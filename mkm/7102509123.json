[
    {
        "doc_title": "Local-LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition",
        "doc_scopus_id": "85090491146",
        "doc_doi": "10.1109/TPAMI.2019.2926459",
        "doc_eid": "2-s2.0-85090491146",
        "doc_date": "2020-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Classification performance",
            "Latent Dirichlet allocation",
            "Low-level features",
            "Object category recognition",
            "State-of-the-art approach",
            "Statistical features",
            "Structural semantics"
        ],
        "doc_abstract": "© 1979-2012 IEEE.Service robots are expected to be more autonomous and work effectively in human-centric environments. This implies that robots should have special capabilities, such as learning from past experiences and real-time object category recognition. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e., visual topics), from low-level feature co-occurrences, for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. In this way, the advantages of both the (hand-crafted) local features and the (learned) structural semantic features have been considered and combined in an efficient way. An extensive set of experiments has been performed to assess the performance of the proposed Local-LDA in terms of descriptiveness, scalability, and computation time. Experimental results show that the overall classification performance obtained with Local-LDA is clearly better than the best performances obtained with the state-of-the-art approaches. Moreover, the best scalability, in terms of number of learned categories, was obtained with the proposed Local-LDA approach, closely followed by a Bag-of-Words (BoW) approach. Concerning computation time, the best result was obtained with BoW, immediately followed by the Local-LDA approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The marks of Cologne and Troyes and the reform of Weights of João II in Portugal (1487-1488)",
        "doc_scopus_id": "85095741147",
        "doc_doi": "10.14195/0870-4147_51_4",
        "doc_eid": "2-s2.0-85095741147",
        "doc_date": "2020-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "History",
                "area_abbreviation": "ARTS",
                "area_code": "1202"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 Universidade de Coimbra. All rights reserved.In Portugal, in the reigns of João II and Manuel I, marked by the reform of charters and the centralization of the administration, it made complete sense to standardize weights and measures. At the European level, the winds of metrological uniformity were also blowing. Early in his reign, João II ordered the measures of capacity to be uniformized by Santarém standards. In 1487-1488, he worked on the reform of weights. Several weight subsystems coexisted for different products. In the documentation already known, some indications suggest that the mark used for haver-de-peso (avoirdupois) was the mark of Troyes/Tria, but they do not allow to draw a consolidated conclusion. In the appendix at the end of this paper, a royal letter is published in which it is the king himself who makes this identification. This clarification, combined with the rest of the documentation, and with the recent evaluation of the Manueline mark, now allows us to draw consolidated conclusions on the equivalence of the weight units prior to the reform. Eventually, the king abolished the Troyes/Tria mark, choosing the Cologne/Colonha mark as the single standard for all products.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning the Scope of Applicability for Task Planning Knowledge in Experience-Based Planning Domains",
        "doc_scopus_id": "85081160340",
        "doc_doi": "10.1109/IROS40897.2019.8968013",
        "doc_eid": "2-s2.0-85081160340",
        "doc_date": "2019-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Abstract representation",
            "Classical planning",
            "Learning from experiences",
            "Logical structure",
            "Planning domains",
            "Problem instances",
            "Task knowledge",
            "Three-valued logic"
        ],
        "doc_abstract": "© 2019 IEEE.Experience-based planning domains (EBPDs) have been proposed to improve problem solving by learning from experience. They rely on acquiring and using task knowledge, i.e., activity schemata, for generating solutions to problem instances in a class of tasks. Using Three-Valued Logic Analysis (TVLA), we extend our previous work to generate a set of conditions that determine the scope of applicability of an activity schema. The inferred scope is an abstract representation of a potentially unbounded set of problems, in the form of a 3-valued logical structure, which is used to test the applicability of the respective activity schema for solving different task problems. We validate this work on two classical planning domains and a simulated PR2 in Gazebo.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive open-ended object, affordance and grasp learning for robotic manipulation",
        "doc_scopus_id": "85071516057",
        "doc_doi": "10.1109/ICRA.2019.8794184",
        "doc_eid": "2-s2.0-85071516057",
        "doc_date": "2019-05-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2019 IEEE.Service robots are expected to autonomously and efficiently work in human-centric environments. For this type of robots, object perception and manipulation are challenging tasks due to need for accurate and real-time response. This paper presents an interactive open-ended learning approach to recognize multiple objects and their grasp affordances concurrently. This is an important contribution in the field of service robots since no matter how extensive the training data used for batch learning, a robot might always be confronted with an unknown object when operating in human-centric environments. The paper describes the system architecture and the learning and recognition capabilities. Grasp learning associates grasp configurations (i.e., end-effector positions and orientations) to grasp affordance categories. The grasp affordance category and the grasp configuration are taught through verbal and kinesthetic teaching, respectively. A Bayesian approach is adopted for learning and recognition of object categories and an instance-based approach is used for learning and recognition of affordance categories. An extensive set of experiments has been performed to assess the performance of the proposed approach regarding recognition accuracy, scalability and grasp success rate on challenging datasets and real-world scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The distribution of weight standards to portuguese cities and towns in the early 16th century: Administrative, demographic and economic factors",
        "doc_scopus_id": "85080937073",
        "doc_doi": "10.18055/finis17728",
        "doc_eid": "2-s2.0-85080937073",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Earth and Planetary Sciences (miscellaneous)",
                "area_abbreviation": "EART",
                "area_code": "1901"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Universidade de Lisboa.This paper studies how the reform of weights of Manuel I, king of Portugal, unfolded. Some key documents overlooked, until now, by the historians of metrology illuminate the early days of the reform. In a letter of May 1503, the king announced the delivery of the new standards to the municipalities and scheduled the entry into force of the new system for January 1504. Municipal documents tell us that the manueline standards were already being delivered in July 1503 and that the new system did come into force in 1504. In the following decades, as the documentation shows, the manueline rules remained in force and the regional authorities sought to ensure their application. It is also known that many municipalities were given exemptions from adopting the standards, considering their smallness, poverty or lack of trade. Crosschecking the recently elaborated inventory of manueline weight piles with data from the 1527-1532 administrative and demographic survey of the whole kingdom allows for a more substantial analysis of how this process unfolded. More than the global population and size of a municipality, it was the population and importance of its chief urban center that mainly influenced on the decision of acquiring a manueline pile.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perceiving, learning, and recognizing 3D objects: An approach to cognitive service robots",
        "doc_scopus_id": "85060470647",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85060470647",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cognitive architectures",
            "Next best view",
            "Object categories",
            "Perception capability",
            "Robotic applications",
            "Service robots",
            "Training data",
            "Unknown objects"
        ],
        "doc_abstract": "Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.There is growing need for robots that can interact with people in everyday situations. For service robots, it is not reasonable to assume that one can pre-program all object categories. Instead, apart from learning from a batch of labelled training data, robots should continuously update and learn new object categories while working in the environment. This paper proposes a cognitive architecture designed to create a concurrent 3D object category learning and recognition in an interactive and open-ended manner. In particular, this cognitive architecture provides automatic perception capabilities that will allow robots to detect objects in highly crowded scenes and learn new object categories from the set of accumulated experiences in an incremental and open-ended way. Moreover, it supports constructing the full model of an unknown object in an on-line manner and predicting next best view for improving object detection and manipulation performance. We provide extensive experimental results demonstrating system performance in terms of recognition, scalability, next-best-view prediction and real-world robotic applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Portuguese Metrology towards the end of the 18th Century and the “Memória sobre Pesos e Medidas” of José de Abreu Bacelar Chichorro (1795)",
        "doc_scopus_id": "85059888745",
        "doc_doi": "10.14195/0870-4147_49_7",
        "doc_eid": "2-s2.0-85059888745",
        "doc_date": "2018-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "History",
                "area_abbreviation": "ARTS",
                "area_code": "1202"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Universidade de Coimbra. All rights reserved.In this paper, the Memória sobre Pesos e Medidas, written by José de Abreu Bacelar Chichorro in 1795, is contextualized and a full transcription is given. At that time, the author was the minister leading the reform of administrative and judicial territories in the province of Estremadura. Besides being one of the earliest studies on Portuguese metrology, the Memória contains the first proposal for the adoption of the Decimal Metric System in Portugal. This proposal did not produce immediate effects. However, the plan proposed by a specialized commission nearly two decades later, and approved by the Prince Regent in 1814, matches Chichorro's proposal in all the essentials aspects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An approach to robot task learning and planning with loops",
        "doc_scopus_id": "85041943595",
        "doc_doi": "10.1109/IROS.2017.8206501",
        "doc_eid": "2-s2.0-85041943595",
        "doc_date": "2017-12-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Action sequences",
            "Planning domains",
            "Problem instances",
            "Robot tasks",
            "Simulated environment"
        ],
        "doc_abstract": "© 2017 IEEE.This paper addresses robot task model learning and planning with loops. By detecting and modeling loops in solved tasks it is possible to learn and solve wider classes of problems. We extend our previous work on experience-based planning domains in robotics to detect, represent and generate loops in action sequences. This approach provides methods for, (i) conceptualizing robot experiences possibly containing loops and learning high-level robot activity schemata with loops; and (ii) instantiating schemata with loops for solving problem instances of the same task with varying sets of objects. Demonstrations of this system in both real and simulated environments prove its potentialities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning robot tasks with loops from experiences to enhance robot adaptability",
        "doc_scopus_id": "85020823138",
        "doc_doi": "10.1016/j.patrec.2017.06.003",
        "doc_eid": "2-s2.0-85020823138",
        "doc_date": "2017-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Conceptualization",
            "Longest common prefixes",
            "Longest common subsequences",
            "Loop detection",
            "Planning domains",
            "Robot tasks",
            "Simulated environment",
            "Task knowledge"
        ],
        "doc_abstract": "© 2017 Elsevier B.V.Learning robot task models with loops helps to increase both the applicability and the compactness of task knowledge. In the framework of Experience-Based Planning Domains (EBPDs), previously formalized by the authors, an approach was developed for learning and exploiting high-level robot task models (the so-called activity schemata) with loops. The paper focuses on the development of: (i) a method—Contiguous Non-overlapping Longest Common Subsequence (CNLCS)—based on the Longest Common Prefix (LCP) array for detecting loops of actions in a robot experience; and (ii) an abstract planner to instanciate a learned task model with loops for solving particular instances of the same task with varying numbers of objects. Demonstrations of this system in both real and simulated environments prove its potentialities.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2017-06-09 2017-06-09 2017-10-23 2017-10-23 2018-07-21T17:18:19 S0167-8655(17)30199-X S016786551730199X 10.1016/j.patrec.2017.06.003 S300 S300.2 FULL-TEXT 2018-07-21T17:19:08.830666Z 0 0 20171101 2017 2017-06-09T16:23:35.30795Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb vol volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref 0167-8655 01678655 true 99 99 C Volume 99 9 57 66 57 66 20171101 1 November 2017 2017-11-01 2017 User Profiling and Behavior Adaptation for Human-Robot Interaction Silvia Rossi Dongheui Lee article sco © 2017 Elsevier B.V. All rights reserved. LEARNINGROBOTTASKSLOOPSEXPERIENCESENHANCEROBOTADAPTABILITY MOKHTARI V 1 Introduction 2 Related work 3 Representation 4 Learning task models 4.1 Generalization and abstraction 4.2 Feature extraction 4.3 Loop detection 5 Using task knowledge to solve new problems 5.1 Loop expansion 5.2 Planning 6 Experimental results 6.1 Real robot demonstration 6.2 Simulated domain 7 Conclusion and future work Acknowledgment References GHALLAB 2004 M AUTOMATEDPLANNINGTHEORYPRACTICE MOKHTARI 2016 463 483 V MOKHTARI 2016 509 517 V 26STINTERNATIONALCONFERENCEAUTOMATEDPLANNINGSCHEDULINGICAPS EXPERIENCEBASEDROBOTTASKLEARNINGPLANNINGGOALINFERENCE CHAUHAN 2013 A 16THPORTUGUESECONFERENCEARTIFICIALINTELLIGENCEEPIA2013 TOWARDSSUPERVISEDACQUISITIONROBOTACTIVITYEXPERIENCESONTOLOGYBASEDAPPROACH MOKHTARI 2016 993 1005 V INTELLIGENTAUTONOMOUSSYSTEMS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES HAMMOND 1986 267 271 K PROCEEDINGSFIFTHNATIONALCONFERENCEARTIFICIALINTELLIGENCE CHEFAMODELCASEBASEDPLANNING BORRAJO 2015 35:1 35:39 D FIKES 1972 251 288 R CHRPA 2010 281 297 L MITCHELL 1986 47 80 T SHAVLIK 1990 39 70 J SRIVASTAVA 2008 991 997 S PROCEEDINGSTWENTYTHIRDCONFERENCEARTIFICIALINTELLIGENCE LEARNINGGENERALIZEDPLANSUSINGABSTRACTCOUNTING SRIVASTAVA 2011 615 647 S LEVESQUE 2005 509 515 H PROCEEDINGSNINETEENTHINTERNATIONALJOINTCONFERENCEARTIFICIALINTELLIGENCEIJCAI PLANNINGLOOPS WINNER 2007 E WORKSHOPAIPLANNINGLEARNINGICAPS LOOPDISTILLLEARNINGDOMAINSPECIFICPLANNERSEXAMPLEPLANS MANBER 1993 935 948 U ZHUO 2009 1804 1810 H PROCEEDINGSTWENTYFIRSTINTERNATIONALJOINTCONFERENCEARTIFICIALINTELLIGENCEIJCAI LEARNINGHTNMETHODPRECONDITIONSACTIONMODELSPARTIALOBSERVATIONS ILGHAMI 2002 131 142 O PROCEEDINGSSIXTHINTERNATIONALCONFERENCEARTIFICIALINTELLIGENCEPLANNINGSYSTEMSAIPS CAMELLEARNINGMETHODPRECONDITIONSFORHTNPLANNING HOGG 2008 950 956 C PROCEEDINGSTWENTYTHIRDAAAICONFERENCEARTIFICIALINTELLIGENCE HTNMAKERLEARNINGHTNSMINIMALADDITIONALKNOWLEDGEENGINEERINGREQUIRED DSACERDOTI 1974 115 135 E HASLUM 2007 1007 1012 P PROCEEDINGSTWENTYSECONDAAAICONFERENCEARTIFICIALINTELLIGENCE DOMAININDEPENDENTCONSTRUCTIONPATTERNDATABASEHEURISTICSFORCOSTOPTIMALPLANNING KNOBLOCK 1991 541 546 C PROCEEDINGSNINTHAAAICONFERENCEARTIFICIALINTELLIGENCE INTEGRATINGABSTRACTIONEXPLANATIONBASEDLEARNINGINPRODIGY BERGMANN 1995 53 118 R SEABRALOPES 1999 294 300 L ASSEMBLYTASKPLANNING1999ISATP99PROCEEDINGS1999IEEEINTERNATIONALSYMPOSIUM FAILURERECOVERYPLANNINGINASSEMBLYBASEDACQUIREDEXPERIENCELEARNINGBYANALOGY SEABRALOPES 2007 65 70 L IFACWORKSHOPINTELLIGENTASSEMBLYDISASSEMBLYIAD2007 FAILURERECOVERYPLANNINGFORROBOTIZEDASSEMBLYBASEDLEARNEDSEMANTICSTRUCTURES TENORTH 2013 643 651 M TENORTH 2015 M HERTZBERG 2014 297 304 J SHAFII 2016 2895 2900 N 2016IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS LEARNINGGRASPFAMILIAROBJECTSUSINGOBJECTVIEWRECOGNITIONTEMPLATEMATCHING RINTANEN 2012 45 86 J HOFFMANN 2001 253 302 J MOKHTARIX2017X57 MOKHTARIX2017X57X66 MOKHTARIX2017X57XV MOKHTARIX2017X57X66XV 2019-10-23T00:00:00.000Z UnderEmbargo © 2017 Elsevier B.V. All rights reserved. item S0167-8655(17)30199-X S016786551730199X 10.1016/j.patrec.2017.06.003 271524 2018-07-21T17:19:08.830666Z 2017-11-01 true 1920868 MAIN 10 56781 849 656 IMAGE-WEB-PDF 1 gr1 6346 65 219 gr10 10340 163 132 gr11 11045 164 200 gr12 10655 164 146 gr13 11610 164 186 gr14 10290 132 219 gr15 7412 128 219 gr16 7575 128 219 gr17 7155 110 219 gr2 4260 27 219 gr3 20243 96 219 gr4 7969 164 176 gr5 5957 163 100 gr6 12590 129 219 gr7 11629 129 219 gr8 4966 26 219 gr9 7118 95 219 gr1 16453 112 376 gr10 44177 347 280 gr11 27146 235 287 gr12 36321 297 265 gr13 32624 260 295 gr14 20792 172 287 gr15 17900 200 341 gr16 17905 200 343 gr17 20758 200 397 gr2 9075 48 386 gr3 24049 169 387 gr4 30347 359 386 gr5 50585 631 386 gr6 41723 228 386 gr7 39961 228 386 gr8 10726 45 386 gr9 20107 167 386 gr1 116627 496 1667 gr10 325844 1539 1243 gr11 195371 1041 1273 gr12 269581 1318 1176 gr13 239069 1152 1309 gr14 146170 765 1273 gr15 160304 886 1511 gr16 169082 886 1521 gr17 182489 886 1761 gr2 56558 214 1708 gr3 214074 750 1713 gr4 242564 1592 1710 gr5 431905 2794 1710 gr6 474511 1008 1708 gr7 441196 1008 1708 gr8 64047 201 1708 gr9 134882 740 1708 si10 144 13 12 si23 481 17 111 si6 151 13 14 si21 432 16 95 si30 218 16 50 si40 413 18 91 si7 235 14 45 si8 162 15 17 si9 172 13 16 si11 194 14 23 si15 267 11 77 si12 247 15 49 si13 468 16 108 si14 487 17 124 si16 251 13 47 si17 583 16 143 si18 220 13 43 si24 274 15 52 si19 428 16 106 si2 471 17 72 si22 390 16 92 si25 280 18 52 si3 248 16 53 si26 278 12 61 si20 300 14 60 si27 2223 51 455 si28 572 17 116 si29 162 13 38 si31 380 16 77 si32 187 16 32 si33 272 16 63 si34 174 17 20 si35 479 17 110 si36 462 16 103 si37 205 13 41 si38 774 17 247 si39 273 15 67 si4 869 17 222 si41 190 18 26 si42 173 15 21 si43 865 17 221 si44 275 13 64 si45 698 17 145 si46 215 13 48 si5 149 13 14 si1 553 15 97 PATREC 6842 S0167-8655(17)30199-X 10.1016/j.patrec.2017.06.003 Elsevier B.V. Fig. 1 An overview of the learning and planning system. Fig. 1 Fig. 2 Suppose (ab)*cd is an abstract plan, in which each letter represents an abstract operator and (ab)* represents a loop. ASBP generates two classes of successors when gets to a loop in an abstract plan: (i) ASBP copies the loop and appends it to the beginning of the abstract plan and generates the successors for this abstract plan; and (ii) ASBP skips the loop and generates the successors for the rest of the abstract plan. ASBP then picks a successor with the lowest cost to develop. This procedure either expands or skips a loop. Fig. 2 Fig. 3 From left to right, robot moves to the cup, picks up the cup from the table, carries the cup, and place it on the tray. Fig. 3 Fig. 4 The learned knowledge for the task (Stack ?table ?pile) in the stripped tower domain. Each action node represents an enriched abstract operator, i.e., an abstract operator on the left side of colon, and a set of features associated with the abstract operator on the right side of colon. The decisions, i.e., diamonds, appear before the loops and specify the cost of two parallel branches to either expand or skip the loops. Depending on the minimum cost of abstract actions, the ASBP planner moves toward the loops or skips the loops, i.e., moves to the next abstract operator, in an abstract plan (see Algorithm 2). Fig. 4 Fig. 5 The learned knowledge for the task (UnstackStack ?pile1 ?pile2) in the stripped tower domain. Fig. 5 Fig. 6 Performance of the SBP, Mp and FF in the task (Stack ?table ?pile), in the stripped tower domain. Fig. 6 Fig. 7 Performance of the SBP and Mp in the task (UnstackStack ?pile1 ?pile2), in the stripped tower domain. FF failed to solve problems in this task. Fig. 7 Listing 1 An abstract operator in EBPD. Listing 1 Listing 2 Representation of a planning operator in EBPD. With respect to the standard PDDL, parent and static are new properties. Listing 2 Listing 3 An experience for ‘clear table’ task problem, in robotic_arm domain. The plan solution to this problem contains 8 primitive actions. Listing 3 Listing 4 A task planning problem for ‘clear table’ task in EBPD. Listing 4 Listing 5 An experience after generalization and abstraction. Compared to Listing 3, the constants in the experience are replaced with variables, and the actions in the plan are replaced with their abstract operator parents. Listing 5 Listing 6 A learned activity schema for ‘clear table’ task after generalization, abstraction and feature extraction. Each abstract operator is associated with a set of features. Listing 6 Listing 7 A learned activity schema for the ‘clear table’ task with a loop. Listing 7 Algorithm 1 Contiguous Non-overlapping Longest Common Prefixes (CNLCP). Algorithm 1 Algorithm 2 Abstract Schema-Based Planner (ASBP). Algorithm 2 Algorithm 3 Schema-Based Planner (SBP). Algorithm 3 Table 1 Abstract and planning operators developed for the r o b o t i c _ a r m domain. Table 1 Abstract operators Planning operators pick(object, table) pickup(arm, object, table) ; pick up an object from a table put(object, tray) putdown(arm, object, tray) ; put down an object on a tray nil grip(arm, old_posture, new_posture) ; change the gripper posture to a new posture move(arm, from, to) ; move arm to a new position Table 2 The arrays SA, LCP and NLCP for the sequence ababa (i.e., pick put pick put pick). Table 2 i SA[i] LCP[i] NLCP[i]* suffix SA [ i ] 0 4 0 0 a 1 2 1 1 aba 2 0 3 2 ababa 3 3 0 0 ba 4 1 2 2 baba * Each number in ith row specifies the length of the non-overlapping longest common prefix between two suffixes in rows i and ( i − 1 ) for i ≥ 1. Table 3 Performance of the planners in task (Stack ?table ?pile), in the stripped tower domain. Table 3 Problem* Total time (s) Memory (MB) Evaluated states Plan length Mp FF SBP Mp FF SBP Mp FF SBP Mp FF SBP p10 0.4 0.0 0.2 27.6 5.4 6.9 95 106 71 39 39 39 p12 0.8 0.0 0.3 37.8 6.2 9.3 196 140 88 48 49 48 p14 0.1 0.0 0.5 47.1 6.7 12.6 407 177 103 56 57 55 p16 0.2 0.1 0.8 61.8 7.7 17.0 766 217 120 63 63 64 p18 0.4 0.1 1.2 79.6 8.5 23.4 1107 263 136 72 73 72 p20 0.7 0.1 1.4 102.1 9.6 22.9 1418 310 152 80 80 80 p22 1.0 0.2 1.9 120.4 10.6 27.5 1160 364 168 87 87 88 p24 1.5 0.4 2.6 151.8 11.7 30.5 1428 422 183 96 97 95 p26 2.2 0.3 3.2 196.2 13.0 36.7 2026 483 199 104 105 103 p28 3.2 0.7 4.2 255.5 14.6 40.5 2832 548 215 112 113 111 p30 4.2 0.6 5.5 278.6 15.9 48.7 2355 616 232 119 119 120 p32 6.2 0.8 6.9 351.6 17.5 54.7 4009 690 248 128 129 128 p34 8.2 0.1 8.9 433.5 19.1 65.3 4575 766 263 135 135 135 p36 11.1 0.1 11.8 534.1 21.0 76.6 5884 847 280 143 143 144 p38 14.9 0.1 15.1 653.7 22.8 85.8 7288 932 295 151 151 151 p40 19.6 0.1 20.0 785.8 24.7 98.6 9276 1020 311 160 160 159 p42 23.8 0.2 26.5 867.3 26.8 117.4 8054 1113 328 168 168 168 p44 30.8 0.2 34.2 1.0k 28.9 137.1 9922 1210 344 176 176 176 p46 39.8 0.3 45.1 1.2k 31.0 152.4 12,124 1311 360 184 184 184 p48 48.5 0.3 60.1 1.4k 33.4 177.6 12,589 1416 376 192 192 192 p50 59.9 0.4 79.2 1.5k 35.8 198.2 13,106 1526 392 199 199 200 * The name of each problem describes the number of blocks used in that problem. Table 4 Performance of the planners in task (UnstackStack ?pile1 ?pile2), in the stripped tower domain. Table 4 Problem Total time (s) Memory (MB) Evaluated states Plan length Mp SBP Mp SBP Mp SBP Mp SBP p10 0.30 0.23 83.4 8.4 3455 152 76 76 p12 0.31 0.31 91.9 11.0 2304 198 91 92 p14 1.25 0.44 211.8 14.1 5093 248 108 107 p16 1.32 0.61 233.0 17.9 3675 305 123 124 p18 1.62 0.83 269.4 22.9 4889 365 140 139 p20 1.56 0.98 263.7 23.6 2498 356 156 156 p22 4.21 1.14 571.5 27.1 6244 387 171 172 p24 4.12 1.37 563.5 30.0 4462 423 188 188 p26 4.78 1.68 586.1 34.4 4133 464 204 204 p28 11.0 2.05 1.2k 39.3 12,485 510 220 220 p30 13.5 2.57 1.4k 43.6 13,106 561 235 236 p32 20.6 3.16 2.1k 50.0 15,367 617 251 252 p34 21.0 3.97 2.0k 55.8 10,645 678 267 268 p36 37.9 5.15 2.7k 63.8 19,213 744 284 284 p38 33.9 6.55 2.7k 72.4 12,353 815 299 300 p40 42.7 8.84 2.8k 82.1 14,503 914 316 316 p42 58.5 11.3 3.0k 93.7 18,362 1000 332 332 p44 – 15.3 – 105.0 – 1091 – 348 p46 – 19.1 – 119.2 – 1187 – 364 p48 – 24.7 – 137.4 – 1288 – 380 p50 – 32.5 – 149.6 – 1394 – 396 Learning robot tasks with loops from experiences to enhance robot adaptability Vahid Mokhtari ⁎ Luís Seabra Lopes Armando J. Pinho Institute of Electronics and Informatics Engineering of Aveiro (IEETA), University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro (IEETA) University of Aveiro Portugal ⁎ Corresponding author. Learning robot task models with loops helps to increase both the applicability and the compactness of task knowledge. In the framework of Experience-Based Planning Domains (EBPDs), previously formalized by the authors, an approach was developed for learning and exploiting high-level robot task models (the so-called activity schemata) with loops. The paper focuses on the development of: (i) a method—Contiguous Non-overlapping Longest Common Subsequence (CNLCS)—based on the Longest Common Prefix (LCP) array for detecting loops of actions in a robot experience; and (ii) an abstract planner to instanciate a learned task model with loops for solving particular instances of the same task with varying numbers of objects. Demonstrations of this system in both real and simulated environments prove its potentialities. Keywords Robot task learning with loops Learning and planning Conceptualization Loop detection Experience-based planning domains 1 Introduction Automated planning is a field of Artificial Intelligence (AI) that studies the computational synthesis of sequences of actions to perform the given tasks [1]. AI planning techniques typically require hand-coding of domain knowledge, e.g., in the form of a set of planning action classes, the so-called planning operators. However, it is not always easy to specify all knowledge that may be necessary. For instance, among different plans to achieve a given task or goal, some plans may be preferable due to factors that may be hard to specify in the planning domain description, such as social conventions. Moreover, despite significant advances in general purpose planning algorithms, i.e., algorithms for solving any problem in any planning domain, AI planners often fail to solve large-scale problems. In this context, the authors have been developing conceptualization techniques that derive generic task models from concrete plans appropriate for solving the given tasks. The acquired and stored task models can then be used to derive solutions for other tasks in the same class. Plans used to solve concrete problems are derived from human demonstrations and/or verbal instructions. Solutions to large-scale problems often include repetitive structures, i.e., loops in which a sequence of actions is successively applied to different objects. Since the set of target objects can vary from task to task, the number of iterations of the loop will also vary. Therefore, the appropriate plan solutions can vary significantly within a class of tasks that involves loops. The inclusion of loops in a task model allows to improve its compactness and helps to increase its applicability by repeating sequences of actions for different sets of objects. Conceptualizing tasks containing loops is therefore particularly useful for solving large problems. This paper contributes towards, (i) detecting and representing loops of actions in a task demonstration; (ii) and adapting and exploiting loops to different instances of the same class of tasks. We assume the basic skills, e.g., pick up or put down, are already implemented in the robot system. We provide the robot with step-by-step operating instructions, i.e., a solution plan possibly containing loops to achieve a task, and allow the robot to conceptualize the task. The obtained task models can later be used for solving different instances of the same task with different number of objects. In previous works, the authors proposed and formalized the notion of Experience-Based Planning Domain [2,3], and presented approaches for collecting experiences through human-robot interaction [4,5], conceptualizing experiences in the form of activity schemata and exploiting the acquired activity schemata to plan solutions to new tasks [2,3]. In this paper, previous work is extended to detect loops in experiences and represent them in the learned activity schemata. An abstract planner was developed and integrated into the system to expand the loops in activity schemata adapting them for given tasks. Fig. 1 provides an overview of the adopted learning and planning framework. The user interface allows a human to instruct a robot to carry out sequences of actions and teach the robot which tasks were achieved. A standard planner is also integrated into the system to generate solutions for tasks, when a human user is not present or when providing a solution is too complex for the human user. When a task is successfully carried out, the experience extractor collects the world information as well as the applied actions as an experience. The conceptualizer builds activity schemata, i.e., abstract task models, from single experiences. The planning module picks an activity schema for a given task problem and generates a plan solution. The robot platform includes perception and execution modules. This paper addresses how the conceptualizer generates a task model with loops and how a symbolic planner 1 1 Throughout this paper the word ‘planner’ means ‘symbolic planner’ as used in the AI planning literature. finds solutions to similar tasks with varying numbers of objects using the learned task model. A ‘clear table’ scenario is used to illustrate the proposed algorithms. A robotic application EBPD, r o b o t i c _ a r m (Table 1 ), was developed and used in a real environment, where a robotic arm learns and performs tasks. To show the potential of our approach we also demonstrate the system in two challenging classes of problems in a simulated domain and compare the performance of the system with two state-of-the-art planners. The rest of this paper is organized as follows. Related works are discussed in Section 2. The formalization of Experience-Based Planning Domains (EBPDs) is recapitulated in Section 3. In Section 4, the approach to learn task models from robot experiences is presented. In addition to summarizing general aspects of the approach, special attention is given in this section to the loop detection algorithm. In Section 5, the planning approach, including an abstract planner, to extend and adapt loops of actions in an activity schema, and a planner, which generates a concrete solution to a given task problem, is described in detail. Finally, Section 6 shows the experimental results and the performance of our system. 2 Related work Several works have focused on acquiring planning knowledge to reduce planning search. The earliest and best known approaches are case based planning [6,7] and macro operators [8,9]. These methods tend to suffer from the utility problem, in which learning more information can be counterproductive due to the difficulty with storage and management of the information and with determining which information should be used to solve a particular problem. Some work has focused on analyzing example plans to generalize them from concrete objects to variables. A well known approach is Explanation-Based Learning (EBL), which generalizes a proof or an explanation of a solution to be applicable to different problem instances [8,10]. One example is BAGGER2 [11], which uses example solutions and domain knowledge to learn an algorithm for problem solving in the form of recursive structures. Other work to find plans with loops includes generalized planning [12–14]. Given a goal condition and a class of initial states with varying number of objects, generalized planning finds a plan that is algorithmic in nature and works for a set of problems. Much of the work on generalized planning leans heavily on theorem-proving. These techniques were mostly applied in simulated domains, and the problems are not expressible in widespread planning languages such as PDDL. LoopDistill [15] addresses the problem of learning loops from examples applied to simulated domains. It identifies the largest matching sub-plan in a given example, and converts the repeating occurrences of these sub-plans into a loop. The output is a domain-specific planning program (dsPlanner), actually a plan with with if statements and while-loops. Our approach builds a task model with loop structures that are used by a domain-specific planner. Our loop detection algorithm uses the standard Longest Common Prefix (LCP) array [16]. LoopDistill does not address how to terminate loops in its learned dsPlanners. A body of work focuses on learning Hierarchical Task Networks (HTNs) [17]. In HTN planning, a plan is generated by decomposing a method for a given task into simpler tasks until primitive tasks are reached, i.e., tasks that can be directly achieved by planning operators. Recursive HTN methods are an alternative approach for representing loops. Research into HTN method learning has been much more application oriented than other AI-planning research. However, identifying a good hierarchical structure is an issue, and most of the techniques in HTN learning rely on the hierarchical structure of the HTN methods specified by a human expert [18,19]. Abstraction has also played a significant role in reducing planning complexity [20,21]. A few works also integrated abstraction with EBL to learn more generic concepts called abstract plans or plan skeletons [22–25]. Similarly, we integrate an abstraction hierarchy with EBL to reduce problem space and learn general concepts. However, we do not propose an automatic procedure to construct the abstraction hierarchy. The Web Ontology Language (OWL) is also becoming increasingly prevalent in cognitive robotics for knowledge representation and for supporting reusability and shareability [26,27]. These techniques combine information of many types from different sources. Our approach was initially integrated into a OWL-based cognitive system [28]. Overall, most of the approaches in AI planning concentrate on the theory and simulated domains, and have seldom been used in real robots. On the other hand, in the LfD approaches, the expressive representation and identification of loops in a task demonstration, as well as the exploitation of the learned knowledge by a planner, are given little attention. 3 Representation A formal definition of Experience-Based Planning Domains (EBPDs), i.e., planning domains that evolve through learning from experience, is proposed in this section. Definition 1 An EBPD is a tuple, D = ( L , Σ , S , A , O , E , M ) , where L is a first-order logic language that has finitely many predicate and constant symbols, Σ is a set of ground atoms of L that are always true, i.e., static world information, S is a set of states in which every state s ∈ S is a set of ground atoms of L which may become false, i.e., transient world information, A is a set of abstract operators, O is a set of planning operators, E is a set of experiences, and M is a set of methods in the form of activity schemata. Definition 2 An abstract operator a ∈ A is a triple, a = ( h , P , E ) , where h is the abstract operator’s head, P is the precondition, and E is the effect of a. A head takes a form n ( x 1 , … , x k ≥ 0 ) , in which n is the name, and x 1 , … , x k are the arguments, e.g., (pick ?object ?table) 2 2 The notation in the Planning Domain Definition Language (PDDL) is used to represent EBPDs. All terms starting with a question mark (?) are variables, and the rest are constants or function symbols. . A ground instance of an abstract operator is called an abstract action. In the proposed EBPDs, an abstract operator is a class of planning operators. Listing 1 shows an abstract operator represented in EBPD. Definition 3 A planning operator o ∈ O is a tuple, o = ( h , a , S , P , E ) , where h is the operator’s head, a is an abstract operator head which specifies the superclass or parent of o, S is the static world information, and P and E are respectively the precondition and effect of o. A ground instance of an operator is called an action. Listing 2 shows a planning operator represented in EBPD, and Table 1 shows the implemented abstract and planning operators in this work. Experiences are episodic descriptions of plan-based robot activities including environment perception, sequences of applied actions and achieved tasks. Definition 4 An experience e ∈ E is a triple of ground structures, e = ( t , K , π ) , where t is the head of a task, taught by a human user to a robot, e.g., (clear table1), K is a set of key propositions, and π is a plan solution to achieve t. K is a subset of the world description captured during an experience. Every key proposition in K is a predicate with a temporal symbol specifying the temporal extent of the predicate in an experience. Three types of temporal symbols are used to represent key propositions, static (always true during an experience), e.g., (static(reach arm1 table1)), init (true at the initial state), e.g., (init(on cup table1)), and end (true at the goal state), e.g., (end(on cup tray1)). Listing 3 shows an experience for the ‘clear table’ task. Extracted experiences are the inputs to acquire task knowledge. An activity schema is a task model obtained from an experience: Definition 5 An activity schema m ∈ M is a pair, m = ( h , Ω ) , where h is the head of a target task (e.g., (clear ?table)), and Ω is an abstract plan to achieve the h, i.e., a sequence of abstract operators or loops of abstract operators enriched with features. Definition 6 An enriched abstract operator ω is a pair, ω = ( a , F ) , where a ∈ A is an abstract operator head (Definition 2), and F is a set of features, i.e., a set of unground key propositions (Definition 4), describing the arguments of a. Listing 7 shows later a learned activity schema in this paper. Definition 7 A task planning problem is a tuple of ground structures, P = ( t , s 0 , g ) , where t is the head of a target task to be planned, s 0 ∈ S is the initial state, and g is the goal, i.e., a set of propositions to be satisfied in a goal state s g ∈ S . Listing 4 shows a task planning problem in EBPD. 4 Learning task models Experiences are the inputs for learning task knowledge. Experiences are generated through human-robot interaction. We previously presented an infrastructure to instruct and teach a robot how to carry out a task using human-robot interaction, and approaches to gathering world information and recording experiences [2]. Obtaining a plan solution from human instructions may have some advantages over the using a standard planner. For instance when there are different alternatives to achieve a goal, some alternatives may be preferable to correctly achieve the goal based on different factors that have not been encoded in the domain specification, such as social norms, physical constraints, etc. Nevertheless, a standard planner is alternatively integrated in the system to generate a plan solution for a given task demonstration when a human user is not present or when providing a plan solution is too complex for the human user. In this section we present methods for conceptualizing experiences. By conceptualization it is meant the process of generating an activity schema from an experience. The conceptualization approach is a combination of different techniques including deductive generalization, abstraction, feature extraction and loop detection. 4.1 Generalization and abstraction Generalization is the first step in conceptualizing an experience. Through deductive generalization, general concepts are formulated from single training examples and domain knowledge. We employed a goal regression algorithm, as in explanation-based generalization (EBG) [10,24,25], to: (i) build an explanation of how an experience is solved with domain operators; and (ii) generalize the obtained explanation. The generalization is carried out by substitution of variables for the observed constants in an experience and propagating this substitution in the whole experience. Abstraction is an important technique for improving problem solving performance. To reduce the level of detail in a generalized experience, we use an operator abstraction hierarchy, which results in more widely applicable task models. This hierarchy is specified in an EBPD using the parent property of planning operators which links to an abstract operator (see Definitions 2 and 3, and Table 1). Abstraction creates an abstract plan by replacing operators with their parents in the operator abstraction hierarchy. In this abstraction, some operators are mapped onto nil, meaning they are excluded from the learned activity schema, and some arguments of operators are excluded in the respective abstract operators. The nil class operators are auxiliary operators that are reconstructed again during instantiation of a learned activity schema for a given task problem. Listing 5 shows an experience after generalization and abstraction. 4.2 Feature extraction A feature is a key proposition that reveals a connection between an abstract operator and the task achieved in an experience. A feature is defined as a key proposition, τ(Pn ), for n ≥ 1, where Pn is an n − a r y predicate and τ is a temporal symbol, i.e., static, init, or end. Each argument of P either appears in an abstract operator’s head or in the task head, or in both. For example in Listing 5, the key proposition, (init(on ?cup ?table)), on line 9, is a feature that connects the ?cup, an argument of the abstract operator pick, on line 21, to the ?table, an argument of the clear task, on line 2. Relevant features are discovered in the generalized key propositions. For each abstract operator in the abstract plan of a generalized and abstracted experience, all possible relationships between the arguments of the abstract operator and the task arguments are extracted and associated to the abstract operator. The set of features, F, for an abstract operator with a set of arguments, A, and a task with a set of arguments, T, is extracted from a set of key propositions, K, as follows: (1) F ( A , T , K ) = { τ ( P ( t 1 , … , t n ) ) ∈ K ∣ { t 1 , … , t n } ∩ A ≠ ϕ , { t 1 , … , t n } ⊆ ( A ∪ T ) , n ≥ 1 } . Listing 6 shows an example of an activity schema after generalization, abstraction and feature extraction. During problem solving, features lead a planner toward a goal state, greatly reducing the probability of backtracking. 4.3 Loop detection Solutions to a class of problems with varying number of objects may differ in the repetition of some actions. The final step in conceptualization is to detect possible loops of actions. A loop is a contiguous repetition of a subsequence of enriched abstract operators for different objects. Two or more contiguous subsequences of enriched abstract operators belong to a loop if, (i) the names and the order of the corresponding abstract operators in each subsequence are the same; (ii) the sets of features describing the corresponding abstract operators in each subsequence, are the same; and (iii) the variables appearing in the corresponding abstract operators and in their corresponding features, in each subsequence, play the same role. For example in Listing 6, two contiguous subsequences of pick and put with the same corresponding features, and the corresponding variables with the same role belong to a loop. We developed a loop detection approach based on the Suffix Array (SA)—an array of integers providing the starting positions of all suffixes of a string, sorted in lexicographical order—and the Longest Common Prefix array (LCP array)—an integer array storing the lengths of the longest common prefixes between all pairs of consecutive suffixes in a suffix array—for an abstract plan [16]. We compute a loop as the Contiguous Non-overlapping Longest Common Prefix (CNLCP) between two consecutive suffixes in a suffix array (Algorithm 1 ). We modified the standard LCP algorithm to find the Non-overlapping Longest Common Prefix (NLCP) between a pair of consecutive suffixes in a suffix array by controlling the size of the common prefix to be at most equal to the difference between the lengths of the two given suffixes (line 13). Algorithm 1 first creates a suffix array, SA, for a given abstract plan, Ω, in line 1, and then builds an NLCP array, in line 3. This is an integer array of size n = l e n g t h ( Ω ) such that NLCP[0] is undefined and NLCP[i], for 1 ≤ i < n, is the length of the non-overlapping longest common prefix between suffixes i − 1 and i in a suffix array, SA (i.e., SA [ i − 1 ] and SA[i]). Table 2 shows the computed SA, LCP and NLCP arrays for an abstract plan example. In this example the NLCP gives the subsequence ab (i.e., pick put) as the non-overlapping longest common prefix between two consecutive suffixes aba and ababa (in rows 1 and 2 in Table 2), in contrast to LCP, which gives the overlapping longest common prefix aba. In Algorithm 1 on lines 5–10, the main function constructs a contiguous non-overlapping longest common prefixes array of strings, CNLCP array, such that each CNLCP[i], for i ≥ 0, is an iteration of a loop that consecutively occurs in a given string, i.e., in an abstract plan. A non-overlapping longest common prefix at NLCP[i] is consecutive if NLCP [ i ] = a b s ( SA [ i ] − SA [ i − 1 ] ) , for 1 ≤ i < n (line 9). Finally, we select a contiguous non-overlapping longest common prefix in the CNLCP array with a largest length, i.e., an iteration of a loop with the largest length, and check that all the features associated with each occurrence of this iteration in the abstract plan are the same. In the current approach, we do not address the possibility of nested loops (i.e., loops inside loops). When a loop is detected, new variables are substituted for the different variables playing the same role in the corresponding abstract operators and in their corresponding features in each subsequence. For example, in Listing 6 the variables ?cup and ?spoon, playing the same role in the respective loop iterations, are replaced with a new variable ?X. Finally, the subsequences of abstract operators are merged and an intersection of their corresponding features is computed. Listing 7 shows the learned activity schema of ‘clear table’ task with a loop of actions. 5 Using task knowledge to solve new problems If there is a learned activity schema for a task, the planning system attempts to generate a plan to achieve any particular instance of that task. If there are several learned activity schemata for a task, the most recent one is used by the planner. In case of failure, the next recent one is used, and so on. Generating a plan for a given task may involve loop expansion, i.e., determining concrete iterations of a loop. 5.1 Loop expansion We developed an abstract planner, Abstract Schema-Based Planner (ASBP), to expand the loops in an activity schema (Algorithm 2 ). The general idea is to search forward while following the activity schema. ASBP takes as input a domain of abstract operators, A , a task problem, P = ( t , s 0 , g ) , and a learned activity schema, m = ( h , Ω 0 ) , for task t such that h = t (i.e., h should be syntactically unifiable with t), and returns a ground abstract plan without loops, Π. Each node in the search tree retains a state, s, the ground abstract plan without loops built so far, Π, the remaining part of the abstract plan, Ω, the total cost, f, and the cost, c, of the path from the start node to the current node. In each planning iteration, a leaf node with the lowest f value is retrieved (line 3). If the front enriched abstract operator, head(Ω), is a loop (line 5), the planner generates successors for two alternatives: it adds an iteration of the loop to the front of Ω (line 6); and skips the loop and moves on to the next abstract operator in Ω (line 7). This way, successors are generated by applying, not only abstract actions instantiated from the front abstract operator inside the loop, but also abstract actions instantiated from the following abstract operator after the loop. Fig. 2 illustrates this idea. On line 9, if head(Ω) is not a loop, successors are generated by applying abstract actions instantiated from head(Ω). Notice that in the current work we assume there are no consecutive loops in an activity schema. The core of ASBP is the procedure step, which generates all successors for the class of the front abstract operator in a given abstract plan. On line 12, step selects the front enriched abstract operator, (a, F), and generates applicable instances of a (line 13). On lines 14–18, the features of each abstract actions, F′, are extracted and verified with the features in F, and a cost is computed as, (2) c n = c + c r e a l · ( k + 1 ) / ( v + 1 ) , where k is the total number of features in F, v is the number of features in F′ that are verified in F, and creal is the real cost of o (e.g., c r e a l = 1 ). It means the applicable abstract actions that verify all features in F, gain the lower cost, and abstract actions with the lower percentage of verified features, gain the higher cost. On line 20, the planner appends o to the current ground abstract plan, Π · o; moves forward in the abstract plan, tail(Ω); and computes the total cost as, c n + h + ( s n ) . The heuristic function, h + , estimates a cost from the current state to the goal g (i.e., we used an additive heuristic). Notice the h + is extremely efficient since the ASBP works with abstract domain. Finally, ASBP stops when the goal g is achieved on line 4. 5.2 Planning A ground abstract plan produced by ASBP forms the main skeleton of the final solution, so the general idea is to generate a plan solution, by substituting base operators for abstract operators and inserting the auxiliary actions into the abstract plan, i.e., actions from the nil class (see Table 1). Schema-Based Planner (SBP), in Algorithm 3 , takes as input a domain, D = ( L , Σ , S , A , O , E , M ) , and a task problem, P = ( t , s 0 , g ) , and generates a plan, π. SBP selects an activity schema m ∈ M for a given task t (line 1). On line 2, a ground abstract plan without loops is built, Π (see Algorithm 2). Line 3 creates the root node. In SBP, each node of the search tree retains a state, s, the plan built so far, π, the remaining part of the ground abstract plan, Π, the total cost, f, and the cost, c, of the path from the root node to the current node. On each planning iteration, a node with the lowest f value is retrieved (line 5). The planner selects the front abstract action, a (line 7), and develops the search tree (lines 8–17). SBP on each iteration either generates all actions that have the parent exactly as a (line 8), and moves forward on Π (line 10); or generates all auxiliary actions from the nil class (line 12). In all cases, the action cost is computed as, c + c r e a l (line 14), and the planner appends o to the current plan, π · o; and computes the total cost as, c n + α · l e n g t h ( Π ) (line 17). We estimate a heuristic (i.e., α · length(Π)) as the length of the remaining abstract plan, Π, multiplied by a factor, α. The factor α is used to calibrate the heuristic value to achieve the admissibility. Empirically, α, can be estimated as an average number of actions in a real plan per abstract operator in the abstract plan, for example, in the r o b o t i c _ a r m domain we estimated α = 2 (see number of actions in Listing 3 compared to number of abstract operators in Listing 5). The lowest value of α may cause to underestimate the cost of reaching the goal, and the highest value may cause to overestimate the actual cost. Determining an accurate value of α may need a little tweaking depending on the used domain. Finally, a plan solution is successfully generated when the goal g is achieved (line 6). 6 Experimental results The performance of the system in three classes of problems in both real and simulated environments is presented. 6.1 Real robot demonstration An EBPD was developed for a real robotic arm application domain. The planning and abstract operators were listed in Table 1 . The system is demonstrated in three different scenarios, one for teaching a task to a robot, and two for evaluating the performance of the learned task knowledge. We designed a ‘clear table’ training scenario with two objects on a table, and instructed and taught a robotic arm how to clear the table using a command-line interface. The main objective in this scenario is to clear the table by removing the two objects from the table and placing them on a tray. Fig. 3 shows snapshots of teaching the ‘clear table’ task to a robot. The plan solution provided by the human user, and the initial and goal states of the problem create an experience for this task. This experience was shown in Listing 3. Conceptualization derived the activity schema, shown in Listing 7, immediately after extracting the experience. To evaluate the performance of the system in solving problems, we demonstrated the system on two test scenarios with three and four objects on thetable respectively, and observed that the robot successfully solved and performed these two tasks. The system was integrated with the work in [29], where a robotic arm learns to grasp different objects through kinesthetic teaching. Videos of this performance are available in goo.gl/RxDCa3. 6.2 Simulated domain In addition to the real robot demonstration, a modified version of the stripped tower domain, originally introduced in [12], was developed. The following planning operators are considered: stack/5, unstack/5, pickup/4, putdown/4, move/4. Additionally, the following abstract operators were defined: stack/4, unstack/4, pick/3, put/3 (i.e., the numbers indicate arities). Two classes of tasks were considered in this domain, Stack and UnstackStack. In the Stack class, a number of red and blue blocks are initially on top of a table. In the UnstackStack class, a pile of red and blue blocks, with red blocks at the bottom and blue blocks on top, is initially given. In both classes, the goal is to construct a pile of alternating red and blue blocks, with a red base block at the bottom and a blue block on top. In each class, a problem with a certain number of objects was randomly created, and a plan solution for that problem was generated using a state-of-the-art planner, Madagascar (Mp) [30]. Based on that, an experience was assembled enabling to drive a task model, i.e., an activity schema. Figs. 4 and 5 represent these learned task models. SBP successfully solved all problems in each class using the learned activity schemata. The performance of SBP was evaluated and compared to two state-of-the-art planners, Mp and Fast-Forward (FF) [31], based on four measurements of time, evaluated nodes, plan length and memory (see Tables 3 and 4). In both classes of problems, SBP was extremely efficient in terms of evaluated nodes in the search tree. During the search process, features allow SBP to find the best actions, thus reducing the number of evaluated nodes. It should be noted that the time evaluation is not completely accurate, since SBP has been implemented in Prolog and Python, in contrast to other planners, which have been implemented in C++. FF was fairly fast to solve the problems in task Stack, however it failed to solve the problems in task UnstackStack. Figs. 6 and 7 summarize the performance of the three planners. SBP also showed the flexibility to solve slightly different problems to the original class. We generated a set of random problems in the class UnstackStack, in which in each problem, the blocks are not necessarily in a pile at the initial state, but some blocks are already on a table. SBP was able to ignore the loops in the given problems and solve the problems using the already learned task knowledge for this class. 7 Conclusion and future work Methods were presented for conceptualizing robot experiences containing loops, and for instanciating the learned concepts (activity schemata) with loops to solve other instances of the same class of tasks. The integration of this system in a real robotic arm was demonstrated. Additional evaluation was carried out in a complex class of problems in a simulated domain. Future work includes extensive evaluation of the proposed system on different task problems and scenarios. Acknowledgment This work is funded by the Portuguese Foundation for Science and Technology (FCT) under the grant SFRH/BD/94184/2013, and the project UID/CEC/00127/2013. References [1] M. Ghallab D. Nau P. Traverso Automated Planning: Theory & Practice 2004 Elsevier [2] V. Mokhtari L. Seabra Lopes A.J. Pinho Experience-based planning domains: an integrated learning and deliberation approach for intelligent robots J. Intell. Robot. Syst. 83 3 2016 463 483 [3] V. Mokhtari L. Seabra Lopes A.J. Pinho Experience-based robot task learning and planning with goal inference 26st International Conference on Automated Planning and Scheduling (ICAPS) 2016 AAAI Press 509 517 [4] A. Chauhan L. Seabra Lopes A.M. Tomé A. Pinho Towards supervised acquisition of robot activity experiences: an ontology-based approach 16th Portuguese Conference on Artificial Intelligence - EPIA’2013 2013 [5] V. Mokhtari G. Lim L. Seabra Lopes A.J. Pinho Gathering and conceptualizing plan-based robot activity experiences E. Menegatti N. Michael K. Berns H. Yamaguchi Intelligent Autonomous Systems 13 Advances in Intelligent Systems and Computing vol. 302 2016 Springer International Publishing 993 1005 [6] K.J. Hammond Chef: a model of case-based planning Proceedings of the Fifth National Conference on Artificial Intelligence 1986 AAAI Press 267 271 [7] D. Borrajo A. Roubíčková I. Serina Progress in case-based planning ACM Comput. Surv 47 2 2015 35:1 35:39 [8] R.E. Fikes P.E. Hart N.J. Nilsson Learning and executing generalized robot plans Artif. Intell. 3 1972 251 288 [9] L. Chrpa Generation of macro-operators via investigation of action dependencies in plans Knowl. Eng. Rev. 25 03 2010 281 297 [10] T.M. Mitchell R.M. Keller S.T. Kedar-Cabelli Explanation-based generalization: a unifying view Mach. Learn. 1 1 1986 47 80 [11] J.W. Shavlik Acquiring recursive and iterative concepts with explanation-based learning Mach. Learn. 5 1 1990 39 70 [12] S. Srivastava N. Immerman S. Zilberstein Learning generalized plans using abstract counting Proceedings of the Twenty-Third Conference on Artificial Intelligence 2008 AAAI Press 991 997 [13] S. Srivastava N. Immerman S. Zilberstein A new representation and associated algorithms for generalized planning Artif. Intell. 175 2 2011 615 647 [14] H.J. Levesque Planning with loops Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI) 2005 509 515 [15] E. Winner M.M. Veloso LoopDISTILL: learning domain-specific planners from example plans Workshop on AI Planning and Learning, ICAPS 2007 [16] U. Manber G. Myers Suffix arrays: a new method for on-line string searches SIAM J. Comput. 22 5 1993 935 948 [17] H.H. Zhuo D.H. Hu C. Hogg Q. Yang H. Munoz-Avila Learning HTN method preconditions and action models from partial observations Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI) 2009 1804 1810 [18] O. Ilghami D.S. Nau H. Munoz-Avila D.W. Aha Camel: learning method preconditions for HTN planning Proceedings of the Sixth International Conference on Artificial Intelligence Planning Systems (AIPS) 2002 131 142 [19] C. Hogg H. Munoz-Avila U. Kuter HTN-MAKER: learning HTNs with minimal additional knowledge engineering required Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence 2008 AAAI Press 950 956 [20] E. D. Sacerdoti Planning in a hierarchy of abstraction spaces Artif. Intell. 5 2 1974 115 135 [21] P. Haslum A. Botea M. Helmert B. Bonet S. Koenig Domain-independent construction of pattern database heuristics for cost-optimal planning Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence vol. 7 2007 AAAI Press 1007 1012 [22] C.A. Knoblock S. Minton O. Etzioni Integrating abstraction and explanation-based learning in PRODIGY Proceedings of the Ninth AAAI Conference on Artificial Intelligence 1991 AAAI Press 541 546 [23] R. Bergmann W. Wilke Building and refining abstract planning cases by change of representation language J. Artif. Intell. Res. 3 1995 53 118 [24] L. Seabra Lopes Failure recovery planning in assembly based on acquired experience: learning by analogy Assembly and Task Planning, 1999. (ISATP’99) Proceedings of the 1999 IEEE International Symposium on 1999 IEEE 294 300 [25] L. Seabra Lopes Failure recovery planning for robotized assembly based on learned semantic structures IFAC Workshop on Intelligent Assembly and Disassembly (IAD’2007) 2007 65 70 [26] M. Tenorth A.C. Perzylo R. Lafrenz M. Beetz Representation and exchange of knowledge about actions, objects, and environments in the RoboEarth framework IEEE Trans. Autom. Sci. Eng. 10 3 2013 643 651 [27] M. Tenorth M. Beetz Representations for robot knowledge in the KnowRob framework Artif. Intell. 2015 [28] J. Hertzberg J. Zhang L. Zhang S. Rockel B. Neumann J. Lehmann K. Dubba A. Cohn A. Saffiotti F. Pecora M. Mansouri Š. Konĕcný M. Günther S. Stock L. Seabra Lopes M. Oliveira G. Lim H. Kasaei V. Mokhtari L. Hotz W. Bohlken The RACE Project KI - Künstliche Intelligenz 28 4 2014 297 304 [29] N. Shafii S.H. Kasaei L. Seabra Lopes Learning to grasp familiar objects using object view recognition and template matching 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2016 2895 2900 [30] J. Rintanen Planning as satisfiability: heuristics Artif. Intell. 193 2012 45 86 [31] J. Hoffmann B. Nebel The ff planning system: fast plan generation through heuristic search J. Artif. Intell. Res. 14 2001 253 302 "
    },
    {
        "doc_title": "Multi-view 6D Object Pose Estimation and Camera Motion Planning Using RGBD Images",
        "doc_scopus_id": "85046250604",
        "doc_doi": "10.1109/ICCVW.2017.260",
        "doc_eid": "2-s2.0-85046250604",
        "doc_date": "2017-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Active Vision",
            "Camera motions",
            "Camera positions",
            "Entropy-based",
            "Multiple objects",
            "Next best view",
            "Pose estimation",
            "Severe occlusions"
        ],
        "doc_abstract": "© 2017 IEEE.Recovering object pose in a crowd is a challenging task due to severe occlusions and clutters. In active scenario, whenever an observer fails to recover the poses of objects from the current view point, the observer is able to determine the next view position and captures a new scene from another view point to improve the knowledge of the environment, which may reduce the 6D pose estimation uncertainty. We propose a complete active multi-view framework to recognize 6DOF pose of multiple object instances in a crowded scene. We include several components in active vision setting to increase the accuracy: Hypothesis accumulation and verification combines single-shot based hypotheses estimated from previous views and extract the most likely set of hypotheses; an entropy-based Next-Best-View prediction generates next camera position to capture new data to increase the performance; camera motion planning plans the trajectory of the camera based on the view entropy and the cost of movement. Different approaches for each component are implemented and evaluated to show the increase in performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning and planning of robot tasks with loops",
        "doc_scopus_id": "85026865200",
        "doc_doi": "10.1109/ICARSC.2017.7964091",
        "doc_eid": "2-s2.0-85026865200",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Action sequences",
            "Planning domains",
            "Real environments",
            "Robot tasks",
            "Task knowledge"
        ],
        "doc_abstract": "© 2017 IEEE.We extend our previous work on experience-based planning domains in robotics to detect and represent loops in actions sequences that achieve certain tasks; and to generate action sequences from loop descriptions. The approach includes methods for conceptualizing robot experiences containing loops; and generating plans based on learned task knowledge with loops. This facilitates the reuse of existing task knowledge for different instances of the same task with varying numbers of objects. A demonstration of this system in a real environment illustrates how a robotic arm can learn and carry out a task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Borges of Carvalhais, Lords of Ferreiros, Avelãs de Cima and Ílhavo",
        "doc_scopus_id": "85041397226",
        "doc_doi": "10.14195/1645-2259_17_5",
        "doc_eid": "2-s2.0-85041397226",
        "doc_date": "2017-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "History",
                "area_abbreviation": "ARTS",
                "area_code": "1202"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The vast seigniorial domain of Carvalhais was held by the Borges family and descendants from the mid-fifteenth century until the liberal reform. It integrated the old counties of Ferreiros, Avelãs de Cima and Ílhavo, in the modern district of Aveiro. There are no in-depth studies on the history of this seigniorie. The chronological series of the so-called \"lords of Carvalhais\" is only well known from António Borges de Miranda, 3rd lord of Carvalhais in the Borges line, an individual born around 1470. In this article, the series of lords of Carvalhais in the Borges lineage is studied in the historical context of the 15th and 16th centuries.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Object learning and grasping capabilities for robotic home assistants",
        "doc_scopus_id": "85006399952",
        "doc_doi": "10.1007/978-3-319-68792-6_23",
        "doc_eid": "2-s2.0-85006399952",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Assistive robots",
            "Manipulation task",
            "Object categories",
            "Object grasping",
            "Object learning",
            "Perception capability",
            "Proposed architectures",
            "Qualitative evaluations"
        ],
        "doc_abstract": "© 2017, Springer International Publishing AG.This paper proposes an architecture designed to create a proper coupling between perception and manipulation for assistive robots. This is necessary for assistive robots, not only to perform manipulation tasks in reasonable amounts of time, but also to robustly adapt to new environments by handling new objects. In particular, this architecture provides automatic perception capabilities that will allow robots to, (i) incrementally learn object categories from the set of accumulated experiences and (ii) infer how to grasp household objects in different situations. To examine the performance of the proposed architecture, quantitative and qualitative evaluations have been carried out. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform object grasping tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Learning Approach for Robotic Grasp Selection in Open-Ended Domains",
        "doc_scopus_id": "85010375402",
        "doc_doi": "10.1109/ICARSC.2016.17",
        "doc_eid": "2-s2.0-85010375402",
        "doc_date": "2016-12-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Global visual features",
            "Interactive objects",
            "Learning approach",
            "Mahalanobis distances",
            "Object representations",
            "Position and orientations",
            "Similarity measure",
            "Similarity metrics"
        ],
        "doc_abstract": "© 2016 IEEE.Enabling a robot to grasp unknown objects is still an ongoing challenge in robotics. The main problem is to find an appropriate grasp configuration including the position and orientation of the arm relative to the object and fingers configuration. One approach is to recognize an appropriate grasp pose for an object based on one or more grasp demonstrations of the same or other objects. We focus on familiar objects, i.e. unknown objects sharing some features with known objects. The underlying assumption in grasping familiar objects is that, where they are similar to known ones, they may be grasped in a similar way. However finding an object representation and a similarity metric are still the main challenge to transfer grasp experiences to new objects. In this paper, we present an interactive object view recognition approach and a similarity metric to grasp familiar objects. Object view recognition is incrementally capable of recognizing object view labels. The grasp pose learning approach learns a grasp template for a recognized object view using local and global visual features of a demonstrated grasp. In grasp pose recognition, a similarity measure based on Mahalanobis distance is used for grasp template matching. The experimental results reveal the high reliability of the developed template matching approach. We also demonstrate how the proposed grasp learning system can incrementally improve its performance in grasping familiar objects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concurrent 3D Object Category Learning and Recognition Based on Topic Modelling and Human Feedback",
        "doc_scopus_id": "85010297513",
        "doc_doi": "10.1109/ICARSC.2016.28",
        "doc_eid": "2-s2.0-85010297513",
        "doc_date": "2016-12-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Learning objects",
            "Limited training data",
            "Object categories",
            "Object exploration",
            "Object recognition systems",
            "On-line evaluation",
            "Service robots",
            "Visual information"
        ],
        "doc_abstract": "© 2016 IEEE.In open-ended domains, autonomous robots must have the ability to continuously process visual information, and execute learning and recognition in a concurrent and interleaved fashion. Because the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by service robots. Topic modelling approaches usually construct the topics from a training set to recognize objects. However, in open-ended domains, the data available for training increases continuously. If limited training data is used, this might lead to non-discriminative topics and, as a consequence, to poor object recognition performance. This paper proposes an object recognition system capable of learning object categories as well as the topics used to encode objects concurrently and in an open-ended manner. This system provides a robot with the capabilities to, (i) use unsupervised object exploration to construct a dictionary of visual words for representing objects and (ii) conceptualize object experiences and learn new object categories using topic modelling and human feedback. To examine the performance of the system, an on-line evaluation protocol is used to assess the performance of the system in an open-ended setting. The experimental results show the fulfilling performance of this approach on different types of objects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning to grasp familiar objects using object view recognition and template matching",
        "doc_scopus_id": "85006365319",
        "doc_doi": "10.1109/IROS.2016.7759448",
        "doc_eid": "2-s2.0-85006365319",
        "doc_date": "2016-11-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Distance measure",
            "Global visual features",
            "Incremental learning",
            "Interactive objects",
            "Mahalanobis distances",
            "Position and orientations",
            "Process of learning",
            "Similarity metrics"
        ],
        "doc_abstract": "© 2016 IEEE.Robots are still not able to grasp all unforeseen objects. Finding a proper grasp configuration, i.e. the position and orientation of the arm relative to the object, is still challenging. One approach for grasping unforeseen objects is to recognize an appropriate grasp configuration from previous grasp demonstrations. The underlying assumption in this approach is that new objects that are similar to known ones (i.e. they are familiar) can be grasped in a similar way. However finding a grasp representation and a grasp similarity metric is still the main challenge in developing an approach for grasping familiar objects. In this paper, interactive object view learning and recognition capabilities are integrated in the process of learning and recognizing grasps. The object view recognition module uses an interactive incremental learning approach to recognize object view labels. The grasp pose learning approach uses local and global visual features of a demonstrated grasp to learn a grasp template associated with the recognized object view. A grasp distance measure based on Mahalanobis distance is used in a grasp template matching approach to recognize an appropriate grasp pose. The experimental results demonstrate the high reliabilityof the developed template matching approach in recognizing the grasp poses. Experimental results also show how the robot can incrementally improve its performance in grasping familiar objects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An orthographic descriptor for 3D object learning and recognition",
        "doc_scopus_id": "85006365227",
        "doc_doi": "10.1109/IROS.2016.7759612",
        "doc_eid": "2-s2.0-85006365227",
        "doc_date": "2016-11-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Classification performance",
            "Computation time",
            "Descriptors",
            "Object reference",
            "Object representations",
            "Real-time application",
            "State of the art"
        ],
        "doc_abstract": "© 2016 IEEE.Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure reliability, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. The performance of the proposed object descriptor is compared with the main state-of-the-art descriptors. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-oftheart descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GOOD: A global orthographic object descriptor for 3D object recognition and manipulation",
        "doc_scopus_id": "84994232513",
        "doc_doi": "10.1016/j.patrec.2016.07.006",
        "doc_eid": "2-s2.0-84994232513",
        "doc_date": "2016-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Classification performance",
            "Disambiguation method",
            "Distribution matrices",
            "Object perception",
            "Object representations",
            "Orthographic projections",
            "Real-time application"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure robustness, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. We propose a novel sign disambiguation method, for computing a unique reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the target object view captured by a 3D sensor. Three principal orthographic projections and their distribution matrices are computed by exploiting the object reference frame. The descriptor is finally obtained by concatenating the distribution matrices in a sequence determined by entropy and variance features of the projections. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications. The estimated object's pose is precise enough for real-time object manipulation tasks.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2016-07-20 2016-07-20 2016-11-03 2016-11-03 2017-03-08T23:40:18 S0167-8655(16)30168-4 S0167865516301684 10.1016/j.patrec.2016.07.006 S300 S300.3 FULL-TEXT 2017-03-08T21:25:26.072496-05:00 0 0 20161101 2016 2016-07-20T22:11:34.444637Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb vol volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0167-8655 01678655 true 83 83 P3 Volume 83, Part 3 11 312 320 312 320 20161101 1 November 2016 2016-11-01 2016 Efficient Shape Representation, Matching, Ranking, and its Applications Xiang Bai Michael Donoser Hairong Liu Longin Jan Latecki article sco © 2016 Elsevier B.V. All rights reserved. GOODAGLOBALORTHOGRAPHICOBJECTDESCRIPTORFOR3DOBJECTRECOGNITIONMANIPULATION KASAEI S 1 Introduction 2 Related work 3 Local reference frame 4 Object descriptor 5 Experimental results 5.1 Descriptiveness 5.2 Scalability 5.3 Robustness 5.3.1 Gaussian noise 5.3.2 Varying point cloud density 5.4 Efficiency 5.4.1 Memory footprint 5.4.2 Computation time 5.5 System demonstration 6 Conclusion Acknowledgments References ALDOMA 2012 A ANDREOPOULOS 2013 827 891 A BO 2011 1729 1736 L IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR2011 OBJECTRECOGNITIONHIERARCHICALKERNELDESCRIPTORS BRO 2008 135 140 R CHEN 2007 1252 1262 H COVER 2012 T ELEMENTSINFORMATIONTHEORY DENG 2009 248 255 J IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION2009CVPR2009 IMAGENETALARGESCALEHIERARCHICALIMAGEDATABASE DINH 2006 863 870 H IEEECOMPUTERSOCIETYCONFERENCECOMPUTERVISIONPATTERNRECOGNITION2006 MULTIRESOLUTIONSPINIMAGES FROME 2004 224 237 A COMPUTERVISIONECCV2004 RECOGNIZINGOBJECTSINRANGEDATAUSINGREGIONALPOINTDESCRIPTORS GUO 2013 86 93 Y GRAPPIVAPP TRISIADISTINCTIVELOCALSURFACEDESCRIPTORFOR3DMODELINGOBJECTRECOGNITION HORN 1984 1671 1686 B JOHNSON 1999 433 449 A KASAEI 2015 537 553 S LAI 2011 1817 1824 K ROBOTICSAUTOMATIONICRA2011IEEEINTERNATIONALCONFERENCE ALARGESCALEHIERARCHICALMULTIVIEWRGBDOBJECTDATASET MARTON 2010 365 370 Z 10THIEEERASINTERNATIONALCONFERENCEHUMANOIDROBOTSHUMANOIDS2010 HIERARCHICALOBJECTGEOMETRICCATEGORIZATIONAPPEARANCECLASSIFICATIONFORMOBILEMANIPULATION MIAN 2010 348 361 A MILLER 1995 39 41 G OLIVEIRA 2014 2216 2223 M IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS20142014 APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS OLIVEIRA 2015 M OSADA 2002 807 832 R PANG 2015 171 179 G INTERNATIONALCONFERENCE3DVISION3DV2015 FASTROBUSTMULTIVIEW3DOBJECTRECOGNITIONINPOINTCLOUDS PASQUALOTTO 2013 608 623 G REGAZZONI 2014 719 728 D RUSU 2009 3212 3217 R IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATION2009ICRA09 FASTPOINTFEATUREHISTOGRAMSFPFHFOR3DREGISTRATION RUSU 2010 2155 2162 R IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS2010 FAST3DRECOGNITIONPOSEUSINGVIEWPOINTFEATUREHISTOGRAM RUSU 2009 47 54 R IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS2009 DETECTINGSEGMENTINGOBJECTSFORMOBILEMANIPULATION RUSU 2008 927 941 R SU 2015 945 953 H PROCEEDINGSIEEEINTERNATIONALCONFERENCECOMPUTERVISION MULTIVIEWCONVOLUTIONALNEURALNETWORKSFOR3DSHAPERECOGNITION TOMBARI 2010 356 369 F COMPUTERVISIONECCV2010 UNIQUESIGNATURESHISTOGRAMSFORLOCALSURFACEDESCRIPTION WOHLKINGER 2011 2987 2992 W IEEEINTERNATIONALCONFERENCEROBOTICSBIOMIMETICSROBIO2011 ENSEMBLESHAPEFUNCTIONSFOR3DOBJECTCLASSIFICATION WU 2015 1912 1920 Z PROCEEDINGSIEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION 3DSHAPENETSADEEPREPRESENTATIONFORVOLUMETRICSHAPES ZHONG 2009 689 696 Y IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS2009 INTRINSICSHAPESIGNATURESASHAPEDESCRIPTORFOR3DOBJECTRECOGNITION KASAEIX2016X312 KASAEIX2016X312X320 KASAEIX2016X312XS KASAEIX2016X312X320XS 2018-11-03T00:00:00.000Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0167-8655(16)30168-4 S0167865516301684 10.1016/j.patrec.2016.07.006 271524 2017-03-08T21:25:26.072496-05:00 2016-11-01 true 2164112 MAIN 9 55193 849 656 IMAGE-WEB-PDF 1 gr1 15364 164 203 gr2 9775 83 219 gr3 14154 141 219 gr4 7518 52 219 gr5 12316 91 219 gr6 8243 54 219 gr7 11732 78 219 gr8 7807 76 219 gr9 15100 86 219 gr1 43088 311 385 gr2 59290 305 809 gr3 33529 246 383 gr4 57188 194 808 gr5 17361 131 316 gr6 66223 200 810 gr7 17689 119 334 gr8 20773 134 385 gr9 24998 150 382 gr1 307699 1377 1704 gr2 563764 1351 3583 gr3 301966 1090 1697 gr4 501105 857 3576 gr5 276100 581 1402 gr6 558076 885 3587 gr7 271243 527 1478 gr8 165674 595 1705 gr9 226874 667 1693 si1 224 14 46 si10 171 12 22 si11 174 12 22 si12 265 11 58 si13 184 16 20 si14 200 20 28 si15 186 17 28 si16 901 22 315 si17 409 13 90 si18 1178 56 198 si19 341 18 83 si2 507 17 143 si20 127 13 23 si21 141 15 24 si22 198 16 63 si23 601 16 145 si24 272 15 44 si25 562 20 132 si26 581 17 173 si27 580 17 173 si28 1151 53 244 si29 1183 53 245 si3 623 52 106 si30 149 24 10 si31 245 24 46 si32 272 24 46 si33 176 13 20 si34 1150 16 322 si35 962 52 206 si36 168 11 22 si37 460 17 93 si38 271 16 59 si39 972 52 207 si4 162 13 17 si40 189 12 26 si41 596 58 112 si42 340 19 57 si43 219 13 44 si44 853 16 261 si5 1066 52 228 si6 382 16 84 si7 461 16 122 si8 699 17 162 si9 146 11 16 PATREC 6593 S0167-8655(16)30168-4 10.1016/j.patrec.2016.07.006 Elsevier B.V. Fig. 1 Visualization of sign disambiguation procedure: (a) orthographic projection of the object on the XoZ and XoY planes; (b) XoY plane is used to determine the sign of Y axis; (c) XoZ plane is used to determine the sign of X axis. The red, green and blue lines represent the unambiguous X, Y, Z axes respectively. Fig. 1 Fig. 2 An illustrative example of the producing a GOOD shape description for a mug object (i.e. d = 5 ): (a) The mug object and its bounding box, reference frame and three projected views; the object’s points are then projected onto three planes; therefore, XoZ (b), YoZ (c) and XoY projections (d) are created. Each plane is partitioned into bins and the number of point falling into each bin is counted. Accordingly, three distribution matrices are obtained for the projections; afterwards, each distribution matrix is converted to a distribution vector, (i.e. (e), (f) and (g)) and two statistic features including entropy and variance are then calculated for each distribution vector; (h) the distribution vectors are consequently concatenated together using the statistics features, to form a single description for the given object. The ordering of the three distribution vectors is first by decreasing values of entropy. Afterwards the second and third vectors are sorted again by increasing values of variance. Fig. 2 Fig. 3 Example of how the projections used to build GOOD can also be used for extracting features relevant for object manipulation (see text): (a) Local reference frame and projections; (b) Projections in multi-view layout. Fig. 3 Fig. 4 Object recognition performance in descriptiveness and scalability experiments; (left) effect of number of bins on performance; (center) scalability of the selected descriptors with respect to varying numbers of categories in the dataset as a function of accuracy vs. Number of categories; (right) scalability experiment time vs. Number of categories. Fig. 4 Fig. 5 An illustration of a Vase object with different levels of Gaussian noise. Fig. 5 Fig. 6 The robustness of the selected descriptors to different level of Gaussian noise and varying point cloud density: (left) different levels of Gaussian noise applied to the test; (center) different levels of downsampling applied to the test data; (right) different levels of downsampling applied to the train data. Fig. 6 Fig. 7 An illustration of a Flask object with different levels of downsampling. Fig. 7 Fig. 8 Average computation time of the selected descriptors on 20 randomly selected objects from the RGB-D dataset. Fig. 8 Fig. 9 Two snapshots showing the object perception system performing object recognition and pose estimation using the GOOD descriptor; (left) the instructor puts a Mug and a Vase on the table. The gray bonding boxes and red, green and blue lines signal the pose of the object and the GOOD descriptions are visualized and computed; this frame shows that the system is able to compute the GOOD description and estimate pose of objects in the scene. Moreover, it demonstrates that the Vase and the Mug are properly recognized; (right) A Plate enters the scene. Its shape description and pose are computed and visualized. Because there is no prior knowledge about plates, it is classified as Unknown [13]. Fig. 9 Table 1 Summary of descriptiveness experiments. Table 1 Number of bins Descriptor size Memory (Kb) Accuracy 5 75 0.3 0.92 10 300 1.2 0.93 15 675 2.7 0.94 20 1200 4.8 0.93 25 1875 7.5 0.94 30 2700 10.8 0.93 35 3675 14.7 0.93 40 4800 19.2 0.93 45 6075 24.3 0.93 50 7500 30.0 0.93 Table 2 Summary of scalability experiments. Table 2 Number of categories Accuracy GOOD(5bins) GOOD(15bins) VFH ESF GFPFH GRSD 5 0.96 0.98 0.98 0.97 0.90 0.97 10 0.94 0.95 0.96 0.96 0.71 0.92 15 0.95 0.95 0.97 0.98 0.75 0.80 20 0.95 0.95 0.97 0.96 0.71 0.81 25 0.95 0.95 0.97 0.97 0.61 0.79 30 0.94 0.94 0.96 0.94 0.61 0.77 35 0.94 0.94 0.94 0.96 0.59 0.76 40 0.93 0.94 0.94 0.93 0.59 0.72 45 0.92 0.94 0.94 0.93 0.60 0.69 50 0.92 0.94 0.94 0.93 0.59 0.68 Table 3 Summary of robustness to Gaussian noise experiments. Table 3 Gaussian noise(mm) Accuracy GOOD 5bins VFH ESF GFPFH GRSD 1 0.94 0.95 0.90 0.40 0.66 2 0.93 0.93 0.74 0.17 0.61 3 0.92 0.93 0.49 0.10 0.51 4 0.91 0.94 0.32 0.09 0.35 5 0.89 0.91 0.24 0.09 0.26 6 0.85 0.85 0.23 0.09 0.19 7 0.83 0.78 0.23 0.09 0.12 8 0.78 0.57 0.22 0.09 0.08 9 0.69 0.42 0.23 0.09 0.10 10 0.67 0.36 0.22 0.09 0.09 Table 4 Length of selected 3D shape descriptors. Table 4 No. Descriptor Feature length (float) Adjustable length Implementation 1 GFPFH 16 No PCL 1.7 2 GRSD 21 No PCL 1.8 3 GOOD 75 Yes – 4 VFH 308 No PCL 1.7 5 ESF 640 No PCL 1.7 ☆ This paper has been recommended for acceptance by Gabriella Sanniti di Baja. GOOD: A global orthographic object descriptor for 3D object recognition and manipulation S. Hamidreza Kasaei ⁎ a Ana Maria Tomé a b Luís Seabra Lopes a b Miguel Oliveira c a IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Aveiro, 3810-193, Portugal IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro Universidade de Aveiro Aveiro 3810-193 Portugal b Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática Universidade de Aveiro Portugal c Instituto de Engenharia de Sistemas e Computadores, Tecnologia e Ciência R. Dr. Roberto Frias, 465, Porto 4200, Portugal Instituto de Engenharia de Sistemas e Computadores Tecnologia e Ciência R. Dr. Roberto Frias 465 Porto 4200 Portugal ⁎ Corresponding author. Tel.: +351 234 370 500; fax: +351 234 370 545. Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure robustness, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. We propose a novel sign disambiguation method, for computing a unique reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the target object view captured by a 3D sensor. Three principal orthographic projections and their distribution matrices are computed by exploiting the object reference frame. The descriptor is finally obtained by concatenating the distribution matrices in a sequence determined by entropy and variance features of the projections. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications. The estimated object’s pose is precise enough for real-time object manipulation tasks. Keywords 3D object recognition Object Perception Orthographic projection 1 Introduction Following the advent of inexpensive depth sensing devices such as Microsoft Kinect or the ASUS Xtion, which record RGB and depth information, the use of 3D data is becoming increasingly popular. One of the primary goals in service robotics is to develop reliable capabilities in the area of perception that will allow robots to robustly recognize objects and interact with the environment by manipulating those objects. For this purpose, a robot must reliably recognize the object. Furthermore, in order to interact with human users, this process of object recognition cannot take more than a fraction of a second. Although many object recognition methods for both 2D and 3D data have been proposed [2], recognizing 3D objects in the presence of noise and variable point cloud resolution is still a challenging task. However, 3D data contains more information about the spatial positioning of objects, which in turn eases the process of object segmentation. Moreover, depth data is more robust than RGB data to the effects of illumination and shadows [25]. Therefore, 3D data can be employed to describe the surface of the objects based on geometric properties 1 1 2D data can also be used to distinguish objects that have same geometric properties with different texture (e.x. a Coke can from a Diet Coke can). . A 3D object recognition system is composed of several software modules such as Object Detection, Object Representation, Object Recognition and Perceptual Memory. Object Detection is responsible for detecting all objects in a scene. Object representation is concerned with the calculation of a set of features for the detected object, which are send to the Object Recognition. Objects are recognized by comparing their description against the descriptions of known objects (stored in the Perceptual Memory). Object Representation plays a prominent role because the output of this module is used for learning as well as recognition. Existing 3D object representation approaches are based on either global or local descriptors. Global descriptors encode the entire 3D object, while local descriptors represent a small area of an object around a specific keypoint. Generally, global descriptors are increasingly used in the context of 3D object recognition, object manipulation, as well as geometric categorization [1]. These must be efficient in terms of computation time as well as memory, to facilitate real-time performance. For example, Ensemble of Shape Functions (ESF) [32], Global Fast Point Feature Histogram (GFPFH) [28], Viewpoint Feature Histogram (VFH) [27] and Global Radius-based Surface Descriptor (GRSD) [16], are global descriptors. Local descriptors tend to handle occlusion and clutter better when compared to global descriptor. However, comparing 3D object views based on their local features tends to be computationally more expensive [1]. Examples in this category include Spin Images (SI) [12], Signature of Histograms of Orientations (SHOT) [31], Fast Point Feature Histogram (FPFH) [26] and Hierarchical Kernel Descriptors [3]. Invariance to the pose of an object is a critical property of any 3D shape descriptor. A number of 3D shape descriptors achieve pose invariance using either a reference axis only (e.x. Spin-Images [12]) or a complete object reference frame (e.x. Intrinsic Shape Signatures [34]). In this paper, a new global 3D shape descriptor named GOOD (i.e. Global Orthographic Object Descriptor) is presented. GOOD provides an appropriate trade-off between descriptiveness, computation time and memory usage. The descriptor is designed to be scale and pose invariant, informative and stable, with the objective of supporting accurate 3D object recognition. A novel sign disambiguation method is proposed to compute a unique and repeatable reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the object. Using this reference frame, three three principal projections, namely XoZ, XoY and YoZ, are created based on orthographical projection. The space of each projection is partitioned into bins and the number of points falling into each bin is counted. From this, three distribution matrices are obtained for the projected views. Two statistic features, namely entropy and variance are then calculated for each distribution matrix. The distribution matrices are consequently concatenated together using the entropy and variance features to form a single description for the given object view. In this paper, we assume that an object has already been segmented from the point cloud of the scene, and we will focus on detailing the 3D object descriptor. This descriptor works directly on 3D point clouds and requires neither triangulation of the object’s points nor surface meshing. For additional details on the object detection and object recognition methodologies, we refer the reader to our previous works on interactive open-ended learning for 3D object recognitions [13,19,20]. The contributions presented in this paper are the following: (i) design a new sign disambiguation method to compute a unique and unambiguous complete local reference frame, from the eigenvectors obtained through Principal Component Analysis of the segmented point cloud of the object and (ii) a novel global object descriptor computed using that local reference frame, that provides a good trade-off between descriptiveness, computation time and memory usage. The remainder of this paper is organized as follows. In Section 2, we discuss related works. The methodology for computing the local reference frame is presented in Section 3. Section 4 describes the proposed global object descriptor. Evaluation of the proposed shape descriptor is presented in Section 5. Finally, in Section 6, conclusions are presented and future research is discussed. 2 Related work Three-dimensional shape description has been under investigation for a long time in various research fields, such as pattern recognition, computer graphics and robotics. Although an exhaustive survey of shape descriptor is beyond the scope of this paper, we will review a few recent efforts. As previously mentioned, some descriptors use LRF to compute a pose invariant description. Therefore, this property can be used to categorize 3D shape descriptors into two categories including (i) shape descriptors without LRF; (ii) shape descriptors with LRF. Most of the shape descriptors of the first category use certain statistic features or geometric properties of the points on the surface like depth value, curvature and surface normal to generate a description. For instance, Shape Distributions descriptor [21] represents an object as a shape distribution sampled from a shape function measuring global geometric properties of the object. Extended Gaussian Images (EGI) descriptor [11] is based on the distribution of surface normals on the Gaussian sphere. Since descriptiveness of the EGI depends on the shape of the object and it is not suitable for non-convex object. Chen and Bhanu [5] proposed a local surface patch (LSP) descriptor that encodes the shape of objects by accumulating points in particular bins along the two dimensions that are the shape index value and the cosine of the angle between the surface normals. Wohlkinger and Vincze [32] introduced a global shape descriptor called Ensemble of Shape Function (ESF) that does not require the use of normals to describe the object and the characteristic properties of an object is represented using an ensemble of ten 64-bin histograms of angle, point distance, and area shape functions. Point Feature Histogram (PFH) [29] can be used as local or global shape descriptor. The PFH represents the relative orientation of normals, as well as distances, between point pairs. For each point p, k-neighborhood points are selected based on a sphere centered at p with radius r. Afterwards, a surface normal for each point is estimated. Subsequently, four features are calculated for every pair of points using their surface normals, positions and angular variations. In a later work [26], in order to improve the robustness of PFH in case of point densities variations, the distance between point pairs is excluded from the histogram of PFH. The computation complexity of a PFH is O(n 2), where n is the number points in the point cloud. Fast Point Feature Histogram (FPFH) [26] is an extension version of PFH. The FPFH estimates the sets of values only between every point and its k nearest neighbors. This is different from PFH, where all pairs of points in the support region are considered. Therefore, the computational complexity is reduced to O(k.n). The FPFH is a scale and pose invariant descriptor which is not suitable for grasping. Viewpoint Feature Histogram (VFH) [27] is another extension of PFH descriptor. The VFH descriptor computes the same angular features as the PFH. Additionally, it computes another statistics between the central viewpoint direction and the normals estimated at each point. The VFH shape descriptor produces a single histogram that encodes the geometry of the whole object and its viewpoint. Because of the global nature of VFH, the computational complexity of VFH is O(n). The descriptiveness of the above shape descriptors are limited because the 3D spatial information either is not taken into account or it is discarded during the description process. Unlike the above approaches, some researchers have recently adopted deep learning algorithms for 3D object representation, learning and recognition [15,30,33]. These works use a collection of 2D images rendered from different view points to learn a shape representation that aggregates information from input views and provides a compact shape descriptor. As it was pointed out in [33], training a deep artificial neural network for 3D object representation requires a large collection of 3D objects to provide accurate representations and typically involves long training times. In contrast, the shape descriptors in the second category encode the spatial information of the objects’ points using a Local Reference Frame (LRF). Some descriptors provide a description using only a Reference Axis. For example, Spin-Images [12] uses surface normal of a vertex as a reference axis and proposed a spin image representation by projecting the surface points to the tangent plane of the vertex. Then, each projected point is represented by a pair (α, β), where α is the distance to the surface normal, i.e., the radius, and β is the perpendicular distance from the point to the tangent plane. Consequently, a histogram is formed by counting the occurrences of different discretized distance pairs. Spin images descriptor has been successfully used in many applications, but one limitation of this descriptor is that it is not scale invariant. Dinh and Kropac [8] proposed multi-resolution pyramids of spin images in order to improve the discrimination of the original spin image and speed up the matching process. Some variants of the spin image shape descriptor also presented such as Tri-Spin-Image descriptor (TriSI) [10] and color spin image [23]. Similar to the SI, 3D Shape Context (3DSC) [9] uses the surface normal of an basis point as its LRF. The 3DSC descriptor is calculated by counting the weighted number of points falling into each bin of an sphere grid centered on the basis point and its north pole oriented with the surface normal. The sphere grid is constructed based on dividing the support area into bins by logarithmically spaced boundaries along the radial dimension and equally spaced boundaries in the azimuth and elevation dimensions. Whenever only an axis is used as a reference frame, there is an uncertainty in the rotation around the axis that should be handled for generating a robust and repeatable description. In order to eliminate this issue, several descriptors (e.g. 3D Shape Context) proposed to compute multiple descriptions for different possible rotations of the object. Since this kind of solutions are caused increasing the computational cost in terms of both execution time as well as memory usage, they are not optimum and real solutions. Furthermore, the recognition process becomes not only significantly slow, but also more laborious. Differently, Zhong [34] proposed a shape descriptor namely Intrinsic Shape Signatures (ISS) using defining a LRF based on the eigenvectors of the scatter matrix of the point cloud of the object and describing the point distribution in the spherical angular space. Similar to Zhong work, Mian et al. [17] introduced LRF computed with eigenvectors of the covariance matrix of the object’s points. However, in both cases the eigenvectors define the principal directions of the data, their sign is not defined unambiguously. Accordingly, different descriptors can be generated for the object. As highlighted before, they are neither computationally efficient nor repeatable. [22] proposed a multi-view 3D object recognition approach. In this approach, each object is projected into 46 projection planes distributed on a sphere, whereas we just compute three principal orthographic projections. Their object representation is clearly not efficient for real time application like robotics. In order to achieve true rotation invariant descriptor, Tombari et al. [31] proposed a 3D shape descriptor namely Signature of Histograms of OrienTations (SHOT). To generate the description for the object, they first applied a sign disambiguation technique to the eigenvectors of the scatter matrix of the object and constructed a unique and unambiguous LRF. The object’s points are then aligned with the LRF. Consequently, similar to 3D Shape Context, a spherical coordinate based approach is used to generate a SHOT description for the given object. 3D object descriptors that use spherical coordinate system suffer from the singularity issue at the poles, because bins at the poles are significantly smaller than bins around the equator. Our shape descriptor differ from all of the listed descriptors above as it is simultaneously unique, unambiguous, and robust to noise and varying low-level point cloud density. Besides, our approach can be used not only for object recognition but also for object manipulation. 3 Local reference frame A Local Reference Frame (LRF), invariant to translations and rotations and robust to noise is important for object recognition as well as object manipulation. Since the repeatability of a LRF directly affects the descriptiveness of the object representation [17], the LRF should be as repeatable and robust as possible to improve the performance of object recognition. In this section, we propose a method to compute a LRF. For this purpose, the three principal axes of a given object are firstly determined based on Principal Component Analysis (PCA). Given a point cloud of an object that contains m points, O = { p 1 , ⋯ , p m } , the geometric center of the object is defined as: (1) c = 1 m ∑ i = 1 m p i , where p i is a three dimensional point in the object’s point cloud. The normalized covariance matrix, C, of the object is constructed: (2) C = 1 m ∑ i = 1 m ( p i − c ) ( p i − c ) T . Then, eigenvalue decomposition is performed on C: (3) C V = E V , where V = [ v 1 , v 2 , v 3 ] contains the three eigenvectors, E = d i a g ( λ 1 , λ 2 , λ 3 ) is a diagonal matrix of the corresponding eigenvalues and λ 1 ≥ λ 2 ≥ λ 3. Since the covariance matrix is symmetric positive, its eigenvalues are positive and the eigenvectors are orthogonal. Eigenvectors define directions which are not unique, i.e. not repeatable across different PCA trials. This is known as the sign ambiguity problem, for which there is no mathematical solution [4]. Since there are two possible directions for each eigenvector, a total of eight reference frames can be created from the same set of eigenvectors. A mechanism is needed to transform this reference frame into a unique object reference frame, which will be always the same across multiple trials. We start with a provisional reference frame, in which the first two axes, X and Y, are defined by the eigenvectors v 1 and v 2 , respectively. However, regarding the third axis, Z, instead of defining it based on v 3 , we define it based on the cross product v 1 × v 2 . This way, because the result of the cross product follows the right-hand rule, the number of alternatives is reduced to four. It is now enough to disambiguate the directions of the X and Y axes. So either the directions of X and Y are both changed or both remain unchanged. To complete the disambiguation, the object’s point cloud, O , is transformed to be placed in the provisional reference frame. Then, the number of points that have positive x, S x + , and the number of points that have negative x, S x − , are counted as follows: (4) S x + = { i : x p i > t } , S x − = { i : x p i < − t } , where t is a threshold (e.g. t = 0.015 m ) that is used to deal with the special case when a point is close to the YoZ plane, and therefore can change from negative to positive X in different trials. Afterwards, the variable Sx is defined as: (5) S x = { + 1 , | S x + | ≥ | S x − | − 1 , o t h e r w i s e , where |.| denotes the number of points of the argument. A similar indication, Sy , is computed for the Y axis. Finally, the sign of the axes is determined as: (6) s = S x . S y , where s can be either − 1 or + 1 . In case of s = − 1 , the directions of X and Y must be changed, otherwise not. Therefore, the final LRF (X, Y, Z) will be defined by ( s v 1 , s v 2 , v 1 × v 2 ) . An illustrative example of the sign disambiguation procedure is provided in Fig. 1 . 4 Object descriptor This section describes the computation of the proposed object descriptor, GOOD, in the obtained LRF centered in the geometric center of the object. The descriptor consists of a concatenation of the orthographic projections of the object on the three orthogonal planes, XoY, YoZ and XoZ. Each projection is described by a distribution matrix. To ensure correct comparison between different object shapes, the number of bins in the distribution matrices must be the same and the bins should be of equal size. Therefore, each distribution matrix must be computed from a square area in the projection plane centered on the object’s center, and this square area must have the same dimensions for the three projections. The side length of these square areas, l, is determined by the largest edge length of a tight-fitting axis-aligned bounding box (AABB) of the object. The dimensions of the AABB are obtained by computing the minimum and maximum coordinate values along each axis. With this setup, the number of bins, n, is the only parameter that must be specified to compute GOOD. For each projection, the l × l projection area is divided into n × n square bins. Finally, a distribution matrix M n × n is obtained by counting the number of points falling into each bin. For each projected point ρ = ( α , β ) ∈ R 2 , where α is the perpendicular distance to the horizontal axis and β is the perpendicular distance to the vertical axis, a row, r ( ρ ) ∈ { 0 , ⋯ , n − 1 } , and a column, c ( ρ ) ∈ { 0 , ⋯ , n − 1 } , are associated as follows: (7) r ( ρ ) = ⌊ α + l 2 l + ϵ n ⌋ = ⌊ n α + l 2 l + ϵ ⌋ , (8) c ( ρ ) = ⌊ β + l 2 l + ϵ n ⌋ = ⌊ n β + l 2 l + ϵ ⌋ , where ϵ is a very small value used to deal with the special cases when a point is projected onto the upper bound of the projection area, and ⌊x⌋ returns the largest integer not greater than x. Note that the projected view is shifted to right and top by l 2 (i.e. α + l 2 and β + l 2 ). Furthermore, to achieve invariance to point cloud density, M is normalized such that the sum of all bins is equal to one (see Fig. 2 ). The matrix M is called distribution matrix, because it represents the 2D spatial distribution of the object’s points. According to standard practice, this matrix is converted to a vector m 1 × n 2 = [ M ( 1 , 1 ) , M ( 1 , 2 ) , ⋯ , M ( n , n ) ] . The three projection vectors will be concatenated producing a vector of dimension 3 × n 2 which is the final object descriptor, GOOD. Statistical features are used to decide the order in which the projection vectors will be concatenated. For the first projection in the descriptor, the one with largest area is preferred. The number of points is not a good indicator of area because all points of the object are represented in the three projections. The number of occupied bins (the ones with a mass greater than 0) could be used as a measure of area. However, this measure tends to be brittle when the boundary of the object is close to boundaries between bins. Therefore, in this work the entropy of the projection is used. Entropy, a measure from Information Theory [6], nicely takes into account both the number of occupied bins and their density. In this work, the entropy of a projection is computed as follows: (9) H ( m ) = − ∑ i = 1 n m i log 2 m i , where m i is the mass in bin i. The logarithm is taken in base 2 and 0 log 2 0 = 0 . The projection with highest entropy is the one that will appear in the first n 2 positions of the descriptor. The next step is to select, from the remaining two projections, which one should appear in the second part of the descriptor (positions n 2 to 2 n 2 − 1 ). It is common that these two projections have similar areas, and therefore similar entropies, leading to instability of the decision if it is made based on entropy. Therefore, instead of entropy, we use variance to make this decision. Since the projection matrices are probability mass functions (pmf), the variance is defined as follows: (10) σ 2 ( m ) = ∑ i = 1 n ( i − μ m ) 2 m i , where μ m is the expected value (i.e. a weighted average of the possible values of i, corresponding to the geometric center of the projection), which is computed as follows: (11) μ m = ∑ i = 1 n 2 i m i , unlike the simple mean, which gives each projection equal weight, the mean of a projection weights each bin, i, according to its probability distribution, m i . The variance measure, σ 2 ( m ) , is used to measure the spread or variability of the spatial distribution of the object’s points in the projection vector. A small variance indicates that the projected points tend to be very close to each other and to the mean of the vector, i.e. the shape of distribution is small and compact. A high variance indicates that the data points in the projection vector are very spread out from the mean. An illustrative example of the proposed shape descriptor is depicted in Fig. 2. In this example, after determining the local reference frame, a mug is projected onto the three orthogonal planes. Based on the entropy criterion, the XoZ projection (Fig. 2b) is selected to appear in the first part of the descriptor. Based on the variance criterion, the YoZ projection (Fig. 2c) is selected to appear in the second part of the descriptor. The remaining projection, XoY (Fig. 2d), appears in the last part of the descriptor. In order to grasp an object, it is necessary to know true dimensions of different parts of the object. Such information is not adequately represented in most shape descriptors (e.g. Viewpoint Feature Histogram [27]). Because GOOD is composed of three orthogonal projections, it is especially rich in terms of information suited for manipulation tasks. In Fig. 3 , again we consider the projections of a mug. Here, we adopt a multi-view orthographic projection layout in which there is a central or front view, a top view and a side view. The central view is the one selected based on the entropy criterion and appearing in the first part of the descriptor. The top view contains the projection in the orthogonal plane formed by the horizontal axis of the central projection and the third axis. The side view contains the projection in the orthogonal plane formed by the vertical axis of the central projection and the third axis. The figure shows that projections can be further processed for object manipulation purposes. In the top view, the gray symbols C, W, D and T represent how the projection can be further processed and some features for manipulation task are extracted, namely inner radius (C), thickness (T), handle length (W) and handle thickness (D). 5 Experimental results Several experiments were carried out to evaluate the performance of the proposed object descriptor concerning descriptiveness, scalability, robustness and efficiency characteristics. The proposed descriptor has a parameter namely number of bins (i.e. d) that must be well selected to provide a good balance between recognition accuracy, memory usage and computation time. For this purpose, 10 experiments were performed for different values of the descriptor’s parameter. The performance of the shape descriptor and its scalability were examined on the Washington RGB-D Object Dataset [14]. Afterwards, various tests were executed to measure the robustness of the proposed shape descriptor concerning different levels of noise and varying mesh resolutions on the Restaurant Object Dataset [13]. Next, two efficiency evaluations relating to computational efficiency and memory usage were performed. Furthermore, a real demonstration was performed to show all the characteristics of the proposed descriptor. The largest publicly available dataset, namely Washington RGB-D Object Dataset [14], consisting of 250,000 views of 300 common household objects. The objects are categorized into 51 categories arranged using WordNet [18] hypernym-hyponym relationships (similar to ImageNet [7]). The Restaurant Object Dataset contains 241 views of one instance of each category (Bottle, Bowl, Flask, Fork, Knife, Mug, Plate, Spoon, Teapot, and Vase) [13]. In all experiments, an instance-based learning approach is used, i.e. object categories are represented by sets of known instances. The instance-based approach is a baseline method for evaluating representations. However, more advanced approaches like SVM and object Bayesian approaches can be easily adapted. Similarly, a simple baseline recognition mechanism in the form of a Euclidean nearest neighbor classifier is used. Moreover, the proposed descriptor was compared with four state-of-the-art object descriptors that are available in the Point-Cloud Library 2 2 (PCL version 1.7 and 1.8) including VFH [27], ESF [32], GFPFH [28] and GRSD [16]. The selected descriptors were evaluated based on a 10-fold cross validation algorithm in terms of Accuracy [24]. In each iteration, a single fold is used for testing, and the remaining data are used as training data. The cross-validation process is then repeated 10 times, which each of the 10 folds used exactly once as the test data. For all selected shape descriptors, the default parameters in the respective PCL implementations were used. All tests were performed with an i7, 2.40GHz processor and 16GB RAM. 5.1 Descriptiveness As mentioned above, GOOD has a parameter called number of bins that has effect on descriptiveness, efficiency and robustness. Therefore, it must be well selected to provide a good balance between recognition performance, memory usage and computation time. The descriptiveness of the proposed descriptor with respect to varying number of bins was evaluated using Washington dataset. Results are presented in Fig. 4 (left) and Table 1 . In these experiments, the configurations that obtained the best precision and recall figures were number 3 and 5. Although, a large number of bins provides more details about the point distribution, it increases computation time, memory usage and sensitivity to noise. Therefore, since the difference to other configurations is not very large, we prefer to use configuration number 1 (i.e. b = 5 ) which displays a good balance between recognition performance, memory usage, and processing speed. The accuracy of the proposed system with this configuration was 92%. It shows that the overall performance of the recognition system is promising and the proposed descriptor is capable of providing distinctive global feature for the given object. The following results are computed using this the default value, unless otherwise noted. 5.2 Scalability A set of experiments was carried out to evaluate the performance of the proposed descriptor on the Washington dataset, concerning its scalability with respect to varying numbers of categories. Results are depicted in Fig. 4 (center) and (right). One important observation is that the accuracy decreases in all approaches as more categories are used (Fig. 4 (center)). This is expected since the number of categories known by the system makes the classification task more difficult and the difference in performance between descriptors becomes smaller. Moreover, it can be concluded from Table 2 that when the number of object categories increases (i.e. more than 35 categories), VFH and GOOD descriptors achieve the best accuracy and stable performance regarding varying numbers of categories Table 3 . It is clear from Fig. 4 (right) that the computation time of our approach is significantly smaller than VFH, GRSD and GFPFH. However, GOOD, VFH and ESF descriptors obtain an acceptable scalability regarding varying numbers of categories, the scalability of GRSD and GFPFH are very low and their performance drops aggressively when the number of categories increases. Although ESF descriptor achieves better performance than our approach with 5 bins (i.e. GOOD 5bins), the length of ESF descriptor (i.e. compactness) is around 8.5 times more than our descriptor (see Table 4 ). It is notable that whenever the size of dataset is larger than 35 object categories, the difference between EFS performance and our approach with 5 bins, is equal or less than 1% and in the similar situation our approach with 15 bins (i.e. GOOD 15bins) works better than ESF descriptor. 5.3 Robustness The robustness of the proposed object descriptor with respect to different levels of Gaussian noise and varying mesh resolutions was evaluated and compared with other global object descriptors. These experiments were run on the mentioned Restaurant Object Dataset. 5.3.1 Gaussian noise Ten levels of Gaussian noise with standard deviations from 1 to 10 mm were added to the test data. For a given test object, Gaussian noise is independently added to the X, Y and Z-axes. As an example, a Vase object with three levels of standard deviation of Gaussian noise ( σ = 3 mm , σ = 6 mm , σ = 9 mm ) is depicted in Fig. 5 . The results are presented in Fig. 6 (left) and Table 3. An important observation can be made from Figs. 6 and 4. Although GOOD, ESF and VFH descriptors achieved a really good performance on noise free data, GOOD outperformed ESF, GFPFH and GRSD descriptors by a large margin under all levels of Gaussian noise. While the performance of VFH descriptor was similar to our approach under a low-level noise (i.e. σ ≤ 6 mm), our shape descriptor outperformed all descriptors under high levels of noise. It can be concluded from this observation that GOOD descriptor is robust to noise due to use an stable, unique and unambiguous object reference frame. In contrast, since the VFH and GFPFH descriptors are rely on surface normals to calculate their shape descriptions, they are highly sensitive to the noise. GRSD employs radial relationships to describe the geometry of points at each voxel cell and ESF uses distances and angels between randomly sampled points to generate a shape description; therefore, GRSD and ESF are also sensitive to the noise and its performance decrease rapidly when the standard deviation of the Gaussian noise increases. In addition, GOOD descriptor uses three distribution matrices that are constructed based on orthographical projection, therefore less affected by noise (i.e. in each orthographic projection one dimension is discarded). 5.3.2 Varying point cloud density Two sets of experiments were carried out to examine the robustness of the proposed descriptor with respect to varying point cloud density. In the first set of experiments, the original density of training objects was kept and the density of testing objects was reduced (downsampling) using a voxelized grid approach 3 3 In the second set of experiments, the original density was kept in testing objects and reduced in training objects. This is initiated with a root volume element (voxel) and the eight children voxels in which each internal node has exactly eight children nodes. These are recursively subdivided until all voxels contain at most one point or the minimum voxel size is reached (i.e. The cloud is divided in multiple voxels with the desired resolution). Afterwards all the points that fall into the same voxel will be downsampled with their centroid. In this evaluation, each object (either test or train) is downsampled using five different voxel sizes including {1, 5, 10, 15, 20} millimetre. An illustration example of a Flask object with four level of downsampling is depicted in Fig. 7 . The robustness results regarding varying point cloud density in test and train data are presented in Fig. 6. From experiments of reducing density of test data (i.e. Fig. 6(left)), it was found that our approach is more robust than the other descriptors concerning low-level downsampling (i.e. DS ≤ 3.5 mm) and works slightly better than the other in high-level downsampling resolution (i.e. DS ≥ 18 mm). In contrast, the performance of VFH, ESF and GRSD were better than GOOD descriptor in mid-level downsampling resolution (i.e. 3.5 mm < DS < 18). The performance of GFPFH was very low under all level levels of point cloud resolution. Besides, it can be concluded from Fig 6 (right) that when the level of up-sampling increases, VFH, ESF and GRSD descriptors achieve better performance than GOOD and GFPFH descriptors. 5.4 Efficiency In this subsection, evaluations regarding computational efficiency and memory footprint (i.e. the amount of main memory that a program uses or references while running) are presented and discussed. 5.4.1 Memory footprint The length or size of a descriptor, has direct influence on memory usage and computation time in object recognition process (see Fig. 4). The length of all descriptors used in this evaluation is given in Table 4. Although GFPFH and GRSD are the tow most compact descriptors in this evaluation (see Table 4), their computation time and are not good as depicted in Figs. 4 and 8 . Our approach is the third compact descriptor that provides good balance between computation time and descriptiveness with 75 floats. However, VFH and ESF descriptors achieve a good description power, their feature length is around 4.1 and 8.5 times larger than our approach and 20 and 40 times larger than GFPFH descriptor respectively. ESF is the lowest compact descriptor compared to all the other descriptors. 5.4.2 Computation time Several experiments were performed to measure computational time of all descriptors used in this evaluation. Since the number of object’s points directly affects the computational time, we calculate the average time required to generate a description for 20 randomly selected objects from the RGB-D dataset. Fig. 8 compares the average computation time of the selected object descriptors in which several observations can be made; first, GOOD descriptor is the most computation time efficient descriptor. In contrast, GFPFH descriptor is the computationally most expensive descriptor. ESF, VFH and GRSD descriptors achieve a medium performance in terms of computation time. Overall, GOOD descriptor achieves the best performance, which is around 10 times better performance than ESF and 44, 50 and 254 times better performance than VFH, GRSD and GFPFH descriptors. VFH, GRSD and GFPFH descriptors are extremely time consuming descriptors. The underlying reason is that GOOD descriptor works directly on 3D point clouds and requires neither triangulation of the object’s points nor surface meshing. According to the evaluations our approach is competent for robotic applications with strict limits on the memory footprint and computation time requirements. 5.5 System demonstration To show all the described functionalities and properties of the proposed GOOD descriptor, a real demonstration was performed. For this purpose, GOOD has been integrated in the object perception system presented in [13,20] and [19] (see Fig. 9 ). In this demonstration a table is in front of a robot and two users interact with the system. During the demonstration, users presented objects to the system and provided the respective category labels. Therefore, throughout this session, the system must be able to detect, conceptualize and recognize unknown (i.e. new) objects. It should be noted that a constraint has been set on the Z axis that the initial direction of Z axis of objects’ LRF should be similar to direction of Z axis of the table. It is assumed that there is no learned categories in the memory at the beginning of the demonstration. It was observed that the proposed object descriptor is capable to provide distinctive global feature for recognizing different type of objects. It also estimates pose of objects and build orthographic projections for object manipulation purposes. A video of this demonstration is available in: 6 Conclusion This paper presented a global object descriptor named GOOD (i.e. Global Orthographic Object Descriptor) that provides a good trade-off between descriptiveness, computation time and memory usage, allowing concurrent object recognition and pose estimation. For an object, GOOD is computed on a unique and repeatable local reference frame. It is calculated with the discretization of the three orthographic projections and their concatenation to form a single description for the given object. A set of experiments were carried out to assess the performance of GOOD and compare it with other state-of-art descriptors with respect to several characteristics including descriptiveness, scalability, robustness (Gaussian noise and varying low-level point cloud density) and efficiency (memory footprint and computation time). Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. GOOD outperformed the selected state-of-the-art descriptors (i.e. VFH, ESF, GRSD and GFPFH descriptors), achieving appropriate descriptiveness and significant robustness to Gaussian noise. GOOD was robust to varying low-level point cloud density too. The accuracy of VFH, ESF and GRSD was better than GOOD in the case of varying medium and high point cloud density. In addition, GOOD obtained the best computation time performance. Besides, GOOD demonstrates the capability of estimating objects’ poses and building orthographic projections for object manipulation purposes. We are currently working on integrating and using the GOOD descriptor for manipulation purposes and we would like to put the source code of the GOOD descriptor available to the research community in the ROS 4 4 repository and Point Cloud Library 5 5 in the near future. Acknowledgments This work was funded by National Funds through FCT project PEst-OE/EEI/UI0127/2014 and FCT scholarship SFRH/BD/94183/2013. References [1] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library IEEE Robot. Autom. Mag. 1070 9932/12 2012 [2] A. Andreopoulos J.K. Tsotsos 50 years of object recognition: Directions forward Comput. Vis. Image Underst. 117 8 2013 827 891 [3] Bo L. Lai K. Ren X. D. Fox Object recognition with hierarchical kernel descriptors IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011 2011 IEEE 1729 1736 [4] R. Bro E. Acar T.G. Kolda Resolving the sign ambiguity in the singular value decomposition J. Chemometr. 22 2 2008 135 140 [5] Chen H. B. Bhanu 3d free-form object recognition in range images using local surface patches Pattern Recogn. Lett. 28 10 2007 1252 1262 [6] T.M. Cover J.A. Thomas Elements of Information Theory 2012 John Wiley & Sons [7] Deng J. Dong W. R. Socher Li L.-J. Li K. Fei-Fei L. Imagenet: A large-scale hierarchical image database IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 2009 IEEE 248 255 [8] H.Q. Dinh S. Kropac Multi-resolution spin-images IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006 1 2006 IEEE 863 870 [9] A. Frome D. Huber R. Kolluri T. Bulow J. Malik Recognizing objects in range data using regional point descriptors Computer Vision-ECCV 2004 2004 Springer 224 237 [10] Guo Y. F.A. Sohel M. Bennamoun Lu M. Wan J. Trisi: A distinctive local surface descriptor for 3d modeling and object recognition. GRAPP-IVAPP 2013 86 93 [11] B.K. Horn Extended gaussian images Proc. IEEE 72 12 1984 1671 1686 [12] A.E. Johnson M. Hebert Using spin images for efficient object recognition in cluttered 3d scenes IEEE Trans. Pattern Anal. Mach. Intell. 21 5 1999 433 449 [13] S. Kasaei M. Oliveira G. Lim L. Seabra Lopes A.M. Tome Interactive open-ended learning for 3d object recognition: an approach and experiments J. Intell. Robot. Syst. 80 3–4 2015 537 553 [14] Lai K. Bo L. Ren X. D. Fox A large-scale hierarchical multi-view rgb-d object dataset Robotics and Automation (ICRA), 2011 IEEE International Conference on 2011 IEEE 1817 1824 [15] Y. Li, S. Pirk, H. Su, C.R. Qi, L.J. Guibas, Fpnn: field probing neural networks for 3d data, arXiv preprint arXiv:1605.06240 (2016). [16] Z.-C. Marton D. Pangercic R.B. Rusu A. Holzbach M. Beetz Hierarchical object geometric categorization and appearance classification for mobile manipulation 10th IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2010 2010 IEEE 365 370 [17] A. Mian M. Bennamoun R. Owens On the repeatability and quality of keypoints for local feature-based 3d object retrieval from cluttered scenes Int. J. Comput. Vis. 89 2–3 2010 348 361 [18] G.A. Miller Wordnet: a lexical database for english Commun. ACM 38 11 1995 39 41 [19] M. Oliveira Lim G.H. L. Seabra Lopes S. Hamidreza Kasaei A.M. Tome A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014), 2014 2014 IEEE 2216 2223 [20] M. Oliveira L.S. Lopes Lim G.H. S.H. Kasaei A.M. Tomé A. Chauhan 3d object perception and perceptual learning in the race project Robot. Auton. Syst. 2015 [21] R. Osada T. Funkhouser B. Chazelle D. Dobkin Shape distributions ACM Trans. Graph. 21 4 2002 807 832 [22] Pang G. U. Neumann Fast and robust multi-view 3d object recognition in point clouds International Conference on 3D Vision (3DV), 2015 2015 IEEE 171 179 [23] G. Pasqualotto P. Zanuttigh G.M. Cortelazzo Combining color and shape descriptors for 3d model retrieval Signal Process.: Image Commun. 28 6 2013 608 623 [24] D.M. Powers, Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation (2011). [25] D. Regazzoni G. de Vecchi C. Rizzi Rgb cams vs rgb-d sensors: low cost motion capture technologies performances and limitations J. Manuf. Syst. 33 4 2014 719 728 [26] R.B. Rusu N. Blodow M. Beetz Fast point feature histograms (fpfh) for 3d registration IEEE International Conference on Robotics and Automation, 2009. ICRA’09. 2009 IEEE 3212 3217 [27] R.B. Rusu G. Bradski R. Thibaux Hsu J. Fast 3d recognition and pose using the viewpoint feature histogram IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2010 2010 IEEE 2155 2162 [28] R.B. Rusu A. Holzbach M. Beetz G. Bradski Detecting and segmenting objects for mobile manipulation IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops), 2009 2009 IEEE 47 54 [29] R.B. Rusu Z.C. Marton N. Blodow M. Dolha M. Beetz Towards 3d point cloud based object maps for household environments Robot. Auton. Syst. 56 11 2008 927 941 [30] H. Su S. Maji E. Kalogerakis E. Learned-Miller Multi-view convolutional neural networks for 3d shape recognition Proceedings of the IEEE International Conference on Computer Vision 2015 945 953 [31] F. Tombari S. Salti L. Di Stefano Unique signatures of histograms for local surface description Computer Vision–ECCV 2010 2010 Springer 356 369 [32] W. Wohlkinger M. Vincze Ensemble of shape functions for 3d object classification IEEE International Conference on Robotics and Biomimetics (ROBIO), 2011 2011 IEEE 2987 2992 [33] Wu Z. Song S. A. Khosla Yu F. Zhang L. Tang X. Xiao J. 3d shapenets: a deep representation for volumetric shapes Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015 1912 1920 [34] Zhong Y. Intrinsic shape signatures: A shape descriptor for 3d object recognition IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops), 2009 2009 IEEE 689 696 "
    },
    {
        "doc_title": "Experience-Based Planning Domains: an Integrated Learning and Deliberation Approach for Intelligent Robots: Robot Task Learning from Human Instructions",
        "doc_scopus_id": "84991220046",
        "doc_doi": "10.1007/s10846-016-0371-y",
        "doc_eid": "2-s2.0-84991220046",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Conceptualization",
            "Learning from human instructions",
            "Planning domains",
            "Robot tasks",
            "Task planning"
        ],
        "doc_abstract": "© 2016, Springer Science+Business Media Dordrecht.Deliberation and learning are required to endow a robot with the capabilities for acquiring knowledge, performing a variety of tasks and interactions, and adapting to open-ended environments. This paper presents the notion of experience-based planning domains (EBPDs) for task level learning and planning in robotics. EBPDs provide methods for a robot to: (i) obtain robot activity experiences from the robot’s performance in a dynamic environment; (ii) conceptualize each experience producing an activity schema; and (iii) exploit the learned activity schemata to make plans in similar situations. Experiences are episodic descriptions of plan-based robot activities including environment perception, sequences of applied actions and achieved tasks. The conceptualization approach integrates different techniques including deductive generalization, abstraction, goal inference and feature extraction. A high-level task planner was developed to find a solution for a task by following an activity schema. The proposed approach is illustrated and evaluated in a restaurant environment where a service robot learns how to carry out complex tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ECG denoising with adaptive filter and singular value decomposition techniques",
        "doc_scopus_id": "84983087094",
        "doc_doi": "10.1145/2948992.2948993",
        "doc_eid": "2-s2.0-84983087094",
        "doc_date": "2016-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Analogue filters",
            "Digital techniques",
            "ECG Denoising",
            "Filtering process",
            "Hardware cost",
            "Noise amplitude",
            "Power line noise",
            "Singular value decomposition technique"
        ],
        "doc_abstract": "Copyright 2016 ACM.An electrocardiogram (ECG) system deals with several challenges related with noise sources. The denoising process is a challenge due to ECG signal amplitude with respect to noise amplitude. The filtering process can be executed by analogue filters or digital filters, however, modern systems uses digital filters mainly because their flexibility, efficiency and hardware costs reduction. This paper compares the ability of two digital techniques used in ECG denoising, namely Adaptive Filter (AF) and Singular Value Decomposition (SVD). The techniques were applied to real ECG signals contaminated with the most common noise sources; artefacts caused by electromyography (EMG) and power line noise (Hum).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hierarchical object representation for open-ended object category learning and recognition",
        "doc_scopus_id": "85019182538",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85019182538",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Hierarchical object representation",
            "Latent Dirichlet allocation",
            "Low-level features",
            "Object categories",
            "State-of-the-art system",
            "Statistical features",
            "Structural semantics"
        ],
        "doc_abstract": "© 2016 NIPS Foundation - All Rights Reserved.Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledgebase that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experience-based robot task Learning and planning with goal inference",
        "doc_scopus_id": "84989889577",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84989889577",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Complex task",
            "Environment perceptions",
            "Planning domains",
            "Robot tasks",
            "Service robots",
            "Task levels",
            "Task modeling",
            "Task planner"
        ],
        "doc_abstract": "Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.Learning and deliberation are required to endow a robot with the capabilities to acquire knowledge, perform a variety of tasks and interactions, and adapt to open-ended environments. This paper explores the notion of experiencebased planning domains (EBPDs) for task-level learning and planning in robotics. EBPDs rely on methods for a robot to: (i) obtain robot activity experiences from the robot's performance; (ii) conceptualize each experience to a task model called activity schema; and (iii) exploit the learned activity schemata to make plans in similar situations. Experiences are episodic descriptions of plan-based robot activities including environment perception, sequences of applied actions and achieved tasks. The conceptualization approach integrates different techniques including deductive generalization, abstraction and feature extraction to learn activity schemata. A high-level task planner was developed to find a solution for a similar task by following an activity schema. In this paper, we extend our previous approach by integrating goal inference capabilities. The proposed approach is illustrated in a restaurant environment where a service robot learns how to carry out complex tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D object perception and perceptual learning in the RACE project",
        "doc_scopus_id": "84949908319",
        "doc_doi": "10.1016/j.robot.2015.09.019",
        "doc_eid": "2-s2.0-84949908319",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Interactive learning",
            "Memory systems",
            "Open-ended learning",
            "Point cloud"
        ],
        "doc_abstract": "© 2015 Elsevier B.V.This paper describes a 3D object perception and perceptual learning system developed for a complex artificial cognitive agent working in a restaurant scenario. This system, developed within the scope of the European project RACE, integrates detection, tracking, learning and recognition of tabletop objects. Interaction capabilities were also developed to enable a human user to take the role of instructor and teach new object categories. Thus, the system learns in an incremental and open-ended way from user-mediated experiences. Based on the analysis of memory requirements for storing both semantic and perceptual data, a dual memory approach, comprising a semantic memory and a perceptual memory, was adopted. The perceptual memory is the central data structure of the described perception and learning system. The goal of this paper is twofold: on one hand, we provide a thorough description of the developed system, starting with motivations, cognitive considerations and architecture design, then providing details on the developed modules, and finally presenting a detailed evaluation of the system; on the other hand, we emphasize the crucial importance of the Point Cloud Library (PCL) for developing such system.1",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2015-10-09 2015-10-09 2015-12-02 2015-12-02 2016-12-26T09:26:02 S0921-8890(15)00214-6 S0921889015002146 10.1016/j.robot.2015.09.019 S300 S300.2 FULL-TEXT 2016-12-26T04:38:02.271089-05:00 0 0 20160101 20160131 2016 2015-10-09T15:05:55.611708Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 75 75 PB Volume 75, Part B 38 614 626 614 626 201601 January 2016 2016-01-01 2016-01-31 2016 Special Section on 3D Perception with PCL; Edited by Matteo Munaro, Radu Bogdan Rusu, and Emanuele Menegatti article fla Copyright © 2015 Elsevier B.V. All rights reserved. 3DOBJECTPERCEPTIONPERCEPTUALLEARNINGINRACEPROJECT OLIVEIRA M 1 Introduction 2 Related work 3 A dual memory approach 4 The RACE object perception system 4.1 Architecture 4.2 The PCL foundation of this work 4.3 Object perception 4.4 User perception 4.5 Addressing computational issues 5 Perceptual learning 6 Profiling and evaluation 6.1 Profiling 6.2 Off-line evaluation of the perceptual learning approach 6.3 Open-ended learning evaluation 7 Conclusions Acknowledgments References HERTZBERG 2014 297 304 J KASAEI 2015 1 17 H MOKHTARIHASSANABAD 2015 V INTELLIGENTAUTONOMOUSSYSTEMS13PROCEEDINGS13THINTERNATIONALCONFERENCEIAS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES COUSINS 2010 12 14 S COUSINS 2010 12 14 S NAU 2003 379 404 D BARSALOU 1999 577 609 L OLIVEIRA 2014 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS LIM 2011 492 509 G CORADESCHI 2003 85 96 S ZAMAN 2013 S IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATION INTEGRATEDMODELBASEDDIAGNOSISREPAIRARCHITECTUREFORROSBASEDROBOTSYSTEMS TULVING 1991 3 32 E MEMORYORGANIZATIONLOCUSCHANGE CONCEPTSHUMANMEMORY TULVING 2005 4 56 E MISSINGLINKINCOGNITION EPISODICMEMORYAUTONOESISUNIQUELYHUMAN WOOD 2011 81 103 R SEABRALOPES 2007 53 81 L SEABRALOPES 2008 277 297 L KIRSTEIN 2012 90 105 S KRUGER 2011 740 757 N ALDOMA 2012 A CLAPES 2013 799 808 A RUSU 2009 3212 3217 R ROBOTICSAUTOMATION2009ICRA09IEEEINTERNATIONALCONFERENCE FASTPOINTFEATUREHISTOGRAMSFPFHFOR3DREGISTRATION SEABRALOPES 2001 10 14 L SAHIB 2013 S LIM 2014 153 160 G ROBOTHUMANINTERACTIVECOMMUNICATION2014ROMAN23RDIEEEINTERNATIONALSYMPOSIUM INTERACTIVETEACHINGEXPERIENCEEXTRACTIONFORLEARNINGABOUTOBJECTSROBOTACTIVITIES EVANS 2008 255 278 J COHENOR 1995 453 461 D BARBER 1996 469 483 C FISCHLER 1981 381 395 M RUSU 2009 R SEMANTIC3DOBJECTMAPSFOREVERYDAYMANIPULATIONINHUMANLIVINGENVIRONMENTS YI 2000 76 78 W JOHNSON 1999 433 449 A CONNOR 2010 599 608 M MUNARO 2013 53 68 M FRONTIERSINTELLIGENTAUTONOMOUSSYSTEMS ASOFTWAREARCHITECTUREFORRGBDPEOPLETRACKINGBASEDROSFRAMEWORKFORAMOBILEROBOT BENTLEY 1975 509 517 J CHAUHAN 2011 341 354 A OLIVEIRAX2016X614 OLIVEIRAX2016X614X626 OLIVEIRAX2016X614XM OLIVEIRAX2016X614X626XM 2017-12-02T00:00:00Z UnderEmbargo item S0921-8890(15)00214-6 S0921889015002146 10.1016/j.robot.2015.09.019 271599 2016-12-26T04:38:02.271089-05:00 2016-01-01 2016-01-31 true 1868217 MAIN 13 53302 849 656 IMAGE-WEB-PDF 1 fx1 12594 28 219 gr1 23670 164 191 gr2 22730 164 219 gr3 20164 142 219 gr4 35390 128 219 gr5 22858 164 135 gr6 25048 164 212 gr7 25354 164 214 gr8 20489 130 219 gr9 19009 87 219 pic1 32766 164 140 pic2 33052 164 140 pic3 29650 164 140 pic4 26625 164 140 pic5 33620 164 140 pic6 30488 164 140 fx1 37984 59 461 gr1 80010 404 472 gr2 59338 275 367 gr3 101946 453 697 gr4 71163 198 339 gr5 217247 903 744 gr6 79263 291 376 gr7 75798 288 376 gr8 110529 378 636 gr9 94838 251 629 pic1 40427 132 113 pic2 44745 132 113 pic3 38146 132 113 pic4 40704 132 113 pic5 42891 132 113 pic6 38759 132 113 fx1 99682 263 2041 gr1 377574 1791 2091 gr2 203931 1218 1625 gr3 571474 2005 3085 gr4 362193 876 1500 gr5 1221470 3998 3295 gr6 269699 1291 1667 gr7 275599 1277 1667 gr8 756745 1677 2818 gr9 562149 1110 2784 pic1 97072 584 500 pic2 111383 584 500 pic3 86166 584 500 pic4 98879 584 500 pic5 122229 584 500 pic6 90792 584 500 si1 60 4 15 si13 914 48 174 si14 227 15 50 si15 1101 53 201 si17 354 15 57 si18 1092 40 178 si19 211 14 25 si2 195 11 42 si20 724 19 149 si21 145 8 14 si22 148 11 19 si23 1862 32 394 si24 171 11 18 si25 124 8 10 si3 113 10 8 si4 124 8 11 si5 1056 62 192 si6 136 13 11 si7 169 12 35 si8 136 12 10 si9 159 11 17 ROBOT 2547 S0921-8890(15)00214-6 10.1016/j.robot.2015.09.019 Elsevier B.V. Fig. 1 A high-level overviews of the RACE architecture. Fig. 2 Abstract cognitive architectures for hybrid reactive–deliberative robots: (a) with a single memory system; (b) with a dual memory system. Fig. 3 Architecture of the developed object perception and perceptual learning system. Fig. 4 Visualization of tracking, pointing and labeling. Fig. 5 Processing time in the object perception pipelines in the video sequence, comparing nodes (left column) with nodelets (right column). Six objects appear in the video, corresponding to pipelines 1 through 6. (a) tracker nodes; (b) feature extraction nodes; (c) object recognition nodes; (d) tracker nodelets; (e) feature extraction nodelets; (f) object recognition nodelets. Fig. 6 Perceptual memory usage during the experiment, in logarithmic scale. The blue (upper) curve represents the total size of all point clouds of object views extracted by the trackers. The green (middle) curve represents the total accumulated size of all point clouds of key views. The red (bottom) curve represents the actual perceptual memory content (shape-based representations of key views). Fig. 7 Object recognition performance (precision, recall and F-measure) during the experiment. Each point in these curves is computed based on the object recognition results in the previous 20 s. Fig. 8 (a) Simulated teacher experiment no. 3; (b) protocol success versus the number of learned categories, for the same experiment. Fig. 9 System performance during simulated user experiments: (a) global success vs. number of learned categories, a measure of how well the system learns; (b) number of learned categories vs. number of question/correction iterations, represents how fast the system learned object categories. Table 1 PCL functionalities used in the RACE object perception and perceptual learning system. The modules listed in the third column are those represented in Fig. 3. PCL class [refs.]/parameters Usage in RACE RACE modules ConditionalRemoval Removing points outside a 3D box a Object DetectorObject Tracker VoxelGrid [30] voxel size=0.015 m Downsampling a point cloud b Object Detector Object Tracker ConvexHull [31,32] Convex hulls of tables c Tabletop Segmenter RandomSampleConsensus [33] RANSAC iterations=200 Table plane detection d Tabletop Segmenter ExtractPolygonalPrismData minimum z=0.01 mmaximum z=0.6 m Tabletop object detection e Object Detector EuclideanClusterExtraction [34] minimum cluster size=10maximum cluster size = 10,000 clustering step=0.08 m Object segmentation f Object Detector PCA [35] Estimating the orientation of objects g Object Tracker ParticleFilterTracker [36] max number of particles=200 Tracking tabletop objects using RGBD h Object Tracking SpinImageEstimation [37] support length=0.1 mimage width=8, support angle=90° Features for representing object views i Feature Extractor KdTree [38] K = 1 Computation of distance between views j Object Recognizer Object Conceptualizer a b c d e f g h i j Table 2 Sequence of events in the experiment (see video). Time (s) Event Description 25 T1 in A Mug (T1) is placed on the table 40 T1 is a Mug T1 is labeled as Mug 60 T2 in A Vase (T2) is placed on the table 75 T2 is a Vase T2 is labeled as a Vase 90 T3 in Another Mug (T3) is placed on the table 135 T3 out T3 is removed from the table 140 T1 out T1 is removed from the table 145 T2 out T2 is removed from the table 165 T4 in A Plate (T4) is placed on the table 170 T4 is a Plate T4 is labeled as a Plate 175 T5 in A Bottle (T5) is placed on the table 190 T5 is a Bottle T5 is labeled as a Bottle 210 T6 in A Spoon (T6) is placed on the table Table 3 Average object recognition performance (F1 measure) for different parameters: voxel size (VS), image with (IW), support length (SL) and classification threshold (CT). Table 4 Summary of simulated teacher experiments a . Exp# #Iterations #Categories #Instances GS APS 1 706 22 14.55 0.65 0.76 2 217 13 8.69 0.68 0.86 3 533 24 10.46 0.68 0.74 4 699 21 15.57 0.63 0.74 5 747 29 11.31 0.69 0.79 6 711 31 9.83 0.72 0.75 7 1041 36 12.06 0.70 0.77 8 252 13 10.08 0.64 0.87 9 412 19 10.37 0.68 0.81 10 393 20 8.9 0.70 0.84 a Exp#, experiment number; #Categories, number of categories learned; #Iterations, number of iterations in the experiment; #Instances, average number of instances per category at the end of the experiment; GS, global success; APS, average protocol success. 3D object perception and perceptual learning in the RACE project Miguel Oliveira a Luís Seabra Lopes a b ⁎ Gi Hyun Lim a S. Hamidreza Kasaei a Ana Maria Tomé a b Aneesh Chauhan c a IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro Portugal b Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro Portugal c Center of Automation and Robotics, Universidad Politécnica de Madrid, Spain Center of Automation and Robotics, Universidad Politécnica de Madrid Spain ⁎ Corresponding author at: IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal. IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro Portugal This paper describes a 3D object perception and perceptual learning system developed for a complex artificial cognitive agent working in a restaurant scenario. This system, developed within the scope of the European project RACE, integrates detection, tracking, learning and recognition of tabletop objects. Interaction capabilities were also developed to enable a human user to take the role of instructor and teach new object categories. Thus, the system learns in an incremental and open-ended way from user-mediated experiences. Based on the analysis of memory requirements for storing both semantic and perceptual data, a dual memory approach, comprising a semantic memory and a perceptual memory, was adopted. The perceptual memory is the central data structure of the described perception and learning system. The goal of this paper is twofold: on one hand, we provide a thorough description of the developed system, starting with motivations, cognitive considerations and architecture design, then providing details on the developed modules, and finally presenting a detailed evaluation of the system; on the other hand, we emphasize the crucial importance of the Point Cloud Library (PCL) for developing such system. 1 1 This paper is a revised and extended version of Oliveira et al. (2014). Keywords 3D object perception Point-Cloud Library Dual memory systems Open-ended learning Interactive learning 1 Introduction One of the primary challenges of service robotics is the adaptation of robots to new tasks in changing environments, where they interact with non-expert users. The European project RACE (Robustness by Autonomous Competence Enhancement [1,2]), recently closed, assumed that versatility and competence enhancement can be obtained by learning from experiences. The project focused on acquiring and conceptualizing experiences about objects [3], scene layouts [4] and activities [5] as a means to enhance robot competence over time thus achieving robustness. Stimuli for learning can be collected, either autonomously by robots, or when they receive appropriate feedback from users. The functional components of the RACE architecture are represented by boxes in Fig. 1 . Each component may contain one or more modules, which are implemented as nodes (or nodelets) over the Robot Operating System (ROS) [6,7]. The Reasoning and Interpretation component includes a temporal reasoner, a spatial reasoner and a description logics reasoner. Perception contains several modules for symbolic proprioception and exteroception, which generate occurrences. The Experience Management and Conceptualization component pre-processes occurrences, extracts relevant experiences, uses them to create new concepts and stores these in the Memory component. The User Interface component receives instructions from the user and relays them to the Planning component. Planning is carried out using SHOP2, a Hierarchical Task Network planner [8]. The produced plans are executed by the Plan Execution Management component. One of the challenges in this type of projects is to ground [9] the semantic representations maintained by the robot, namely the model of the world state and the learned concepts, into the perception and action capabilities of the robot itself [10,11]. At least two types of grounding are involved here. For internal symbols that refer to real-world objects (e.g. “mug23”), the robot must maintain a perception-mediated mapping of symbols to objects. This is often called anchoring [12] and relies to some extent on (visual) tracking capabilities at the perception level and on semantic interpretation capabilities at the reasoning level. For category symbols (e.g. “Mug”), the robot must ground their meanings on concrete observations of instances of the categories. A related challenge is how to combine semantic (i.e. symbolic, relational, logic-based) with perceptual (numeric, pattern-based) representations, and how to store different types of representations. After analyzing the requirements of the different components of the RACE architecture, a key decision was made by the project: instead of a single memory system, two independent memory systems would be used, one for semantic information (the Semantic Memory) and the other for perceptual information (the Perceptual Memory) [2]. Through perceptual learning capabilities, the developed object perception system can be applied to open-ended environments. In this case, “open-ended” means that the robot does not know in advance which object categories it will have to learn, which observations will be available, and when they will be available to support this learning. This kind of perception system must comprise a significant number of software modules, which must be closely coupled in their structure and functionality [13]. Three main design options address the key computational issues involved in processing and storing perception data. First, a lightweight, NoSQL database, is used to implement the perceptual memory. Second, a thread-based approach with zero copy transport of messages is used in implementing the modules. Finally, a multiplexing scheme, for the processing of the different objects in the scene, enables parallelization. This way, the system is capable of real time object detection, tracking and recognition. The developed perception and perceptual learning capabilities target objects in table-top scenes, e.g. in a restaurant environment. These capabilities are fully integrated in the RACE architecture and are running on the PR2 robot used by the project. This work heavily relies on Point Cloud Library (PCL) functionalities, as will be detailed in Section 4. Because the developed perception system is a complex network of processing nodes, the whole paper is organized in such a way that the organization of the system and the module functionalities are well justified and presented in detail. However, since PCL is used in nearly every module of the system, the system could not have been developed easily without PCL. Therefore, this work shows the current importance of PCL in building sophisticated 3D perception systems in robotics and other domains. The remaining part of this paper is organized as follows. In the next section, related works are discussed. Memory and cognitive architecture issues are discussed in Section 3, leading to the choice of a dual memory approach and to the development of a perceptual memory system. The RACE object perception system and the perceptual learning approach are described in Sections 4 and 5. Profiling and evaluation of the developed system is the topic of Section 6. Finally, in Section 7, the conclusion is presented and future research is discussed. 2 Related work As robots are expected to increasingly interact and collaborate closely with humans, robotics researchers need to look at human cognition as a source of inspiration. Learning is closely related to memory in human cognition. In the cognitive science literature, the existence of multiple memory systems is widely accepted [14,15]. Biological findings about memory and learning have served as inspiration for the development of computational models and applications. Wood et al. [16] present a thorough review and discussion on memory systems in animals as well as artificial agents, having in mind further developments in artificial intelligence and cognitive science. In [17], an open-ended object category learning system, based on one-class learning and human–robot interaction, is described. The authors also proposed a teaching protocol for performance evaluation in open-ended learning. In [18], a multi-classifier system with similar goals is described in which a meta-learning component monitors classifier performance, reconfigures classifier combinations and chooses the classifier to be used for prediction. These works are based on 2D images collected in static scenes. Since there is no continuous stream of data being stored, memory requirements are easily satisfied. Kirstein et al. [19] proposed a lifelong approach for interactive learning of multiple categories from 2D perception, in this case based on vector quantization. This involves selecting the most crucial features from a series of high dimensional feature vectors that almost exclusively belong to each specific category. However, they still follow a standard train-and-test procedure, which is not plausible in open-ended scenarios. Moreover, the authors did not provide details on their memory system or computational architecture. The work is also not integrated in a hybrid perceptual/semantic processing system. Kruger et al. [20] use the so-called “object–action complexes” (OAC) to bind objects, actions and attributes associated with an agent in a causal way. These OACs are learnable/refinable semantic representations. To ground OACs, an agent requires an object perception and learning system, such as the one we propose below. Heintz et al. [21] propose a hierarchical framework designed to anchor symbols to continuous streams of sensor data. The approach dynamically constructs and maintains data association hypotheses at multiple levels. A traffic monitoring application is used to illustrate the system. This work proposes a general approach to the anchoring problem, but does not address object perception and learning. Willow Garage developed the Object Recognition Kitchen (ORK), 2 2 a 3D object recognition system built on top of the Ecto framework. 3 3 Ecto organizes computation as a directed acyclic graph, which implies important limitations in the architecture of the perception system. Moreover, in ORK, training/learning and detection/recognition are two separate stages. Such approach is not suitable for developing open-ended learning agents. In contrast, our system allows for concurrent or interleaved learning and recognition, and real-time performance is achieved through nodelets and multiplexing. Although the Point-Cloud Library (PCL) is increasingly popular, we do not know of other systems building upon PCL to integrate 3D object perception, memory, learning, recognition and interaction. Aldoma et al. [22] reviewed several state-of-the-art 3D shape descriptors from PCL to develop 3D object recognition and pose estimation capabilities. Throughout the paper, the properties, advantages and disadvantages of different local and global shape descriptors are considered. They also proposed two pipelines for object recognition based on PCL. In the first pipeline, an object view is described by a set of local shape features, which are computed around keypoints. Afterward, each feature is compared against all the features of all models in a database using Euclidean distance. The second pipeline is based on global descriptors, i.e. high-dimensional representations usually calculated for object candidates (subsets of the scene’s point cloud obtained through segmentation). Clapés et al. [23] proposed an automatic surveillance system for user identification and object recognition. In this work, the position of the RGB-D camera is fixed and the authors employed a background subtraction strategy to segment users and objects in the scene. In the case of object detection, the remaining connected components (those not previously selected as being part of the user) are considered as object candidates. During the recognition stage, Fast Point Feature Histogram (FPFH) [24] features are computed for each detected object view and matched against the training models. There is no learning process involved. 3 A dual memory approach Arguably, robots that interact closely with non-expert users should be [25]: animate, meaning that they react appropriately to different events, based on a tight coupling of perception and action; adaptive, to cope with changing users, tasks and environments, which requires reasoning and learning capabilities; and accessible, that is, they should be easy to command and instruct, and they should also be able to explain their beliefs, motivations and intentions. In an abstract architecture for intelligent robots, as shown in Fig. 2 (a), a Perception component processes all momentary information coming from sensors, including sensors that capture the actions and utterances of the user. A Reasoning component updates the world model and determines plans to achieve goals. An Action component reactively dispatches and monitors the execution of actions, taking into account the current plans and goals. Action processing ranges from low-level control to high-level execution management. Finally, a Learning component, which typically runs in the background, analyzes the trace of foreground activities recorded in a Memory component and extracts and conceptualizes possibly interesting experiences. The resulting conceptualizations are stored back in memory. Each component in such abstract architecture decomposes into a set of software modules, possibly distributed across multiple computers. The reasoning component manipulates primarily semantic representations of the current world state, goals and plans, that is, representations that are symbolic and relational in nature. In RACE, where case studies were carried out in a restaurant environment, semantic representations describe tables, chairs, table-top objects, guests, the robot, etc., the categories of these objects, the relations between them, and the actions and events that change these relations. The semantic information flowing between reasoning, execution management and memory is typically of small size, and its processing tends to be slow. One of the challenges in a project like RACE was to combine and store semantic and perceptual representations. Standard SQL databases do not cope well neither with semantic data nor with perception data, as both tend to be partially unstructured and/or of variable size. This suggests that modern NoSQL databases [26] should be used. Semantic data represents the world in terms of instances, categories and relations between them. A semantic representation of the state of the world can be simply a set of subject–predicate–object triples. A special kind of database, the triplestore, which shares some features with both SQL and graph databases, is especially optimized to store information in the form of a set of triples. Triplestores are clearly one of the database types to take into account when developing memory systems for robots. An RDF triplestore was in fact the choice for the initial memory component in the RACE architecture [1]. The contents of this memory system, which is used as blackboard for all processes, is semantic in nature. It keeps track of the evolution of both the internal state of the robot and the events observed in the environment. Access to the triplestore is granted via a ROS node that provides database query and write services for all other nodes (an interface node). Information exchange is performed using either publisher/subscriber or client/server mechanisms. ROS communications are a robust framework [13]. However, when the size of the messages is large (e.g., when passing 3D point clouds), the communication between processes is slow. In the case of perception related data, its large size implies large ROS messages to be passed between the database interface node and the other nodes. This is a major constraint, especially considering that, unlike semantic data, perceptual data flows continuously at the sensor output frequency. Using a database interface node creates a bottleneck for accessing the database, since it handles access requests in a first in, first out basis. Moreover, although triplestores are well suited for storing semantic information, they can hardly be considered suited for storing perception data. In fact, the perception modules will primarily process numeric information organized in structures like vectors and matrices, possibly grouped in sets. For instance the raw perception data about an object, after detection, can be a 3D point cloud, which is a set of points described by their 3D coordinates and possibly RGB information. Based on the point cloud, shape features can be extracted, and the object can be represented by a set of local shape features, where each of them can be a 2D shape histogram. To ensure timely reaction to events in the environment, perception modules run continuously at the frame rate of the used sensors. Although raw data tends to be massive (high-dimensional), the perception modules must run fast, and whatever memory support they use, must also be lightweight. In the context of RACE, to accommodate semantic and perceptual information in the same database, the only option would be to replace the triplestore with a more generic kind of database. However, we would loose the special features of triplestores, which are optimized for storing triples. In alternative, two different databases can be used, one for semantic information, and the other for perceptual information. The second alternative, which seems more promising, allows to use databases that are well suited for the kinds of data that each will store. In RACE, we converged to the second option. Fig. 2(b) shows an abstract architecture diagram in which we make explicit the dual memory approach. In what concerns reasoning, we make explicit both interpretation and planning capabilities. One of the most basic interpretation capabilities is anchoring, i.e. connecting object symbols used in the semantic memory to the perception of those objects that is recorded in the perceptual memory. Interpretation also includes computing spatial relations between objects to keep an updated relational model of the scene around the robot. In turn, this scene model can be taken into account for anchoring. The perceptual memory contains, not only object perception data, but also object category knowledge, in the form of perceptual categories that enable to recognize instances of those categories. These perceptual categories are learned in an open-ended fashion with user mediation [27]. The perceptual learning component primarily uses data from perceptual memory (e.g. shape features of objects) as well as from the semantic memory (e.g. teaching instructions from the user). In RACE, the implementation of the perceptual memory was carried out using a flexible and scalable NoSQL database which operates in memory (see the next section for details). It is worth emphasizing that, although our design choices were guided primarily by engineering criteria, we converged to a solution that is biologically and cognitively plausible. In fact, as previously pointed out, human memory is not a single monolithic system, but rather a combination of several memory subsystems specialized for storing different types of information and supporting different functionalities [14,15]. In particular, our perceptual memory resembles the so-called Perceptual Representation Memory System, used in human cognition for enhancing the identification of objects as structured physical–perceptual entities, a process referred to as perceptual priming [14]. Another key distinction in cognitive science is between processes that are fast, automatic and unconscious, and processes that are slow, deliberative and conscious [28]. Our dual semantic/perceptual memory approach is also in line with these findings. 4 The RACE object perception system The work presented in this paper was developed as an extension to the initial RACE architecture [1] (see also Fig. 1). In particular, the work focused on extending the perceptual capabilities of the system. The perception system developed around the perceptual memory supports the anchoring of object symbols into perceived object data as well as the grounding of category symbols into perceptual categories. Since the initial integration of the object perception system, the RACE system included basic capabilities for object symbol anchoring, allowing perceived objects to be represented not only in the perceptual memory, but also in the semantic memory. The object perception system targets table-top scenes in a restaurant scenario. This system became a salient portion of the full RACE system [2]. 4.1 Architecture The developed perception system is composed of six functional components: Object Detection, Multiplexed Object Perception, User Interface, Reasoning and Interpretation, Memory and Conceptualization. These are represented by the dashed rectangles in Fig. 3 . Functional components in Fig. 3 correspond to those highlighted in bold in the high-level RACE architecture (Fig. 1) and to the perception, interpretation, memory and perceptual learning components in Fig. 2(b). In turn, each functional component contains one or more software modules (solid line rectangles in Fig. 3). Arrows signal the exchange of information between software modules. Each software module is organized into a ROS package and will typically correspond to a node or a nodelet 4 4 at runtime. The implementation of the perceptual memory was carried out using LevelDB, a lightweight, flexible and scalable NoSQL database developed by Google. 5 5 LevelDB is a key–value storage database that provides an ordered mapping from string keys to string values. In addition, LevelDB operates in memory and is copied to the file system asynchronously. This significantly improves its access speed. 4.2 The PCL foundation of this work This work heavily relies on PCL [29] 6 6 functionalities. Table 1 lists the PCL classes used by the object perception and perceptual learning system along with values we typically use for the main configuration parameters. The reason why they are used as well as the modules in which they are used are also given. It can be seen from Table 1 that several PCL functionalities are used by our system. For point cloud size reduction, we use both conditional removal filter as well as voxel grid filters. Table planes are detected using RANSAC, and their boundaries extracted by estimating the 2D convex hull. Points belonging to tabletop objects are extracted using the polygonal prism extraction method. Tabletop objects are segmented using Euclidean cluster extraction. The pose of newly detected tabletop objects is estimated using PCA, which is useful to define well oriented bounding boxes. Objects are tracked over time using the particle filter tracker. Object views are represented by spin image feature descriptors, and object recognition uses an optimized view-to-view distance calculation based on K-d trees. Since PCL is used in nearly every module of the system, the system could not have been developed easily without PCL. This work shows the current importance of PCL in building sophisticated 3D perception systems in robotics and other domains. 4.3 Object perception An RGB-D sensor is used for the perception of both the user and the table-top scene. The starting point for the perception of the table-top scene is the Table-Top Segmenter (TTS) module, which uses ROS 7 7 and PCL functionalities to isolate (partial) point clouds of the objects placed on the table (see Table 1) [29,39]. The Object Detector (OD) module periodically requests the current list of objects from TTS. Then, OD will check if any of those objects is already being tracked. To do this, OD matches the point clouds of all objects on the table with the estimated bounding boxes of all objects currently being tracked. The percentages of points of the tabletop objects that lie inside the bounding boxes of the tracked objects are computed. A large percentage indicates that the tracked object and the segmented object are the same. Point clouds that cannot be matched with any of the tracked bounding boxes are assumed to represent new objects just added to the scene. OD will assign a new identifier (track_id) to each newly detected object. Also for each new object, OD will launch an object perception pipeline which contains three modules: Object Tracking, Feature Extraction and Object Recognition. Fig. 4 shows a situation where two objects are segmented and tracked, i.e., they have bounding boxes around them. Object Tracking (OT) is responsible for keeping track of the target object over time while it remains visible. Tracking is an essential base for anchoring. On initialization, OT receives the point cloud of the detected object and computes a bounding box for that point cloud, the center of which defines the pose of the object. A particle filter tracking approach from PCL (see Table 1) is then used to predict the next probable pose of the object. In each cycle, OT sends out the tracked pose of the object both to OD (as mentioned above) and to the Interpretation component. At a lower rate, OT sends the point cloud of the object (i.e. containing the points inside the predicted bounding box) to Feature Extraction. As expected, the system is sensitive to the speed with which objects move. If an object moves very fast, tracking is lost, then a new object is detected, and a new object perception pipeline is initiated. Nonetheless, our experiments have shown that the system is able to cope with users picking up the objects and moving them around in natural movements with typical speeds. The Feature Extraction (FE) module computes and stores object representations in the perceptual memory. Objects are represented by sets of local shape features computed in certain keypoints. For efficiency reasons, the number of keypoints should be much smaller than the total number of points. To select keypoints, a voxelized grid approach from PCL is used (see Table 1). We select, for each voxel, the point that is closest to the voxel center [3]. Thus, there will be one keypoint per voxel. The surrounding shape in each keypoint is described by a spin-image [37]. Spin-images are pose invariant, and therefore a suitable local shape descriptor for 3D perception in service robots. They are computed by projecting the 3D surface points of the object to the keypoint’s tangent plane We use an implementation of spin-image estimation available from PCL (see Table 1). In addition to storing object representations in the perceptual memory, FE also sends them to Object Recognition (OR). The perceptual categories learned so far and stored in the perceptual memory are used by OR to predict the category of the target object. OR is a low frequency module, which runs at 1 Hz. Accordingly, FE receives object point clouds from OT and sends the extracted representations for recognition at the same frequency. Thus, only OT itself uses object point clouds at the frame rate of the sensor (30 Hz). For better representing an object, it is important to store different views, which is possible when the object is moved (and thus its pose relative to the sensor changes). In contrast, storing all object representations computed by FE while the object is static would lead to unnecessary accumulation of highly redundant data. On a different line, it is important to minimize noise effects possibly affecting object views. Thus, to optimize memory usage while keeping potentially relevant and distinctive information, a heuristic is used to select key views, that is, object views that should be stored. Whenever the tracking of an object is initialized, or when it becomes static again after being moved, three consecutive object views are stored, provided that the hands of the user are not detected near the object. In case the hands are detected near the object, storing key views is postponed until the hands are withdrawn. OT is responsible for marking object views as key views. Then, when FE receives a point cloud marked as key view, it will store the respective representation in the perceptual memory. Object recognition results are also written to the perceptual memory, where the Interpretation component can fetch them to support symbol anchoring. The current implementation is capable of anchoring symbols that refer to objects only while these remain visible. Further work is ongoing to enable anchoring object symbols when the visual tracking is lost. 4.4 User perception The perceptual memory supports, not only the anchoring of object symbols into perceived object data, but also the grounding of category symbols into perceptual categories. Perceptual categories are acquired with user mediation, that is, the user points to objects and provides their category names. Verbal input is provided through interactive markers in RVIZ, a 3D visualization tool for ROS. Point gesture recognition is based on tracking the skeleton of the user. The Skeleton Tracker (ST) module is the one available in OpenNI. 8 8 It tracks the user skeleton pose over time based on RGB-D data. The skeleton pose information is passed to the Gesture Recognizer (GR) module, which computes a pointing direction. Currently, the pointing direction is assumed to be the direction of the right forearm (see an example in Fig. 4). The pointing direction is then passed to the Interpretation component. Upon receiving verbal input, the Interpretation component checks if the received pointing direction intersects the bounding box of any of the objects currently on the table according to the world state recorded in the semantic memory. If that is the case, then a teaching instruction is recorded in the semantic memory, stating that the target object was taught to belong to the given category. Teaching instructions trigger perceptual learning to create and/or update object categories. 4.5 Addressing computational issues In contrast with the reasoning processes supported by the semantic memory, the processes developed around the perceptual memory must run fast to cope with the continuous stream of massive sensor data. As pointed out, one of the reasons for using LevelDB to implement the perceptual memory is the fact that it operates in RAM. There is, however, the limitation that simultaneous access to LevelDB is only possible by threads within the same process. To comply with this constraint while keeping ROS as the framework for the newly developed modules, we use ROS nodelets. 9 9 Nodelets, which run as threads of a single process, were designed to provide a way of concurrently running different modules with zero copy transport between publisher and subscriber calls (as an example, see [40]). The motivation for ROS nodelets comes from systems with high throughput data flows as is common in perception systems. It is not surprising, therefore, that the developers of PCL and ROS nodelets are the same. In our system, in addition to handling high throughput data flows, nodelets come handy to implement modules that need to simultaneously access the perceptual memory (LevelDB). Another way of optimizing perception is to parallelize computations. In our system, instead of tracking all objects in a single tracking module, there is a tracker for each object. Similar strategy is used for feature extraction and object recognition. In other words, object perception is designed to be multiplexed. Every time a new object is detected, a corresponding instance of the object perception pipeline (see Fig. 3) is launched. Thus there are as many object perception pipelines as the number of currently tracked objects, and each pipeline targets a specific object. Since the modules in an object perception pipeline run as independent nodes/nodelets, they can be distributed to different CPU cores, thus improving the overall computational efficiency of perception. Note that the three modules in the object perception pipeline are traditionally amongst the heaviest in terms of computational requirements. The parallelization is aimed at the hotspot or bottleneck of the computation flow and takes full advantage of modern multi-core machines. In fact, experiments with a non-multiplexed version of this architecture show that it cannot run in real-time. We can easily configure the perception and perceptual learning modules to be launched with different runtime configurations, that is, using ROS nodelets only, ROS nodes only, or a combination of both. By default, the object perception pipelines, the perceptual learning module and the perceptual memory run as a set of nodelets of a single process. When debugging is necessary, we use a configuration where all modules run as nodes. In this configuration, the modules access the perceptual memory (LevelDB) using ROS services provided by a database interface node. 5 Perceptual learning Although other kinds of perceptual categories could be considered, perceptual learning currently focuses on object categories. We approach object category learning from a long-term perspective and with emphasis on open-endedness, i.e. not assuming a pre-defined set of categories [17,18]. For example, when learning how to serve a coffee [1], if the robot does not know how a mug looks like, it may ask the user to point to one. Such situation provides an opportunity to collect an experience for learning. Concerning category formation, a purely memory-based learning approach is adopted, in which a category is represented by a set of views of instances of the category. The recording of a teaching instruction in the semantic memory triggers the perceptual learning component. If a new category was taught, the key views of the target object stored by the feature extraction module (see above Section 4.3) are used to initialize the category. If the category is previously known, the key views are added to the existing category representation only if the agent cannot correctly recognize the category of the target object. The current approach differs from our recent previous work [3,10], in the distance measures used for classification, as will be pointed out. In order to estimate the dissimilarity between a target object view, t , and an object view, o , contained in a category model in the perceptual memory, the following distance function is used: (1) D ( t , o ) = ∑ l = 1 q min k d ( t l , o k ) q , where t l , l = 1 , …, q , are the spin-images of the target object, o k represents the spin-images of the model object, and q is the number of target object’s spin-images (see Section 4.3 and [37] on spin-images). Since a linear search in high-dimensional spaces has a high computational cost, which is not suitable in the case of an autonomous service robot, a fast approximate nearest neighbor search based on k-d trees [41] from PCL (see Table 1) is used instead of a traditional nearest neighbor search. The next step is to compute the object–category distance between the target object, t , and a certain category, C, as the average distance of the instances of C to t : (2) O C D ( t , C ) = ∑ u ∈ C D ( t , u ) n , where n = | C | is the total number of category instances. In object recognition, instances are more spread in some categories than in others. Normalizing distances will help to prevent misclassification. Distance normalization is based on the following intra-category distance: (3) I C D ( C ) = ∑ u ∈ C ∑ v ∈ C , v ≠ u D ( u , v ) n . ( n − 1 ) . The normalized distance of target object, t , to the category, C, N D ( t , C ) , is computed as follows: (4) N D ( t , C ) = 2 × O C D ( t , C ) I C D ( C ) + I C D ¯ where I C D ¯ is the average of the intra-category distances of all categories, i.e. I C D ¯ = ∑ i = 1 m I C D ( C i ) / m , and m is the number of categories. Finally, the target object is classified based on the minimum normalized distance of the known categories to the object. If, for all categories, the normalized distance is larger than a given Classification Threshold, C T , then the object is predicted to belong to an unknown category 10 10 By default, CT=2.0 is used. See Section 6.2 for an evaluation of alternatives. : (5) C a t e g o r y ( t ) = { unknown , if m i n C N D ( t , C ) > C T a r g m i n C N D ( t , C ) , otherwise . In open-ended environments where some objects may belong to not yet known categories, recognizing that an object belongs to an unknown category is important. It may prevent the agent from making further decisions based on wrong assumptions. On the other hand, detecting that an object belongs to an unknown category may be used by the agent to trigger some exploration or interaction process leading to the acquisition of a new category. In our recent previous work [3,10], the object–category distance was measured as the minimum distance between the target object and known instances of a given category. This measure was then normalized by an intra-category distance. If the normalized measure was larger than a given classification threshold, the target object could not belong to that category. The combination of minimum distance with normalization and threshold led to bad decisions in some limit situations. The new formulation here presented solves these problems. In addition, by taking into account the average intra-category distance, the current normalization is also more stable than the previously used normalization method. 6 Profiling and evaluation This section presents three sets of results. First, based on a session where users manipulate objects on a table and interact with the developed perception and perceptual learning system, we carry out a profiling analysis of the main modules of the system. Second, we present an off-line evaluation of the perceptual learning approach under different configurations of the system. Finally, we report on open-ended learning experiments carried out on a public domain dataset. 6.1 Profiling For profiling the modules, two human users interacted with the system in a short session of nearly 4 min (Table 2 ). All raw data from the RGB-D sensor as well as verbal input from the users during the session was recorded in a rosbag, which was then used to test different configurations of our system. Note that, when the system starts, the set of categories known to the system is empty. There is a video 11 11 The video can be found at that illustrates the behavior of the main modules of the system, from user/object tracking to learning and recognition. As discussed in Section 4.5, nodelets can significantly improve efficiency since they support zero copy transport and they enable simultaneous access to LevelDB. Fig. 5 compares the processing time of the object perception modules. The tracker modules (Fig. 5(a) nodes and (d) nodelets) tend to display a stable processing time shortly after their initialization. This is explained by the fact that the size of the input data is more or less stable over time. In this case, nodelets are more efficient when compared to nodes: for example for pipelines 1–3 in the 100–150 time interval, nodes display an average processing time of 45 ms, compared to 25 ms in the case of nodelets. Since the trackers do not access the database, the main factor contributing to the increase in efficiency is the zero copy transport. The messages that are received (sensor point cloud) and sent (partial object point cloud) by the trackers are of large size, which explains why zero copy transport enables such a significant improvement. The feature extraction modules (Fig. 5(b) nodes and (e) nodelets) show a different behavior. These modules periodically compute the spin-image representation from the partial object point cloud. At some points, the point cloud is signaled to belong to a key view, which will trigger the writing of that representation to the perceptual memory. The curves show these points in time with a rapid increase in processing time. Nodelets also display these peaks, but because access to the database is much faster, the peaks are smaller, as is the average processing time. The object recognition modules (Fig. 5 (c) nodes and (f) nodelets) receive a representation of the current object view from the feature extraction, and compare it against the representations of all known category views. Thus, they are continuously reading the database in the search for an update to the known categories. As a result, the larger the size of the database, the slower the reading of the complete set of categories. However, in the case of nodelets, this deterioration is minor when compared with nodes, since accessing the database is much more efficient. Fig. 6 shows the memory usage of the system. Notice that at the end of the experiment the memory size would be above 1 MB if all object point clouds extracted by the trackers would be stored (roughly 5 Kb/s). In a continuously running system, this rate of data accumulation would be hard to handle, and would not bring any real benefit. The total size of the point clouds of all the selected key views is much smaller (one order of magnitude in this experiment). The data actually accumulated in memory (shape representations based on spin-images) is even smaller. Fig. 7 shows the evolution of object recognition performance throughout the experiment. When the first Mug (T1) is placed on the table the system recognizes it as Unknown. After some time, the instructor labels T1 as a Mug and the system starts displaying a precision of 1.0. However, the recall score is under 0.2, because the system classified T1 as Unknown several times before the user labeled the object. After the labeling, the recall starts improving continuously. The instructor then places a Vase (T2) on the table. Because the category Vase has not been taught yet, the performance goes down. After labeling T2 as Vase, performance starts going up again. When a second Mug (T3) enters the scene, the system can correctly recognize it and the scores continue to increase. Then, a Plate (T4) enters the scene, causing recall to drop. Successively, the Plate is taught, a Bottle is placed on the table and then taught, and eventually performance starts going up again. This illustrates the process of acquiring categories in an open-ended fashion with user mediation. 6.2 Off-line evaluation of the perceptual learning approach More systematic experiments have been performed to evaluate the object category learning and recognition approach. An object dataset has been acquired, which contains 339 views of 10 categories of objects: bottle, bowl, flask, fork, knife, mug, plate, spoon, teapot and vase. In addition, there are 31 views of unknown and false objects. The point clouds of the objects were segmented using an offline version of the Object Detection (OD) module. Detected objects were manually labeled. The performance of the system was measured using a leave-one-out cross validation scheme. A total of 24 experiments were performed for different values of four parameters of the system, namely the voxel size (VS) which is related to number of key points extracted from each object view, the image width (IW) and support length (SL) of spin images, and the classification threshold (CT). Results are presented in Table 3 . The parameters that obtained the best average F1 score were selected as the default system parameters. They are the following: VS=0.03, IW=8, SL=0.1 and CT=2. The F-measure of the proposed system with the default parameters was 0.94. Results show that the overall performance of the recognition system is promising. Spin images are capable of collecting distinctive traits of the local surface patches of each object. 6.3 Open-ended learning evaluation Although there are well established methodologies to evaluate learning systems, e.g., k-fold cross validation, leave-one-out, etc., these approaches follow the classical train-and-test procedure, i.e., two separate stages, training followed by testing. Training is accomplished offline, and once it is complete the testing is performed. These methodologies are not well suited to evaluate open-ended learning systems, because they do not abide to the simultaneous nature of learning and recognition and because the number of categories must be predefined. A teaching protocol for evaluating open-ended learning systems was proposed in [17,42]. The protocol relies on three basic actions to interact with an object recognition system: teach, used for teaching a new object category; ask, used to ask the system what is the category of an object view; and correct, used for providing the system with an additional (labeled) view of an existing category. The idea is to continuously ask the system to recognize previously unseen views of known categories and provide corrections when needed. This way, the system is trained, and at the same time the recognition performance of the system is continuously estimated. A simulated teacher was developed to automate experiments following the teaching protocol [3]. To operate, the simulated teacher must be connected to a data-set of object views. In this work, we use the Washington RGB-D Object Dataset [43]. This dataset contains images of 300 common household objects from 51 categories. In the experiments presented, the system begins without category knowledge, i.e., it knows zero categories at start, and the training instances are gradually given to the system. Thus, object category models are incrementally built. Although the evaluation protocol is designed to test the system only using views of known categories, the system is designed to identify unknown categories when the views to be recognized are too far from all models in memory. Therefore, we use F-measure, which combines precision and recall, as the indicator of recognition performance. Protocol success is a local F-measure, computed in a sliding window of size 3 n (as defined in [42]), where n is the number of categories that have already been introduced. According to the teaching protocol, the system is ready to learn a new object category when the protocol success is above a threshold (0.67 in all experiments presented), and at least one instance of every known category has been tested. Simulated teacher experiments can be used to evaluate the performance of open-ended learning systems using several measures, namely: • The number of learned categories at the end of an experiment, an indicator of how much the system is capable of learning; • The number of question/correction iterations required to learn those categories and the average number of stored instances per category, indicators of time and memory resources required for learning; • Global success, an F-measure computed using all predictions in a complete experiment, and the (local) protocol success defined above, indicators of how well the system learns. Since the order of introducing new categories may affect the performance of the system, ten experiments were performed using random sequences of introduction of categories. Table 4 summarizes the 10 experiments. Fig. 8 (a) shows the evolution of the teaching protocol success in experiment 3. The introduced categories are signaled in the plot. Fig. 8(b) shows the protocol success as a function of the number of learned categories. Fig. 9 (a) shows the global success (i.e. since the beginning of the experiment) as a function of the number of learned categories. In this figure we can see that the global success decreases as more categories are learned. This is expected since the number of categories known by the system makes the classification task more difficult. Finally, Fig. 9(b) shows the number of learned categories as a function of the number of protocol iterations. This gives a measure of how fast the learning occurred in each of the experiments. 7 Conclusions This paper describes the 3D object perception and learning system developed for the RACE project. The system is designed to detect and track tabletop objects. It is also capable of classifying the tracked objects according to the object categories that are known by the system. Furthermore, this object category knowledge may be enhanced in real time through an RVIZ interface that enables humans to teach additional object categories. The paper describes a dual memory approach in which two databases with different characteristics are used to store semantic and perceptual data. The proposed nodelet software architecture is designed for efficiency purposes, since it enables actual simultaneous access to the perceptual database. Results show that the nodelet based approach is significantly faster when compared to the standard node approach. In addition, we also propose a multiplexed object perception pipeline, in which a set of nodes is launched during execution to handle each newly detected object. This mechanism allocates separate computational resources for each tracked object, which creates a dynamic computation graph in run time, and facilitates the parallelization of processing. As a consequence, the system is capable of simultaneously tracking several objects moving in the scene. The RACE object perception and learning system contains a large number of nodes that process 3D data for different purposes, e.g., segmentation, feature extraction, tracking, etc. Thus, it also serves as a good example of how PCL functionalities can be used to create an efficient and complex 3D perception system. This paper presented a perceptual memory system designed to enable grounding of object symbols and object category symbols in an open ended fashion. This system is integrated in a dual memory architecture which also includes a semantic memory component. The dual memory approach contributes to the optimization of both the semantic and perceptual components, and is in line with findings in cognitive science regarding the human memory system. The perceptual memory implementation was carried out using a lightweight, NoSQL database which, when combined with a nodelet based infrastructure, allows the simultaneous access of several modules to the storage. The system also supports runtime multiplexing of the object perception pipelines, which leads to the parallelization of the bottlenecks in the data processing. Results show that the perceptual memory combined with a nodelet infrastructure significantly outperforms a node based approach. The presented architecture can seamlessly integrate user-mediated experience acquisition, conceptualization and recognition. The system is open-ended since it can continuously acquire new object categories. Because the system is open-ended and receives a continuous stream of data, ongoing work is using data stream clustering methods to update a dictionary of local features [44]. Acknowledgments This work was funded by the EC 7th FP theme FP7-ICT-2011-7, Grant agreement No. 287752 (project RACE — Robustness by Autonomous Competence Enhancement). We would like to thank the other RACE project partners for their efforts in the integration and the demonstrations, and especially to the Technical Aspects of Multimodal Systems (TAMS) group, University of Hamburg, for making the PR2 robot available to the project. References [1] S. Rockel, B. Neumann, J. Zhang, K.S.R. Dubba, A.G. Cohn, S˘. Konec˘ný, M. Mansouri, F. Pecora, A. Saffiotti, M. Günther, S. Stock, J. Hertzberg, A.M. Tomé, A.J. Pinho, L. Seabra Lopes, S. von Riegen, L. Hotz, An ontology-based multi-level robot architecture for learning from experiences, in: Designing Intelligent Robots: Reintegrating AI II, AAAI Spring Symposium on, Stanford, USA, 2013. [2] J. Hertzberg J. Zhang L. Zhang S. Rockel B. Neumann J. Lehmann K.S.R. Dubba A.G. Cohn A. Saffiotti F. Pecora M. Mansouri Š Konec˘ný M. Günther S. Stock L. Seabra Lopes M. Oliveira G.H. Lim H. Kasaei V. Mokhtari L. Hotz W. Bohlken The RACE project KI—Künstliche Intelligenz 28 4 2014 297 304 [3] H. Kasaei M. Oliveira G.H. Lim L. Seabra Lopes A.M. Tomé Interactive open-ended learning for 3D object recognition: An approach and experiments J. Intell. Robot. Syst. 2015 1 17 [4] K. Dubba, M. de Oliveira, G. Lim, H. Kasaei, L. Seabra Lopes, A. Tomé, A. Cohn, Grounding language in perception for scene conceptualization in autonomous robots, in: AAAI 2014 Spring Symposium on Qualitative Representations for Robots, 2014. [5] V. Mokhtari~Hassanabad G.H. Lim L. Seabra~Lopes A.J. Pinho Gathering and conceptualizing plan-based robot activity experiences E. Menegatti N. Michael K. Berns H. Yamaguchi Intelligent Autonomous Systems 13: Proceedings of the 13th International Conference IAS-13 Advances in Intelligent Systems and Computing vol. 302 2015 Springer [6] S. Cousins B. Gerkey K. Conley W. Garage Welcome to ROS topics IEEE Robot. Autom. Mag. 17 1 2010 12 14 [7] S. Cousins B. Gerkey K. Conley W. Garage Sharing software with ROS IEEE Robot. Autom. Mag. 17 2 2010 12 14 [8] D.S. Nau T.-C. Au O. Ilghami U. Kuter J.W. Murdock D. Wu F. Yaman Shop2: An htn planning system J. Artificial Intelligence Res. 20 2003 379 404 [9] L. Barsalou Perceptual symbol systems Behav. Brain Sci. 22 4 1999 577 609 [10] M. Oliveira G.H. Lim L. Seabra Lopes H. Kasaei A. Tome A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2014 IEEE Chicago, Illinois [11] G.H. Lim I.H. Suh H. Suh Ontology-based unified robot knowledge for service robots in indoor environments Systems, IEEE Trans. Syst. Man Cybern. 41 3 2011 492 509 [12] S. Coradeschi A. Saffiotti An introduction to the anchoring problem Robot. Auton. Syst. 43 2–3 2003 85 96 special issue on perceptual anchoring [13] S. Zaman G. Steinbauer J. Maurer P. Lepej S. Uran An integrated model-based diagnosis and repair architecture for ROS-based robot systems IEEE International Conference on Robotics and Automation 2013 Karlsruhe Germany [14] E. Tulving Concepts of human memory L. Squire G. Lynch N. Weinberger J. McGaugh Memory: Organization and Locus of Change 1991 Oxford Univ. Press 3 32 [15] E. Tulving Episodic memory and autonoesis: Uniquely human? J.M.H.S. Terrace The Missing Link in Cognition 2005 Oxford Univ. Press NewYork, NY 4 56 [16] R. Wood P. Baxter T. Belpaeme A review of long-term memory in natural and synthetic systems Adapt. Behav. 20 2011 81 103 [17] L. Seabra~Lopes A. Chauhan How many words can my robot learn?: An approach and experiments with one-class learning Interact. Stud. 8 1 2007 53 81 [18] L. Seabra~Lopes A. Chauhan Open-ended category learning for language acquisition Connect. Sci. 20 4 2008 277 297 [19] S. Kirstein H. Wersing H.-M. Gross E. Körner A life-long learning vector quantization approach for interactive learning of multiple categories Neural Netw. 28 2012 90 105 [20] N. Krüger C. Geib J. Piater R. Petrick M. Steedman F. Wörgötter A. Ude T. Asfour D. Kraft D. Omrčen A. Agostini R. Dillmann Object-action complexes: Grounded abstractions of sensory-motor processes Robot. Auton. Syst. 59 10 2011 740 757 [21] F. Heintz, J. Kvarnstrom, P. Doherty, A stream-based hierarchical anchoring framework, in: Intelligent Robots and Systems, 2009, IROS 2009, IEEE/RSJ International Conference on, 2009, pp. 5254–5260. [22] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library IEEE Robot. Autom. Mag. 1070 9932 2012 [23] A. Clapés M. Reyes S. Escalera Multi-modal user identification and object recognition surveillance system Pattern Recognit. Lett. 34 7 2013 799 808 [24] R.B. Rusu N. Blodow M. Beetz Fast point feature histograms (FPFH) for 3D registration Robotics and Automation, 2009, ICRA’09, IEEE International Conference on 2009 IEEE 3212 3217 [25] L. Seabra~Lopes J. Connell Semisentient robots: routes to integrated intelligence IEEE Intell. syst. 16 5 2001 10 14 [26] S. Sahib A review of non relational databases, their types, advantages and disadvantages Int. J. Eng. Technol. 2 2 2013 [27] G.H. Lim M. Oliveira V. Mokhtari S. Hamidreza~Kasaei A. Chauhan L. Seabra Lopes A.M. Tomé Interactive teaching and experience extraction for learning about objects and robot activities Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on 2014 IEEE 153 160 [28] J.S. Evans Dual-processing accounts of reasoning, judgment, and social cognition Ann. Rev. Psychol. 59 1 2008 255 278 [29] R. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: Robotics and Automation, ICRA, 2011 IEEE International Conference on, 2011, pp. 1–4. [30] D. Cohen-Or A. Kaufman Fundamentals of surface voxelization Graph. Models Image Process. 57 6 1995 453 461 [31] C.B. Barber D.P. Dobkin H. Huhdanpaa The Quickhull algorithm for convex hulls ACM Trans. Math. Softw. 22 4 1996 469 483 [32] N.M. Amato, F.P. Preparata, An NC parallel 3D convex hull algorithm, in: Proceedings of the ninth annual symposium on Computational geometry, SCG’93, 1993, pp. 289–297. [33] M.A. Fischler R.C. Bolles Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [34] R.B. Rusu Semantic 3D object maps for everyday manipulation in human living environments (Ph.D. thesis) 2009 Computer Science department, Technische Universitaet Muenchen Germany October [35] W. Yi S. Marshall Principal component analysis in application to object orientation Geo-spatial Inf. Sci. 3 3 2000 76 78 [36] Y. Salih, A. Malik, 3D tracking using particle filters, in: Instrumentation and Measurement Technology Conference, I2MTC, 2011 IEEE, 2011, pp. 1–4. [37] A. Johnson M. Hebert Using spin images for efficient object recognition in cluttered 3D scenes IEEE Trans. Pattern Anal. Mach. Intell. 21 5 1999 433 449 [38] M. Connor P. Kumar Fast construction of k-nearest neighbor graphs for point clouds IEEE Trans. Vis. Comput. Graphics 16 4 2010 599 608 [39] R. Rusu, N. Blodow, Z. Marton, M. Beetz, Close-range scene segmentation and reconstruction of 3D point cloud maps for mobile manipulation in domestic environments, in: Intelligent Robots and Systems, 2009, IROS 2009, IEEE/RSJ International Conference on, 2009, pp. 1–6. [40] M. Munaro F. Basso S. Michieletto E. Pagello E. Menegatti A software architecture for RGB-D people tracking based on ROS framework for a mobile robot Frontiers of Intelligent Autonomous Systems Studies in Computational Intelligence vol. 466 2013 53 68 [41] J.L. Bentley Multidimensional binary search trees used for associative searching Commun. ACM 18 9 1975 509 517 [42] A. Chauhan L. Seabra~Lopes Using spoken words to guide open-ended category formation Cogn. Process. 12 2011 341 354 [43] K. Lai, L. Bo, X. Ren, D. Fox, A large-scale hierarchical multi-view RGB-D object dataset, in: Robotics and Automation, ICRA, 2011 IEEE International Conference on, 2011, pp. 1817–1824. [44] M. Oliveira, L. Seabra Lopes, G.H. Lim, H. Kasaei, A.D. Sappa, A. Tome, A. Chauhan, Concurrent learning of visual codebooks and object categories in open-ended domains, in: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, IEEE (in press). Miguel Oliveira received the B.Sc., and M.Sc., degrees in Mechanical Engineering from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering with specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Luís Seabra Lopes is an Associate Professor of Informatics in the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, Portugal. He received a Ph.D. in Robotics and Integrated Manufacturing from the New University of Lisbon, Portugal, in 1998. He has longstanding interests in robot learning, cognitive robotic architectures, and human–robot interaction. Gi Hyun Lim received the B.S. degree in Metallurgical Engineering and the M.S. and Ph.D degrees in Electronics and Computer Engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Post-doctoral Researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal. His research interests lie in the area of intelligence and learning for robots, including perception and semantics. From 1997 to 1998, he was an Engineer with the Dongbu Electronics, Ltd., Chungcheongbuk-do, Korea, where he was involved in research on semi-conductor factory automation. From 1999 to 2005, he was a Senior Research Engineer at a venture business, where he was involved in research on real-time operating systems and embedded systems. S. Hamidreza Kasaei obtained B.Sc. (2010) and M.Sc. (2012) in Computer Engineering field of Artificial Intelligence from the University of Isfahan (Iran). Currently he is a Ph.D. student at the University of Minho, Aveiro and Porto (Portugal), where he works on 3D object category learning and recognition in open-ended domains as a research student at the Institute of Electronics and Telematics Engineering of Aveiro. He worked on Middle size soccer robot and Humanoid robot and obtained different ranks in Robocup competition. His main research interest are in computer vision, robotics and multi agent systems. Ana Maria Tomé is an Associate Professor of Electrical Engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as machine learning applications. Aneesh Chauhan is a Post-doctoral Researcher in the Computer Vision Group at the Centre of Automatics and Robotics at Universidad Politecnica de Madrid. He holds a Bachelor of Engineering degree in Computer Science and Engineering from Baba Ambedkar Marathwada University, Maharashtra, India, a Master of Science degree in Autonomous Systems from the University of Exeter, UK and Ph.D. in Informatics Engineering from Universidade de Aveiro, Aveiro, Portugal. His research interests include intelligent robotics, language grounding, human–robot interaction as well as the application of computer vision and machine learning approaches for autonomous perception tasks. "
    },
    {
        "doc_title": "Gathering and conceptualizing plan-based robot activity experiences",
        "doc_scopus_id": "84945936412",
        "doc_doi": "10.1007/978-3-319-08338-4_72",
        "doc_eid": "2-s2.0-84945936412",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Abstraction and generalization",
            "Conceptualization",
            "Ego networks",
            "Experience gathering",
            "Plan schema",
            "Plan-based"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.Learning from experiences is an effective approach to enhance robot’s competence. This paper focuses on developing capabilities for a robot to obtain robot activity experiences and conceptualize the experiences as plan schemata, which are used as heuristics for the robot to make plans in similar situations. The plan-based robot activity experiences are obtained through human-robot interactions where a teaching action from a command-line user interface triggers recording of an experience. To represent human-robot interaction activities, ontologies for experiences and user instructions are integrated into a robot ontology. Recorded experiences are episodic descriptions of the robot’s activities including relevant perceptions of the environment, the goals pursued, successes, and failures. Since the amount of experience data is large, a graph simplification algorithm based on ego networks is investigated to filter out irrelevant information in an experience. Finally, an approach to robot activity conceptualization based on deductive generalization and abstraction is presented. The proposed systemwas demonstrated in a scenario where a PR2 robot is taught how to “serve a coffee” to a guest, in the EU project RACE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An experimental protocol for the evaluation of open-ended category learning algorithms",
        "doc_scopus_id": "84964005038",
        "doc_doi": "10.1109/EAIS.2015.7368779",
        "doc_eid": "2-s2.0-84964005038",
        "doc_date": "2015-12-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Cross validation",
            "Evaluation methods",
            "Experimental protocols",
            "Incremental approach",
            "Incremental learning",
            "Key characteristics",
            "Language grounding",
            "Test method"
        ],
        "doc_abstract": "© 2015 IEEE.There has been a steady surge in various sub-fields of machine learning where the focus is on developing systems that learn in an open-ended manner. This is particularly visible in the fields of language grounding and data stream learning. These systems are designed to evolve as new data arrive, modifying and adjusting learned categories, as well as, accommodating new categories. Although some of the features of incremental learning are present in open-ended learning, the latter can not be characterized as standard incremental learning. This paper presents and discusses the key characteristics of open-ended learning, differentiating it from the standard incremental approaches. The main contribution of this paper is concerned with the evaluation of these algorithms. Typically, the performance of learning algorithms is assessed using traditional train-test methods, such as holdout, cross-validation etc. These evaluation methods are not suited for applications where environments and tasks can change and therefore the learning system is frequently facing new categories. To address this, a well defined and practical protocol is proposed. The utility of the protocol is demonstrated by evaluating and comparing a set of learning algorithms at the task of open-ended visual category learning.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concurrent learning of visual codebooks and object categories in open-ended domains",
        "doc_scopus_id": "84958162360",
        "doc_doi": "10.1109/IROS.2015.7353715",
        "doc_eid": "2-s2.0-84958162360",
        "doc_date": "2015-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Bag-of-words models",
            "Gaussian Mixture Model",
            "Multiple levels",
            "Object categories",
            "On-line fashion",
            "Updated using",
            "Visual object recognition",
            "Visual objects"
        ],
        "doc_abstract": "© 2015 IEEE.In open-ended domains, robots must continuously learn new object categories. When the training sets are created offline, it is not possible to ensure their representativeness with respect to the object categories and features the system will find when operating online. In the Bag of Words model, visual codebooks are usually constructed from training sets created offline. This might lead to non-discriminative visual words and, as a consequence, to poor recognition performance. This paper proposes a visual object recognition system which concurrently learns in an incremental and online fashion both the visual object category representations as well as the codebook words used to encode them. The codebook is defined using Gaussian Mixture Models which are updated using new object views. The approach contains similarities with the human visual object recognition system: evidence suggests that the development of recognition capabilities occurs on multiple levels and is sustained over large periods of time. Results show that the proposed system with concurrent learning of object categories and codebooks is capable of learning more categories, requiring less examples, and with similar accuracies, when compared to the classical Bag of Words approach using codebooks constructed offline.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Planning with activity schemata: Closing the loop in experience-based planning",
        "doc_scopus_id": "84933055924",
        "doc_doi": "10.1109/ICARSC.2015.35",
        "doc_eid": "2-s2.0-84933055924",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Abstract task knowledge",
            "Cognitive capability",
            "Conceptualization",
            "Effective approaches",
            "Environment perceptions",
            "Learning tasks",
            "Planning domains",
            "Task planning"
        ],
        "doc_abstract": "© 2015 IEEE.Learning task knowledge from robot activity experiences has been recognized as an effective approach to improve robot task planning performance. Cognitive capabilities are required to enable a robot to learn new activities from its human partners as well as to refine and improve already learned skills. This paper presents an approach for a robot to conceptualize plan-based robot activity experiences as activity schemata - enriched abstract task knowledge - as well as to exploit them to make plans in similar situations. The experiences are episodic descriptions of plan-based robot activities including environment perceptions, sequences of applied actions and achieved tasks. In this work, the robot activity experiences are obtained through human-robot interaction. The adopted conceptualization approach constructs an activity schema through deductive generalization, abstraction and feature extraction. A high-level task planner was developed to find a solution for a similar task by following an activity schema. The paper proposes a formalization for experience-based planning domains. The proposed learning and planning approach is illustrated in a restaurant environment where a service robot learns how to carry out complex tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An adaptive object perception system based on environment exploration and Bayesian learning",
        "doc_scopus_id": "84933040723",
        "doc_doi": "10.1109/ICARSC.2015.37",
        "doc_eid": "2-s2.0-84933040723",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Bayesian learning",
            "Cognitive robotics",
            "Environment exploration",
            "Object perception",
            "Perception capability",
            "Probabilistic models",
            "Qualitative analysis",
            "Quantitative evaluation"
        ],
        "doc_abstract": "© 2015 IEEE.Cognitive robotics looks at human cognition as a source of inspiration for automatic perception capabilities that will allow robots to learn and reason out how to behave in response to complex goals. For instance, humans learn to recognize object categories ceaselessly over time. This ability to refine knowledge from the set of accumulated experiences facilitates the adaptation to new environments. Inspired by such abilities, this paper proposes an efficient approach towards 3D object category learning and recognition in an interactive and open-ended manner. To achieve this goal, this paper focuses on two state-of-the-art questions: (i) How to use unsupervised object exploration to construct a dictionary of visual words for representing objects in a highly compact and distinctive way. (II) How to learn incrementally probabilistic models of object categories to achieve adaptability. To examine the performance of the proposed approach, a quantitative evaluation and a qualitative analysis are used. The experimental results showed the fulfilling performance of this approach on different types of objects. The proposed system is able to interact with human users and learn new object categories over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive Open-Ended Learning for 3D Object Recognition: An Approach and Experiments",
        "doc_scopus_id": "84945464134",
        "doc_doi": "10.1007/s10846-015-0189-z",
        "doc_eid": "2-s2.0-84945464134",
        "doc_date": "2015-01-31",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Feature descriptors",
            "High level knowledge",
            "Leave-one-out cross validations",
            "Nearest neighbor classification",
            "Open-ended learning",
            "Precision and recall",
            "Spin images"
        ],
        "doc_abstract": "© 2015, Springer Science+Business Media Dordrecht.3D object detection and recognition is increasingly used for manipulation and navigation tasks in service robots. It involves segmenting the objects present in a scene, estimating a feature descriptor for the object view and, finally, recognizing the object view by comparing it to the known object categories. This paper presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In this paper, “open-ended” implies that the set of object categories to be learned is not known in advance. The training instances are extracted from on-line experiences of a robot, and thus become gradually available over time, rather than at the beginning of the learning process. This paper focuses on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D scenes in an open-ended manner? (2) How to acquire and use high-level knowledge obtained from the interaction with human users, namely when they provide category labels, in order to improve the system performance? This approach starts with a pre-processing step to remove irrelevant data and prepare a suitable point cloud for the subsequent processing. Clustering is then applied to detect object candidates, and object views are described based on a 3D shape descriptor called spin-image. Finally, a nearest-neighbor classification rule is used to predict the categories of the detected objects. A leave-one-out cross validation algorithm is used to compute precision and recall, in a classical off-line evaluation setting, for different system parameters. Also, an on-line evaluation protocol is used to assess the performance of the system in an open-ended setting. Results show that the proposed system is able to interact with human users, learning new object categories continuously over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Batch Reinforcement Learning for Robotic Soccer Using the Q-Batch Update-Rule",
        "doc_scopus_id": "84945484531",
        "doc_doi": "10.1007/s10846-014-0171-1",
        "doc_eid": "2-s2.0-84945484531",
        "doc_date": "2015-01-09",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adversarial environments",
            "Amount of interaction",
            "Batch reinforcement learning",
            "Class of methods",
            "Q-batch",
            "Reinforcement learning method",
            "Robotic soccer",
            "Robotics applications"
        ],
        "doc_abstract": "© 2015, Springer Science+Business Media Dordrecht.Reinforcement Learning is increasingly becoming a valuable alternative to tackle many of the challenges existing in a semi-structured, non-deterministic and adversarial environment such as robotic soccer. Batch Reinforcement Learning is a class of Reinforcement Learning methods characterized by processing a batch of interactions. By storing all past interactions, Batch RL methods are extremely data-efficient which makes this class of methods very appealing for robotics applications, specially when learning directly on physical robotic platforms.This paper presents the application of Batch Reinforcement Learning to obtain efficient robotic soccer controllers on physical platforms. To learn the controllers we propose the application of Q-Batch, a novel update-rule that exploits the episodic nature of the interactions in Batch Reinforcement Learning. The approach was validated in three different tasks with increasing difficulty. Results show the proposed approach is able to outperform hand-coded policies, for all the tasks, in a reduced amount of time. Additionally, for one of the tasks, a comparison between Q-Batch and Q-learning is carried out, and results show that, Q-Batch obtains better policies than Q-learning for the same amount of interaction time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hierarchical nearest neighbor graphs for building perceptual hierarchies",
        "doc_scopus_id": "84951750360",
        "doc_doi": "10.1007/978-3-319-26535-3_74",
        "doc_eid": "2-s2.0-84951750360",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Agglomerative clustering",
            "Graphs",
            "Hierarchical",
            "Incremental learning",
            "Nearest",
            "Neighbour",
            "Open-ended environments"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Humans tend to organize their knowledge into hierarchies, because searches are efficient when proceeding downward in the tree-like structures. Similarly, many autonomous robots also contain some form of hierarchical knowledge. They may learn knowledge from their experiences through interaction with human users. However, it is difficult to find a common ground between robots and humans in a low level experience. Thus, their interaction must take place at the semantic level rather than at the perceptual level, and robots need to organize perceptual experiences into hierarchies for themselves. This paper presents an unsupervised method to build view-based perceptual hierarchies using hierarchical Nearest Neighbor Graphs (hNNGs), which combine most of the interesting features of both Nearest Neighbor Graphs (NNGs) and self-balancing trees. An incremental construction algorithm is developed to build and maintain the perceptual hierarchies. The paper describes the details of the data representations and the algorithms of hNNGs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ECG signal prediction for destructive motion artefacts",
        "doc_scopus_id": "84937468480",
        "doc_doi": "10.1007/978-3-319-19695-4_10",
        "doc_eid": "2-s2.0-84937468480",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Algorithm implementation",
            "Burg algorithm",
            "Contact loss",
            "Daily activity",
            "E health",
            "Linear prediction",
            "Motion artefacts",
            "Portable device"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.This paper addresses the ability of Burg algorithm to predict the ECG signal when it was completely destroyed by motion artefacts. The application focus of this study is portable devices used in telemedicine and healthcare, where the daily activity of patients produces several contact losses and movements of electrodes on the skin. The paper starts with a short analysis of noise sources that affects the ECG signal, followed by the algorithm implementation and the results. The obtained results show that Burg algorithm is a very promising technique to predict the ECG signal for at least three sequential heart beats.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The RACE Project: Robustness by Autonomous Competence Enhancement",
        "doc_scopus_id": "84933054481",
        "doc_doi": "10.1007/s13218-014-0327-y",
        "doc_eid": "2-s2.0-84933054481",
        "doc_date": "2014-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Control architecture",
            "Early integration",
            "European program",
            "General systems",
            "Learn+",
            "Object categories",
            "Perceptual memory",
            "Performance based",
            "Plan execution",
            "Systems architecture"
        ],
        "doc_abstract": "© 2014, Springer-Verlag Berlin Heidelberg.This paper reports on the aims, the approach, and the results of the European project RACE. The project aim was to enhance the behavior of an autonomous robot by having the robot learn from conceptualized experiences of previous performance, based on initial models of the domain and its own actions in it. This paper introduces the general system architecture; it then sketches some results in detail regarding hybrid reasoning and planning used in RACE, and instances of learning from the experiences of real robot task execution. Enhancement of robot competence is operationalized in terms of performance quality and description length of the robot instructions, and such enhancement is shown to result from the RACE system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A perceptual memory system for grounding semantic representations in intelligent service robots",
        "doc_scopus_id": "84911489131",
        "doc_doi": "10.1109/IROS.2014.6942861",
        "doc_eid": "2-s2.0-84911489131",
        "doc_date": "2014-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational issues",
            "High-level reasoning",
            "Intelligent Service robots",
            "Low level control",
            "Memory requirements",
            "Multiplexing schemes",
            "Perceptual memory",
            "Semantic representation"
        ],
        "doc_abstract": "© 2014 IEEE.This paper addresses the problem of grounding semantic representations in intelligent service robots. In particular, this work contributes to addressing two important aspects, namely the anchoring of object symbols into the perception of the objects and the grounding of object category symbols into the perception of known instances of the categories. The paper discusses memory requirements for storing both semantic and perceptual data and, based on the analysis of these requirements, proposes an approach based on two memory components, namely a semantic memory and a perceptual memory. The perception, memory, learning and interaction capabilities, and the perceptual memory, are the main focus of the paper. Three main design options address the key computational issues involved in processing and storing perception data: a lightweight, NoSQL database, is used to implement the perceptual memory; a thread-based approach with zero copy transport of messages is used in implementing the modules; and a multiplexing scheme, for the processing of the different objects in the scene, enables parallelization. The system is designed to acquire new object categories in an incremental and open-ended way based on user-mediated experiences. The system is fully integrated in a broader robot system comprising low-level control and reactivity to high-level reasoning and learning.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive teaching and experience extraction for learning about objects and robot activities",
        "doc_scopus_id": "84937567763",
        "doc_doi": "10.1109/ROMAN.2014.6926246",
        "doc_eid": "2-s2.0-84937567763",
        "doc_date": "2014-10-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Continuous interactions",
            "Human users",
            "Intelligent Service robots",
            "Interactive learning",
            "Learning methods",
            "Perceptual domain",
            "Robot architecture",
            "Teaching activities"
        ],
        "doc_abstract": "© 2014 IEEE.Intelligent service robots should be able to improve their knowledge from accumulated experiences through continuous interaction with the environment, and in particular with humans. A human user may guide the process of experience acquisition, teaching new concepts, or correcting insufficient or erroneous concepts through interaction. This paper reports on work towards interactive learning of objects and robot activities in an incremental and open-ended way. In particular, this paper addresses human-robot interaction and experience gathering. The robot's ontology is extended with concepts for representing human-robot interactions as well as the experiences of the robot. The human-robot interaction ontology includes not only instructor teaching activities but also robot activities to support appropriate feedback from the robot. Two simplified interfaces are implemented for the different types of instructions including the teach instruction, which triggers the robot to extract experiences. These experiences, both in the robot activity domain and in the perceptual domain, are extracted and stored in memory, and they are used as input for learning methods. The functionalities described above are completely integrated in a robot architecture, and are demonstrated in a PR2 robot.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Portable decision support system for heart failure detection and medical diagnosis",
        "doc_scopus_id": "84906818980",
        "doc_doi": "10.1145/2528194.2628204",
        "doc_eid": "2-s2.0-84906818980",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Ambient conditions",
            "Automatic Detection",
            "Clinical history",
            "Decision support algorithms",
            "Electronic systems",
            "Myocardial Infarction",
            "Problematic issues",
            "Signal variations"
        ],
        "doc_abstract": "Heart disorders are one of the most problematic issues of human health. There are currently many efforts to reduce the time for first assistance based ou electronic systems that continuously records the electric heart activity (EGG), for further inspection and anomalies detection. However, an efficient automatic detection of heart problems still being a big challenge mainly clue to difficulty of accessing some specific cardiac problems, signal variations between different patients and the level of uncertainty caused by the absence of significant data or medical elements from patient. This research explores the use of several health condition signals and indicators for heart monitoring and injuries detection. This paper presents the first approach of a portable solution running decision support algorithms to produce medical diagnosis based on electrical and sound heart signals, ambient conditions, patient mobility, patient personal data and clinical history. copyright 2014 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An interactive open-ended learning approach for 3D object recognition",
        "doc_scopus_id": "84905034271",
        "doc_doi": "10.1109/ICARSC.2014.6849761",
        "doc_eid": "2-s2.0-84905034271",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Descriptors",
            "High level knowledge",
            "Leave-one-out cross validations",
            "Nearest-neighbor classifications",
            "open-ended learning",
            "Precision and recall",
            "Three-dimensional object"
        ],
        "doc_abstract": "Three-dimensional object detection and recognition is increasingly in manipulation and navigation applications in autonomous service robots. It involves clustering points of the point cloud from an unstructured scene into objects candidates and estimating features to recognize the objects under different circumstances such as occlusions and clutter. This paper presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In this paper, 'open-ended' implies that the set of object categories to be learned is not known in advance. The training instances are extracted from actual experiences of a robot, and thus become gradually available, rather than being available at the beginning of the learning process. This paper focuses on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D unstructured scenes in an open-ended manner? (2) How to acquire and utilize high-level knowledge obtained from the user (e.g. category label) to improve the system performance? This approach starts with a pre-processing phase to remove unnecessary information and prepare a suitable point cloud. Clustering is then applied to detect object candidates. Subsequently, all object candidates are described based on a 3D shape descriptor called spin-image. Finally, a nearest-neighbor classification rule is used to assign category labels to the detected objects. To examine the performance of the proposed approach, a leave-one-out cross validation algorithm is utilized to compute precision and recall. The experimental results show the fulfilling performance of this approach on different types of objects. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning robotic soccer controllers with the Q-Batch update-rule",
        "doc_scopus_id": "84905017754",
        "doc_doi": "10.1109/ICARSC.2014.6849775",
        "doc_eid": "2-s2.0-84905017754",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Competitive environment",
            "Data collection",
            "Learning tasks",
            "Parameter-tuning",
            "Physical robots",
            "RoboCup",
            "Robotic soccer",
            "Simulated environment"
        ],
        "doc_abstract": "Robotic soccer provides a rich environment for the development of Reinforcement Learning controllers. The competitive environment imposes strong requirements on performance of the developed controllers. RL offers a valuable alternative for the development of efficient controllers while avoiding the hassle of parameter tuning a hand coded policy. This paper presents the application of a recently proposed Batch RL update-rule to learn robotic soccer controllers in the context of the RoboCup Middle Size League. The Q-Batch update-rule exploits the episodic structure of the data collection phase of Batch RL to efficiently evaluate and improve the learned policy. Three different learning tasks, with increasing difficulty, were developed and applied on a simulated environment and later on the physical robot. The performance of the learned controllers is mostly compared to hand-tuned controllers while some comparisons with other RL methods were performed. Results show that the proposed approach is able to learn the tasks in a reduced amount of time, even outperforming existing hand-coded solutions. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Grounding language in perception for scene conceptualization in autonomous robots",
        "doc_scopus_id": "84904861654",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84904861654",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cognitive architectures",
            "Grounding language",
            "Human supervision",
            "Learning frameworks",
            "Linguistic descriptions",
            "Natural languages",
            "Spatial structure",
            "Textual description"
        ],
        "doc_abstract": "In order to behave autonomously, it is desirable for robots to have the ability to use human supervision and learn from different input sources (perception, gestures, verbal and textual descriptions etc). In many machine learning tasks, the supervision is directed specifically towards machines and hence is straight forward clearly annotated examples. But this is not always very practical and recently it was found that the most preferred interface to robots is natural language. Also the supervision might only be available in a rather indirect form, which may be vague and incomplete. This is frequently the case when humans teach other humans since they may assume a particular context and existing world knowledge. We explore this idea here in the setting of conceptualizing objects and scene layouts. Initially the robot undergoes training from a human in recognizing some objects in the world and armed with this acquired knowledge it sets out in the world to explore and learn more higher level concepts like static scene layouts and environment activities. Here it has to exploit its learned knowledge and ground language into perception to use inputs from different sources that might have overlapping as well as novel information. When exploring, we assume that the robot is given visual input, without explicit type labels for objects, and also that it has access to more or less generic linguistic descriptions of scene layout. Thus our task here is to learn the spatial structure of a scene layout and simultaneously visual object models it was not trained on. In this paper, we present a cognitive architecture and learning framework for robot learning through natural human supervision and using multiple input sources by grounding language in perception. Copyright © 2014, Association for the Advancement of Artificial Intelligence. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of a dialogue manager for a mobile robot",
        "doc_scopus_id": "84889583545",
        "doc_doi": "10.1109/ROMAN.2013.6628466",
        "doc_eid": "2-s2.0-84889583545",
        "doc_date": "2013-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Dialogue manager",
            "Experimental evaluation",
            "Information state",
            "Intelligent Service robots",
            "Usability evaluation"
        ],
        "doc_abstract": "This paper presents an evaluation of the dialogue manager (DM) used on Carl, a prototype of an intelligent service robot, designed and developed having in mind hosting tasks in a building or event. The developed DM, based on the 'Information State' approach, is described. In an experimental evaluation, in which 10 participants attempted to complete several interaction tasks with the robot, 81% of tasks were performed successfully. The results of an usability evaluation are also presented and discussed. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A portable spatial monitoring system for autonomous heart diagnosis",
        "doc_scopus_id": "84894172239",
        "doc_doi": "10.1109/HealthCom.2013.6720718",
        "doc_eid": "2-s2.0-84894172239",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Heart disorders are one of the most problematic issues of human health. There are currently many efforts to reduce the time for first assistance based on electronic systems that continuously records the electric heart activity for further inspection and anomalies detection. The most popular are portable monitoring systems based on the Electrocardiogram (ECG) signal. However, an efficient detection of heart problems still being a big challenge mainly due to the difficulty of accessing some specific cardiac problems and signal variations between different patients. A different technique for heart diagnosing is based on spatial recording of electrical heart activity, commonly known as Vectorcardiogram (VCG). VCG is pointed by several authors as a more efficient tool than ECG for heart inspection and problem detection. This research explores the possibility of using VCG signals in a portable device for constant heart monitoring and injuries detection. It is presented a portable solution with VCG recording and digital signal processing for automatic diagnosis. This paper covers the aspects of the VCG signal and its ability to be converted into a 12-lead ECG signal. It is also presented a system for automatic diagnosis based on VCG which is based on a multichannel portable hardware platform to support noise cancellation, parameter extraction and decision making algorithms. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The representation of weighted action-coupled semantic network and spreading activation model for improvisational action",
        "doc_scopus_id": "84893609545",
        "doc_doi": "10.1109/SMC.2013.692",
        "doc_eid": "2-s2.0-84893609545",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Action-coupled",
            "Bi-directional",
            "Experience",
            "Inference mechanism",
            "Real environments",
            "Semantic network",
            "Service robots",
            "Spreading activations"
        ],
        "doc_abstract": "Knowledge plays significant roles in real environments to successfully complete service tasks. Almost all knowledge inference mechanisms assume complete and correct knowledge about environments. However, these environments are said to be partially observable and uncertain with respect to service robots. Thus, a robot cannot often complete the given service tasks. To cope with such a difficulty, we propose a novel weighted action-coupled semantic network (wASN), which represents the world by symbolic relationships and their weights, and its association mechanism using bi-directional spreading activation model which recommends goal-oriented actions from action-coupled knowledge. The improvisational actions provide more opportunities for a service robot to achieve goals. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Aerial ball perception based on the use of a single perspective camera",
        "doc_scopus_id": "84884705454",
        "doc_doi": "10.1007/978-3-642-40669-0_21",
        "doc_eid": "2-s2.0-84884705454",
        "doc_date": "2013-10-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color segmentation",
            "Hybrid process",
            "Perception-based",
            "Perspective cameras",
            "Position estimation",
            "Regions of interest",
            "Size and position",
            "Vision systems"
        ],
        "doc_abstract": "The detection of the ball when it is not on the ground is an important research line within the Middle Size League of RoboCup. A correct detection of airborne balls is particularly important for goal keepers, since shots to goal are usually made that way. To tackle this problem on the CAMBADA team , we installed a perspective camera on the robot. This paper presents an analysis of the scenario and assumptions about the use of a single perspective camera for the purpose of 3D ball perception. The algorithm is based on physical properties of the perspective vision system and an heuristic that relates the size and position of the ball detected in the image and its position in the space relative to the camera. Regarding the ball detection, we attempt an approach based on a hybrid process of color segmentation to select regions of interest and statistical analysis of a global shape context histogram. This analysis attempts to classify the candidates as round or not round. Preliminary results are presented regarding the ball detection approach that confirms its effectiveness in uncontrolled environments. Moreover, experimental results are also presented for the ball position estimation and a sensor fusion proposal is described to merge the information of the ball into the worldstate of the robot. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised Internet-based category learning for object recognition",
        "doc_scopus_id": "84884489702",
        "doc_doi": "10.1007/978-3-642-39094-4_88",
        "doc_eid": "2-s2.0-84884489702",
        "doc_date": "2013-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "category learning",
            "Internet based",
            "Internet images",
            "K-means clustering",
            "Object categories",
            "Object extraction",
            "Training objects",
            "Unsupervised method"
        ],
        "doc_abstract": "This paper describes a new method for acquiring object categories by searching images on the Internet. An unsupervised method is proposed that, starting from a set of objects extracted from the Internet images automatically fetched for a given category name, selects a subset of objects suitable for building a model of the category. The method is based on repeated k-means clustering. Object relevance scoring is based on properties of the clusters in which they are placed. The approach is evaluated on generic categories (i.e. those usually referenced through common nouns). We demonstrate that the proposed approach significantly improves the quality of the training object collections. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Contour-based object extraction and clutter removal for semantic vision",
        "doc_scopus_id": "84884474684",
        "doc_doi": "10.1007/978-3-642-39094-4_20",
        "doc_eid": "2-s2.0-84884474684",
        "doc_date": "2013-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Contour segments",
            "Contour tracing",
            "Diverse applications",
            "Multiple objects",
            "Object categories",
            "Object detectors",
            "Object extraction",
            "Original images"
        ],
        "doc_abstract": "This paper focuses on object extraction from images, a functionality that can be relevant both for category learning and object recognition in diverse applications. The described object extraction approach, which doesn't take into account any prior knowledge about the target objects, works on the edge-based counterpart of the original image. In a first step, groups of neighboring edge pixels are traced to form contour segments. These contour segments are then coherently aggregated to reconstruct the shapes of the different objects present in the original image. The approach is particularly relevant for extracting objects with few if any distinctive local features, thus objects mainly characterized by their shape. The developed functionalities can be used to segment and extract objects from images with multiple objects, as those obtained from the Internet by searching for a specific object category name. They can also be used to discard clutter from image sub-windows expected to contain a single object, as those delivered by an object detector. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An ontology-based multi-level robot architecture for learning from experiences",
        "doc_scopus_id": "84883297909",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84883297909",
        "doc_date": "2013-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Eu projects",
            "High level semantics",
            "Learning from experiences",
            "Ontology-based",
            "Robot architecture",
            "Robot performance",
            "Service robots"
        ],
        "doc_abstract": "One way to improve the robustness and flexibility of robot performance is to let the robot learn from its experiences. In this paper, we describe the architecture and knowledge-representation framework for a service robot being developed in the EU project RACE, and present examples illustrating how learning from experiences will be achieved. As a unique innovative feature, the framework combines memory records of low-level robot activities with ontology-based high-level semantic descriptions. © 2013, Association for the Advancement of artificial intelligence.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A COP-based controller for adaptive motion planning of a single-legged robot",
        "doc_scopus_id": "84876462364",
        "doc_doi": "10.1109/ROBIO.2012.6490941",
        "doc_eid": "2-s2.0-84876462364",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            }
        ],
        "doc_keywords": [
            "Adaptive motion",
            "Centre of gravity",
            "Centre of pressure",
            "Ground reaction forces",
            "Inverse kinematics algorithms",
            "Planning and control",
            "Reaction forces",
            "Single-legged robots"
        ],
        "doc_abstract": "This paper presents a motion planning and control method for determining the joint velocities of a single-legged robot based on the desired Centre of Pressure (COP) motion, the Centre of Mass (COM) dynamics and the ground reaction forces. A closed-loop inverse kinematics algorithm is implemented using the Centre of Gravity (COG) Jacobian, while the control system relies on the reaction force data to estimate the real COP. The main idea is to manipulate the dynamics of the COM such as the COP shows a desired behavior. The implementation of the COP-based controller is described to demonstrate the possibility of keeping a single-legged robot in balance, while adapting to unexpected changes in a slope surface or during execution of a specified motion task. Simulation results are illustrated throughout the paper to validate the proposed controller. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Manhattan-Pyramid Distance: A solution to an anomaly in pyramid matching by minimization",
        "doc_scopus_id": "84874576349",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84874576349",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Classification approach",
            "Image datasets",
            "Multi-resolutions",
            "Object classification",
            "Systematic evaluation"
        ],
        "doc_abstract": "In the field of computer vision, pyramid matching by minimization has gained increasing popularity. This paper points out and discusses an inherent anomaly in pyramid matching by minimization that can affect the performance of classification approaches based on this type of matching. As a solution, a new multiresolution measure, called Manhattan-Pyramid Distance (MPD), is proposed. Systematic evaluations are carried out at the task of instance-based object classification on four object image datasets. Results show that MPD improves object classification performance with respect to a standard approach based on pyramid matching by minimization. © 2012 ICPR Org Committee.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A brief survey of commercial robotic arms for research on manipulation",
        "doc_scopus_id": "84864248186",
        "doc_doi": "10.1109/ISRA.2012.6219361",
        "doc_eid": "2-s2.0-84864248186",
        "doc_date": "2012-07-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Comparative analysis",
            "Functional capacity",
            "Light weight",
            "manipulation",
            "Research objectives",
            "State of the art"
        ],
        "doc_abstract": "Presently a vast variety of robust robotic arms are available commercially, some of which are extremely reliable in precision and repeatability. This makes them an ideal tool for research focused on manipulation. However, there is a lack of an easily accessible comparative analysis that can assist researchers in choosing an arm that fits their research objectives. With an objective to provide such an analysis, this paper provides a comparative survey of the state of the art in commercial robotic arms - classified based on their price, performance and suitability for research on manipulation. These arms are categorized into four classes: cheap educational arms, low price industrial arms, research oriented arms and modular light weight arms. Within each classification, some typical robotic arms are analyzed in detail to provide an overview of the functional capacities of the arms in that particular class. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using spoken words to guide open-ended category formation",
        "doc_scopus_id": "81255185578",
        "doc_doi": "10.1007/s10339-011-0407-y",
        "doc_eid": "2-s2.0-81255185578",
        "doc_date": "2011-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Experimental and Cognitive Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3205"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Naming is a powerful cognitive tool that facilitates categorization by forming an association between words and their referents. There is evidence in child development literature that strong links exist between early word-learning and conceptual development. A growing view is also emerging that language is a cultural product created and acquired through social interactions. Inspired by these studies, this paper presents a novel learning architecture for category formation and vocabulary acquisition in robots through active interaction with humans. This architecture is open-ended and is capable of acquiring new categories and category names incrementally. The process can be compared to language grounding in children at single-word stage. The robot is embodied with visual and auditory sensors for world perception. A human instructor uses speech to teach the robot the names of the objects present in a visually shared environment. The robot uses its perceptual input to ground these spoken words and dynamically form/organize category descriptions in order to achieve better categorization. To evaluate the learning system at word-learning and category formation tasks, two experiments were conducted using a simple language game involving naming and corrective feedback actions from the human user. The obtained results are presented and discussed in detail. © 2011 Marta Olivetti Belardinelli and Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Robot team coordination using dynamic role and positioning assignment and role based setplays",
        "doc_scopus_id": "79952629487",
        "doc_doi": "10.1016/j.mechatronics.2010.05.010",
        "doc_eid": "2-s2.0-79952629487",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Coordinated procedures",
            "Dynamic role assignment",
            "Information sharing",
            "Multi-robot teams",
            "Performance measure",
            "Priority-based algorithms",
            "Robotic soccer team",
            "Strategic positioning"
        ],
        "doc_abstract": "The coordination methodologies of CAMBADA, a robotic soccer team designed to participate in the RoboCup Middle-Size League (MSL), are presented in this paper. The approach, which relies on information sharing and integration within the team, is based on formations, flexible positionings and dynamic role and positioning assignment. Role assignment is carried out locally on each robot to increase its reactivity. Positioning assignment is carried out at a lower frequency by a coach agent following a new priority-based algorithm that maintains a competitive formation, covering the most important positionings when malfunctions lead to a reduction of the team size. Coordinated procedures for passing and setplays have also been implemented. With this design, CAMBADA reached the 1st place in RoboCup'2008 and the 3rd place in RoboCup'2009. Competition results and performance measures computed from logs and videos of real competition games are presented and discussed. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-07-02 2010-07-02 2011-03-08T22:21:19 S0957-4158(10)00101-7 S0957415810001017 10.1016/j.mechatronics.2010.05.010 S300 S300.1 FULL-TEXT 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-07-02T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 10 445 454 445 454 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright © 2010 Elsevier Ltd. All rights reserved. ROBOTTEAMCOORDINATIONUSINGDYNAMICROLEPOSITIONINGASSIGNMENTROLEBASEDSETPLAYS LAU N 1 Introduction 2 Player architecture 3 Information sharing and integration 4 Positionings and roles in open play 4.1 Formations and strategic positionings 4.2 Roles in open play 4.3 Role and positioning assignment 5 Coordinated procedures 5.1 Passes 5.2 Set plays 6 Performance evaluation 6.1 General game features 6.2 Roles and behaviors 6.3 Coordination 6.4 Competition results 7 Conclusion Acknowledgments References NOREILS 1993 79 98 F WANG 1991 177 195 P BURGARD 2005 376 386 W CARPIN 2008 305 316 S GERKEY 2004 939 954 B STONE 1999 241 273 P REIS 2001 175 197 L BALANCINGREACTIVITYSOCIALDELIBERATIONINMULTIAGENTSYTEMSROBOCUPREALWORDAPPLICATIONS SITUATIONBASEDSTRATEGICPOSITIONINGFORCOORDINATINGATEAMHOMOGENEOUSAGENTS PAGELLO 2006 1370 1383 E BALCH 1998 926 939 T PEDREIRAS 2007 598 607 P ROBOTICSOCCER TASKMANAGEMENTFORSOFTREALTIMEAPPLICATIONSBASEDGENERALPURPOSEOPERATINGSYSTEMS NEVES 2007 499 507 A PROGRESSINARTIFICIALINTELLIGENCE OMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTS KOK 2005 99 114 J ZWEIGLE 2006 651 659 O LAUX2011X445 LAUX2011X445X454 LAUX2011X445XN LAUX2011X445X454XN item S0957-4158(10)00101-7 S0957415810001017 10.1016/j.mechatronics.2010.05.010 271456 2011-03-10T12:04:28.246113-05:00 2011-03-01 2011-03-31 true 703428 MAIN 10 59227 849 656 IMAGE-WEB-PDF 1 gr2 34159 270 292 gr2 8280 164 177 gr3 25214 219 354 gr3 5645 135 219 gr4 33839 357 308 gr4 4859 163 141 gr5 21014 231 356 gr5 10214 142 219 gr6 17002 151 327 gr6 4599 101 219 gr7 19059 220 355 gr7 7426 136 219 gr8 27976 248 356 gr8 14225 153 219 gr1 41827 254 355 gr1 19189 156 219 MECH 1164 S0957-4158(10)00101-7 10.1016/j.mechatronics.2010.05.010 Elsevier Ltd Fig. 1 CAMBADA robotic team. Fig. 2 Layered software architecture of CAMBADA players, from [28]. Fig. 3 Target player positions for several different ball positions. Fig. 4 CAMBADA positioning assignment algorithm. Fig. 5 Sequence of passes demonstrated in the free challenge of RoboCup’2008. Fig. 6 Replacer role algorithm for corner kicks. Fig. 7 Placement of RoleBarrier players. Fig. 8 Shoot locations in the final CAMBADA (black, on the left)–Tech United (white, on the right) game in RoboCup’2008 (shoots are circles and goals are sun-like forms). Table 1 Coordinated actions in a pass. RolePasser RoleReceiver PassFlag TRYING_TO_PASS Align to receiver Align to Passer PassFlag READY Kick the ball PassFlag BALL_PASSED Move to next position Catch ball Table 2 Time distribution for different classes of game states. Game state % Time Open play 53.1 Set piece for 21.5 Set piece against 25.4 Table 3 Percentage of game time for different numbers of playing field robots. Number of robots 0 1 2 3 4 5 Time (%) 0.3 4.5 3.5 16.1 39.3 36.3 Table 4 Goal scoring performance. Result Number Missed 1 Post/bar 2 Defended 5 Goal 7 Total 15 Table 5 Average time spent by players in different roles (in %) and respective standard deviation. Role % Time RoleStriker 10.4±5.2 RoleSupporter 45.2±10.0 RoleToucher 5.9±4.1 RoleReplacer 5.6±4.6 RoleBarrier 28.4±6.5 RoleParking 4.4±6.4 Table 6 Average time spent by players in different roles (in %) for different game states. Role Open play Set piece for Set piece against RoleStriker 24.3 0.3 0.4 RoleSupporter 75.3 51.5 0.3 RoleToucher 0.4 23.7 0.0 RoleReplacer 0.0 24.5 0.0 RoleBarrier 0.0 0.0 99.3 Table 7 Average time (±standard deviation) spent by players running different behaviors. Behavior % Time (any role) % Time (striker) bMove 4.9±3.0 43.7±4.4 bMoveToAbs 74.7±12.6 25.3±4.7 bDribble 1.4±1.2 13.4±4.5 bKick 1.8±1.5 14.6±7.7 bCatchBall 0.2±0.3 Table 8 Measures related to ball possession (average±standard deviation). Average minimum distance to the ball (m) 1.25±0.33 Time with ball visible (%) 91.7±3.5 Time with ball engaged (%) 9.8±4.7 Table 9 Set-piece performance in the RoboCup’2008 final game. Set piece # Occurrences # Correct Kick-off 2 2 Free kick 1 1 Throw in 6 5 Goal kick 10 8 Corner kick 2 2 Total 21 18 Table 10 Goal scoring performance in set piece situations. Set piece # Occurrences # Success Kick-off 2 2 Free kick 1 0 Throw in 3 2 Total 6 4 Table 11 Set-piece performance in the RoboCup’2009 semi-final game. Set piece # Occurrences # Correct Kick-off 3 3 Free kick 6 4 Throw in 13 11 Goal kick 2 1 Corner kick 1 1 Total 25 20 Table 12 Outcomes of set pieces correctly executed in the RoboCup’2009 semi-final game. Outcome # Occurrences % Receiver blocked by opponent 12 60 Ball off the field through goal line 3 15 Ball hits goal framework 2 10 Goalkeeper defense 2 10 Receiver dribbling with ball 1 5 Total 20 100 Table 13 RoboCup’2008 competition results. # Games # Goals scored # Goals suffered # Points Round-robin 1 5 41 2 15 Round-robin 2 4 16 3 9 Round-robin 3 2 5 2 3 Semi-final 1 4 3 3 Final 1 7 1 3 Total 13 73 11 33 Table 14 RoboCup’2009 competition results. # Games # Goals scored # Goals suffered # Points Round-robin 1 6 38 6 15 Round-robin 2 4 23 2 12 Round-robin 3 2 7 2 6 Semi-final 1 0 2 0 3rd Place 1 3 1 3 Total 14 71 13 36 Robot team coordination using dynamic role and positioning assignment and role based setplays Nuno Lau ⁎ Luis Seabra Lopes Gustavo Corrente Nelson Filipe Ricardo Sequeira Instituto de Engenharia Electrónica e Telemática de Aveiro (IEETA), Dep. Electrónica Telecomunicações e Informática (DETI), Universidade de Aveiro, Portugal ⁎ Corresponding author. The coordination methodologies of CAMBADA, a robotic soccer team designed to participate in the RoboCup Middle-Size League (MSL), are presented in this paper. The approach, which relies on information sharing and integration within the team, is based on formations, flexible positionings and dynamic role and positioning assignment. Role assignment is carried out locally on each robot to increase its reactivity. Positioning assignment is carried out at a lower frequency by a coach agent following a new priority-based algorithm that maintains a competitive formation, covering the most important positionings when malfunctions lead to a reduction of the team size. Coordinated procedures for passing and setplays have also been implemented. With this design, CAMBADA reached the 1st place in RoboCup’2008 and the 3rd place in RoboCup’2009. Competition results and performance measures computed from logs and videos of real competition games are presented and discussed. Keywords Multi-robot team coordination Strategic positioning Dynamic role assignment Coordinated procedures 1 Introduction As robots become increasingly available in different areas of human activity, researchers are naturally prompted to investigate how robots can cooperate with each other in order to perform different tasks. Moreover, progress in wireless communication technologies enables information sharing and explicit coordination between robots. These are basic capabilities needed to support sophisticated cooperation and coordination algorithms. Given this increasing availability of robots and communication technologies, multi-robot systems have, in the last two decades, been receiving more and more attention from researchers [1–3]. Interest on multi-robot systems is further justified by the advantages they offer with respect to single robots. First, some tasks are simply too difficult or impossible to be carried out by a single robot. In other cases, by providing a larger work force, multi-robot systems can carry out tasks faster. Multi-robot systems also facilitate scalability, as larger problems can often be solved by adding more robots to the team. Finally, through their inherent redundancy, multi-robot systems offer robustness, as they may still work when a team member is damaged or malfunctioning. These advantages make multi-robot systems useful in a variety of domains, such as exploration of unknown or changing environments [4–6] (including such diverse applications as ecological monitoring, rescue, de-mining or planetary exploration), mapping [7], foraging [8], transportation [9], manufacturing [10], intrusion detection and patrolling [11,12], or even entertainment [13]. The development of multi-robot systems raises many new research issues, not found in isolated robots. These new issues are concerned with how the individual robots can coordinate their actions to carry out the assigned tasks as efficiently as possible. Among other issues, the following can be mentioned: How are different sub-tasks assigned to different robots [14,8,15]? How can different roles be assigned to different robots [16–18]? If robots need to move in formation, how can the formation be controlled [2,19,20]? How can multi-robot plans be generated and/or executed [21,22]? Which information should robots exchange in order to enable coordination [23,24]? How can multi-robot systems be debugged [25,26]? The authors have been addressing several of these issues in the robotic soccer domain, currently a popular scenario and application for research in multi-robot systems. In particular, the authors contributed to the development of CAMBADA, a RoboCup Middle-Size League (MSL) team (Fig. 1 ). The MSL is one of the most challenging leagues in RoboCup. Robotic players must be completely autonomous and must play in a field of 12m×18m [27]. Teams are composed of at most five robots with a maximum height of 80cm. Human interference is allowed only for removing malfunctioning robots and re-entering robots in the game. Building a team for the MSL is a very challenging task, both at the hardware and software levels. To be competitive, robots must be robust, fast and possess a comprehensive set of sensors. At the software level they must have an efficient set of low-level skills and must coordinate themselves to act as a team. Research conducted within CAMBADA has led to developments concerning hardware [28], computational and communications infrastructure [29–31], vision system [32,33], monitoring/debugging [25] and high-level deliberation and coordination [24,34]. This paper focuses on the last aspect, providing a detailed and up-to-date account of the currently used algorithms and their performance. The complexity inherent to the MSL and, in particular, the difficulty of developing robots with robust sensorimotor capabilities and informative perception capabilities explains why most teams have implemented relatively simple coordination capabilities. The more advanced teams achieve coordination through the assignment of different roles to the robots [35,36,18]. Typically there is, at least, an attacker, a defender, a supporter and a goalie. As perception and sensorimotor capabilities become more sophisticated it will be possible to develop more sophisticated coordination algorithms. This trend is pushed further by the increase in team size (number of robots) as well as field size. A natural source of inspiration is the RoboCup Soccer Simulation League, where teams have been using coordination layers with strategy, tactics and formations [37,16], coordination graphs [38] and reinforcement learning [39,40]. CAMBADA participated in several national and international competitions, including RoboCup world championships (5th place in 2007, 1st place in 2008, 3rd pace in 2009) and the Portuguese Open Robotics Festival (3rd place in 2006, 1st place in 2007, 2008 and 2009). The excellent results obtained in RoboCup’2008 and RoboCup’2009 are largely due to the developed coordination methodologies, as the CAMBADA robots are among the slowest in the international competitions. This paper is organized as follows: Section 2 presents the hardware and software architectures of CAMBADA players and provides details on the main software components involved in individual decisions of the players. Section 3 describes how players share information with teammates and how they integrate shared information. Sections 4 and 5 describe the adopted coordination methodologies. Section 6 presents and discusses competition results and various performance measures. Section 7 concludes the paper. 2 Player architecture CAMBADA robots (Fig. 1) were designed and completely built at the University of Aveiro. Each robot fits into a cylindrical envelope with 485mm in diameter. The mechanical structure of the players is layered and modular. Each layer can easily be replaced. The components in the lower layer, namely motors, wheels, batteries and an electromechanical kicker, are attached to an aluminum plate placed 8cm above the floor. The second layer contains the control electronics. The third layer contains a laptop computer, at 22.5cm from the floor, a catadioptric omnidirectional vision system, a frontal vision system (single camera) and an electronic compass, all close to the maximum height of 80cm. The players are capable of holonomic motion, based on three omnidirectional roller wheels. With the current motion system, the robots can move at a maximum speed of 2.0m/s. As mentioned, this is less than in many of the other MSL teams, which can currently move at speeds typically between 2.5 and 4.0m/s (e.g. [41–44]). The mentioned vision system allows detecting objects, the ball, players, and field lines on a radius of 5m around each player. The frontal camera allows detecting the ball further away but is currently not used due to software stability problems encountered when using both cameras simultaneously. Each player also carries encoders, battery status sensors and, for detecting if the ball is kickable, an infra-red presence sensor. The computational system in each robot is a set of processing nodes (several small microcontrollers for basic perception and sensorimotor control plus a laptop for high-level deliberation) connected through a Controller Area Network (CAN). All communications within the team are based on the standard wireless LAN protocol IEEE 802.11x profiting from large availability of complying equipment. The team receives referees instructions through a wired LAN TCP link. On the main processing node (laptop), CAMBADA players run several software processes that execute different activities, such as image acquisition, image analysis, integration/deliberation and communication with the low-level modules (Fig. 2 ). The order and schedule of activation of these processes is performed by a so-called process manager (Pman [31]). Pman stores the characteristics of each process to activate and allows the activation of recurrent tasks, settling phase control (through the definition of temporal offsets), precedence restrictions, priorities, etc. The Pman services allow changes in the temporal characteristics of the process schedule during run-time. The top-level processing loop starts by integrating perception information gathered locally by the player. This includes information coming from the vision processes, odometry information coming from the holonomic base, compass information and ball presence information. All this information is stored in a shared data structure called Real-Time Data Base (RTDB) [29]. The RTDB has a local area, shared only among local processes, and a global area, where players share their world models to the other players. The global area is transparently updated and replicated in all players in real-time. Every 100ms the shared area of the RTDB of each robot (and of the coach) is communicated to the other robots using UDP multicast. Sending times are chosen using an adaptive TDMA algorithm that tries to avoid collisions among packets [45]. Selflocalization uses a sensor fusion engine based on the publicly available engine described in [46]. By integrating information from field line detection, this engine produces self position estimates with a high level of confidence. Compass information is used to resolve ambiguities and detect self-localization errors. The final fusion step is to integrate local information with information shared by teammates. After this integration, part of the world state is written to the global area of the RTDB. Deliberation in CAMBADA considerably relies on the concepts of role and behavior. Behaviors are the basic sensorimotor skills of the robot, like moving to a specific position or kicking the ball. The set of behaviors that are implemented in the CAMBADA agent are adapted to its catadioptric omnidirectional vision and holonomic driving systems. The combination of these technologies enhances the set of possible behaviors when compared to a differential drive robot or to an holonomic drive robot with a limited field of view. In brief, the current set of behaviors is the following: bMove uses two symbolic parameters: the target position where to move; and the position which the CAMBADA player should be facing in its path to the target. The symbols used are OBall, TheirGoal and OurGoal. This behavior may activate the functions of avoiding obstacles and avoiding the ball (used during the game repositions to avoid collisions with the ball). bMoveToAbs is another moving behavior; it allows the movement of the player to an absolute position in the game field, and also allows the player to face any given position. Obstacle avoidance is also included. bPassiveInter moves the player to the closest point in the ball trajectory and waits there for the ball. bDribble is used to dribble the ball towards a given relative player direction. bCatchBall is used to receive a pass. The player aligns itself with the ball path and, when the ball is close, moves backwards to soften the impact and more easily engage the ball. bKick is used to kick the ball accurately to one 3D position, either for shooting to goal or passing to a teammate. Preparing for the kick involves determining the kick direction and power. Polynomial functions, whose coefficients were determined by experimentation, are used to compute kick power based on distance to target. Different functions are used according to the expected number of ball bounces, given the distance. bGoalieDefend is the main behavior of the goalie. Roles select the active behavior at each time step. During open play, the CAMBADA agents use only three roles: RoleGoalie, RoleSupporter and RoleStriker. The RoleGoalie is activated for the goalkeeper. Further details about the developed roles and respective coordination mechanisms will be presented in Sections 4 and 5. Another important component of the deliberation process in CAMBADA is based on a coach agent that runs in an external computer. The coach communicates with the robot agents using the RTDB. This agent is used to define the positionings of the robots inside the current formation, as will be explained in Section 4.3. 3 Information sharing and integration Sharing perceptional information in a team can improve the accuracy of world models and, indirectly, the team coordination [23]. Therefore, information sharing and integration is one of the key aspects in multi-robot teams. In CAMBADA, each robot uses the information shared by the other robots, obtained through the RTDB, to improve its knowledge about the current positions and velocities of the other robots and of the ball. It is very important for our coordination model that each robot keeps an accurate estimate of the absolute position of the ball, its own position and its teammates positions. The role assignment algorithm is based on the absolute positions of the ball, the robot and its teammates. The teammates positions are not obtained through the vision system. They are obtained from the teammates themselves through the RTDB. Each agent communicates its own absolute position and velocity to all teammates as well as its ball information (position, velocity, visibility and engagement in robot), current role and current behavior. Multi-robot ball position integration has been used in the Middle-Size League by several teams [35,47]. In CAMBADA, multi-robot ball position integration is used to maintain an updated estimate of the ball position, when the vision subsystem cannot detect the ball, and to validate robot’s own ball position estimate, when the vision subsystem detects a ball. Currently, a simple integration algorithm is used. When the agent does not see the ball, it analyzes the ball information of playing teammates. The analysis consists in the calculation of the mean and standard deviation of the ball positions, then discarding the values considered as outliers of ball position, and finally using the ball information of the teammate that has a shorter distance to the ball. To determine if the agent sees a fake ball, i.e., to validate the robot’s own perception, we use a similar algorithm. Communication is also used to convey the coordination status of each robot allowing robots to detect uncoordinated behavior (e.g., several robots with the same exclusive role) and to correct this situation reinforcing the reliability of coordination algorithms. The communication between the base station and the robots informs the team of the active play mode (decided by the referee). During development, the base station can be used to control several robotic agent characteristics like fixed roles, manually activated self-positioning, etc, all managed through the RTDB. 4 Positionings and roles in open play For open play, CAMBADA uses an implicit coordination model based on notions like strategic positioning, role and formation. These notions and related algorithms have been introduced and/or extensively explored in the RoboCup Soccer Simulation League [16,17]. The concept of formation adopted in CAMBADA is mostly the same as the one presented in [16]. The model that was used to define the strategic positions of the formation members for each situation is derived from [17]. However, these and other methods developed in the Simulation League assume that team size is constant. In the MSL we must deal with incomplete formations, resulting from referee orders or malfunctioning robots. 4.1 Formations and strategic positionings A formation defines a movement model for the robotic players. Formations are sets of strategic positionings, where each positioning is a movement model for a specific player. The assignment of players to specific positionings is dynamic, and it is done according to some rules described below. Each positioning is specified by three elements: Home position which is the target position of the player when the ball is at the center of the field. Region of the field where the player can move. Ball attraction parameters used to compute the target position of the player in each moment based on the current ball position. All these items of information are given in a strategy configuration file. Using different home positions and attraction parameters for the positionings allows a simple definition of defensive, wing, midfielder and attack strategic movement models. Fig. 3 shows the formation of the team used in RoboCup’2008 for several ball positions. The definition of a formation in terms of strategic positionings was introduced in the SBSP model [17] for the Soccer Simulation League. This model also introduced specific notions of tactic and strategy, which are currently not used in CAMBADA. 4.2 Roles in open play Each role has an associated hierarchical finite-state machine that decides the behavior to be used for each of its states based on the current world state [24]. Using an hierarchical finite state machine enables the modeling of transitions between states with different levels of importance. Certain super-states (states that include other states) may be exited by the activation of one of these high priority conditions, without having to consider an identical transition from each of the their substates. As mentioned before, the CAMBADA players use only three roles in play-on mode: RoleGoalie, activated for the goalkeeper, RoleSupporter and RoleStriker. RoleStriker is an “active player” role. It tries to catch the ball and score goals. The striker activates several behaviors that try to engage the ball (bMove, bMoveToAbs), get into the opponent’s side avoiding obstacles (bDribble) and shoot to the goal (bKick). The bDribble behavior can perform 180degrees turns while keeping possession of the ball. In a consistent role assignment, only one player at a time takes on the role of striker. The striker is helped by other teammates which take on RoleSupporter [24]. Supporters maintain their target positions as determined by their current positioning assignments and the current ball position. To this end, they use essentially the bMoveToAbs behavior. As a result, supporters accompany the striker as it plays along the field, without interfering. In case the ball is captured by the opponent, some supporter hopefully will be in a good position to become the new striker. Occasionally, supporters can take a more active behavior. This happens when the striker cannot progress with the ball towards the opponent goal and, instead, the ball remains behind the striker for more than some pre-defined time (2s in the adopted configuration). In this case, the closest supporter to the ball also approaches the ball, acting as “backup striker”. 4.3 Role and positioning assignment Previous work on role assignment algorithms for robotic soccer is based on the concept of role exchange, measuring the utility of that exchange to decide its activation [37,16]. However, in MSL the number of available players varies as a result of several common situations, namely hardware and software malfunctions and referee orders. As the number of robots is small and varies a lot, the usefulness of role exchanges is reduced. The algorithms used in CAMBADA for role and positioning assignment are based on considering different priorities for the different roles and positionings, so that the most important ones are always covered [34]. In CAMBADA, the algorithms for role assignment and positioning assignment are separated and run at different rates. Role assignment is decided locally by each robot, every cycle (40ms), based on its current world model. The positioning assignment is decided by the coach and communicated to the agents, through the RTDB, every second. We believe this is an improvement over previous approaches [37,16], in which role and positioning assignment were integrated. The adopted separation provides a very reactive role assignment to cope with the high dynamics of MSL games, and a more stable and consistent positioning assignment. In case the coach fails, robots are prepared to run the positioning assignment algorithm locally. During open play, from the robots that see the ball, the one that estimates having the closest distance to the ball takes on RoleStriker, and all others, except the goalie, take on RoleSupporter. Because world models are not identical, in some situations more than one robot may be assigned RoleStriker, but the results provided in Section 6 show that this situation is very rare. The positioning assignment algorithm decides the place in the formation that each robot should occupy (see Fig. 4 ). Consider a formation with N positionings and a team of K ⩽ N available field players (not counting the goalkeeper which has a fixed role). To assign the positioning to each robot, the distances of each of the robots to each of the target positions are calculated. Then the closest robot to the highest priority strategic positioning is assigned to that positioning, which is in turn the closest to the ball. From the remaining K −1 robots, the closest to the defensive positioning (second highest priority) is assigned to this positioning, then the closest to the third level priority positioning is assigned next and the algorithm continues until all active robots have positionings assigned. The robot assigned to the highest priority positioning will in most cases be locally assigned to RoleStriker and will not move to that positioning, but will position itself close to the ball assuring the stability of the assignment. This algorithm results in the RoleStriker having top priority, followed by the defensive positioning, followed by the other supporter positionings. 5 Coordinated procedures Coordinated procedures are short plans executed by at least two robots. These plans in some cases involve communication resulting in explicit coordination. In the case of CAMBADA coordinated procedures are used for passes and set plays. 5.1 Passes Passing is a coordinated behavior involving two players, in which one kicks the ball towards the other, so that the other can continue with the ball. Until now, MSL teams have shown limited success in implementing and demonstrating passes. In RoboCup’2004, some teams had already implemented passes, but the functionality was not robust enough to actually be useful in games [13,48]. The CoPS and Tribots team also support pass play [49,40]. Two player roles have recently been developed for coordinated passes in the CAMBADA team. In the general case, the player running RoleStriker may decide to take on RolePasser, choosing the player to receive the ball. After being notified, the second player takes on the RoleReceiver. These roles have not been used yet for open play in international competition games, but they have been demonstrated in RoboCup’2008 MSL Free Technical Challenge and a similar mechanism has been used for corner kicks (see below). In the free challenge, two robots alternately took on the roles of passer and receiver until one of them was in a position to score a goal (Fig. 5 ). The sequence of actions on both players is described in Table 1 . They start from their own side of the field and, after each pass, the passer moves forward in the field, then becoming the receiver of the next pass. The coordination between passer and receiver is based on passing flags, one for each player, which can take the following values: READY, TRYING_TO_PASS and BALL_PASSED. In the case of a normal game, another pass coordination variable would identify the receiver. 5.2 Set plays Another methodology implemented in CAMBADA is the use of coordinated procedures for set plays, i.e. situations when the ball is introduced in open play after a stoppage, such as kick-off, throw-in, corner kick, free kick and goal kick. Set play procedures define a sequence of behaviors for several robots in a coordinated way. For that purpose, the involved players take on specific roles. This role-based implementation of set plays not only was easy to integrate within the previous agent architecture, but also facilitated the test and tune of different possibilities allowing for very efficient final implementations. RoleToucher and RoleReplacer are used to overcome the 2008 MSL indirect rule in the case of indirect set pieces against the opponent [27]. The purpose of RoleToucher is to touch the ball and leave it to the RoleReplacer player. The replacer handles the ball only after it has been touched by the toucher. This scheme allows the replacer to score a direct goal if the opportunity arises. Two toucher–replacer procedures are implemented. In the case of corner kicks, the toucher passes the ball to the replacer and the replacer continues with the ball (see pseudo-code in Fig. 6 ). The passing algorithm is as explained above. Another toucher–replacer procedure is used in the case of throw-in, goal kick and free kick set plays. Here, the toucher approaches and touches the ball pushing it towards the replacer until the ball is engaged by the replacer, then withdraws leaving the ball to the replacer. The replacer also moves towards the ball, grabs it, waits that the toucher moves away and then shoots to the opponent goal. It should be noted that both the toucher and the replacer position themselves on the shoot line, so that, as soon as the toucher moves away, the replacer is ready to shoot. For the kick-off, a similar procedure is followed, but without reference to the shoot line, since the involved robots must be in their own side of the field. This scheme has been updated in 2009 to comply with the new rule that only allows one robot of the team performing the set piece (and none from the opponent team) within the 1m circle around the ball and obliges the ball to be immediately kicked and to roll free on the field for at least 0.5m. In 2009, the RoleReplacer passes the ball to one of, possibly multiple, robots acting as RoleReceiver. Before passing, an evaluation of the passing corridors is performed jointly by the Replacer and all Receivers and results are shared through the RTDB. It is the responsibility of the Replacer to choose the destination of the pass, which is also communicated through the RTDB before pass execution. Finally, in the case of set pieces against CAMBADA, RoleBarrier is used to protect the goal from a direct shoot. The line connecting the ball to the own goal defines the barrier positions. One player places itself on this line, as close to the ball as it is allowed. Two players place themselves near the penalty area. One player is placed near the ball, 45degrees from the mentioned line, so that it can observe the ball coming into play and report that to teammates. Finally, one player positions itself in such a way that it can oppose to the progression of the ball through the closest side of the field. The placement of players is illustrated in Fig. 7 . The assignment of the RoleBarrier, RoleReceiver, RoleReplacer and RoleToucher roles is executed by sorting the agents according to their perceived distances to the ball and selecting the closest ones, up to the maximum number of agents in each role. When selecting a role like the RoleReplacer, which is exclusive, the agent looks at the other teammates role decisions and if it finds a RoleReplacer with a lower uniform number it will never select that role. A similar approach is performed for the other exclusive roles. This assignment is always performed locally by each robot. Robots that are not assigned setplay specific roles are assigned the supporter role with a positioning that does not interfere with the setplay. As soon as the setplay finishes, either because of a timeout or because all the setplay actions have been performed with success, the robots assigned with specific setplay roles return to an open play role using the role assignment algorithm previously described. 6 Performance evaluation The CAMBADA team participated and won the MSL world championship in RoboCup’2008 (Suzhou, China, July 2008) and achieved a distinct 3rd place in RoboCup’2009 (Graz, Austria, July 2009). Most performance evaluation measures presented in this Section were obtained by analyzing log files and videos of games in the RoboCup championships. The logs are created by the coach agent. At 1s intervals, the coach takes a snapshot of relevant information retrieved from each robot, including current role, strategic positioning, behavior, self position and ball position. A software tool was developed to analyze game logs and extract relevant evaluation measures. Most of the information presented below was extracted from the RoboCup’2008 logs. As the CAMBADA team made it to the final, it was scheduled to play 13 games. One of them was not played due to absence of the opponent. For two other games, the log files were lost. Thus, the results presented below are extracted from log files of the remaining 10 games. Some additional results were extracted from the semi-final game in RoboCup’2009. Finally, RoboCup’2008 and RoboCup’2009 competition results will also be presented. 6.1 General game features Three main classes of game states are open play, set piece against CAMBADA and set piece for CAMBADA. Table 2 shows the respective time distribution in percentage of full game duration, computed over the 10 game logs mentioned above. The time spent in set pieces, considerably higher than what might be expected, results from the dynamics in MSL games. In fact, robots fast moving capabilities (up to 4m/s) and powerful ball kicking capabilities are not accompanied by sufficiently effective ball control capabilities, thus causing various types of set pieces. The time spent in set pieces justifies the investment in the development of the replacer/toucher combination in CAMBADA. A high efficiency rate in set pieces makes a real difference in the final team performance. Another common feature in MSL teams is that, due to reliability issues, the number of playing field robots is often less than the maximum of five. Table 3 shows the average percentage of game time (in the 10 mentioned game logs) for different numbers of playing field robots in the CAMBADA team. The average number of running field robots for the CAMBADA team was 3.98. This reveals the reliability problems that were experienced mostly in the beginning of the championship. These were solved to some extent during the championship and reliability improved in later games. In the final game the average number of running field robots was 4.33. Capabilities for shooting to goal, although not directly based on coordination methodologies, are essential for a team’s success. Fig. 8 shows the location from where the ball was shot to goal in the RoboCup’2008 MSL final (CAMBADA-TechUnited). CAMBADA showed good scoring abilities in the competition. Table 4 shows the results of all the shots made in the final game within 9m of the opponent goal (for larger distances, a shot does not have enough power to pose a real threat to the opponent team). A total of 15 shots were made, of which 1 was missed, 1 hit the post and another hit the bar. The remaining 12 hit the intended target within the goal. This gives an accuracy rating of 80%. From all the 15 shots made, 7 resulted in a goal being scored. This gives a goal scoring success rate (within 9m) of 46.7%. This high success rate is the result of accurate ball placing when kicking. In 5 of the 7 scored goals, the goalkeeper was actually well positioned and in the path of the ball. However, the accurate calibration and power selection for each kick made the ball reach the opponent goal at an height slightly above 80cm which effectively caused it to go over the goalkeeper, and thus creating a shot that is very difficult to defend. 6.2 Roles and behaviors Table 5 shows the average percentage of time any given player spends in each role, with respect to the total time the player is active in each game. It can be observed that players spend a considerable amount of time (45.2%) as RoleSupporter. This is to be expected since there may be up to four players with the Supporter role in open play, while there is at most one player acting as RoleStriker. This largely explains why the RoleStriker time is approximately 1/4 of the RoleSupporter time. The small deviation from the exact 1/4 relation is explained by two main factors: first, RoleSupporter is also taken by some players during set plays for CAMBADA; and, second, the number of field robots is often less than the maximum of five, as described above. It can also be seen that more time is spent in set plays against CAMBADA (28.4%, since usually four players take the Barrier role in these situations) than in set plays against the opponent (11.5% in Toucher and Replacer roles). RoleParking moves robots outside of the field at the end of the first half and at the end of the game. A more in-depth perspective is given by Table 6 , which shows the role time distribution across the three classes of game states. It can be seen that in open play basically only RoleStriker and RoleSupporter are used. In set pieces for CAMBADA, players take the roles of RoleReplacer, RoleToucher and RoleSupporter. In set pieces against CAMBADA, all field robots act is RoleBarrier. Underlying the numbers in Table 6 is the fact, already mentioned above, that CAMBADA had an average of nearly four field players. That explains why the time spent as supporter in open play is approximately three times that of striker, and the time spent as supporter in set pieces for CAMBADA is approximately two times that of toucher or replacer. Table 7 shows the average percentage of time any given player spends running each implemented behavior. The second column of the table shows such percentages irrespective of the role taken. The third column shows the percentages of time considering only the periods in which players are acting as RoleStriker. These values highlight the specificity of RoleStriker: much less time moving to absolute positions, since the striker most of the time ignores its strategic positioning assignment; much more time in moving (to the ball), dribbling and kicking. 6.3 Coordination In the final game of RoboCup’2008 (CAMBADA-TechUnited), the ball was in the opponent’s side 73% of time, mainly in the center of the field towards the opponent’s side. While this certainly results from the combination of several factors, the CAMBADA’s coordination approach has certainly helped in achieving such high field dominance. Some measures of coordination performance have been extracted. According to the logs, players change roles 2.02±1.02 times per minute. As role assignment is distributed (implicit coordination), it occasionally happens that two players take on RoleStriker at the same time. On average, all inconsistencies in the assignment of the Striker role have a combined total duration of 20.9±27.4s in a game (∼30min), i.e., the mean inconsistency time is about 1.2% of game duration. The high standard deviation results mainly from one game in which, due to magnetic interference, localization errors were higher than normal. In that game, role inconsistencies occurred 45 times for a combined total of 101s. Concerning strategic positionings, relevant mainly to supporters, the average distance of the player to its target position is 1.38±0.48m. The strategic positioning assignment for each player is changed on average 9.83±2.23 times per minute. As the CAMBADA players do not track the positions and actions of the opponent players, it is not possible to compute an exact measure of ball possession. However, the game logs enable to compute related measures, as shown in Table 8 . The closest player to the ball is at an average distance of 1.2m from the ball (the field is 18m×12 m). The ball is perceived by at least one robot of the CAMBADA team 91.7% of the time. The ball is engaged in a robots grabber device 9.8% of the time. Some additional analysis was carried out based on the logs of the RoboCup’2008 final game. Table 9 provides information on set pieces, identifying the total number of times each set piece was executed as well as the number of times it was correctly executed. In RoboCup’2008 final game there were 21 set pieces, of which 18 were correctly executed (85.7%). The failed throw-in occurred due to magnetic interference in one area of the field, causing the robot to mislocalize itself. The two missed goal kicks occurred because the movement of the robot acting as RoleToucher, while pushing the ball towards the Replacer, was not accurately aligned and did not succeed in delivering the ball to the Replacer. This can be due to some small localization errors experienced near the goal kick marker. Table 10 provides information on goal scoring success in set piece situations in which the set piece procedure was correctly executed and the distance to the opponent goal was less than 9m. In the six set pieces for CAMBADA, carried out under these conditions, four resulted in a goal being scored. This is a very good success rate. It should be noted that from the seven goals scored in this game, four resulted from set pieces. This shows the importance of having accurate, reliable and swift set pieces in MSL games. These high values were observed consistently throughout the whole championship. They were crucial in the teams success, proving to be a powerful asset for achieving victories. An identical analysis was performed based on the logs of the RoboCup’2009 semi-final game, in which CAMBADA played against the same opponent of the 2008 final: Tech United. Table 11 shows the obtained results. In RoboCup’2009 the number of set pieces in the semi-final (against Tech United) was 25, of which 20 were correctly executed (80%). It should be noticed that 2009 rules make it much more difficult to control the ball during the execution of the coordinated procedure following a set piece. While in 2008 the ball moved very little during the execution of the coordinated procedure (robots moved to touch the ball and then the replacer shoots to goal), the 2009 MSL rules make it obligatory for the ball to roll free for at least 0.5m. This gives more time for the opponent team to react and forces the interception of a moving ball, a capability that is still not perfectly performed with the current robots. The outcomes of the 20 set pieces that were correctly executed in that semi-final can be observed in Table 12 . 6.4 Competition results Tables 13 and 14 present the competition results of CAMBADA in RoboCup’2008 and RoboCup’2009. In 2008, the team won 11 out of 13 games, scoring a total of 73 goals and suffering only 11 goals. The participation in 2009 also resulted in the team winning 12 of the 14 played games, scoring a total of 71 goals, far more than any other team, and suffering 13 goals. 7 Conclusion This paper presented and evaluated the coordination methodologies of CAMBADA, one of the top teams in RoboCup MSL world championships (champion in RoboCup’2008, 3rd place winner in RoboCup’2009). During open play, an implicit coordination approach, based on formations, flexible positionings and dynamic role and positioning assignment, is used. The positioning of the team adapts to the external game conditions and maintains a strong defense and a good backup to the striker role. This is achieved through priority-based positioning/role assignment algorithms that maintain a competitive formation even when robot malfunctions decrease the number of field players. The positioning assignment algorithm is focused on covering the most important roles/positionings and differs substantially from previously presented algorithms that were based on role exchange. The success of the approach can be seen, not only from the competition results, but also from the detailed analysis of game logs and videos, as presented in the paper. More importantly, and this is one of the clearest evidences, the good competition results were obtained despite the fact that CAMBADA robots clearly move at low speed (2m/s), when compared to most of the main competitors which move faster (2.5–4m/s). The development of pre-defined role-based set plays proved to be very efficient both during the development phase, and during their execution in games. More than half of the 73 scored goals are direct result of these set plays. One of the most significant aspects of this work is the integration of the described coordination methodologies in a complex multi-robot system and their validation in the challenging RoboCup MSL competition scenario. This contrasts with many other approaches described in the literature, which are often validated in more controlled robotic environments, if not in simulation. Acknowledgments The CAMBADA team was funded by the Portuguese Government – FCT-POSI/ROBO/43908/2002 (CAMBADA) and currently FCT, PTDC/EIA/70695/2006 (ACORD). We would also like to thank the rest of the CAMBADA team for an excellent work environment. References [1] F. Noreils Toward a robot architecture integrating cooperation between mobile robots: application to indoor environment Int J Robotics Res 12 1993 79 98 [2] P. Wang Navigation strategies for multiple autonomous mobile robots moving in formation J Robotic Syst 8 2 1991 177 195 [3] Balch T, Parker L. Robot teams: from diversity to polymorphism. Natick (Massachusetts): A K Peters Ltd.; 2002. [4] Low KH, Gordon GJ, et al. Adaptive sampling for multi-robot wide-area exploration. In: Proc IEEE int conf on robotics and automation, Rome, Italy; 2007. p. 755–60. [5] Yamauchi B. Frontier-based exploration using multiple robots. In: Proc second int conf on autonomous agents; 1998. p. 47–53. [6] W. Burgard M. Moors C. Stachniss F. Schneider Coordinated multi-robot exploration IEEE Trans Robotics Autom 21 3 2005 376 386 [7] S. Carpin Fast and accurate map merging for multi-robot systems Auton Robots 25 3 2008 305 316 [8] Dahl T, Mataric M, Sukhatme G. Emergent robot differentiation for distributed multi-robot task allocation. In: Proc 7th international symposium on distributed autonomous robotic systems, Toulouse, France; 2004. p. 201–10. [9] Figueiredo L, Jesus I, Machado J, Ferreira J, de Carvalho JM. Towards the development of intelligent transportation systems. In: Proc IEEE intellig transport syst; 2001. p. 1206–11. [10] Tewolde G, Wu G, et al. Distributed multi-robot work load partition in manufacturing automation. In: Proc IEEE int conf on autom science and eng, Arlington, VA, USA, 2008. p. 504–9. [11] Fagiolini A, Pellinacci M, et al. Consensus-based distributed intrusion detection for multi-robot systems. In: Proc IEEE int conf robotics and automation, Pasadena, CA, USA; 2008. p. 120–7. [12] Agmon N, Kraus S, Kaminka G. Multi-robot perimeter patrol in adversarial settings. In: Proc IEEE int conf on robotics and automation, Pasadena CA, USA; 2008. p. 2339–45. [13] Lima P, Custdio L, Akin I, Jacoff A, Kraezschmar G, Ng BK, et al. RoboCup 2004 competitions and symposium: a small kick for robots a giant score for science. AI Mag 2005;6(2):36–61. [14] B.P. Gerkey M. Mataric A formal analysis and taxonomy of task allocation in multi-robot systems Int J Robotics Res 23 9 2004 939 954 [15] Michael N, Zavlanos M, et al. Distributed multi-robot task assignment and formation control. In: Proc IEEE int conf on robotics and automation, Pasadena CA; 2008, pp. 128–33. [16] P. Stone M. Veloso Task decomposition, dynamic role assignment and low bandwidth communication for real time strategic teamwork Artif Intell 110 2 1999 241 273 [17] L. Reis N. Lau E. Oliveira Situation based strategic positioning for coordinating a team of homogeneous agents M. Hannenbauer Balancing reactivity and social deliberation in multiagent sytems: from RoboCup to real word applications LNAI vol. 2103 2001 Springer-Verlag 175 197 [18] E. Pagello A. DGAngelo E. Menegatti Cooperation issues and distributed sensing for multirobot systems Proc IEEE 94 7 2006 1370 1383 [19] T. Balch R. Arkin Behavior-based formation control for multirobot teams IEEE Trans Robotics Autom 14 6 1998 926 939 [20] Cheah C, Hou S, Slotine J. Region following formation control for multi-robot systems. In: Proc IEEE int conf on robotics and automation, Pasadena CA, USA; 2008. p. 3796–801. [21] Joyeux S, Alami R, Lacroix S. A plan manager for multi-robot systems. In: Proc 6th int conf on field and service robotics, Chamonix, France; 2007. p. 443–52. [22] Lesser V, Decker K, Wagner T, Carver N, Garvey A, Horling B, et al. Evolution of the GPGP/TAEMS domain-independent coordination framework. In: Autonomous agents and multi-agent systems, vol. 9(1); 2004. p. 87–143. [23] Dietl M, Gutmann J-S, Nebel B. Cooperative sensing in dynamic environments. In: Proc IEEE/RSJ int conf on intelligent robots and systems (IROS’01), Maui, Hawaii; 2001. [24] Lau N, Seabra Lopes L, Corrente G. Cambada: information sharing and team coordination. In: Autonomous robot systems and competitions: proc of the 8th conference. Aveiro (Portugal): Universidade de Aveiro; 2008. p. 27–32. [25] Figueiredo J, Lau N, Pereira A. Multi-agent debugging and monitoring framework. In: Proc first IFAC workshop on multivehicle systems (MVS’06), Brazil; 2006. [26] Rosa MD, Campbell J, et al. Distributed watchpoints: debugging large multi-robot systems. In: Proc IEEE int conf on robotics and automation, Rome, Italy; 2007. p. 3723–9. [27] M.T.C. 1997–2009. Middle size robot league rules and regulations for 2009. Version – 13.1 20081212 [December 2008]. [28] Azevedo J, Cunha M, Almeida L. Hierarchical distributed architectures for autonomous mobile robots: a case study. In: Proc ETFA2007 – 12th IEEE conference on emerging technologies and factory automation, Patras, Greece; 2007. p. 973–80. [29] Almeida L, Santos F, Facchinetti T, Pedreira P, Silva V, Seabra Lopes L. Coordinating distributed autonomous agents with a real-time database: the CAMBADA project. In: C A, et al., editors. Computer and information sciences – ISCIS 2004: proc 19th international symposium, LNCS, vol. 3280, Antalya, Turkey; 2004. p. 876–86. [30] Pedreiras P, Teixeira F, Ferreira N, Almeida L, Pinho A, Santos F. Enhancing the reactivity of the vision subsystem in autonomous mobile robots using real-time techniques. In: Noda I, A J, et al., editors. RoboCup-2005: robot soccer world cup IX, LNAI, vol. 4020. Berlin: Springer; 2006. p. 371–83. [31] P. Pedreiras L. Almeida Task management for soft real-time applications based on general purpose operating systems P. Lima Robotic soccer 2007 Itech Education and Publishing Vienna (Austria) 598 607 [32] A. Neves G. Corrente A. Pinho An omnidirectional vision system for soccer robots Progress in artificial intelligence LNCS vol. 4874 2007 Springer Berlin 499 507 [33] Cunha B, Azevedo J, Lau N, Almeida L. Obtaining the inverse distance map from a non-svp hyperbolic catadioptric robotic vision system. In: Visser, U, et al., editors. RoboCup-2007: robot soccer world cup XI, LNAI. Berlin: Springer Verlag; 2008. [34] Lau N, Seabra Lopes L, Corrente G, Filipe N. Multi-robot team coordination through roles, positionings and coordinated procedures. In: Proc of the 2009 IEEE/RSJ international conference on intelligent robots and systems – IROS 2009, St. Louis, MO, USA; 2009. [35] Weigel T, Auerbach M, et al. Cs freiburg: doing the right thing in a group. In: P S., et al., editors. RoboCup-2000: robot soccer world cup IV, LNAI, vol. 2019. Springer-Verlag; 2001. p. 52–63. [36] M A, et al. Creating a robot soccer team from scratch: the brainstormers tribots. In: Proc of RoboCup 2003, Padua, Italy; 2003. [37] Reis L, Lau N. FC Portugal team description: RoboCup 2000 simulation league champion. In: Stone P, et al., editors. RoboCup-2000: robot soccer world cup IV, LNCS, vol. 2019. Springer; 2001. p. 29–40. [38] J. Kok M. Spaan N. Vlassis Non-communicative multi-robot coordination in dynamic environments Robotics Auton Syst 50 2-3 2005 99 114 [39] Riedmiller M, Gabel T. On experiences in a complex and competitive gaming domain: reinforcement learning meets RoboCup. In: Proc of the 3rd IEEE symposium on computational intelligence and games (CIG 2007). Honolulu (Hawaii):IEEE Press; 2007. p. 17–23. [40] H M, et al. Making a robot learn to play soccer using reward and punishment. In: KI 2007: Advances in artificial intelligence, LNCS, vol. 4667. Springer; 2007. p. 220–34. [41] Oubbati M, Schanz M, Buchheim T, Levi P. Velocity control of an omnidirectional RoboCup player with recurrent neural networks. In: A B, et al., editors. RoboCup 2005: robot soccer world cup IX, LNAI, vol. 4020. Springer; 2006. p. 691–701. [42] Hafner R, Lange S, Lauer M, Riedmiller M. Brainstormers tribots team description; 2008. [43] Sato Y, et al. Hibikino-musashi team description paper; 2008. [44] E.T. Group. Ethercat robots win german open, May 2008. [45] Santos F, Almeida L, Seabra Lopes L, Azevedo JL, Cunha MB. Communicating among robots in the RoboCup middle-size league. In: Baltes J, Lagoudakis MG, Naruse T, Ghidary SS, editors. RoboCup 2009: robot soccer world cup XIII, LNAI, vol. 5949. Berlin/Heidelberg: Springer; 2010. p. 320–31. [46] Lauer M, Lange S, Riedmiller M. Calculating the perfect match: an efficient and accurate approach for robot self-localisation. In: Bredenfeld A, et al., editors. RoboCup 2005: robot soccer world cup IX, LNAI, vol. 4020. Springer; 2006. [47] Ferrein A, Hermanns L, Lakemeyer G. Comparing sensor fusion techniques for ball position estimation. In: A B, et al., editors. RoboCup 2005: robot soccer world cup IX, LNAI, vol. 4020. Springer; 2006. 154–65. [48] van der Vecht B, Lima P. Formulation and implementation of relational behaviours for multi-robot cooperative systems. In: RoboCup 2004: robot soccer world cup VIII, LNAI, vol. 3276. Springer; 2005. p. 516–23. [49] O. Zweigle R. Lafrenz T. Buchheim U.-P. Kppeler H. Rajaie F. Schreiber Cooperative agent behavior based on special interaction nets Intell Auton Syst 2006 651 659 "
    },
    {
        "doc_title": "Experiments with single-class support vector data descriptions as a tool for vocabulary grounding",
        "doc_scopus_id": "78649375200",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78649375200",
        "doc_date": "2010-11-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            }
        ],
        "doc_keywords": [
            "Language grounding",
            "Learning models",
            "Robotic agents",
            "Semi-automated",
            "Sigmoid kernels",
            "Support vector",
            "Support vector data description",
            "Vocabulary acquisition"
        ],
        "doc_abstract": "This paper explores support vectors as a tool for vocabulary acquisition in robots. The intention is to investigate the language grounding process at the single-word stage. A social language grounding scenario is designed, where a robotic agent is taught the names of the objects by a human instructor. The agent grounds the names of these objects by associating them with their respective sensor-based category descriptions. A system for grounding vocabulary should be incremental, adaptive and support gradual evolution. A novel learning model based on single-class support vector data descriptions (SVDD), which conforms to these requirements, is presented. For robustness and flexibility, a kernel based implementation of support vectors was realized. For this purpose, a sigmoid kernel using histogram pyramid matching has been developed. The support vectors are trained based on an original approach using genetic algorithms. The model is tested over a series of semi-automated experiments and the results are reported.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Communicating among robots in the RoboCup Middle-Size League",
        "doc_scopus_id": "77951004199",
        "doc_doi": "10.1007/978-3-642-11876-0_28",
        "doc_eid": "2-s2.0-77951004199",
        "doc_date": "2010-04-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Best practice",
            "Communication protocols",
            "Fundamental component",
            "Limited bandwidth",
            "Mobile autonomous robots",
            "Multirobots",
            "RoboCup",
            "Robotic soccer",
            "Wireless communication protocols",
            "Wireless communications",
            "Wireless medium"
        ],
        "doc_abstract": "The RoboCup Middle-Size League robotic soccer competitions pose a real cooperation problem for teams of mobile autonomous robots. In the current state-of-practice cooperation is essential to overcome the opponent team and thus a wireless communication protocol and associated middleware are now fundamental components in the multi-robots system architecture. Nevertheless, the wireless communication has relatively low reliability and limited bandwidth. Since it is shared by both teams, it is a fundamental resource that must be used parsimoniously. Curiously, to the best of our knowledge, no previous study on the effective use of the wireless medium in actual game situations was done. In this paper we show how current teams use the wireless medium and we propose a set of best practices towards a more efficient utilization. Then, we present a communication protocol and middleware that follow such best practices and have been successfully used by one particular MSL team in the past four years. © 2010 Springer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Acquiring vocabulary through human robot interaction: A learning architecture for grounding words with multiple meanings",
        "doc_scopus_id": "79960126314",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960126314",
        "doc_date": "2010-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "Language grounding",
            "Learning architectures",
            "Learning models",
            "Life long learning",
            "Nearest neighbor based classifiers",
            "Robotic agents",
            "Vocabulary acquisition"
        ],
        "doc_abstract": "This paper presents a robust methodology for grounding vocabulary in robots. A social language grounding experiment is designed, where, a human instructor teaches a robotic agent the names of the objects present in a visually shared environment. Any system for grounding vocabulary has to incorporate the properties of gradual evolution and lifelong learning. The learning model of the robot is adopted from an ongoing work on developing systems that conform to these properties. Significant modifications have been introduced to the adopted model, especially to handle words with multiple meanings. A novel classification strategy has been developed for improving the performance of each classifier for each learned category. A set of six new nearest-neighbor based classifiers have also been integrated into the agent architecture. A series of experiments were conducted to test the performance of the new model on vocabulary acquisition. The robot was shown to be robust at acquiring vocabulary and has the potential to learn a far greater number of words (with either single or multiple meanings).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-robot team coordination through roles, positionings and coordinated procedures",
        "doc_scopus_id": "76249133341",
        "doc_doi": "10.1109/IROS.2009.5354286",
        "doc_eid": "2-s2.0-76249133341",
        "doc_date": "2009-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Dynamic roles",
            "Information sharing",
            "Multi-robot teams",
            "Performance measure",
            "Priority-based",
            "RoboCup",
            "Robotic soccer team",
            "Team size"
        ],
        "doc_abstract": "The coordination methodologies of CAMBADA, a robotic soccer team designed to participate in the RoboCup middle-size league (MSL), are presented. The approach, which relies on information sharing and integration within the team, is based on formations, flexible positionings and dynamic role and positioning assignment. Role/positioning assignment follows a new priority-based algorithm that maintains a competitive formation, covering the most important roles/positionings when malfunctions lead to a reduction of the team size. Coordinated procedures for passing and setplays have also been implemented. With this design, CAMBADA reached the 1st place in the RoboCup'2008 world championship. Competition results and performance measures computed from logs and videos of real competition games are presented and discussed. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A real-time distributed software infrastructure for cooperating mobile autonomous robots",
        "doc_scopus_id": "70449370206",
        "doc_doi": null,
        "doc_eid": "2-s2.0-70449370206",
        "doc_date": "2009-11-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adaptive technique",
            "Application process",
            "Communication channel",
            "Coordinated behavior",
            "COTS technology",
            "Demining",
            "Distributed perception",
            "Distributed shared memory",
            "Distributed software",
            "In-field",
            "LINUX- operating system",
            "Local operations",
            "Mobile autonomous robots",
            "Non-blocking",
            "Open-source",
            "Real time",
            "Real-time database",
            "Refresh rate",
            "Remote data",
            "Remote state",
            "RoboCup middle-size soccer",
            "Task managers",
            "Team members",
            "Wireless communication protocols"
        ],
        "doc_abstract": "Cooperating mobile autonomous robots have been generating a growing interest in fields such as rescue, demining and security. These applications require a real time middleware and wireless communication protocol that can effecient and timely support the fusion of the distributed perception and the development of coordinated behaviors. This paper proposes an affordable middleware, based on low-cost and open-source COTS technologies, which relies on a real-time database partially replicated in all team members, containing both local and remote state variables, in a distributed shared memory style. This provides seamless access to the complete team state, with fast non-blocking local operations. The remote data is updated autonomously in the background by a WiFi-based wireless communication protocol, at an adequate refresh rate. The software infrastruture is complemented with a task manager that provides scheduling and synchronization services to the application processes on top of the Linux operating system. Such infrastructure has been successfully used for four years in one RoboCup middle-size soccer team, and it has proved to be dependable in the presence of uncontrolled spurious traffic in the communication channel, using an adaptive technique to synchronizating the robots in the team and reconfiguring the communications dynamically and automatically according to the number of currently active team members.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DarkBlade: A program that plays diplomacy",
        "doc_scopus_id": "71049195443",
        "doc_doi": "10.1007/978-3-642-04686-5_40",
        "doc_eid": "2-s2.0-71049195443",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Board position",
            "Potential field",
            "Search Algorithms",
            "Search spaces"
        ],
        "doc_abstract": "Diplomacy is a 7-player game that requires coordination between players in order to achieve victory. Its huge search space makes existing search algorithms useless. In this paper we present Darkblade, a player designed as a Multi-Agent System that uses potential fields to calculate moves and evaluate board positions. We tested our player against other recent players. Although there are some limitations, the results are promising. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Roles, positionings and set plays to coordinate a RoboCup MSL team",
        "doc_scopus_id": "71049186908",
        "doc_doi": "10.1007/978-3-642-04686-5_27",
        "doc_eid": "2-s2.0-71049186908",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Coordinated procedures",
            "Coordination model",
            "Dynamic role assignment",
            "Multi-robot team coordination",
            "Performance measure",
            "Priority-based",
            "Real robot",
            "RoboCup",
            "Robotic soccer team",
            "Soccer simulation",
            "Strategic positioning",
            "Team coordination"
        ],
        "doc_abstract": "This paper presents the team coordination methodologies of CAMBADA, a robotic soccer team designed to participate in the RoboCup middle-size league (MSL). The coordination model extends and adapts previous work in the Soccer Simulation League to the MSL environment. The approach is based on flexible positionings and priority-based dynamic role/positioning assignment. In addition, coordinated procedures for passing and setplays have been implemented. With the described design, CAMBADA reached the 1st place in the RoboCup'2008 world championship, becoming the first Portuguese real robot team to win in RoboCup. Competition results and performance measures computed from logs and videos of real competition games are presented and discussed. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning visual object categories with global descriptors and local features",
        "doc_scopus_id": "71049177078",
        "doc_doi": "10.1007/978-3-642-04686-5_19",
        "doc_eid": "2-s2.0-71049177078",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data sets",
            "Descriptors",
            "Local feature",
            "Nearest neighbor rule",
            "Real-world application",
            "Scaling-up",
            "Shape descriptors",
            "Shape representation",
            "Visual objects",
            "Voting strategies"
        ],
        "doc_abstract": "Different types of visual object categories can be found in real-world applications. Some categories are very heterogeneous in terms of local features (broad categories) while others are consistently characterized by some highly distinctive local features (narrow categories). The work described in this paper was motivated by the need to develop representations and categorization mechanisms that can be applied to domains involving different types of categories. A second concern of the paper is that these representations and mechanisms have potential for scaling up to large numbers of categories. The approach is based on combinining global shape descriptors with local features. A new shape representation is proposed. Two additional representations are used, one also capturing the object's shape and another based on sets of highly distinctive local features. Basic classifiers following the nearest-neighbor rule were implemented for each representation. A meta-level classifier, based on a voting strategy, was also implemented. The relevance of each representation and classifier to both broad and narrow categories is evaluated on two datasets with a combined total of 114 categories. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic image search and subset selection for classifier training in object recognition",
        "doc_scopus_id": "71049160418",
        "doc_doi": "10.1007/978-3-642-04686-5_28",
        "doc_eid": "2-s2.0-71049160418",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Classifier training",
            "Digital collections",
            "Image clustering",
            "Local feature",
            "Object extraction",
            "Ranking methods",
            "Semantic images",
            "Semantic search",
            "Shape contexts",
            "Subset selection",
            "Training objects"
        ],
        "doc_abstract": "Robots need to ground their external vocabulary and internal symbols in observations of the world. In recent works, this problem has been approached through combinations of open-ended category learning and interaction with other agents acting as teachers. In this paper, a complementary path is explored, in which robots also resort to semantic searches in digital collections of text and images, or more generally in the Internet, to ground vocabulary about objects. Drawing on a distinction between broad and narrow (or general and specific) categories, different methods are applied, namely global shape contexts to represent broad categories, and SIFT local features to represent narrow categories. An unsupervised image clustering and ranking method is proposed that, starting from a set of images automatically fetched on the web for a given category name, selects a subset of images suitable for building a model of the category. In the case of broad categories, image segmentation and object extraction enhance the chances of finding suitable training objects. We demonstrate that the proposed approach indeed improves the quality of the training object collections. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Embodied language acquisition: A proof of concept",
        "doc_scopus_id": "71049118629",
        "doc_doi": "10.1007/978-3-642-04686-5_22",
        "doc_eid": "2-s2.0-71049118629",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Categorization strategies",
            "Color blobs",
            "Common languages",
            "Component based",
            "Field of views",
            "Geometric relations",
            "Graph matchings",
            "Human users",
            "Language acquisition",
            "Language grounding",
            "Language levels",
            "Object representations",
            "Proof of concept",
            "Robotic agents",
            "Semi-automated"
        ],
        "doc_abstract": "For robots to interact with humans at the language level, it becomes fundamental that robots and humans share a common language. In this paper, a social language grounding paradigm is adopted to teach a robotic arm basic vocabulary about objects in its environment. A human user, acting as an instructor, teaches the names of the objects present in their shared field of view. The robotic agent grounds these words by associating them to visual category descriptions. A component-based object representation is presented. An instance based approach is used for category representation. An instance is described by its components and geometric relations between them. Each component is a color blob or an aggregation of neighboring color blobs. The categorization strategy is based on graph matching. The learning/grounding capacity of the robot is assessed over a series of semi-automated experiments and the results are reported. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Beyond the individual: New insights on language, cognition and robots",
        "doc_scopus_id": "56749180541",
        "doc_doi": "10.1080/09540090802518661",
        "doc_eid": "2-s2.0-56749180541",
        "doc_date": "2008-12-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cognition",
            "Distributed languages",
            "Language"
        ],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Open-ended category learning for language acquisition",
        "doc_scopus_id": "56749154177",
        "doc_doi": "10.1080/09540090802413228",
        "doc_eid": "2-s2.0-56749154177",
        "doc_date": "2008-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Instance based learning",
            "Long-term learning",
            "Metacognition",
            "Open-ended category learning",
            "Vocabulary acquisition"
        ],
        "doc_abstract": "Motivated by the need to support language-based communication between robots and their human users, as well as grounded symbolic reasoning, this paper presents a learning architecture that can be used by robotic agents for long-term and open-ended category acquisition. To be more adaptive and to improve learning performance as well as memory usage, this learning architecture includes a metacognitive processing component. Multiple object representations and multiple classifiers and classifier combinations are used. At the object level, the main similarity measure is based on a multi-resolution matching algorithm. Categories are represented as sets of known instances. In this instance-based approach, storing and forgetting rules optimise memory usage. Classifier combinations are based on majority voting and the Dempster-Shafer evidence theory. All learning computations are carried out during the normal execution of the agent, which allows continuous monitoring of the performance of the different classifiers. The measured classification successes of the individual classifiers support an attentional selection mechanism, through which classifier combinations are dynamically reconfigured and a specific classifier is chosen to predict the category of a new unseen object. A simple physical agent, incorporating these learning capabilities, is used to test the approach. A long-term experiment was carried out having in mind the open-ended nature of category learning. With the help of a human mediator, the agent incrementally learned 68 categories of real-world objects visually perceivable through an inexpensive camera. Various aspects of the approach are evaluated through systematic experiments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Self-configuration of an adaptive TDMA wireless communication protocol for teams of mobile robots",
        "doc_scopus_id": "56349163751",
        "doc_doi": "10.1109/ETFA.2008.4638554",
        "doc_eid": "2-s2.0-56349163751",
        "doc_date": "2008-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Actual systems",
            "Communication layers",
            "Data exchanges",
            "Demining",
            "Mobile autonomous agents",
            "Openess",
            "RF communications",
            "Tdma protocols",
            "Team members",
            "Wireless communication protocols"
        ],
        "doc_abstract": "Interest on using mobile autonomous agents lias been growing, recently, due to their capacity to cooperate for diverse purposes, from rescue to demining and security. However, such cooperation requires the exchange of state data that is time sensitive while achieving timeliness with RF communication is intrinsically difficult due to the openess of the medium. This paper describes a communication layer that improves the timeliness of periodic data exchanges among the team reducing the chances of lost packets caused by collisions between team members. In particular, the paper extends a previous proposal for an adaptive TDMA protocol with new self-configuration capabilities according to the current number of active team members. This feature further reduces the likelyhood of collisions within the team. Several experimental results with an actual system implementation show the effectiveness of the proposed solution. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A dialogue manager for an intelligent mobile robot",
        "doc_scopus_id": "58149121474",
        "doc_doi": null,
        "doc_eid": "2-s2.0-58149121474",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Dialogue managers",
            "Pronoun resolutions"
        ],
        "doc_abstract": "This paper focuses on a dialogue manager developed for Carl, an intelligent mobile robot. It uses the Information State (IS) approach and it is based on a Knowledge Acquisition and Management (KAM) module that integrates information obtained from various interlocutors. This mixed-initiative dialogue manager handles pronoun resolution, it is capable of performing different kinds of clarification questions and to comment information based on the current knowledge acquired.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An information state based dialogue manager for a mobile robot",
        "doc_scopus_id": "56149100697",
        "doc_doi": null,
        "doc_eid": "2-s2.0-56149100697",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Dialogue managers",
            "Dialogue system",
            "Human-robot interaction",
            "Information state system",
            "Information states",
            "Knowledge acquisition and managements",
            "Natural language understanding",
            "Pronoun resolutions"
        ],
        "doc_abstract": "The paper focuses on an Information State (IS) based dialogue manager developed for Carl, an intelligent mobile robot. It uses a Knowledge Acquisition and Management (KAM) module that integrates information obtained from various interlocutors. This mixed-initiative dialogue manager (DM) handles pronoun resolution, is capable of performing different kinds of clarification/ confirmation questions and generates observations based on the current knowledge acquired. An evaluation of the DM on knowledge acquisition tasks is shown.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "How many words can my robot learn? An approach and experiments with one-class learning",
        "doc_scopus_id": "39049110696",
        "doc_doi": "10.1075/is.8.1.05lop",
        "doc_eid": "2-s2.0-39049110696",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Animal Science and Zoology",
                "area_abbreviation": "AGRI",
                "area_code": "1103"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper addresses word learning for human-robot interaction. The focus is on making a robotic agent aware of its surroundings, by having it learn the names of the objects it can find. The human user, acting as instructor, can help the robotic agent ground the words used to refer to those objects. A lifelong learning system, based on one-class learning, was developed (OCLL). This system is incremental and evolves with the presentation of any new word, which acts as a class to the robot, relying on instructor feedback. A novel experimental evaluation methodology, that takes into account the open-ended nature of word learning, is proposed and applied. This methodology is based on the realization that a robot's vocabulary will be limited by its discriminatory capacity which, in turn, depends on its sensors and perceptual capabilities. The results indicate that the robot's representations are capable of incrementally evolving by correcting class descriptions, based on instructor feedback to classification results. In successive experiments, it was possible for the robot to learn between 6 and 12 names of real-world office objects. Although these results are comparable to those obtained by other authors, there is a need to scale-up. The limitations of the method are discussed and potential directions for improvement are pointed out. © John Benjamins Publishing Company.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Failure recovery planning for robotized assembly based on learned semantic structures",
        "doc_scopus_id": "79961064845",
        "doc_doi": "10.3182/20070523-3-es-4907.00011",
        "doc_eid": "2-s2.0-79961064845",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Classical problems",
            "Complex task",
            "Failure recovery",
            "Intelligent manufacturing system",
            "Semantic knowledge",
            "Semantic structures",
            "System failures"
        ],
        "doc_abstract": "In complex task domains such as assembly and disassembly, robots need to reason about the tasks and the environment in order to make decisions. Reasoning should be based on experience acquired by the robot through interaction with users and other robots. This paper focuses on the use of learned semantic structures for failure recovery in assembly, a classical problem that, nevertheless, is far from receiving from the robotics community the due attention. A method for failure recovery planning guided by learned semantic knowledge is described, formally analyzed and empirically evaluated. Copyright © 2007 IFAC.",
        "available": true,
        "clean_text": "serial JL 314898 291210 291718 291882 291883 31 IFAC Proceedings Volumes IFACPROCEEDINGSVOLUMES 2016-04-23 2016-04-23 2016-04-23 2016-04-23 2016-04-23T09:07:51 S1474-6670(15)31255-6 S1474667015312556 10.3182/20070523-3-ES-4907.00011 S350 S350.1 HEAD-AND-TAIL 2022-05-24T08:00:36.180846Z 0 0 20070101 20071231 2007 2016-04-23T09:37:32.564995Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate isbn isbns isbnnorm isbnsnorm issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1474-6670 14746670 978-3-902661-22-7 9783902661227 false 40 40 2 2 Volume 40, Issue 2 10 59 64 59 64 2007 2007 2007-01-01 2007-12-31 2007 1st IFAC Workshop on Intelligent Assembly and Disassembly article fla Copyright © 2007 IFAC. Published by Elsevier Ltd. All rights reserved. FAILURERECOVERYPLANNINGFORROBOTIZEDASSEMBLYBASEDLEARNEDSEMANTICSTRUCTURES LOPES L CAMARINHAMATOS 1996 202 219 L KOLODNER 1993 J CASEBASEDREASONING MINTON 1990 363 392 S PETTERSSON 2005 73 88 O SEABRALOPES 2001 10 14 L TOGUYENI 2003 57 85 A LOPESX2007X59 LOPESX2007X59X64 LOPESX2007X59XL LOPESX2007X59X64XL item S1474-6670(15)31255-6 S1474667015312556 10.3182/20070523-3-ES-4907.00011 314898 2016-04-23T04:37:32.564995-04:00 2007-01-01 2007-12-31 true 106363 MAIN 6 51808 849 656 IMAGE-WEB-PDF 1 FAILURE RECOVERY PLANNING FOR ROBOTIZED ASSEMBLY BASED ON LEARNED SEMANTIC STRUCTURES Luís Seabra Lopes Transverse Activity on Intelligent Robotics IEETA/DETI ­ Universidade de Aveiro 3810-193 Aveiro, Portugal Abstract: In complex task domains such as assembly and disassembly, robots need to reason about the tasks and the environment in order to make decisions. Reasoning should be based on experience acquired by the robot through interaction with users and other robots. This paper focuses on the use of learned semantic structures for failure recovery in assembly, a classical problem that, nevertheless, is far from receiving from the robotics community the due attention. A method for failure recovery planning guided by learned semantic knowledge is described, formally analyzed and empirically evaluated. Copyright © 2007 IFAC Keywords: Intelligent manufacturing systems, assembly, robotics, system failure and recovery, artificial intelligence, planning, machine learning 1. INTRODUCTION In flexible manufacturing and assembly systems (FMS/FAS), the keyword flexibility is generally understood as the ability to cope with change. This is a particularly important topic, especially in what concerns failure detection, diagnosis and recovery. In the assembly area, if other flexibility elements, such as sophisticated end-effectors and flexible, especially modular fixtures, are introduced, then robots will potentially be able to perform a greater variety of tasks with a higher complexity. This implies that the robot should be able to choose which actions to perform in each situation. My main research interests are concerned with the development of integrated robot architectures that support reasoning and learning at the task level (Seabra Lopes and Connell, 2001). In this paper, I focus on work concerned with failure recovery for robotized assembly. Failure recovery and related problems must be addressed at multiple levels, including prevention, fault tolerance, action-level failure recovery, task-level failure recovery, reactive and deliberative issues, human-robot interaction issues, etc. The failure recovery problem still needs a lot of research from the Artificial Intelligence, Robotics and Automation communities Literature review (Loborg, 1994; Toguyéni et al. 2003; Zielinski, 2002) shows that only a very small number of papers on failure recovery are published each year in the main journals and conferences. A distinction is often made between local (action-level) and global (tasklevel) recovery (Lopez-Mellado Alami, 1990). Local recovery is often handled through pre-defined procedures (Loborg and Törne, 1994; Lopez-Mellado Alami, 1990; Meijer, 1991). When such procedures fail, typically the human operator is called. For global recovery, López-Mellado and Alami (1990) propose to execute any actions that, according to the partially ordered task plan, do not depend on the failed action. Meanwhile, the human operator is called to solve the detected failure. Only in very few cases, has automated planning been used for failure recovery. Meijer (1991), while proposing a similar \"task re-scheduling\", additionally proposes the use of a limited-horizon planner to try to devise a failure recovery plan automatically. Only when that is not possible will the human operator be called. Evans and Lee (1994) use planning for failure recovery, but the generated plans are still simple enough to be converted to reactive procedures. After these efforts in the early 1990's, very little work has been concerned with failure recovery planning. There are several, complementary, approaches for tackling the failure recovery planning problem: 1. A search-based planner is able to come up with a failure recovery strategy. 2. The robot has enough time to go through trials and eventually comes up with a solution; this is a kind of search, but carried out in the physical world (Ferch and Zhang, 2001). 3. The robot has been through a similar problem before and was able to solve it; the strategy that was used can now be adapted. 4. If the above approaches fail, an external agent (e.g. a human teacher) can provide an appropriate recovery strategy. The major problem faced by artificial intelligence planners in the domain of failure recovery is concerned with the search complexity involved in real-world planning. A robot has limited time to plan and recover from a situation. Even in very circumscribed domains, planning complexity prevents planners from finding solutions to problems that require long action sequences. In real-world domains, of course, the number of action alternatives is much bigger and complexity becomes a problem, even when only a short sequence of actions is to be determined. For instance, in the assembly domain, a sophisticated hand or a modular fixture offer a variety of possibilities for solving a variety of tasks. However, finding the right sequence of arm moves and hand and fixture configurations for solving an unexpected problem is not easy. Heuristics are often used to focus search. The problem is that it is extremely difficult to define heuristics for the domain of failure recovery planning. Going through trials in a framework of reinforcement learning is something that the robot can do in spare time or in a training period. Work of this type in the domain of cooperative grasping and assembly is reported in (Ferch and Zhang, 2001). However, this does not seem viable for failure recovery in normal operation. Avoiding planning complexity by using knowledge about solutions to similar, previously encountered problems is a way out that has been attracting increasing attention from researchers in various domains. I have developed a failure recovery approach that involves recognizing the basic principles underlying solutions to concrete cases and the application of those principles to new situations. (Seabra Lopes, 1999). In the failure recovery domain, the descriptions of failure categories and operator schemata make up a domain theory that must be taken into account when explaining (or understanding) the recovery strategy that was applied in a given situation. Learning the basic principles of the applied solutions involves a series of deductive and inductive inference steps. In recovery planning, the inverse transformations are applied. In this paper, I take up previous work by providing formal definitions, formal complexity analysis of the planning algorithm and experimental results. For the learning algorithm, refer to (Seabra Lopes, 1999). This work takes the common assumption in AI planning, namely that the world normally changes as described in action models. In case of failure, this work also assumes that failure analysis capabilities will enable to update the world model and start the recovery planning activity. The work finally assumes that the manipulated representations are perceptually grounded. 2. FAILURE RECOVERY KNOWLEDGE A symbolic and logic-based approach is adopted. The world model is a set of logic formulas that represent, not only permanent information about the world, but also the momentary situation. The world can change through actions of the robot and unexpected events. The types of actions the robot is able to perform are represented by PDDL type operator schemata (Gerevini and Long, 2005). An operator schema specifies the pre-conditions and effects of an action. Plans are obtained by stringing together operator applications, i.e. instanciated operator schemata. The execution of a plan can fail to deliver its goals. An execution failure is, therefore, a deviation between the expected and the actual situation of the world detected during plan execution. Failure detection requires a functionality usually called execution monitoring (Pettersson, 2005). Execution failures can be caused by system faults (malfunctions in equipment), external exceptions (e.g. some unexpected occurrence in the robot environment) or previous execution failures (Camarinha-Matos et al., 1996). An execution failure is characterized by a certain number of effects on the situation of the world. Thus, the proposed representation resembles that of operators: Definition 1 - A failure category describes a class of execution failures in terms of the execution context and failure effects. It is represented as a tuple FC = <FT,OT, DL,AL>, where: - FT is a functional expression representing the failure category template. - OT is the failed operation template. (In general, this could be some pointer to the execution context) - DL, called the delete list, is a list of atomic (logic) formulas that should be deleted from the description of the situation of the world prior to executing the failed operation. - AL, called the add list, is a list of atomic formulas that should be added to the situation description. The description of a failure actually occurred in the physical scenario is an instanciation of a failure category and is identified by the instanciated failure template. Determining failure effects after failure detection can be done through a failure diagnosis module, which should be part of the agent architecture. Failure diagnosis includes failure confirmation, classification and explanation, and concludes with status identification. Based on the results of failure diagnosis, a failure recovery strategy can be devised. Applying a failure recovery strategy is a learning opportunity, whether it is successful or not. Success means, of course, that the planned recovery operations are completely executed, allowing to resume execution of the nominal plan. If some exception, unrelated to the initial failure, causes a new failure which is handled recursively, and then the initial recovery strategy is resumed and completed, recovery is also considered successful. Unsuccessful recovery may happen due either to the application of an incorrect recovery strategy, given the initial failure diagnosis, or to errors in the initial diagnosis itself. The first case can usually be avoided by verifying the effects of the recovery strategy, according to the applied operator schemata, before execution. In the second case, learning is more difficult because it involves criticizing and reformulating the used diagnostic model. In this paper, learning is attempted only from successful failure recovery episodes. The following representation is adopted: Definition 2 - A failure recovery episode is a tuple <OT,FT,RP>, where OT is the failed operation template, FT is the failure instance template and RP is a plan (i.e. a sequence of operator applications) used to recover from the failure (recovery plan). The basic principles that explain the success of a failure recovery strategy are then extracted based on several deductive as well as inductive transformations, namely deductive generalization, abstraction, feature extraction and clustering of repeated plan patterns (Seabra Lopes, 1999). The final result of these learning computations is a failure recovery schema, which can now be formally defined: Definition 3 - A failure recovery schema describes guidelines for building solutions for instances of a given failure category. It consists of a tuple <OT,FT,RSK>, where: - OT is the template of the applied operator. - FT is the failure category template. - RSK is the recovery plan skeleton, consisting of a sequence of abstract steps described as pairs <Action,Features>, where Action is either an abstract operator or a sequence of abstract operators and Features is a list of features of the objects manipulated by Action. Each abstract operator can be concretized through a sequence of task-level operators. Failure recovery schemata can be thought of as conceptual categories, but not in the usual sense of the term. Although these categories organize similar concepts, their members cannot be enumerated. They can, however, be reconstructed as necessary. An example of a failure recovery schema, learned with this approach from a 24-steps plan in the assembly domain described in (Seabra Lopes, 1999), is presented in figure 1. [ assemble(R,T,Obj1,Comp1,Type1,Prod,Fix), defective_assembly(DefObj,Comp4,Type4,Prod,Fx), [ [ place(Obj1), [ ] ] [ [disassemble(X,C), place(X)], [ initially_assembled(C), assemble_after(C,Comp4), assemble_before(C,Comp1) ] ] [disassemble(DefectiveObj, Comp4), [ ] ] [place(DefectiveObj), [ ] ] [pick(NewObj), [ same_type_as(NewObj,DefectiveObj) ] ] [assemble(NewObj,Comp4), [ same_type_as(NewObj, DefectiveObj) ] ] [ [pick(X), assemble(X,C) ], [ initially_assembled(C), assemble_after(C,Comp4), assemble_before(C,Comp1)]] [pick(Obj1), [ ] ] [assemble(Obj1,Comp1), [ ] ] ]] Fig. 1. Example of learned failure recovery schema (see Definition 3.) It covers a range of assembly failure situations caused by a defect in an assembled component and the subsequent impossibility of assembling another component. The learned strategy contains two clusters, one for the disassembly phase, [disassemble(X,C), place(X)], and the other [pick(X), for the assembly phase assemble(X,C)], where variables X and C are constrained by certain features (given in italic). For instance, feature assemble_before(C,Comp1) specifies that assembly component C must be assembled before Comp1, i.e. before the component whose assembly failed. 3. PLANNING GUIDED BY AN ABSTRACT PLAN When a failure is detected and the diagnosis function is able to produce the failure description, recovery planning is attempted. The first step is to look for similar failure situations previously encountered. It shouldn't be ignored that a so-called \"utility problem\" has been identified for learning systems, particularly for explanation-based learning systems (Minton, 1990). It arises when the added costs of matching learned control rules exceed the search guidance benefits. Similar problems can occur in case-based systems, which is based on storing cases. In the proposed approach, the combination of deductive generalization with different forms of abstraction (including clustering) enables to learn failure recovery schemata that cover a wide range of situations, therefore reducing the utility problem considerably. If needed, indexing/retrieval techniques developed within CBR (Kolodner, 1993) should be used. In the work described here, the failed operation template plus the failure instance template are used as indexing key to locate and retrieve a suitable a failure recovery schema. Then, the plan skeleton in that schema is concretized to solve the current situation. 3.1 Planning Guided by an Abstract Plan Concretizing a plan skeleton, i.e. adapting it such that it solves a particular situation, can be done by planning a solution for the situation and using the skeleton simply to guide the search. If the plan skeleton contains no clusters, an A* graph-based progression planning algorithm can be directly applied. In this case, all that must be done is to redefine the cost function to take into account the information contained in the plan skeleton. The previously extracted features play an important role in cost computations. The following cost estimation method was implemented: - At the root node of the search space, representing the failure situation, the entire plan skeleton is considered as the guide to reach a situation in which the failure is considered recovered. In each node of the search space, there is some remaining part of the initial plan skeleton that provides guidance for going from that node to a solution node. - When expanding a node, the first abstract operation in the remaining part of the plan skeleton, is used to judge the utility of different operator applications and assign them different costs. Let OP be a legal operator application in the current node and let CR be the real cost of executing it 1. Let AO be the head of (i.e. the first abstract operation in) the current plan skeleton. The cost of OP actually considered by the planner, CP, is defined by the following rules: o If OP is an auxiliary operation, therefore belonging to the nil class in the operator abstraction hierarchy, then CP will be the real cost CR. o If OP belongs to the category of AO, then let K be the number of features documenting AO in the plan skeleton and let V d K be the number of these features actually verified (i.e. true) in the case of OP. In this case, we have CP = CR (K+1)/(V+1) This way, operator applications that verify a greater number of features of the abstract operation will be preferred. If V = K, then 1 - CP will be the real cost. In all other cases, CP will be infinity. In this case, the planner won't even create the corresponding child node in the search tree. When a node n is created, its total cost is defined as f(n) = g(n) + h(n), where: g(n) is the sum of the CP costs of all operations in the path leading from the root node to n; the estimated cost of reaching a solution node is h( n) D L , where L is the length of the current abstract plan and D is an estimate of the average cost of operations in a task-level plan per each operation in the abstract plan 2. o 3.2 Expanding Plan Clusters The planning process becomes more complex when the plan skeleton contains clusters, due to the uncertainty with respect to the number of cluster instances that are required for the particular problem. Since this problem interacts with the structure of the entire plan skeleton, the best approach is to carry out an abstract planning phase, in which the clusters in the plan skeleton are replaced by sequences of abstract operations and the parameters of the abstract operations are instanciated. A greedy progression planning approach is followed in converting a plan skeleton with clusters into a mere sequence of abstract operator applications. When the planner encounters a cluster, it can either skip it or append a cluster instance to the current plan skeleton. In this approach, cost is not taken into account. Only a measure of preference of a particular abstract operator application over alternative applications is used. Let AO be an abstract operation in the plan skeleton and let K be the number of features that documents it. Let AA be a possible application of AO and let V be the number of features in AO verified in AA. AA will be preferred over other operator applications if it has the highest value of the ratio (V+1)/(K+1). Note that, as a direct consequence of taking into account features in cost computations, recovery planning will be easier for those failure episodes that are more similar to the failure episode that originally enabled learning. Precisely the same occurs with human learning. 4. ANALYSIS AND EVALUATION 4.1 Computational complexity of planning Planning from experience, following the method described above, assumes that the selected failure recovery schema is indeed applicable to the situation being addressed. As already mentioned, such analysis is to be carried out by a diagnosis module. If the schema is not applicable to the situation, planning 2 For tests in the assembly domain reported below, all costs were set to 1.0. For tests in the assembly domain reported below, D = 2.5 was used. will eventually fail. This happens because, in order to focus search, the plan skeleton is used to actually cut off a large portion of the search space. When planning from scratch, the average branching factor is B = Nao u Bao + Bnil where Nao is the number of abstract operators defined for the domain, Bao is the average number of instantiations of an abstract operator applicable on any given state and Bnil is the average number of auxiliary operations applicable on any given state. In the particular domain used for the examples of this paper, B has been experimentally determined to be around 10 operations (Seabra Lopes, 1999). When planning from experience, according to the algorithm presented in section 3.1, the average branching factor is reduced to B = Bnil + Bao. If, for instance, we have a domain in which Nao = 4 and Bnil = Bao, we see that the branching factor is reduced to 40% of the original branching factor. This leads to a considerable reduction in the search space. For larger domains, this improvement will be even more significant. However, it does not prevent an exponential growth in the general case. The actual performance of the proposed planning approach in a particular situation depends on the following two main conditions: a) The current situation is similar enough to the situation that originally enabled learning, so that the same basic solution can be applied, without any extra search effort; b) The features used to document each step of the abstract plan skeleton are sufficiently informative, therefore allowing an appropriate similarity assessment. Situations meeting these two conditions are the main target of the proposed planning approach. Under these conditions, the approach has been experimentally observed to exhibit a linear computational complexity (see section 4.2). More specifically, the number of created nodes in the search tree grows linearly with the size of the solution plan, resulting in a linear complexity both in terms of memory usage and running time. This can also be intuitively seen through an analysis of the planning process. Typically, since the abstract plan skeleton doesn't provide any guidance with respect to auxiliary operations, most of the search effort is consumed in finding appropriate auxiliary operations to insert between the other task-level operations. The search for intermediate auxiliary operations terminates as soon as a state is generated where an appropriate instance of the next abstract operation can be applied. Assuming that the features characterizing the abstract operation are informative and, therefore, the right task-level operation is selected in the first attempt, the average depth of the local search for auxiliary operations is given by l = 1 + O/(1-O), where O is the proportion of auxiliary operations in the solution plan. In these circumstances, the size of the local search space, S, grows exponentially 3 with l, but can be considered independent of the length of the full solution plan. The number of expanded nodes for a full recovery plan of size L will be in the order of (1O) u L u S which is linear in L. When features are not enough to pinpoint the right task-level operation to implement a given abstract operation, the local search space will tend to expand beyond that point. The more often that happens, the less linear the search process will be. 4.2 Empirical evaluation Empirical evaluation was carried out in the assembly domain described in (Seabra Lopes, 1999). Parts are available to the assembly robot through feeders or in pallets and tools are picked up from a tool magazine. The product used in the examples is the Cranfield Benchmark, a classical pendulum-like structure used for evaluation of research in the assembly domain (Rathmill and Collins, 1984). 13 different action categories (planning operators) are used. An assembly plan for the Cranfield benchmark, considering assembly operations and auxiliary operations for part feeding and fetching, has a length of around 50 operations. The average branching factor of the planning search space has been observed to be approximately 10. This means that, if there are no heuristics for guiding the search, a search space of n 10 may have to be explored for determining a plan with n operations. For this evaluation, the failure recovery schema presented in fig. 1 was used to solve five recovery planning problems (Table 1). First of all, the learned schema was used to solve the problem based on which it was learned. An optimal solution, equivalent to the one used for learning, is obtained, after expanding a search tree with an Average Branching Factor of 4.299 and an Effective Branching Factor of 1.197 (first line in Table 1). Other problems were formulated for testing the same learned schema. For instance, the second problem is the following: after assembling the side plate (sp1), two of the spacer pegs (peg1, peg2), the cross bar (cb1) and the shaft (sft), the robot fails to assemble the lever lv and realizes that the failure was due to a defect in the initial sideplate (sp1). The situation is similar to the one that enabled learning, but more complex, because there is a larger set of components that must be disassembled in order to replace the defective component. However, applying the planning strategy proposed above, guided by the plan skeleton contained in the learned recovery schema, an optimal solution for the new problem is easily 3 It can be estimated as S = B u (Bl-1) / (B-1) . derived. As can be seen, at the surface, the plan for the new problem is quite different from the plan used in the episode that enabled learning. Note that, in this search, branching factors (second line in Table 1) are comparable to those of the previous case. As Table 1 summarizes, several problems, with solution sizes varying between 15 and 40 operations, can be solved using the same basic solution (the schema in fig. 1). Variations in branching factors are minor in these problems. The key achievement of the approach, here illustrated, is that it keeps the Effective Branching Factor very near to 1. We also see that the Effective Branching Factor decreases (approaching 1.0) as the length of the target solution increases. In fact, it was nearly 1.3 when finding a plan with 15 operations, but decreased to 1.1 when finding a plan with 40 operations. This is related to the fact that, as argued before, the computational cost of the planning algorithm grows linearly with the size of the solution. The linearity claim is supported in Table 1 (3rd column), where it can be seen that the ratio between the number of nodes in the search tree and the length of the solution oscillates around 14. Table 1 Failure recovery planning performance measures REFERENCES Camarinha-Matos, L.M., L. Seabra Lopes, J. Barata (1996) Integration and Learning in Supervision of Flexible Assembly Systems, IEEE Trans Robotics and Automation, vol. 12, p. 202-219. Evans, E.Z. and C.S.G. Lee (1994) Automatic Generation of Error Recovery Knowledge through Learned Reactivity, Proc. 1994 IEEE International Conference on Robotics and Automation, San Diego, California, v. 4, p. 2915-2920. Ferch, M. and J. Zhang (2001) Learning Cooperative Grasping with the Graph Representation of a StateAction Space, Proc. 9th European Workshop on Learning Robots, Prague, p. 65-74. Gerevini, A. and D. Long (2005) BNF Description of PDDL3.0, October 2005. Kedar-Cabelli, S.T. and L.T. McCarty (1987) ExplanationBased Generalization as Resolution Theorem Proving, Proc. 4th Int'l Conf. on Machine Learning, Irvine, CA, p. 383-389. Kolodner, J. (1993) Case-Based Reasoning, Morgan Kaufmann Publishers. Loborg, P. (1994) Error Recovery in Automation: an Overview, AAAI'94 Spring Symposium on Detecting and Resolving Errors in Manufacturing Systems, Stanford, California, p. 94-100. Loborg, P. and A. Törne (1994) Manufacturing Control Systems Principles supporting Error Recovery, AAAI Spring Symposium on Detecting and Resolving Errors in Manufacturing Systems, Stanford, CA, p. 101-108. Lopez-Mellado, E. and R. Alami (1990) A Failure Recovery Scheme for Assembly Workcells, Proc. 1990 IEEE International Conference on Robotics and Automation, Cincinnati, p. 702-707. McDermott, D., chair (1998) PDDL: The Planning Domain Definition Language, Yale Center for Computational Vision and Control, Tech. Rep. TR-98-003. Meijer, G.R. (1991) Autonomous Shopfloor Systems. A Study into Exception Handling for Robot Control, PhD thesis, Universitaet van Amsterdam. Minton, S. (1990) Quantitative Results Concerning the Utility of Explanation-Based Learning, Artificial Intelligence, vol. 42, p. 363-392. Pettersson, O. (2005) Execution Monitoring in Robotics: a Survey, Robotics and Autonomous Systems, vol. 53(2), p. 73-88. Rathmill, K. and K. Collins (1984) Development of an European Benchmark for the Comparison of Assembly Robot Programming Systems, Proc. First Robotics Conference, Brussels. Seabra Lopes, L. (1999) Failure Recovery Planning in Assembly Based on Acquired Experience: Learning by Analogy, Proc. 1999 IEEE International Symposium on Assembly and Task Planning, Porto, pp. 294-300. Seabra Lopes, L. and J.H. Connell (2001) \"Semisentient Robots: Routes to Integrated Intelligence\", IEEE Intelligent Systems, vol. 16, n. 5, p. 10-14. Toguyéni, A.K.A., P. Berruet and E. Craye (2003) «Models and Algorithms for Failure Diagnosis and Recovery in FMSs», The International Journal of Flexible Manufacturing Systems, 15 (1), p. 57-85. Zielinski, C. (2002) «Reaction to errors in robot systems», RoMoCo '02. Proc Third International Workshop on Robot Motion and Control, Bukowy Dworek, Poland, p. 201-208. # nodes # non Avg Effective Plan # nodes / plan termina branching branching length length l nodes factor factor 24 374 15.6 87 4.299 1.197 32 494 15.4 114 4.333 1.143 15 160 10.7 36 4.444 1.296 21 281 13.4 64 4.391 1.217 40 564 14.1 129 4.372 1.109 5. CONCLUSIONS In this paper, a method was proposed for solving failure recovery problems based on analogies with previous similar problems, whose basic solution was learned. An expressive semantic representation is used to guide failure recovery planning in new similar situations. The inefficiency of the classical planning-from-scratch approaches led robotics researchers to avoid artificial intelligence methods. However, as the method presented in this paper illustrates, the introduction of learning reduces planning complexity from exponential to linear in the number of required operations. This way, the task planning and failure recovery module is able to make decisions in the time scale of the normal execution of the task. This means that, if some unexpected problem arises and there is previous experience about problems of the same kind, the problem can be solved with the guarantee that the total time to complete the task won't increase significantly. Globally speaking, the approach enables the robot system to become increasingly effective, not only within a task, but also across different tasks. Generation of Error Recovery Knowledge through Learned Reactivity, Proc. 1994 IEEE International Conference on Robotics and Automation, San Diego, California, v. 4, p. 2915-2920. Ferch, M. and J. Zhang (2001) Learning Cooperative Grasping with the Graph Representation of a StateAction Space, Proc. 9th European Workshop on Learning Robots, Prague, p. 65-74. Gerevini, A. and D. Long (2005) BNF Description of PDDL3.0, October 2005. Kedar-Cabelli, S.T. and L.T. McCarty (1987) ExplanationBased Generalization as Resolution Theorem Proving, Proc. 4th Int'l Conf. on Machine Learning, Irvine, CA, p. 383-389. Kolodner, J. (1993) Case-Based Reasoning, Morgan Kaufmann Publishers. Loborg, P. (1994) Error Recovery in Automation: an Overview, AAAI'94 Spring Symposium on Detecting and Resolving Errors in Manufacturing Systems, Stanford, California, p. 94-100. Loborg, P. and A. Törne (1994) Manufacturing Control Systems Principles supporting Error Recovery, AAAI Spring Symposium on Detecting and Resolving Errors in Manufacturing Systems, Stanford, CA, p. 101-108. Lopez-Mellado, E. and R. Alami (1990) A Failure Recovery Scheme for Assembly Workcells, Proc. 1990 IEEE International Conference on Robotics and Automation, Cincinnati, p. 702-707. McDermott, D., chair (1998) PDDL: The Planning Domain Definition Language, Yale Center for Computational Vision and Control, Tech. Rep. TR-98-003. Meijer, G.R. (1991) Autonomous Shopfloor Systems. A Study into Exception Handling for Robot Control, PhD thesis, Universitaet van Amsterdam. Minton, S. (1990) Quantitative Results Concerning the Utility of Explanation-Based Learning, Artificial Intelligence, vol. 42, p. 363-392. Pettersson, O. (2005) Execution Monitoring in Robotics: a Survey, Robotics and Autonomous Systems, vol. 53(2), p. 73-88. Rathmill, K. and K. Collins (1984) Development of an European Benchmark for the Comparison of Assembly Robot Programming Systems, Proc. First Robotics Conference, Brussels. Seabra Lopes, L. (1999) Failure Recovery Planning in Assembly Based on Acquired Experience: Learning by Analogy, Proc. 1999 IEEE International Symposium on Assembly and Task Planning, Porto, pp. 294-300. Seabra Lopes, L. and J.H. Connell (2001) \"Semisentient Robots: Rout IPV 31255 S1474-6670(15)31255-6 10.3182/20070523-3-ES-4907.00011 IFAC FAILURE RECOVERY PLANNING FOR ROBOTIZED ASSEMBLY BASED ON LEARNED SEMANTIC STRUCTURES Luís Seabra Lopes Transverse Activity on Intelligent Robotics IEETA/DETI – Universidade de Aveiro 3810-193 Aveiro, Portugal Transverse Activity on Intelligent Robotics IEETA/DETI Universidade de Aveiro Aveiro 3810-193 Portugal In complex task domains such as assembly and disassembly, robots need to reason about the tasks and the environment in order to make decisions. Reasoning should be based on experience acquired by the robot through interaction with users and other robots. This paper focuses on the use of learned semantic structures for failure recovery in assembly, a classical problem that, nevertheless, is far from receiving from the robotics community the due attention. A method for failure recovery planning guided by learned semantic knowledge is described, formally analyzed and empirically evaluated. Keywords Intelligent manufacturing systems assembly robotics system failure and recovery artificial intelligence planning machine learning References Camarinha-Matos et al., 1996 L.M. Camarinha-Matos L. Seabra Lopes J. Barata Integration and Learning in Supervision of Flexible Assembly Systems IEEE Trans Robotics and Automation 12 1996 202 219 Evans et al., 1994 Evans, E.Z. and C.S.G. Lee (1994) Automatic Generation of Error Recovery Knowledge through Learned Reactivity, Proc. 1994 IEEE International Conference on Robotics and Automation, San Diego, California, v. 4, pp. 2915-2920. Ferch et al., 2001 Ferch, M. and J. Zhang (2001) Learning Cooperative Grasping with the Graph Representation of a State-Action Space, Proc. 9th European Workshop on Learning Robots, Prague, pp. 65-74. Gerevini et al., 2005 Gerevini, A. and D. Long (2005) BNF Description of PDDL3.0, October 2005. Kedar-Cabelli et al., 1987 Kedar-Cabelli, S.T. and L.T. McCarty (1987) Explanation-Based Generalization as Resolution Theorem Proving, Proc. 4th Int'l Conf. on Machine Learning, Irvine, CA, pp. 383-389. Kolodner, 1993 J. Kolodner Case-Based Reasoning 1993 Morgan Kaufmann Publishers Loborg et al., 1994 Loborg, P. (1994) Error Recovery in Automation: an Overview, AAAI'94 Spring Symposium on Detecting and Resolving Errors in Manufacturing Systems, Stanford, California, pp. 94-100. Loborg et al., 1994 Loborg, P. and A. Törne (1994) Manufacturing Control Systems Principles supporting Error Recovery, AAAI Spring Symposium on Detecting and Resolving Errors in Manufacturing Systems, Stanford, CA, pp. 101-108. Lopez-Mellado et al., 1990 Lopez-Mellado, E. and R. Alami (1990) A Failure Recovery Scheme for Assembly Workcells, Proc. 1990 IEEE International Conference on Robotics and Automation, Cincinnati, pp. 702-707. McDermott et al., 1998 McDermott, D., chair (1998) PDDL: The Planning Domain Definition Language, Yale Center for Computational Vision and Control, Tech. Rep. TR-98-003. Meijer et al., 1991 Meijer, G.R. (1991) Autonomous Shopfloor Systems. A Study into Exception Handling for Robot Control, PhD thesis, Universitaet van Amsterdam. Minton, 1990 S. Minton Quantitative Results Concerning the Utility of Explanation-Based Learning Artificial Intelligence 42 1990 363 392 Pettersson, 2005 O. Pettersson Execution Monitoring in Robotics: a Survey Robotics and Autonomous Systems 53 2 2005 73 88 Rathmill et al., 1984 Rathmill, K. and K. Collins (1984) Development of an European Benchmark for the Comparison of Assembly Robot Programming Systems, Proc. First Robotics Conference, Brussels. Seabra Lopes et al., 1999 Seabra Lopes, L. (1999) Failure Recovery Planning in Assembly Based on Acquired Experience: Learning by Analogy, Proc. 1999 IEEE International Symposium on Assembly and Task Planning, Porto, pp. 294-300. Seabra Lopes and Connell, 2001 L. Seabra Lopes J.H. Connell Semisentient Robots: Routes to Integrated Intelligence IEEE Intelligent Systems 16 5 2001 10 14 Toguyéni et al., 2003 A.K.A. Toguyéni P. Berruet E. Craye ≪Models and Algorithms for Failure Diagnosis and Recovery in FMSs≫ The International Journal of Flexible Manufacturing Systems 15 1 2003 57 85 Zielinski et al., 2002 Zielinski, C. (2002) «Reaction to errors in robot systems», RoMoCo '02. Proc Third International Workshop on Robot Motion and Control, Bukowy Dworek, Poland, pp. 201-208. "
    },
    {
        "doc_title": "A knowledge representation and reasoning module for a dialogue system in a mobile robot",
        "doc_scopus_id": "78651300617",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78651300617",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (miscellaneous)",
                "area_abbreviation": "COMP",
                "area_code": "1701"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Classical semantics",
            "Dialogue systems",
            "Inductive inference",
            "Intelligent mobile robot",
            "Knowledge representation and reasoning",
            "Knowledge representation language",
            "Question Answering"
        ],
        "doc_abstract": "The recent evolution of Carl, an intelligent mobile robot, is presented. The paper focuses on the new knowledge representation and reasoning module, developed to support high-level dialogue. This module supports the integration of information coming from different interlocutors and is capable of handling contradictory facts. The knowledge representation language is based on classical semantic networks, but incorporates some notions from UML. Question answering is based on deductive as well as inductive inference.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From robust spoken language understanding to knowledge acquisition and management",
        "doc_scopus_id": "33745207534",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33745207534",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Knowledge representation and reasoning (KRR)",
            "Semantic networks",
            "Spoken language understanding (SLU)"
        ],
        "doc_abstract": "The recent evolution of Carl, an intelligent mobile robot, is presented. The paper focuses on robust spoken language understanding (SLU) and on knowledge representation and reasoning (KRR). Robustness in SLU is achieved through the combination of deep and shallow parsing, tolerating non-grammatical utterances. The KRR module supports the integration of information coming from different interlocutors and is capable of handling contradictory facts. The knowledge representation language is based on semantic networks. Question answering is based on deductive as well as inductive inference. A preliminary evaluation of the efficiency of the SLU/KRR system, for the purpose of knowledge acquisition, is presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "One-class learning for Human-Robot Interaction",
        "doc_scopus_id": "79952042826",
        "doc_doi": "10.1007/0-387-22829-2_53",
        "doc_eid": "2-s2.0-79952042826",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Class modeling",
            "Class models",
            "Classification mechanism",
            "Face classification",
            "Hidden Markov tree model",
            "One-class Classification",
            "One-class learning",
            "Wavelet features"
        ],
        "doc_abstract": "A Suitable learning and classification mechanism is a crucial premise for Human-Robot Interaction. To this purpose, several one-class classification methods have been investigated using wavelet features (parameters of Hidden Markov Tree model) in this paper. Only target class patterns are used to train class models. Good discrimination over outlier (never seen non-target) patterns is still kept based on their distances to class model. Face and non-face classification is used as an example and some promising results are reported. © 2005 by International Federation for Information Processing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The modio-measure and the modio of the prices in the 11th to 13th centuries",
        "doc_scopus_id": "70350491269",
        "doc_doi": "10.3989/aem.2005.v35.i1.134",
        "doc_eid": "2-s2.0-70350491269",
        "doc_date": "2005-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "History",
                "area_abbreviation": "ARTS",
                "area_code": "1202"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In this paper, the value of the modius (modio, moyo, moio) in Portugal in the 11th to the 13th centuries is studied. By this time, the modius was both a measure of capacity and a unit used in the specification of prices. The author concludes that, while the modius-measure had a capacity not under the horse charge (210 to 220 litters) in the legal systems used in Portugal from the end of the 11th century onwards, some smaller modii continued to be used in rural areas far away from the main urban centers. These smaller modii had a capacity around 50 to 75 litters. With respect to the modius-price, the information gathered by the author indicates a value between 25 and 55 litters of cereal. Therefore, it becomes clear that the modius-price was not equivalent to the standard modius-measure, although it could be equivalent to the smaller modii still in use in rural areas. The gathered evidence also suggests that, from the 10th to the 13th centuries, independently of inflation, the modius-price remained approximately equivalent to the currency unit known as solidus.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual object recognition through one-class learning",
        "doc_scopus_id": "35048885031",
        "doc_doi": "10.1007/978-3-540-30125-7_58",
        "doc_eid": "2-s2.0-35048885031",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Classification methods",
            "Face classification",
            "Natural languages",
            "One-class Classification",
            "One-class classifier",
            "One-class learning",
            "PCA (principal component analysis)",
            "Visual object recognition"
        ],
        "doc_abstract": "In this paper, several one-class classification methods are investigated in pixel space and PCA (Principal component Analysis) subspace having in mind the need of finding suitable learning and classification methods to support natural language grounding in the context of Human-Robot Interaction. Face and non-face classification is used as an example to demonstrate effectiveness of these one-class classifiers. The idea is to train target class models with only target (face) patterns, but still keeping good discrimination over outlier (never seen non-target) patterns. Some discussion is given and promising results are reported. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Coordinating distributed autonomous agents with a real-time database: The GAMBADA project",
        "doc_scopus_id": "35048815976",
        "doc_doi": "10.1007/978-3-540-30182-0_88",
        "doc_eid": "2-s2.0-35048815976",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cooperative autonomous mobile robots",
            "Distributed architecture",
            "Local operations",
            "Management systems",
            "Mobile autonomous agents",
            "Real-time database",
            "Robotic soccer team",
            "Temporal coherency"
        ],
        "doc_abstract": "Interest on using mobile autonomous agents has been growing, recently, due to their capacity to cooperate for diverse purposes, from rescue to demining and security. However, such cooperation requires the exchange of state data that is time sensitive and thus, applications should be aware of data temporal coherency. In this paper we describe the architecture of the agents that constitute the CAMBADA (Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture) robotic soccer team developed at the University of Aveiro, Portugal. This architecture is built around a real-time database that is partially replicated in all team members and contains both local and remote state variables. The temporal coherency of the data is enforced by an adequate management system that refreshes each database item transparently at a rate specified by the application. The application software accesses the state variables of all agents with local operations, only, delivering both value and temporal coherency. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Indoor object recognition through human interaction using wavelet features",
        "doc_scopus_id": "84990902039",
        "doc_doi": "10.1109/RISSP.2003.1285568",
        "doc_eid": "2-s2.0-84990902039",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Feature extraction methods",
            "Haar wavelet transform",
            "Human interactions",
            "Life long learning",
            "Statistical features",
            "Vision information",
            "Wavelet features",
            "Wavelet-based Feature"
        ],
        "doc_abstract": "© 2003 IEEE.In this paper a preliminary work towards grounded concept learning for a service robot through its vision and human interaction is presented. With a lifelong learning server (LLL), described in [L. S. Lopes and Q.H. Wang, 2002], the robot can incrementally learn to recognize instances of such concepts of indoor objects as \"person\", \"trash-can\" and \"triangle sign\" using simple intra-band statistical features extracted from the Haar wavelet transform of its vision information under the instruction of a human teacher. Experimental results show that these simple wavelet-based features can efficiently describe the characteristics of different objects in an office-like environment. Comparison with some other feature extraction methods is also given.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a personal robot with language interface",
        "doc_scopus_id": "78349285382",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78349285382",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Human robots",
            "Interactivity",
            "Language interface",
            "Multi-modal",
            "Natural language interfaces",
            "Personal robot"
        ],
        "doc_abstract": "The development of robots capable of accepting instructions in terms of familiar concepts to the user is still a challenge. For these robots to emerge it?s essential the development of natural language interfaces, since this is regarded as the only interface acceptable for a machine which expected to have a high level of interactivity with Man. Our group has been involved for several years in the development of a mobile intelligent robot, named Carl, designed having in mind such tasks as serving food in a reception or acting as a host in an organization. The approach that has been followed in the design of Carl is based on an explicit concern with the integration of the major dimensions of intelligence, namely Communication, Action, Reasoning and Learning. This paper focuses on the multi-modal human-robot language communication capabilities of Carl, since these have been significantly improved during the last year.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A robot with natural interaction capabilities",
        "doc_scopus_id": "77954482142",
        "doc_doi": "10.1109/ETFA.2003.1247762",
        "doc_eid": "2-s2.0-77954482142",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Current capability",
            "Human-robot communication",
            "Intelligent Service robots",
            "Multi-modal",
            "Natural interactions"
        ],
        "doc_abstract": "© 2003 IEEE.This paper describes the architecture and current capabilities of Carl, a prototype of an intelligent service robot, designed having in mind such tasks as serving food in a reception or acting as a host in an organization. The approach that has been followed in the design of Carl is based on an explicit concern with the integration of the major dimensions of intelligence, namely communication, action, reasoning and learning. The paper focuses on the multi-modal human-robot communication capabilities of Carl, since these have been significantly improved during the last year.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards grounded human-robot communication",
        "doc_scopus_id": "15744404347",
        "doc_doi": "10.1109/ROMAN.2002.1045641",
        "doc_eid": "2-s2.0-15744404347",
        "doc_date": "2002-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Dimensionality reduction",
            "Human users",
            "Human-robot communication",
            "Incremental learning",
            "Natural language expressions",
            "Natural languages"
        ],
        "doc_abstract": "Future robots are expected to communicate with humans using natural language. The naïve human user will expect a robot to easily understand what he/she is meaning by instructions concerning robot's tasks. This implies that the robot will need to have a means of grounding, in its own sensors, the natural language terms and constructions used by the human user. This paper presents an approach to solve this problem that is based on the integration of a \"learning server\" in the software architecture of the robot. Such server should be capable of on-line, incremental learning from examples; it should handle multiple problems concurrently and it should have meta-tearning capabilities. A learning server already developed by the authors is presented. Complementarity, the dimensionality reduction problem is also addressed, using a Blocked DCT approach. Experimental results are obtained in a scenario in which three concepts (corresponding to natural language expressions) are concurrently learned. © 2002 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Carl: From situated activity to language level interaction and learning",
        "doc_scopus_id": "0036448808",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0036448808",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Communication action reasoning and learning in robotics",
            "Language-level communication",
            "Sensor fusion",
            "Software architecture",
            "Spoken language processing"
        ],
        "doc_abstract": "Carl is a prototype of an intelligent service robot, designed having in mind such tasks as serving food in a reception or acting as a host in an organization. The approach that has been followed in the design of Carl is based on an explicit concern with the integration of the major dimensions of intelligence, namely Communication, Action, Reasoning and Learning. Although different communities have thoroughly studied these dimensions in the past, their integration has seldom been attempted in a systematic way. This paper describes the software architecture of Carl as well as the main modules, from sensor fusion and navigation to language-level communication and learning.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semisentient robots: Routes to integrated intelligence",
        "doc_scopus_id": "0035438679",
        "doc_doi": "10.1109/5254.956076",
        "doc_eid": "2-s2.0-0035438679",
        "doc_date": "2001-09-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Case-based approach to reasoning",
            "Natural language understanding system",
            "Semisentient robots"
        ],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semisentient Robots: Routes to Integrated Intelligence",
        "doc_scopus_id": "85008035923",
        "doc_doi": "10.1109/MIS.2001.956076",
        "doc_eid": "2-s2.0-85008035923",
        "doc_date": "2001-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human-robot interaction through spoken language dialogue",
        "doc_scopus_id": "0034448735",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034448735",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Body and soul",
            "Communication,Action,Reasoning and Learning in robotics",
            "Human-robot interaction",
            "Sensory motor skill",
            "Spoken language dialogue"
        ],
        "doc_abstract": "The development of robots able to accept, via a friendly interface, instructions in terms of the concepts familiar to the human user remains a challenge. It is argued that designing and building such intelligent robots can be seen as the problem of integrating four main dimensions: human-robot communication, sensory motor skills and perception, decision-making capabilities and learning. Although these dimensions have been thoroughly studied in the past, their integration has seldom been attempted in a systematic way. It is further argued that, for the common user, the only sufficiently practical interface is spoken language. The \"body and soul\" of Carl, a robot currently under construction in our lab, are presented. The spoken language interface is given particular attention.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Intelligent control and decision-making demonstrated on a simple compass-guided robot",
        "doc_scopus_id": "0034498071",
        "doc_doi": "10.1109/ICSMC.2000.884354",
        "doc_eid": "2-s2.0-0034498071",
        "doc_date": "2000-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Compass guided robots",
            "Environmental exploration",
            "Task planning"
        ],
        "doc_abstract": "This paper presents the architecture and algorithms developed for Dom Dinis, a simple compass-guided robot built by the authors. This includes environment exploration, task planning and task execution. Environment exploration, based on repeating a reactive goal search, enables a progressive construction of a grid-based map. Based on the (possibly incomplete) map, the robot is able to plan its tasks. The execution capabilities of the robot include exception handling. Essential to all these capabilities is the knowledge of the robot's position in the world. The position is computed based on tracking traversed distances and followed orientations. Orientation is given by a compass. Dom Dinis doesn't use wheel encoders at all.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Failure recovery planning in assembly based on acquired experience: Learning by analogy",
        "doc_scopus_id": "0033364782",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0033364782",
        "doc_date": "1999-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Deductive transforms",
            "Failure recovery planning",
            "Inductive transforms",
            "Inverse transforms"
        ],
        "doc_abstract": "For complex tasks in flexible manufacturing as well as service applications, robots need to reason about the tasks and the environment in order to make decisions. This is a topic that is far from receiving from the robotics community the due attention. This paper presents a method for recovering from execution failures based on analogies with previous failure recovery episodes. The basic principles that explain the success of a failure recovery strategy are extracted based on several deductive as well as inductive transformations. In recovery planning based on these learned principles, the inverse transformations are applied.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning failure recovery knowledge for mechanical assembly",
        "doc_scopus_id": "0030393122",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030393122",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Flexible assembly system",
            "Mechanical assembly"
        ],
        "doc_abstract": "A framework for planning and supervision of robotized assembly tasks is initially presented, with emphasis on failure recovery. The approach to the integration of services and the modeling of tasks, resources and environment is briefly described. A planning strategy and domain knowledge for nominal plan execution is presented. Through the use of machine learning techniques, the supervision architecture will be given capabilities for improving its performance over time. In particular, an approach for memorizing failure recovery episodes, based on abstraction, deductive generalization and feature construction, is presented. Recovery planning consists of adapting plan skeletons from similar episodes previously occurred.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards intelligent execution supervision for flexible assembly systems",
        "doc_scopus_id": "0030380440",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030380440",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Failure diagnosis",
            "Failure recovery",
            "Flexible assembly systems",
            "Intelligent execution supervision"
        ],
        "doc_abstract": "Research results concerning error detection and recovery in robotized assembly systems, key components of flexible manufacturing systems, are presented. The approach to the integration of services and the modelling of tasks, resources and environment is described. A planning strategy and domain knowledge for nominal plan execution and for error recovery is presented. A supervision architecture provides, at different levels of abstraction, functions for dispatching actions, monitoring their execution, and diagnosing and recovering from failures. Through the use of machine learning techniques, the supervision architecture will be given capabilities for improving its performance over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integration and learning in supervision of flexible assembly systems",
        "doc_scopus_id": "0030125708",
        "doc_doi": "10.1109/70.488941",
        "doc_eid": "2-s2.0-0030125708",
        "doc_date": "1996-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Flexible assembly systems",
            "Supervision"
        ],
        "doc_abstract": "A generic architecture for evolutive supervision of robotized assembly tasks, in a context of integrated manufacturing systems, is presented. This architecture provides, at different levels of abstraction, functions for dispatching actions, monitoring their execution, and diagnosing and recovering from failures. The problem of integration of legacy systems is discussed and an implementation approach described. Modeling execution failures through taxonomies and causal relations plays a central role in diagnosis and recovery. Through the use of machine learning techniques, the supervision architecture will be given capabilities for improving its performance over time. Particular attention is given to the inductive generation of structured classification knowledge for diagnosis. Methodologies used, performed experiments, and obtained results are described in detail. © 1996 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Machine learning approach to error detection and recovery in assembly",
        "doc_scopus_id": "0029205584",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0029205584",
        "doc_date": "1995-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Domain knowledge",
            "Nominal plan execution"
        ],
        "doc_abstract": "Research results concerning error detection and recovery in robotized assembly systems, key components of flexible manufacturing systems, are presented. A planning strategy and domain knowledge for nominal plan execution and for error recovery is described. A supervision architecture provides, at different levels of abstraction, functions for dispatching actions, monitoring their execution, and diagnosing and recovering from failures. Through the use of machine learning techniques, the supervision architecture will be given capabilities for improving its performance over time. Particular attention is given to the inductive generation of structured classification knowledge for diagnosis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inductive generation of diagnostic knowledge for autonomous assembly",
        "doc_scopus_id": "0029200661",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0029200661",
        "doc_date": "1995-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Autonomous assembly",
            "Diagnostic knowledge",
            "Dispatching",
            "Failure diagnosis",
            "Failure recovery",
            "Generic architecture",
            "Global coordination",
            "Hierarchical structure",
            "Intelligent execution supervisor"
        ],
        "doc_abstract": "A generic architecture for evolutive supervision of robotized assembly tasks is presented. This architecture provides, at different levels of abstraction, functions for dispatching actions, monitoring their execution, and diagnosing and recovering from failures. Modeling execution failures through taxonomies and causal networks plays a central role in diagnosis and recovery. Through the use of machine learning techniques, the supervision architecture will be given capabilities for improving its performance over time. Particular attention is given to the inductive generation of structured classification knowledge for diagnosis. The applied methodologies, performed experiments and obtained results are described in detail.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning to diagnose failures of assembly tasks",
        "doc_scopus_id": "0038890646",
        "doc_doi": "10.1016/0066-4138(94)90049-3",
        "doc_eid": "2-s2.0-0038890646",
        "doc_date": "1994-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "An architecture for execution supervision of Robotic Assembly Tasks is presented. This architecture provides, at different levels of abstraction, functions for dispatching actions, monitoring their execution, and diagnosing and recovering from failures. Modeling execution failures through taxonomies and causal networks plays a central role in diagnosis and recovery. A discussion on the knowledge acquisition process, through the use of machine learning techniques, is made. Preliminary results in this area are presented and planned extensions discussed. © 1995.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "P, δ, α, λ(a): a rapid method for calculating population trends",
        "doc_scopus_id": "0028681169",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0028681169",
        "doc_date": "1994-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Earth and Planetary Sciences (miscellaneous)",
                "area_abbreviation": "EART",
                "area_code": "1901"
            }
        ],
        "doc_keywords": [
            "demographic method",
            "historical method",
            "methodological evaluation",
            "population trend",
            "Portugal"
        ],
        "doc_abstract": "This article discusses methods for establishing population trends making use of historic demographic material for Portugal. The major part of the discussion covers the 19th century and the present century, but baptismal trends are plotted back to the 16th century. Other case studies of population change are also taken back to the 16th century. -H.Clout",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Execution monitoring in assembly with learning capabilities",
        "doc_scopus_id": "0027927791",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027927791",
        "doc_date": "1994-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Execution failures",
            "Execution monitoring",
            "Supervision architecture"
        ],
        "doc_abstract": "A generic architecture for execution supervision of Robotic Assembly Tasks is presented. This architecture provides, at different levels of abstraction, functions for dispatching actions, monitoring their execution, and diagnosing and recovering from failures. Modeling execution failures through taxonomies and causal networks plays a central role in diagnosis and recovery. A discussion on the process of acquisition of such monitoring knowledge is made. Through the use of machine learning techniques, the supervision architecture will be given capabilities for improving its performance over time. Preliminary results of applying machine learning in this area are presented and planned extensions discussed.",
        "available": false,
        "clean_text": ""
    }
]