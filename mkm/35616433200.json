[
    {
        "doc_title": "Real-Time LSTM-RNN Classification of Floors with Different Friction Coefficients for a Walking Humanoid Robot Wearing a 3D Force System",
        "doc_scopus_id": "85118675966",
        "doc_doi": "10.1109/JSEN.2021.3124854",
        "doc_eid": "2-s2.0-85118675966",
        "doc_date": "2021-12-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Computational intelligence techniques",
            "Floor classification",
            "Foot",
            "Humanoid robot",
            "Humanoid robot locomotion",
            "Instrumented systems",
            "Legged locomotion",
            "Robot sensing system",
            "Task analysis",
            "Wearable instrumented system"
        ],
        "doc_abstract": "© 2001-2012 IEEE.In the study of biped humanoid robots it is crucial to achieve high precision and robustness in locomotion. Humanoid robots that operate in real world environments need to be able to physically recognize different grounds to best adapt their gait without losing their dynamic stability. This work proposes a technique to classify in real time the type of floor from a set of possibilities learnt off-line. Hence, the paper describes the collection and preparation of a dataset of contact forces, obtained with a wearable instrumented system, mixed with the information of the robot internal inertial sensor to classify the type of underlying surface of a walking humanoid robot. For this classification, the data are acquired for four different slippery floors at a rate of 100 Hz and it is used as input for a long short-term memory (LSTM) recurrent neural network (RNN). After testing different learning models architectures and tuning the models parameters, a good mapping between inputs and targets is achieved with a test classification accuracy greater than 92%. A real time experiment is presented to demonstrate the suitability of the proposed approach for the multi-classification problem addressed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Editorial: Special issue on Autonomous Driving and Driver Assistance Systems — Some main trends",
        "doc_scopus_id": "85109560838",
        "doc_doi": "10.1016/j.robot.2021.103832",
        "doc_eid": "2-s2.0-85109560838",
        "doc_date": "2021-10-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2021-06-12 2021-06-12 2021-07-12 2021-07-12 2021-09-02T10:55:37 S0921-8890(21)00117-2 S0921889021001172 10.1016/j.robot.2021.103832 S300 S300.1 FULL-TEXT 2021-09-02T10:29:16.817797Z 0 0 20211001 20211031 2021 2021-06-12T04:38:12.168906Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav body acknowledge affil articletitle auth authfirstini authfull authlast grantnumber grantsponsor grantsponsorid pubtype ref 0921-8890 09218890 true 144 144 C Volume 144 7 103832 103832 103832 202110 October 2021 2021-10-01 2021-10-31 2021 simple-article edi © 2021 Published by Elsevier B.V. EDITORIALSPECIALISSUEAUTONOMOUSDRIVINGDRIVERASSISTANCESYSTEMSMAINTRENDS SANTOS V 1 Introduction 2 Trends in this special issue Perception Deep learning Planing, control and management 3 Conclusions Acknowledgments References RATO 2021 103714 D BERSANI 2021 103662 M ZHAO 2020 103597 X BARADARANKHALKHALI 2020 103596 M OLIVEIRA 2020 103558 M ABDENNOUR 2021 103707 N LOBIANCO 2020 103623 L ALMEIDA 2020 103605 T LIKMETA 2020 103568 A LUCET 2021 103706 E KANAPRAM 2020 103652 D FERNANDEZ 2020 103624 F QIN 2020 103606 Z SANTOSX2021X103832 SANTOSX2021X103832XV 2023-07-12T00:00:00.000Z 2023-07-12T00:00:00.000Z © 2021 Published by Elsevier B.V. 2021-07-14T22:52:35.066Z FCT UID/CEC/00127/2020 FCT Fundação para a Ciência e a Tecnologia Generalitat de Catalunya CIDIS-205-2020 FIEC-16-2018 Generalitat de Catalunya This work has been supported by: the Spanish Government under Project TIN2017-89723-P , the “ CERCA Programme/ Generalitat de Catalunya ”, the ESPOL projects CIDIS-205-2020 and FIEC-16-2018 , FCT — Foundation for Science and Technology , in the context of project UID/CEC/00127/2020 . The authors gratefully acknowledge the support of the CYTED Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” ( REF-518RT0559 ). 0 item S0921-8890(21)00117-2 S0921889021001172 10.1016/j.robot.2021.103832 271599 2021-09-02T10:29:16.817797Z 2021-10-01 2021-10-31 true 328924 MAIN 3 68383 849 656 IMAGE-WEB-PDF 1 ROBOT 103832 103832 S0921-8890(21)00117-2 10.1016/j.robot.2021.103832 Editorial Editorial: Special issue on Autonomous Driving and Driver Assistance Systems — Some main trends Vitor Santos IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Angel D. Sappa ⁎ 1Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador 1Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863 Guayaquil Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador 2Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Bellaterra, Barcelona, Spain 2Computer Vision Center, Universitat Autònoma de Barcelona Bellaterra Barcelona 08193 Spain Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain ⁎ Corresponding editor. Miguel Oliveira IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Arturo de la Escalera Universidad Carlos III de Madrid, Escuela Politécnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid), Spain Universidad Carlos III de Madrid, Escuela Politécnica Superior C/ Butarque, 15 - 28911 Leganes (Madrid) Spain Universidad Carlos III de Madrid, Escuela Politcnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid) Spain 1 Introduction This special issue covers several topics associated to the main point of Autonomous Driving and Driving Assistance Systems, and most papers span a wide range of concerns. Therefore, there is not a single set of categories where papers can be associated to. So, in order to simplify the overview, a simple clustering was done. Clearly, perception is the most common theme. A relevant focus is also the learning based techniques, especially the Deep Learning front, which serves mainly as a tool for several fields of the entire robotics research domain, sometimes to solve new problems and some other times to tackle classical issues of perception and navigation that still remain challenging. A third large scope of the papers cover topics like planning and control up to full systems management; that covers higher levels of software organization and the integration of complete or very large systems covering many fronts from perception, to data fusion and process and task management at the level of the Intelligent Transportation Systems (ITS) realm. So, the 13 papers of this special issue are divided primarily into these three groups: 1. Perception [1] [2] [3] [4] [5] 2. Deep Learning [6] [7] [8] [9] 3. Planing, control and management [10] [11] [12] [13] As mentioned, this categorization is not very strict since topics overlap and in some cases more than one of these topics is covered in the paper. The intended idea is mainly to mark some major trends observed in the Autonomous Driving community. 2 Trends in this special issue Next sections provide a further insight of each paper within its major category but where their remainder components are also addressed. Perception In this section, there is one paper that focuses on the classical problem of Simultaneous Localization and Mapping (SLAM) [3]. Other papers include the detection or tracking of vehicles, obstacles or the road [2,4] [1]. Perception is carried out using several different sensors, from RGB sensors [3,4] to LiDAR [1] and RaDAR [2]. The fusion of information from these sensors requires accurate estimations of the geometric transformations between them. This problem of the calibration of the sensors in intelligent vehicles is addressed in [5]. Rato and Santos propose a new methodology for the detection of road curbs based on the density of the accumulated point cloud generated by the motion of the vehicle equipped with a 4-Layer LiDAR. Results from the algorithm are compared with data annotated on top of satellite images, which show that the detection is accurate [1]. Khalkhali et al. [4] propose a method that combines Situation Assessment (SA) information extracted from surveillance cameras in a driving environment with Kalman Filter (KF) equations. The resulting modified model is able to track objects in different situation scenarios based on SA information and state estimation from KF. Results in video sequences from different datasets show an average 25 percent performance improvement in vehicle tracking [4]. Zhao et al. [3] address the problem of SLAM from a perspective of enhancing the robustness of the method to challenging situations such as large-baseline motion, textureless environments, and great illumination changes. Several improvements are proposed across the pipeline, including the usage of more robust visual features, online photometric calibration, and a multi-scale analysis. Results indicate that the system improves both the accuracy and robustness of localization. Bersani et al. [2] present an integrated algorithm for the estimation of ego-vehicle and obstacles’ positioning and motion along a given road, modeled in curvilinear coordinates. Data from RaDARs and a LiDAR is fused in order to identify and track obstacles. The algorithm is tested and validated in a prototype intelligent vehicle. Oliveira et al. [5] propose a novel framework for the calibration of multi-sensor and multi-modal intelligent vehicle platforms. The proposed methodology is general and can be used for any intelligent vehicle. Results include a case study where the calibration of the four sensors of and intelligent vehicle is carried out. Deep learning The usage of machine learning based solutions is growing in most of computer vision applications, in the current special issue four papers are deep learning-based approaches, which represent about the 30% of publications. Two of these papers are focused on road and objects detection through deep learning models while the remaining papers are focused on the driving topic as detailed next. Regarding the publications focused on road and objects detection both of them target the lane detection problem under different schemes. Almeida et al. [8] propose a new representation based on the usage of two simultaneous deep learning techniques for road and lane detection in the context of mixed scenarios (i.e., structured, unstructured, lane based, curb based limits, etc.). The proposed approach has been evaluated in five different road scenarios (e.g., roundabout entrance, highway scenario, road occlusion, etc.) achieving a better overall performance than each algorithm individually applied. The proposed approach allows to change the number of simultaneous algorithms used in the detection, each of the used algorithms can be updated or replaced by another with more confidence. In contrast to the approach presented above, Lo Bianco et al. [7] propose a single architecture for detecting both road lane and road participants (e.g., pedestrian, vehicles). The proposed multi-task approach takes advantage of detected features to reduce computational requirements, hence a real time performance is reached even in configurations with limited hardware. The proposed approach is validated through a newly generated public dataset that contains about 20K images of different real scenarios. Obtained results show both the validity of the proposed multi-task model for road applications as well as the good overall performance in a real-time computation, which make it suitable for on-board operations. On the other hand, the publications tackling the driving action propose deep learning based approaches for driver identification [6] and a decision-making strategy for autonomous driving [9]. In Abdennour et al. [6] the authors propose to identify driver by means of a lighweight deep learning model that is trained with just the CAN-Bus vehicle data. The proposed method requires less than two hours of training to achieve an accuracy higher than the 99%. The driving problem is also tackled in Likmeta et al. [9], but in this case for autonomous vehicles by proposing a high-level decision-making system. The authors propose a combination of traditional rule-based strategies together with a reinforcement learning (RL) model. The usage of handcrafted rule-based controllers allows to always determine why a given decision was made; on the other hand the RL architecture enable to deal with complex scenarios, which are usually difficult to interpret. The best features of each approach are combined by designing parametric rule-based controllers, where rules can be provided by domain experts and their parameters are learned via RL. The manuscript presents an extensive numerical simulation in both highways and urban scenarios showing the validity of proposed approach. Planing, control and management There are two papers concerning issues related to parking systems: [10] [13], one for abnormality detection based on Self-awareness [11], and the last one shows how to use a cognitive multi-layer map to develop collaborative human–machine systems [12]. Lucet et al. [10] present an autonomous bus navigation and parking system in a bus depot. Several peculiarities make this case harder than cars: the environment is a confined area, dimensions and weight higher and the need of a better accuracy. The authors use a predictive controller, based on its model linearized around the changing path curvature value, to perform accurate curved paths tracking. The controller has been implemented in an industrial vehicle and tested under realistic conditions. Quin et al. [13] present an Automated Valet Parking system. The methodology is based on the directional graph search and it is divided into three parts: global path planning, path coordination strategy and parking path planning. The global path planning uses a directional Hybrid A* algorithm. The path coordination strategy gives a transitional path to connect the end node of the global path to the parking planning start node. The parking path planning generates a parking path to guide the vehicle using a modified C-type vertical parking path planning algorithm. The system is validated using simulations on Matlab and PreScan. Kanaram et al. [11] use self-awareness for the detection of abnormalities instead of a manually programmed set of nested conditions checking for their presence. Multi-sensory time-series data are used to develop Dynamic Bayesian Network models used for state prediction and the detection of abnormalities. Real experiments of autonomous vehicles performing various tasks under real conditions are used to develop and test the algorithm. Fernandez et al. [12] introduce a cognitive layer called Associated Reality to enhance the information, knowledge and communication processes needed for Advanced Driver Assistance Systems (ADAS) and Automated and Autonomous Vehicles. The proposed architecture includes an augmented Local Dynamic Map and an augmented Graph Database. They show its use for vehicle localization and mapping in road tunnels. 3 Conclusions For some time now, it is clear that the Autonomous Driving challenges and the Driver Assistance Systems share similar tools to a common purpose, which is essentially to ensure the safety of drivers, passengers and all agents in the road. Perception continues to be the most common topic, which is clear from the proportion of papers in this special issue which are devoted to this topic. Multi-modality seems to be a trend too, and issues of calibration in multi-sensorial and multi-modality setups are also found as relevant concerns by authors. After perception, authors address the problem of modeling and controlling vehicles and related systems, both on board or on the road. The classical approaches, especially in planning and controlling are still exploited and improved by authors, but it is undeniable that learning techniques, mainly Deep Learning, seem to have established as critical tools, mainly in perception for classification, with very good results in semantic segmentation of images, which used to be a huge challenge some years ago. As a final corollary, and to bring the challenge to higher levels, we also find works, usually based on large projects, that combine many of the tools described an others to propose entire setups and demonstrators, ready to merge in full featured Intelligent Transportation Systems. We have not yet observed abundant full end-to-end systems, but there is a feeling that the path is being left open to that, perhaps to be covered in future versions of these special issues in ADDAS. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work has been supported by: the Spanish Government under Project TIN2017-89723-P, the “CERCA Programme/ Generalitat de Catalunya”, the ESPOL projects CIDIS-205-2020 and FIEC-16-2018, FCT — Foundation for Science and Technology, in the context of project UID/CEC/00127/2020. The authors gratefully acknowledge the support of the CYTED Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” (REF-518RT0559). References [1] Rato D. Santos V. Lidar based detection of road boundaries using the density of accumulated point clouds and their gradients Robot. Auton. Syst. 138 2021 103714 D. Rato, V. Santos, Lidar based detection of road boundaries using the density of accumulated point clouds and their gradients, Robotics and Autonomous Systems 138 (2021) 103714. [2] Bersani M. Mentasti S. Dahal P. Arrigoni S. Vignati M. Cheli F. Matteucci M. An integrated algorithm for ego-vehicle and obstacles state estimation for autonomous driving Robot. Auton. Syst. 139 2021 103662 M. Bersani, S. Mentasti, P. Dahal, S. Arrigoni, M. Vignati, F. Cheli, M. Matteucci, An integrated algorithm for ego-vehicle and obstacles state estimation for autonomous driving, Robotics and Autonomous Systems 139 (2021) 103662. [3] Zhao X. Liu L. Zheng R. Ye W. Liu Y. A robust stereo feature-aided semi-direct slam system Robot. Auton. Syst. 132 2020 103597 X. Zhao, L. Liu, R. Zheng, W. Ye, Y. Liu, A robust stereo feature-aided semi-direct slam system, Robotics and Autonomous Systems 132 (2020) 103597. [4] Baradaran Khalkhali M. Vahedian A. Sadoghi Yazdi H. Vehicle tracking with Kalman filter using online situation assessment Robot. Auton. Syst. 131 2020 103596 M. Baradaran Khalkhali, A. Vahedian, H. Sadoghi Yazdi, Vehicle tracking with kalman filter using online situation assessment, Robotics and Autonomous Systems 131 (2020) 103596. [5] Oliveira M. Castro A. Madeira T. Pedrosa E. Dias P. Santos V. A ros framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Robot. Auton. Syst. 131 2020 103558 M. Oliveira, A. Castro, T. Madeira, E. Pedrosa, P. Dias, V. Santos, A ros framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach, Robotics and Autonomous Systems 131 (2020) 103558. [6] Abdennour N. Ouni T. Amor N.B. Driver identification using only the can-bus vehicle data through an rcn deep learning approach Robot. Auton. Syst. 136 2021 103707 N. Abdennour, T. Ouni, N. B. Amor, Driver identification using only the can-bus vehicle data through an rcn deep learning approach, Robotics and Autonomous Systems 136 (2021) 103707. [7] Lo Bianco L.C. Beltrán J. López G.F. García F. Al-Kaff A. Joint semantic segmentation of road objects and lanes using convolutional neural networks Robot. Auton. Syst. 133 2020 103623 L. C. Lo Bianco, J. Beltrán, G. F. López, F. García, A. Al-Kaff, Joint semantic segmentation of road objects and lanes using convolutional neural networks, Robotics and Autonomous Systems 133 (2020) 103623. [8] Almeida T. Lourenco B. Santos V. Road detection based on simultaneous deep learning approaches Robot. Auton. Syst. 133 2020 103605 T. Almeida, B. Lourenço, V. Santos, Road detection based on simultaneous deep learning approaches, Robotics and Autonomous Systems 133 (2020) 103605. [9] Likmeta A. Metelli A.M. Tirinzoni A. Giol R. Restelli M. Romano D. Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving Robot. Auton. Syst. 131 2020 103568 A. Likmeta, A. M. Metelli, A. Tirinzoni, R. Giol, M. Restelli, D. Romano, Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving, Robotics and Autonomous Systems 131 (2020) 103568. [10] Lucet E. Micaelli A. Russotto F.-X. Accurate autonomous navigation strategy dedicated to the storage of buses in a bus center Robot. Auton. Syst. 136 2021 103706 E. Lucet, A. Micaelli, F.-X. Russotto, Accurate autonomous navigation strategy dedicated to the storage of buses in a bus center, Robotics and Autonomous Systems 136 (2021) 103706. [11] Kanapram D.T. Marin-Plaza P. Marcenaro L. Martin D. de la Escalera A. Regazzoni C. Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection Robot. Auton. Syst. 134 2020 103652 D. T. Kanapram, P. Marin-Plaza, L. Marcenaro, D. Martin, A. de la Escalera, C. Regazzoni, Self-awareness in intelligent vehicles: Feature based dynamic bayesian models for abnormality detection, Robotics and Autonomous Systems 134 (2020) 103652. [12] Fernandez F. Sanchez A. Velez J.F. Moreno B. Associated reality: A cognitive human–machine layer for autonomous driving Robot. Auton. Syst. 133 2020 103624 F. Fernandez, A. Sanchez, J. F. Velez, B. Moreno, Associated reality: A cognitive human–machine layer for autonomous driving, Robotics and Autonomous Systems 133 (2020) 103624. [13] Qin Z. Chen X. Hu M. Chen L. Fan J. A novel path planning methodology for automated valet parking based on directional graph search and geometry curve Robot. Auton. Syst. 132 2020 103606 Z. Qin, X. Chen, M. Hu, L. Chen, J. Fan, A novel path planning methodology for automated valet parking based on directional graph search and geometry curve, Robotics and Autonomous Systems 132 (2020) 103606. "
    },
    {
        "doc_title": "A general approach to hand-eye calibration through the optimization of atomic transformations",
        "doc_scopus_id": "85103787190",
        "doc_doi": "10.1109/TRO.2021.3062306",
        "doc_eid": "2-s2.0-85103787190",
        "doc_date": "2021-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Atomic transformation",
            "Calibration algorithm",
            "Calibration problems",
            "Calibration procedure",
            "Initial configuration",
            "Nonlinear least squares methods",
            "Optimization procedures",
            "Robot operating system"
        ],
        "doc_abstract": "© 2021 IEEE.This article proposes a general approach to solve the hand-eye calibration problem. The system is general since it is able to calibrate any number of cameras and, moreover, is able to simultaneously perform the calibration of several instances of the two common hand-eye calibration use cases: eye-on-hand and eye-to-base. The calibration is solved with a nonlinear least squares method, and the reprojection error is used as a metric to guide the optimization procedure. Our approach is seamlessly integrated with the robot operating system framework and allows for the interactive positioning of sensors and labeling of data, facilitating both the data acquisition and labeling and the calibration procedures. Results show that the proposed approach is able to handle any calibration use case with a minimal initial configuration. The approach is compared with several other state-of-the-art hand-eye calibration algorithms. Results show that the proposed approach produces very accurate calibrations when compared to the state of the art.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparative Analysis of Deep Neural Networks for the Detection and Decoding of Data Matrix Landmarks in Cluttered Indoor Environments",
        "doc_scopus_id": "85112339177",
        "doc_doi": "10.1007/s10846-021-01442-x",
        "doc_eid": "2-s2.0-85112339177",
        "doc_date": "2021-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Automated guided vehicles",
            "Comparative analysis",
            "Conventional methods",
            "Indoor environment",
            "Learning network",
            "Processing time",
            "Self localization",
            "Visual landmarks"
        ],
        "doc_abstract": "© 2021, The Author(s).Data Matrix patterns imprinted as passive visual landmarks have shown to be a valid solution for the self-localization of Automated Guided Vehicles (AGVs) in shop floors. However, existing Data Matrix decoding applications take a long time to detect and segment the markers in the input image. Therefore, this paper proposes a pipeline where the detector is based on a real-time Deep Learning network and the decoder is a conventional method, i.e. the implementation in libdmtx. To do so, several types of Deep Neural Networks (DNNs) for object detection were studied, trained, compared, and assessed. The architectures range from region proposals (Faster R-CNN) to single-shot methods (SSD and YOLO). This study focused on performance and processing time to select the best Deep Learning (DL) model to carry out the detection of the visual markers. Additionally, a specific data set was created to evaluate those networks. This test set includes demanding situations, such as high illumination gradients in the same scene and Data Matrix markers positioned in skewed planes. The proposed approach outperformed the best known and most used Data Matrix decoder available in libraries like libdmtx.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Trends of human-robot collaboration in industry contexts: Handover, learning, and metrics",
        "doc_scopus_id": "85107883362",
        "doc_doi": "10.3390/s21124113",
        "doc_eid": "2-s2.0-85107883362",
        "doc_date": "2021-06-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Coordinated actions",
            "Future challenges",
            "Human-robot collaboration",
            "Human-robot communication",
            "Industry contexts",
            "Literature overview",
            "Physical interactions",
            "Task accomplishment",
            "Artificial Intelligence",
            "Benchmarking",
            "Hand Strength",
            "Humans",
            "Industry",
            "Robotics"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.Repetitive industrial tasks can be easily performed by traditional robotic systems. However, many other works require cognitive knowledge that only humans can provide. Human-Robot Collaboration (HRC) emerges as an ideal concept of co-working between a human operator and a robot, representing one of the most significant subjects for human-life improvement.The ultimate goal is to achieve physical interaction, where handing over an object plays a crucial role for an effective task accomplishment. Considerable research work had been developed in this particular field in recent years, where several solutions were already proposed. Nonetheless, some particular issues regarding Human-Robot Collaboration still hold an open path to truly important research improvements. This paper provides a literature overview, defining the HRC concept, enumerating the distinct human-robot communication channels, and discussing the physical interaction that this collaboration entails. Moreover, future challenges for a natural and intuitive collaboration are exposed: the machine must behave like a human especially in the pre-grasping/grasping phases and the handover procedure should be fluent and bidirectional, for an articulated function development. These are the focus of the near future investigation aiming to shed light on the complex combination of predictive and reactive control mechanisms promoting coordination and understanding. Following recent progress in artificial intelligence, learning exploration stand as the key element to allow the generation of coordinated actions and their shaping by experience.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Robust texture mapping using rgb-d cameras",
        "doc_scopus_id": "85105402165",
        "doc_doi": "10.3390/s21093248",
        "doc_eid": "2-s2.0-85105402165",
        "doc_date": "2021-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D meshes",
            "Camera misalignments",
            "Camera pose estimation",
            "Estimation methodologies",
            "Pose estimation errors",
            "Rgb-d cameras",
            "Texture mapping",
            "Visual artifacts"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.The creation of a textured 3D mesh from a set of RGD-D images often results in textured meshes that yield unappealing visual artifacts. The main cause is the misalignments between the RGB-D images due to inaccurate camera pose estimations. While there are many works that focus on improving those estimates, the fact is that this is a cumbersome problem, in particular due to the accumulation of pose estimation errors. In this work, we conjecture that camera poses estimation methodologies will always display non-neglectable errors. Hence, the need for more robust texture mapping methodologies, capable of producing quality textures even in considerable camera misalignments scenarios. To this end, we argue that use of the depth data from RGB-D images can be an invaluable help to confer such robustness to the texture mapping process. Results show that the complete texture mapping procedure proposed in this paper is able to significantly improve the quality of the produced textured 3D meshes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "LIDAR based detection of road boundaries using the density of accumulated point clouds and their gradients",
        "doc_scopus_id": "85100004919",
        "doc_doi": "10.1016/j.robot.2020.103714",
        "doc_eid": "2-s2.0-85100004919",
        "doc_date": "2021-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Detection capability",
            "Driver assistance",
            "Horizontal surfaces",
            "Measurements of",
            "Processing speed",
            "Traditional computers",
            "Vertical surface"
        ],
        "doc_abstract": "© 2020 Elsevier B.V.Autonomous driving and driver assistance require a continuous and reliable perception of the road boundaries, namely curbs and berms, including also other minor, or not so minor, obstacles in the neighborhood of the car. This paper proposes to use a 4-layer LIDAR placed close to the ground to capture measurements of the road ahead of the car and allow the detection of the boundaries. This setup provides a special point of view that allows the accumulation of points on vertical surfaces on the road as the car moves, which increases the point density in vertical surfaces but keeps it limited in horizontal surfaces. This technique allows to successfully distinguish curbs from the flat parts of the road. However, this approach has some limitations, namely to detect berms, and another approach had to be developed using the gradient of point density, which extends the detection capabilities to berms and negative obstacles. This is achieved by flattening the point clouds to 2D and use traditional computer vision gradient and edge detection techniques, which also improves the processing speed. Results are obtained on the ATLASCAR real system, at different velocities, and a good performance is reached when comparing to a manually created ground truth.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2021-01-22 2021-01-22 2021-01-27 2021-01-27 2021-02-15T14:56:19 S0921-8890(20)30554-6 S0921889020305546 10.1016/j.robot.2020.103714 S300 S300.1 FULL-TEXT 2021-04-22T02:41:57.075141Z 0 0 20210401 20210430 2021 2021-01-22T16:02:57.761404Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 138 138 C Volume 138 3 103714 103714 103714 202104 April 2021 2021-04-01 2021-04-30 2021 article fla © 2020 Elsevier B.V. All rights reserved. LIDARBASEDDETECTIONROADBOUNDARIESUSINGDENSITYACCUMULATEDPOINTCLOUDSGRADIENTS RATO D 1 Introduction 2 Related work 3 Proposed approach 3.1 Initial approach 3.1.1 Simulation 3.1.2 Parameter definition 3.1.3 Limitations of the initial approach 3.2 Density grids — accumulated point cloud density 3.3 Gradient and edge detection 3.4 Ground truth definition 4 Tests and results 4.1 Qualitative evaluation 4.2 Quantitative evaluation 5 Conclusion Acknowledgment References SALVADO 2012 P RECONSTRUCAODINAMICADEMAPALOCALPARAOATLASCAR BARHILLEL 2014 727 745 A PETERSON 2008 612 619 K 2008IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS FASTFEATUREDETECTIONSTOCHASTICPARAMETERESTIMATIONROADSHAPEUSINGMULTIPLELIDAR ZHANG 2010 845 848 W 2010IEEEINTELLIGENTVEHICLESSYMPOSIUM LIDARBASEDROADROADEDGEDETECTION MARQUES 2017 T DETECTIONROADNAVIGABILITYFORATLASCAR2USINGLIDARINCLINOMETERDATA FRITSCH 2013 1693 1700 J 16THINTERNATIONALIEEECONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC2013 ANEWPERFORMANCEMEASUREEVALUATIONBENCHMARKFORROADDETECTIONALGORITHMS RATO 2019 D DETECTIONNAVIGABLEROADLIMITSBYANALYSISACCUMULATEDPOINTCLOUDDENSITY RATOX2021X103714 RATOX2021X103714XD 2023-01-27T00:00:00.000Z 2023-01-27T00:00:00.000Z © 2020 Elsevier B.V. All rights reserved. 2020-12-26T10:21:41.255Z item S0921-8890(20)30554-6 S0921889020305546 10.1016/j.robot.2020.103714 271599 2021-04-22T02:41:57.075141Z 2021-04-01 2021-04-30 true 4081300 MAIN 13 61569 849 656 IMAGE-WEB-PDF 1 gr1 56934 347 368 gr4 63515 228 575 pic2 5195 132 113 gr18 51994 252 539 gr11 33375 165 506 gr3 19970 81 581 gr16 21302 266 504 gr13 33602 138 696 gr20 25396 207 697 gr10 45289 334 519 gr6 20719 208 501 pic1 6703 132 113 gr14 17828 219 367 gr21 35377 206 368 gr8 37186 308 697 gr15 24412 261 504 gr7 72072 274 689 gr19 36953 225 695 gr9 29260 294 294 gr12 25755 177 511 gr22 52717 286 676 gr5 19874 247 304 gr2 11502 124 559 gr17 56681 253 539 gr1 24498 164 174 gr4 10947 87 219 pic2 18390 163 140 gr18 18546 102 219 gr11 6560 72 219 gr3 3476 30 219 gr16 3563 115 219 gr13 4152 43 219 gr20 4941 65 219 gr10 6635 141 219 gr6 5940 91 219 pic1 19488 163 140 gr14 6028 131 219 gr21 24012 123 219 gr8 5364 97 219 gr15 3750 114 219 gr7 8035 87 219 gr19 7937 71 219 gr9 8291 164 164 gr12 4281 76 219 gr22 4792 93 219 gr5 7899 164 202 gr2 2682 49 219 gr17 19342 103 219 gr1 574695 1537 1630 gr4 660475 1010 2546 pic2 39992 583 500 gr18 450521 1117 2388 gr11 411632 732 2240 gr3 133065 357 2572 gr16 158727 1177 2233 gr13 265570 610 3083 gr20 192884 917 3087 gr10 352459 1479 2298 gr6 155553 923 2218 pic1 60929 583 500 gr14 139606 973 1627 gr21 428145 913 1630 gr8 291914 1366 3088 gr15 178871 1158 2233 gr7 869603 1212 3050 gr19 391049 995 3078 gr9 251935 1305 1305 gr12 211170 782 2263 gr22 360350 1266 2992 gr5 146477 1093 1347 gr2 106336 550 2477 gr17 482091 1119 2388 si13 1646 si78 29749 si47 11308 si35 35859 si66 30141 si12 3537 si24 1679 si42 6847 si79 12696 si15 23903 si9 761 si73 1757 si53 2998 si3 963 si10 1582 si49 6158 si40 5186 si80 13461 si63 14594 si34 1571 si55 3715 si74 3149 si64 2269 si59 7111 si51 27262 si27 4874 si39 6001 si31 11129 si54 3338 si77 7753 si41 3800 si61 15334 si76 13702 si28 6875 si33 17109 si57 4604 si75 11801 si62 14958 si50 3627 si30 13954 si21 2843 si48 5112 si46 6704 si65 10773 si32 8513 si60 13754 am 9319293 ROBOT 103714 103714 S0921-8890(20)30554-6 10.1016/j.robot.2020.103714 Elsevier B.V. Fig. 1 Placement of AtlasCar2 sensors. Fig. 2 LIDAR placement in the AtlasCar2 and illustration of the 4 measurement beams. Fig. 3 Flowchart of road/road-edge detection algorithm employed by [8]. Fig. 4 The difference of perspective between a Velodyne and the used LIDAR sensor mounted in the front of the car. Fig. 5 Top view of the four laser scan that intersect the road plane in a real world situation. Fig. 6 Diagram of the working principle to assemble the four scans into a single accumulation cloud. Fig. 7 Process of road reconstruction using the accumulated point cloud (grid with 20m × 20m). Fig. 8 Frames used to reconstruct the environment in AtlasCar2. Fig. 9 Illustration of the displacement and vertical line caused by a side-walk. Fig. 10 Illustration of higher local concentration of points in vertical planes and lower local concentration of points in horizontal planes. Fig. 11 Simplified illustration of the laser beam paths in four scanning planes, with h = 0.4 m , α = 0.6 ∘ , ϕ = 5 ∘ and δ x = 0 m . Fig. 12 Simplified side-view illustration of the laser beams in four scanning planes during movement, with h = 0.4 m , α = 0.6 ∘ , ϕ = 5 ∘ , δ x = 4 m and v c a r = 50 km/h . Fig. 13 Types of road profiles [11]. Fig. 14 Setup of the simulation scenario. Fig. 15 Influence in accumulated point cloud density of the filtering circle radius, for accumulation in a vertical plane. Fig. 16 Accumulated point cloud density on the road plane using different radius for the filtering circle. Fig. 17 Road boundary detection in different situations. Fig. 18 Road boundary detection during heavy slowing and stopping of the vehicle. Fig. 19 Camera image, point cloud and density grid with a reference frame. Fig. 20 Results of several edge detection grids in the identification of navigable space in a straight road situation. Fig. 21 Satellite view of a path used for quantitative evaluation. In red, the GPS path of the car. In green, the ground truth road limits. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 22 Influence of car velocity and cell resolutions, in the statistical measures for Simple Gradient algorithm with 0.4 m/cell and no threshold (10 to 30 m). Table 1 Performance of algorithms in the presence of Type I curbs (values in %). Filter Precision Specificity NPV Sensitivity F-measure Accuracy Laplace 84.1 90.5 70.7 60.5 70.0 75.9 Gradient 83.8 86.6 84.5 83.3 83.0 84.6 Sobel 83.9 89.8 71.8 63.1 71.7 76.7 Prewitt 81.6 87.5 78.9 72.6 76.0 80.2 Kirsh 86.9 89.9 72.3 66.7 75.4 78.1 Canny 86.0 91.5 66.6 52.3 62.5 71.7 Table 2 Performance of algorithms in the presence of Type III curbs (values in %). Filte Precision Specificity NPV Sensitivity F-measure Accuracy Laplace 94.6 97.9 76.0 52.8 67.6 80.1 Gradient 85.0 91.8 83.6 72.0 77.9 84.0 Sobel 91.0 96.9 76.2 51.4 65.5 79.4 Prewitt 86.9 93.8 79.5 63.0 73.0 81.6 Kirsh 94.6 97.8 78.9 59.6 73.0 82.7 Canny 78.9 95.2 67.5 27.3 39.6 68.8 Table 3 Algorithms performance with the chosen parameters (values in %). Filter Precision Specificity NPV Sensitivity F-measure Accuracy Laplace 88.9 88.1 84.8 85.6 87.1 86.8 Gradient 89.4 89.0 83.4 83.9 86.5 86.3 Sobel 87.1 87.6 80.4 79.6 83.0 83.5 Prewitt 87.6 88.5 77.2 74.4 80.0 81.4 Kirsch 86.6 86.0 84.8 85.3 85.9 85.7 Canny 87.3 91.1 67.5 56.0 66.3 73.3 ☆ This work was partially supported by project UID/CEC/00127/2019. LIDAR based detection of road boundaries using the density of accumulated point clouds and their gradients Daniela Rato ⁎ Vítor Santos DEM, IEETA, Portugal DEM, IEETA Portugal DEM, IEETA ⁎ Corresponding author. Autonomous driving and driver assistance require a continuous and reliable perception of the road boundaries, namely curbs and berms, including also other minor, or not so minor, obstacles in the neighborhood of the car. This paper proposes to use a 4-layer LIDAR placed close to the ground to capture measurements of the road ahead of the car and allow the detection of the boundaries. This setup provides a special point of view that allows the accumulation of points on vertical surfaces on the road as the car moves, which increases the point density in vertical surfaces but keeps it limited in horizontal surfaces. This technique allows to successfully distinguish curbs from the flat parts of the road. However, this approach has some limitations, namely to detect berms, and another approach had to be developed using the gradient of point density, which extends the detection capabilities to berms and negative obstacles. This is achieved by flattening the point clouds to 2D and use traditional computer vision gradient and edge detection techniques, which also improves the processing speed. Results are obtained on the ATLASCAR real system, at different velocities, and a good performance is reached when comparing to a manually created ground truth. Keywords LIDAR Road curbs Point clouds Occupancy grid Gradient 1 Introduction There is an increasing popularity of Autonomous Driving (AD) and driving assistance systems, but large and complex challenges remain both at control and perception levels. Road boundaries are a crucial element for car navigation, and their detection is certainly one of the biggest challenges when a general solution is needed. The ultimate solution will likely require multi-modal perception fusion using multiple complementary or redundant techniques for increased robustness and reliability. One part of the overall problem can focus on the detection of curbs and road berms, and that is the main subject of this paper, which proposes the usage of near-to-ground range scanning using Light Detection And Ranging (LIDAR). The proposed approach provides a solution to this problem using a 4-layer 3D LIDAR mounted in the front of a car, close to the ground. LIDAR sensors measure the distance to the closest object, thus allowing the creation of a point cloud with three-dimensional points corresponding to the intersection of laser beams with the objects ahead. Fig. 1 shows the multiple sensors mounted on AtlasCar2, the main setup for this work, to assist the study on AD. The most relevant sensor for the work of this paper is the SICK LD-MRS4000001, a 4-layer 3D LIDAR prepared for outdoor usage. In the current setup, the sensor is configured to output data at 50Hz with an angular resolution of 0.5 ∘ and an aperture of 85 ∘ . Novatel SPAN-IGM-A1 sensor and Novatel GPS-702-GG Dual-Frequency Antenna, not visible in the figure, mounted on top of the car, are also used to localize the car and its frames in the world, allowing the road reconstruction. The Inertial Measurement Unit (IMU) and Global Positioning System (GPS) combo constitute a powerful and precise global positioning solution. To calculate the inclination of the car (roll and pitch), the SICK DT20 Hi sensors are used with a refresh rate of up to 400Hz, measuring distances between 200.0mm and 700.0mm. In the early history of the Atlas project, the accumulation of laser readings with the car movement has produced interesting results in real-time road reconstruction ahead of the car, but no automatic detection of the road limits was performed at the time [1]. The LIDAR was placed on the rooftop pointing down at a given angle, and laser readings were converted into point clouds for further representation and processing. In the current work, the goal is the automatic identification of road curbs or similar obstacles at the road level. The fact that the study of density and accumulation of point cloud scans are approaches that are not deeply developed on the literature. This, added to the differentiating position of the LIDAR sensor in the front of the car, forms an interesting and unique approach. The advantage of placing the sensor in front of the car, as shown in Fig. 2, unlike other solutions, is to focus more on objects at road level and to have a detailed perspective near the ground rather than a general vision of the road with larger surroundings that, most of the times, are not required for navigation purposes. By doing this, road curbs, holes, and similar obstacles are identified with more precision. The general idea was in the first place to take advantage of point cloud density to detect vertical surfaces, but a later approach extends the technique to the density gradient analysis by using edge detection tools with filters like Sobel and Canny to identify density changes that usually define obstacles. As will be shown, this technique is a step ahead of the simple density analysis by considering both positive and negative variations of density, allowing to also identify negative obstacles. In general, the approach produces good results and can detect both positive and negative curb profiles and is also a good complement to Advanced Driver’s Assistance Systems (ADAS) when combined with other visual techniques to detect lanes and soft limits. Note that image semantic segmentation is a visual-based approach, and was not compared directly with the proposed LIDAR technique. Moreover, this LIDAR technique is expected to complement the visual based perception, not necessarily to replace it. 2 Related work The problem of road detection and lane perception is an essential tool for ADAS. As such, it has been an active field of research for the past decades with considerable progress made in recent years. The problem is faced under various scenarios, with different task definitions, leading to the usage of diverse sensing modalities and techniques [2]. When it comes to road detection and identifying road limits, there are two main approaches: vision-based methods and laser-based methods. The first is more affected by external factors such as lane and road appearance diversity, image clarity issues and poor visibility conditions [2]. On the other hand, the second technique represents a more robust system yet more difficult to work with, mainly used for identification of lane marks and road boundaries, estimating planes and detecting curbs and berms. The ideal solution would be to incorporate both techniques and merge the results to take advantage of individual strengths. When it comes to road limits or curb detection using LIDAR, there are different methodologies. Contrarily to the proposed solution in this paper, most methodologies take advantage of 360 ∘ LIDAR on top of the car, like [3–6], allowing the detection of higher objects like trees and other cars, however, showing a lack of accuracy in ground-level objects. Many methods also take advantage of elevation profiles, by filtering objects according to their elevation difference and grouping the ones with similar values. For example, Huang et al. [7] use a roof-mounted 3D LIDAR, a prediction method is used to find the height difference between two points and create an elevation map with the predicted measures. Also with a 360 ∘ 3D-LIDAR, Zhang et al. [3] use a sliding beam method for road segmentation and a search based method applied to detect the curbs in each frame. The sliding beam method contains two sections: one bottom layer beam model that allows finding the beam angles, that represent the road direction in the current location of the vehicle in a beam zone inside the region of interest, and a top layer beam model, that allows building segments of the road ahead of the vehicle. After this, spatial features of curbs are defined and extracted; the features’ thresholds are defined and the search-based method is applied to detect curbs in each segment of the road. In the end, the algorithm checks if the parameters are within the thresholds and makes the decision if a point belongs or not to a road curb. This method proposed by Zhang et al. proves to have more efficient results when compared with similar approaches. Zhang [8] developed a framework applied in the “Boss” vehicle for the DARPA Urban Challenge. That approach uses a point array gathered by a two-dimensional LIDAR mounted on top of the vehicle with a downwards pointing angle. This data is processed according to the cascade method shown in Fig. 3, allowing the identification of the road and the detection of the road edges. The road and road-edge points are first detected in elevation data, and then validated on the ground plane. An elevation-based road signal is extracted from the LIDAR range data and the signal is processed by a filter to select candidate regions for road-segment classification. In each candidate region, features are extracted and classified as road-segment or not. The detected road edges are the left-most and right-most boundaries of the detected road-segments. The LIDAR range data is also projected onto the ground plane to identify curb lines using a road curb detection module and compared to a simple road model the top-down view to determining whether the candidate region is a road segment with its road-edges. These algorithms then gather the points belonging to the curb and accumulate them according to the movement of the vehicle, and filter them to eliminate noise. The method developed in [8] has fast processing speeds, and produces attractive results in the straight roads with the same elevation, but falls short in occluded, sunken or uphill road areas. Xu et al. in [9] use an approach with some similar concept to the one proposed in this paper, based on a density gradient of point clouds. The methodology consists of calculating the difference of density in adjacent voxels in 2D and then adding the third dimension as the difference of elevation between voxels. The road classification is then based on three principles: voxel within the surface (one large gradient), voxel intersecting two surfaces (two large gradient) and voxel in the intersection of three mutually non-parallel surface (three large gradient). Concerning this work, Xu et al.’s approach is the one that comes the closest to the proposed work, by also using point cloud density and gradients. However, Xu’s methodology diverges by adding the third component of gradient as elevation differences and follows a completely different path by doing road classification and identifying features as opposed to focusing on road limits. Additionally, all the previously described methodologies use a 3D 360 ∘ LIDAR, offering a completely different perspective, not so close to the car and focusing on a wider view with bigger obstacles like tree or people (Fig. 4(a)). The LIDAR used in the proposed method in this paper only has a field of view 85 ∘ and is placed to cover objects close to the car and at ground level, like holes, depressions, road curbs, etc. (Fig. 4(b)). 3 Proposed approach The goal of this study is to develop a methodology to obtain the navigable road limits on-the-fly during navigation. Numerous other approaches are used to obtain the road limits but, through the years, in the Atlas Project, the accumulation of laser readings (i.e. point clouds) with the car movement proved to be an interesting and less studied approach. This accumulation, further studied by T. Marques [10], allows instant identification of road obstacles for decision making. To develop a new and effective methodology to detect road limits, the concept was to take advantage of point cloud density to detect not only positive obstacles, defined as obstacles above the road surface, like curbs, but also negative obstacles, defined as obstacles below the road surface, as depressions or inverted curbs (berms). And although other authors have also applied gradient filters to laser readings, this method takes it a step further by applying edge detection techniques to flattened point clouds. 3.1 Initial approach To reconstruct the environment around the car as it navigates, the laser readings from the SICK LD-MRS sensor were gathered and accumulated to increase the 3D information, forming a point cloud representative of the perceived environment. Given the limited amount of scan planes (four), the resulting projection is, in ideal conditions, a set of points arranged in four lines at specific distances, shown in Fig. 5, which represents this projection on a real section of road. However, this information proved to be insufficient to consistently extract the road features and boundaries. To counteract this effect, the number of data points used to represent the road needed to be increased. By taking advantage of the vehicle’s movement, causing the displacement of the sensor along with it, different sections of the road are scanned as the car moves. As an example, if the sensor has a measuring frequency of 50Hz, and the car is moving at 50km/h, the sensor performs a measurement every 0.29m. By accumulating these successive measurements, it is possible to form a point cloud that represents the road and its features with good accuracy and density to allow the extraction of the road boundaries. To accumulate those LIDAR readings, successive data from each four of the laser scans are assembled, relative to an input frame, in a rolling buffer of a predefined size (Fig. 6), allowing to control the size of the road reconstruction point cloud. The assembled four laser scans are then merged into a single point cloud ready to be processed. This method is very efficient and there is almost no delay between the acquisition of the data and its assembly into the point cloud, making it a good solution even for a fast-moving vehicle, as can be seen in Fig. 7(a), that represents the laser scans in red and the previously accumulated points in white. Fig. 7(b) illustrates a reconstruction of the road accomplished through the described method, where the point cloud is colored based on height relative to the ground. This point cloud is also corrected by the inclinometry of the car, calculated with the four SICK DT20 Hi sensors placed in each corner of the car. This helps to reduce the effects of deceleration, acceleration, turning and breaking. The reconstructed section of the road illustrated in Fig. 7(b) proved to be a good representation of the environment, with rich information about the road and curb profiles. This reconstruction also presents some interesting properties regarding the representation of the road boundaries, which can easily be identified in the image due to the higher concentration of points characteristic of these regions. Additionally, in this particular image, it can be observed that, despite only using the two lowest laser scans, the reconstruction is performed up to a distance of approximately 20m from the front of the vehicle. This suggests that it is viable to perform the road boundary extraction from this point cloud to use in navigation algorithms. To reconstruct the point clouds in relation to the car movement, the location of the LIDAR (and other sensors) needs to be calculated in relation to the car. Coordinate systems of their own are essential to represent these relations at each moment. As seen in Figs. 8(a) and 8(b), the map frame is placed in the first GPS coordinates received by the program; the transformation to the ground frame is directly below the GPS antenna, clamped to the ground; the car_center frame is placed on the center of the vehicle to allow roll-pitch-yaw (RPY) calculations; the moving_axis frame is placed on the front of the car, following the car movement, and finally, lms151_E corresponds to the frame of the left SICK LMS151 and serves as a reference to all the other sensors’ frames. 3.1.1 Simulation Through the analysis of the projection from the laser beams in a side-walk, a big displacement was observed along the direction of the road between the points where the scan hits the top of the side-walk and the road plane, shown in Fig. 9. Additionally, the sections of the laser beam that hit the vertical face of the curb, and form the diagonal between the aforementioned points, produce a long line at the edge of the road that also follows the direction of the curb. When the accumulation of the successive scans is taken into account, these diagonal lines created by the vertical projections form a plane of closely adjacent points that correspond to the curb surface. Fig. 10 illustrates a simplification of the laser projection from a side (Fig. 10(a)) and a top (Fig. 10(b)) views. By analyzing these projections, and despite the points in the diagonal projection being more spaced than the ones in the horizontal planes (ground and side-walk), the accumulation produces a higher concentration of points in the vertical plane as demonstrated by the circles in Fig. 10. This difference between the concentration of points in the vertical and horizontal planes decreases with α , in which smaller values of α produce higher concentrations on the vertical plane while keeping the concentration of the points in the horizontal planes constant. Decreasing the angle would mean that the projections would be too far away from the vehicle but, since the sensor used in AtlasCar2 is placed very close to the ground, this problem is rectified. Another factor that affects the local point density is the velocity of the vehicle, causing large variations in the distance between successive readings of the sensor ( δ x ), with greater velocities producing bigger distances between each scan, and therefore reducing the overall density of the measured section of road. The behavior that results from the combination of sensor placement, vehicle velocity, and laser scan accumulations, provides a way to distinguish between the ground and the curb planes. When analyzing the local concentration of points in the road reconstruction point cloud, called accumulated point cloud density, the conclusion is that lower density regions correspond to horizontal planes and higher density regions correspond to vertical planes. This distinction supports the methodology for the algorithm of road boundary detection. To study the influence of the LIDAR placement and the curb configuration on the accumulation of points a simulation setup was built. The sensor placement is controlled by two variables: the height h and the tilt angle relative to the ground plane ( α ). Eq. (1) relates all the involved variables and Fig. 11 illustrates several simulated laser beams using this equation. (1) ∀ X ∈ R : Y = X − δ x tan ( ϕ i ) Z = h − ( x − δ x ) ⋅ tan ( α ) Eq. (1) reflects the path of the laser beams based on the sensor placement defined by the variables indicated in Figs. 10(a) and 10(b), where: • h (height) - Distance in meters from the center of the sensor to the ground; • α (tilt) - Angle between the scanning plane and a horizontal plane parallel to the ground; • δ x (Displacement) - Distance between the sensor and the origin in the direction of movement, increasing this value with time, allows to simulate vehicle movement; • ϕ i (Beam angle) - Angle formed by the combination of the sensor aperture (85 ∘ total) and the angular resolution (0.5 ∘ ) for each beam i ; The value of the angle between scanning planes (0.8 ∘ ) is added to the tilt of the sensor ( α ) to produce the four scanning planes characteristic of this sensor. Another important feature implemented in the simulation is the capability to include vehicle motion. This functionality is important since the methodology of accumulated point cloud density heavily relies on the accumulation of successive sensor readings. For this, the information about vehicle velocity ( v c a r ) is given as input, which combined with the data acquisition frequency of the LIDAR sensor ( f s e n s o r =50Hz) gives the distance between consecutive LIDAR readings, according to Eq. (2). In the simulation, this is represented by the displacement variable δ x . Fig. 12 represents a side-view of the successive scans according to the calculated displacement. (2) δ x = v c a r f s e n s o r To simulate curb configuration, a plane was created to represent a curb defined by a point p = ( x p , y p , z p ) and a normal vector n → = ( a , b , c ) is given by the parametric equation (3); these two components are respectively responsible for the position and orientation of the road curb. (3) ( x p − X ) a + ( y p − Y ) b + ( z p − Z ) c = 0 To obtain the points that are to be kept in the filtering algorithms, Eqs. (3) and (1) are combined to contemplate the LIDAR placement and the curb profile, only keeping the points that are coincident with the vertical or inclined plane defined by Eq. (3) contained inside the sphere with a pre-defined radius r , resulting in Eq. (4). (4) ∀ X ∈ R : = Y = X − δ x tan ( ϕ i ) Z = h − ( x − δ x ) tan ( α ) ( x p − X ) a + ( y p − Y ) b + ( z p − Z ) c < r 2 3.1.2 Parameter definition The two parameters needed for the filtering algorithm are the radius of the filtering sphere and the number of intended laser scan points contained within that sphere. To test the influence of the various external parameters that cause fluctuations in the measured accumulated point cloud density (e.g. vehicle velocity, sensor placement, curb geometry), one of the parameters is fixed and the other is varied to access the influence of the external parameters in the analysis. As such, an initial simulation was performed to determine the ideal sphere radius as a parameter for the filter, and use this value in the remainder simulations that evaluate the influence of external parameters in accumulated point cloud density. To assess the influence of these parameters, the laser scans of a vehicle moving at a standard speed of 50km/h were simulated with Type I curb (Fig. 13) road limits. This translates in a vertical curb on each side of the vehicle (1.5m to the right and 4.5mto the left). The accumulation point is located at 14m from the initial position of the vehicle, and the simulation ends when the car moves 12m. Fig. 14 shows a representation of the simulation scenario used in this experiment and all the ones that follow, representing the constant variables common to these simulations. Using this configuration, the accumulated point cloud density was defined using Eq. (5) for multiple radii of the filtering circle, where N t o t a l is the total of points present in a predefined radius and A c i r c l e is the area of the circle with that same radius. Fig. 15 shows the result of the simulation at 50km/h, where the Density refers to the accumulated point cloud density in points ∕ m 2 , and X refers to the distance traveled by the vehicle in meters. (5) Accumulated Point Cloud Density = N t o t a l A c i r c l e From the graph shown in Fig. 15, it can be observed that the 0.2m radius is the one that produces the highest accumulated point cloud density followed by the 0.1m radius, but this one shows a more inconsistent behavior since it has some intervals where no points where contained in the circle due to its too small size. Another important aspect of this methodology for road boundary detection is the difference between the accumulated point cloud density measured on the curb and the road plane. It is important to maximize that difference since it will facilitate the separation of the two regions. For this reason, another simulation was performed, in the same conditions as the previous, to evaluate the influence of the filtering radius in accumulated point cloud density, but now using a region on the road plane. Fig. 16 shows the results of the simulation with a car speed of 50km/h. From this graph, it can be observed that, in general, the values of the accumulated point cloud density are much smaller (about 25 times) than the ones registered in the vertical curb from the previous experiment. Even so, the 0.2m radius shows to be the one that results in lower accumulated point cloud density values, making it the preferable option. Given the results gathered from these two experiments, the 0.2m radius proved to be the best size of the filtering sphere in both situations resulting in greater densities on the curb plane, and lower ones in the ground plane, meaning that it is the one that best differentiates between the two surfaces. 3.1.3 Limitations of the initial approach This initial technique, although very important to understand the behavior of the accumulated point cloud density, has some limitations, namely when the vehicle slows down during a significant amount of time, for example when reaching an intersection (Fig. 17) or a roundabout (Fig. 17(a)): this causes the local density of points to greatly increase, deceiving the algorithm. The extreme case is when the vehicle completely stops, causing a heavy concentration of points on the road plane (Fig. 18(a)), followed by a halt in road boundary detection (Fig. 18(b)) since with no movement, there is no accumulation and, therefore, no information about the reconstructed road. One other issue with using a static parameter for the filtering algorithm is caused by the difference in accumulated point cloud density on both sides of the road. Because the left and right curbs are at a different distance from the sensor, the values of the accumulated point cloud density in these curb planes will also be significantly different, as evidenced by the results of the simulation presented in Figs. 15 and 16. These results show that for the same accumulation distance the density values measured on the left side of the road are almost double the ones measured on the right side. In practical terms this means that the boundary detection on the right side will happen closer to the car than the one on the left side, since it needs to accumulate during a greater distance to reach the same density levels, causing the misalignment of the curb detection. 3.2 Density grids — accumulated point cloud density As a solution to the limitations found on the initial approach, density gradients are used to identify road limits instead of point cloud density alone. The advantage of density gradients is that a positive gradient represents an increase of density, i.e. positive curb, and a negative gradient represents a decrease of density, i.e. negative curb. By creating a map with the absolute values of density gradients, it is possible to visualize both positive and negative road profiles as a single map of road limits. This map was created from the concept of occupancy grid by using a density grid and also different gradient grids calculated with different edge detection algorithms. The basic idea of an occupancy grid is to represent a map of the environment as an evenly spaced grid with each cell representing the likelihood of the presence of an obstacle at that location in the environment. Occupancy grids have an associated frame identification, resolution and size that can be customized according to the situation. The flattened point cloud (Fig. 19(b)) was converted into a density grid (Fig. 19(c)) with the density of the point cloud in each grid cell, allowing to perform more complex operations with a significantly lower computational effort. The density grid was built under the following principles: 1. The density inside each cell is equal to the number of points within the coordinates of that cell. 2. The altitude component is not relevant due to the positioning of the sensor, thus only x and y components were considered. 3. The grid base frame is a frame placed on the center of the LIDAR and consequentially the point cloud must be transformed from the world frame to the target frame. 4. Since there is no advantage in considering the information behind the car front, the grid was defined with 40m ahead of the car and 20m to each side of the car, making a grid with a final size of 40m × 40m. And although the influence of grid resolution in the detection of road limits is discussed further on, the base resolution is considered 0.4m/cell. Also, Fig. 19(a) shows a camera image from the camera installed in the car of the same moment where the point cloud and density grid of Figs. 19(b) and 19(c). This image shows the real environment of the road and the road curbs detected on the right side. To develop the occupancy grid, the accumulated point cloud is transformed into the moving_axis frame. Then, for each point in the point cloud, the cell coordinates (line and column) to which it belongs are calculated. This grid can be visualized along with the car movement. The car also has four SICK DT20Hi placed in each four corners of the car and we calculate the inclination of the car (approximate roll and pitch on a flat road). This inclination is used to correct the point cloud in real time and helps compensate the cases of acceleration, braking, etc. Additionally, to standardize the values of the density in each frame, and minimize the impact of the car’s velocity and variations in the RPY values, the values of the occupancy grid density are normalized to the maximum density value in each frame, returning values between 0 and 100. 3.3 Gradient and edge detection With the density occupancy grid defined, new grids with horizontal and vertical gradient components, and the overall gradient magnitude, were created individually to evaluate the most effective for this particular case. To calculate the horizontal ( G y ) and vertical ( G x ) gradients, the simple linear filters used were, respectively, − 1 1 0 and − 1 1 0 ⊺ . The magnitude of the gradient was used in all further calculations: G = | G x | + | G y | . As the manual implementation of filters was limited and also to allow the exploration of different pixel weights in edge detection and different types of algorithms, the gradient grids were converted to images, opening a new set of possibilities for edge detection tools. This conversion has two steps: convert the occupancy grid into a GridMap, and convert the grid map to an image, with a CV_8UC1 encoding (8-bit unsigned type with one channel). To this image, new edge detection operations can be applied with edge detection algorithms from image processing libraries, and converted back into new occupancy grids through the same process. The edge detection algorithms applied to the images were Simple Gradient, Sobel, Prewitt, Laplace, Kirsch, and Canny. In general, when using edge operators, three parameters need to be given as input: the kernel size, an odd number that defines the size of the filter, set to 3, the scale, if necessary to scale the output, and Delta, an optional value that is added to the results before storing them in a matrix; scale and Delta were left in their default values, meaning not used. Also, the timestamps and headers are preserved in all conversions. After applying the edge detection operator, the image is converted to absolute values as both positive and negative gradients are relevant. This process allows the application of a custom threshold when converting the image back to a grid and eliminate low-interest regions in the edge detection process corresponding to road-noise, etc. 3.4 Ground truth definition To facilitate the process of marking the ground truth, the car coordinates are saved in each frame in a Keyhole Markup Language (KML) file, allowing the visualization of the car path in the Google Earth application. The process of reading the road limits in the form of ground truth is more complex, involving manually drawing the data corresponding to the right and left side of the road overlapped on satellite images, that can be read and transformed in vectors by the program. The coordinates placed in the World Geodetic System (WGS 84) frame need to be transformed into the correct car frame, moving_axis (in the front of the car). This requires the following steps: 1. Convert the car latitude and longitude to the Universal Transverse Mercator (UTM) frame, which generates the following coordinates: ( x u t m _ c a r , y u t m _ c a r ) . 2. Convert every road limit point to the UTM frame, obtaining ( x u t m _ p o i n t , y u t m _ p o i n t ) . 3. With both points in a metric scale, calculate the difference between each point coordinates and the car coordinates, x u t m _ p o i n t − x u t m _ c a r and y u t m _ p o i n t − y u t m _ c a r . 4. Rotate the obtained coordinates to the correct frame orientation (see Fig. 19(c)) by performing a z rotation corresponding to the yaw angle ψ (azimuth in the GPS message — clockwise rotation) through eq. (6), and obtaining ( x c o r r e c t , y c o r r e c t ) . 5. Add 2.925m to the x coordinate of each point, correspondent to the translation from the GPS sensor frame to the LIDAR sensor frame. (6) x c o r r e c t y c o r r e c t = cos ( ψ ) sin ( ψ ) − sin ( ψ ) cos ( ψ ) x u t m _ p o i n t − x u t m _ c a r y u t m _ p o i n t − y u t m _ c a r To create a continuous line, an interpolation function was implemented to create new points within the lines, assuming a straight line between points. These limits are then filled to define the navigable space of the road. 4 Tests and results 4.1 Qualitative evaluation Images in Fig. 20 show a representation of the navigable space obtained from each edge detector at the same moment. By analyzing the images, the conclusion is that: • Although being the only one that results relatively well close to the car, the Canny edge detection produces poorer results further from the car, with many gaps in the detection. • All the edge detectors, apart from Canny produce poor results in the approximately 10m following the car. • The Gradient filter is the one with fewer gaps in the detection and more clear road apart from the initial meters. • The Laplace and Prewitt algorithms produce similar results with some gaps in the middle of the road. • Sobel and Kirsh filters have defined road but further away from the car than the rest of the algorithms. 4.2 Quantitative evaluation The statistical measures described in Eqs. (7)–(11) are commonly used to quantify performance of automotive applications [12]. These measures are based on a binary classification of negatives and positives. A cell is considered positive if it is inside the navigable space, and negative if not, thus a true positive (TP) is a cell that is correctly identified as road, a false positive (FP) is a cell misidentified as road, a true negative (TN) is a cell correctly identified out-of-limits and a false negative (FN) is a misidentified out-of-limits cell. In the F-measure, the β parameter rules the importance ratio between Precision and Sensitivity, and β = 1 was considered. (7) Precision = T P T P + F P (8) Sensitivity = T P T P + F N (9) N P V = T N T N + F N (10) F-measure = ( 1 + β 2 ) Precision × Sensitivity β 2 × Precision + Sensitivity (11) Accuracy = T P + T N T P + F P + T N + F N (12) Specificity = T N T N + F P Concerning this work, the most relevant parameters are the F-measure and Accuracy, as these include the global performance of the algorithm in terms of the detection of both positives and negatives. Yet, the remaining parameters are also taken into consideration, especially for particularly low values that may indicate an unreliable algorithm. With the quantitative evaluation method, the influence of the grid resolution and the algorithm’s threshold was also tested. The threshold can be applied when converting images to Occupancy Grids and means removing the filtered values below that number, which is useful to remove low values corresponding to road-noise, etc. To perform these evaluations, several paths were used accordingly to their characteristics. An example is shown in Fig. 21. To evaluate the performance of algorithms when faced with negative obstacles, a test was performed on a straight road with Type III curbs (see Fig. 13). The results obtained are presented in Table 2. When comparing the obtained results with the ones obtained with Type I curbs shown in Table 1, the numbers are very similar and, in some cases, even better. In both these tests, the segment of the road was 280m long. Note that the size of the road as a parameter for our analysis and not the number of frames because it is very dependent on the car speed and when evaluating its influence, it was impossible to consider the same number of frames for the same road. The Canny edge detector produces poor results in both types of curbs, with even lower F-measure and Sensitivity, proving once again not to be suitable for this application. The Simple Gradient filter still produces the best results, although in general, all algorithms had a decrease of Sensitivity when comparing them with the Type I curbs, which measures the fraction of actual positives correctly identified. By the analysis of the right chart in Fig. 22, which shows a plot of the influence of cell resolution in the performance of the Simple Gradient edge detector, it can be concluded that a resolution of 0.4 m/cell is the one that produces the best results. Given that a pedestrian can be fitted in a square of about 0.5m × 0.5m, a resolution of 0.4m is considered enough to even identify a person in the road. Logically, if there is a thin object in the road, like a road pin, this resolution can dissolve the points in that cell and lead to the non-identification of the obstacle, but there has to be a trade-off between a very thin resolution that results in a worse outcome and a size that can still detect small objects. One of the major concerns with this work was its robustness to the different car speeds. To evaluate this, the same road was tested at different speeds, evaluating the system in about 200m. Logically, the number of frames evaluated varies according to the car velocity since lower speed results in more time to travel the same distance and, consequently, more frames. The left of Fig. 22 shows the results of the algorithm by varying car speed from 20 to 60 km/h. By analyzing the graphic, it can be concluded that the performance remains more or less constant until 50km/h. From this value up the performance of the algorithm starts decreasing but still above 50%. Concerning the studies performed on the algorithm’s threshold (further information in [13]), the Simple Gradient and Canny work better with 0 threshold, the Prewitt and Sobel with 25 and the Laplace and Kirsch with 50. This study is relevant to eliminate lower values that correspond to road or filtering noise. The threshold value varies from filter to filter as some are more susceptible to noise than others, like Kirsch versus Gradient. For this reason, the study was performed individually for each algorithm to optimize individual performance. Table 3 shows the results of the best parameters’ combination for each algorithm with the chosen thresholds, car speed, and cell resolution values within the tested range. Simple Gradient and the Laplace, followed by Kirsch filter, have the best results. Canny edge detector, for the reasons already mentioned, is not suitable for this application and the Sobel and Prewitt filters, very similar to each other, although having good results, fall short of the best algorithms. 5 Conclusion This paper proposes an effective way to correctly identify road limits and road navigable space. The argument was that the density of the accumulated cloud, obtained from car movement with an 85 ∘ range 3D LIDAR, is a new methodology to perform road perception. Unlike most of the works in this area, that use 360 ∘ Velodyne LIDARs, seeing the road at ground level offers a completely different perspective, possibly allowing to make real-time driving decisions given the identified road limits. To solve the problem, the used approach was to transform accumulated point clouds in a density occupancy grid, calculate a two-dimensional gradient and subsequently apply edge detection algorithms to find density patterns that define obstacles. When compared to the initial approach that used an algorithm to remove points outside a radius for each voxel, the improved approach can detect both positive and negative obstacles by considering density increases and decreases. Regarding the edge detection, the Simple Gradient showed the best performance, although all algorithms except Canny have good results in general. The developed solution also showed decent performance at different car speeds, only decreasing in performance at higher speeds. This methodology is not expected to serve as a single algorithm for ADAS, but rather to assist or complement other algorithms to create a more robust system and providing useful information about the physical road limits. And although this technique alone may be limited to provide data for a navigation system, when integrated with the other sensors in the AtlasCar2, it creates essential navigational information of hard limits, leaving open a wide range of possibilities. On the one hand, road detection for navigational purposes can be largely improved by combining it with lane detection using cameras (soft limits), and creating a multi-sensorial algorithm with the possibility to create a common navigation Occupancy Grid with different levels of probability according to the detected features (lane less serious, hard limits more serious, etc.). Fusing the results of several edge detection algorithms may also be interesting to obtain more complete and robust information. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment Part of this work was funded by Project SeaAI-FA_02_2017_011 References [1] Salvado P. Reconstrução Dinâmica de Mapa Local para o AtlasCar 2012 Salvado, P., 2012. Reconstrucao dinamica de mapa local para o AtlasCar. [2] Bar Hillel A. Lerner R. Levi D. Raz G. Recent progress in road and lane detection: a survey Mach. Vis. Appl. 25 3 2014 727 745 10.1007/s00138-011-0404-2 Bar Hillel, A., Lerner, R., Levi, D., Raz, G., 2014. Recent progress in road and lane detection: a survey. Machine Vision and Applications 25, 727–745,. 10.1007/s00138-011-0404-2. [3] Y. Zhang, J. Wang, X. Wang, J.M. Dolan, Road-Segmentation-Based Curb Detection Method for Self-Driving via a 3D-LiDAR Sensor, 19 (12) 3981–3991, [4] Peterson K. Ziglar J. Rybski P.E. Fast feature detection and stochastic parameter estimation of road shape using multiple LIDAR 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems 2008 612 619 10.1109/IROS.2008.4651161 Peterson, K., Ziglar, J., Rybski, P.E., 2008. Fast feature detection and stochastic parameter estimation of road shape using multiple LIDAR, in: 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, 612–619,. 10.1109/IROS.2008.4651161. [5] J. Jung, S.-H. Bae, Real-Time Road Lane Detection in Urban Areas Using LiDAR Data, 7, (11) (2018) 276, [6] D. Zai, J. Li, Y. Guo, M. Cheng, Y. Lin, H. Luo, C. Wang, 3-D Road Boundary Extraction From Mobile Laser Scanning Data via Supervoxels and Graph Cuts, 19, 2018 802–813, [7] R. Huang, J. Chen, J. Liu, L. Liu, B. Yu, Y. Wu, A Practical Point Cloud Based Road Curb Detection Method for Autonomous Vehicle, 8, (2017) 93, [8] Zhang W. LIDAR-based road and road-edge detection 2010 IEEE Intelligent Vehicles Symposium 2010 845 848 10.1109/IVS.2010.5548134 Zhang, W., 2010. Lidar-based road and road-edge detection, in: 2010 IEEE Intelligent Vehicles Symposium, 845–848,. 10.1109/IVS.2010.5548134. [9] S. Xu, R. Wang, H. Zheng, Road Curb Extraction from Mobile LiDAR Point Clouds, 55 (2) (2017) 996–1009, arXiv:1610.04673, [10] Marques T. Detection of road navigability for ATLASCAR2 using LIDAR and inclinometer data 2017 Marques, T., 2017. Detection of road navigability for ATLASCAR2 using LIDAR and inclinometer data. /Perception/2018TiagoMarques [11] B. Yang, L. Fang, J. Li, Semi-automated extraction and delineation of 3D roads of street scene from mobile laser scanning point clouds, 79 (2013) 80–93 [12] Fritsch J. Kuhnl T. Geiger A. A new performance measure and evaluation benchmark for road detection algorithms 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013) 2013 IEEE 978-1-4799-2914-6 1693 1700 10.1109/ITSC.2013.6728473 Fritsch, J., Kuhnl, T., Geiger, A., 2013. A new performance measure and evaluation benchmark for road detection algorithms, in: 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013), IEEE. 1693–1700,. 10.1109/ITSC.2013.6728473. [13] Rato D. Detection of the navigable road limits by analysis of the accumulated point cloud density 2019 URL: Rato, D., 2019. Detection of the navigable road limits by analysis of the accumulated point cloud density Daniela Rato graduated from Mechanical Engineering in 2019. The focuses of her work are mainly robotics, artificial vision and artificial intelligence, having worked with robotic arms, LIDARs, thermal cameras, among others. Currently, she is in a research scholarship at the University of Aveiro, inserted in the project SeaAI — Filament Winding System with Robotic Arm and Vision with Artificial Intelligence Assistance. Vítor Santos graduated in Electronics Engineering and Telecommunications in 1989 and obtained a Ph.D. in Electrical Engineering in 1995. He was a researcher in mobile robotics at the Joint Research Center, Italy. Currently, he is an Associate Professor at the Department of Mechanical Engineering, lecturing courses related to advanced perception and robotics. He has managed research activity on mobile robotics, advanced perception, and humanoid robotics, with the supervision or co-supervision of more than 100 graduate and post-graduate students, and more than 140 publications. He has been in the program committee of several national and international conferences and acts regularly as a reviewer for several international conferences and journals. At the University of Aveiro, he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and the development of ATLASCAR and ATLASCAR2, the first car with autonomous navigation capabilities in Portugal that won the first prize in the Freebots competition in 2011. He is one of the founders of Portuguese Robotics Open and co-founder of the Portuguese Society of Robotics and a researcher at IEETA, in the Intelligent Robotics and Systems Group, focused on autonomous driving and driver assistance. "
    },
    {
        "doc_title": "Multi-sensor extrinsic calibration using an extended set of pairwise geometric transformations",
        "doc_scopus_id": "85096588033",
        "doc_doi": "10.3390/s20236717",
        "doc_eid": "2-s2.0-85096588033",
        "doc_date": "2020-11-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Automatic method",
            "Calibration procedure",
            "Calibration process",
            "Experimental methods",
            "Extrinsic calibration",
            "Geometric transformations",
            "Human intervention",
            "Multiple sensors"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Systems composed of multiple sensors for exteroceptive perception are becoming increasingly common, such as mobile robots or highly monitored spaces. However, to combine and fuse those sensors to create a larger and more robust representation of the perceived scene, the sensors need to be properly registered among them, that is, all relative geometric transformations must be known. This calibration procedure is challenging as, traditionally, human intervention is required in variate extents. This paper proposes a nearly automatic method where the best set of geometric transformations among any number of sensors is obtained by processing and combining the individual pairwise transformations obtained from an experimental method. Besides eliminating some experimental outliers with a standard criterion, the method exploits the possibility of obtaining better geometric transformations between all pairs of sensors by combining them within some restrictions to obtain a more precise transformation, and thus a better calibration. Although other data sources are possible, in this approach, 3D point clouds are obtained by each sensor, which correspond to the successive centers of a moving ball its field of view. The method can be applied to any sensors able to detect the ball and the 3D position of its center, namely, LIDARs, mono cameras (visual or infrared), stereo cameras, and TOF cameras. Results demonstrate that calibration is improved when compared to methods in previous works that do not address the outliers problem and, depending on the context, as explained in the results section, the multi-pairwise technique can be used in two different methodologies to reduce uncertainty in the calibration process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Road detection based on simultaneous deep learning approaches",
        "doc_scopus_id": "85089817944",
        "doc_doi": "10.1016/j.robot.2020.103605",
        "doc_eid": "2-s2.0-85089817944",
        "doc_date": "2020-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Driving assistance systems",
            "Lane detection",
            "Learning approach",
            "Learning models",
            "Multiple data sources",
            "Road detection"
        ],
        "doc_abstract": "© 2020 Elsevier B.V.One of the most important challenges for Autonomous Driving and Driving Assistance systems is the detection of the road to perform or monitor navigation. Many works can be found in the literature to perform road and lane detection, using both algorithmic processing and learning based techniques. However, no single solution is mentioned to be applicable in any circumstance of mixed scenarios of structured, unstructured, lane based, line based or curb based limits, and other sorts of boundaries. So, one way to embrace this challenge is to have multiple techniques, each specialized on a different approach, and combine them to obtain the best solution from individual contributions. That is the central concern of this paper. By improving a previously developed architecture to combine multiple data sources, a solution is proposed to merge the outputs of two Deep Learning based techniques for road detection. A new representation for the road is proposed along with a workflow of procedures for the combination of two simultaneous Deep Learning models, based on two adaptations of the ENet model. The results show that the overall solution copes with the alternate failures or under-performances of each model, producing a road detection result that is more reliable than the one given by each approach individually.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2020-08-20 2020-08-20 2020-08-26 2020-08-26 2020-09-28T14:55:57 S0921-8890(20)30445-0 S0921889020304450 10.1016/j.robot.2020.103605 S300 S300.1 FULL-TEXT 2020-10-09T16:35:14.36346Z 0 0 20201101 20201130 2020 2020-08-20T15:37:38.613677Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref specialabst vitae 0921-8890 09218890 true 133 133 C Volume 133 2 103605 103605 103605 202011 November 2020 2020-11-01 2020-11-30 2020 article fla © 2020 Elsevier B.V. All rights reserved. ROADDETECTIONBASEDSIMULTANEOUSDEEPLEARNINGAPPROACHES ALMEIDA T 1 Introduction 2 Related work 3 Proposed approach 3.1 The ENet architecture 3.2 ENet for semantic segmentation 3.3 ENet for road lane lines segmentation 3.4 Output post-processing 3.5 The combination approach 4 Experimental infrastructure 5 Experiments and results 6 Conclusion Acknowledgments References ASSIDIQ 2008 82 88 A 2008INTCONFCOMPUTERCOMMUNICATIONENGINEERING REALTIMELANEDETECTIONFORAUTONOMOUSVEHICLES ALY 2008 7 12 M INTELLIGENTVEHICLESSYMPOSIUM2008IEEE REALTIMEDETECTIONLANEMARKERSINURBANSTREETS KLUGE 1995 54 59 K ADEFORMABLETEMPLATEAPPROACHLANEDETECTION BOUNINI 2015 1 6 F 2015IEEEVEHICLEPOWERPROPULSIONCONFERENCEVPPC AUTONOMOUSVEHICLEREALTIMEROADLANESDETECTIONTRACKING LIU 2018 5222 5227 S 201837THCHINESECONTROLCONFCCC EFFECTIVEROADLANEDETECTIONTRACKINGMETHODUSINGLINESEGMENTDETECTOR HOU 2016 6999 7004 C 201635THCHINESECONTROLCONFCCC EFFICIENTLANEMARKINGSDETECTIONTRACKINGMETHODBASEDVANISHINGPOINTCONSTRAINTS LONG 2014 J FULLYCONVOLUTIONALNETWORKSFORSEMANTICSEGMENTATION RONNEBERGER 2015 O UNETCONVOLUTIONALNETWORKSFORBIOMEDICALIMAGESEGMENTATION BADRINARAYANAN 2015 V SEGNETADEEPCONVOLUTIONALENCODERDECODERARCHITECTUREFORIMAGESEGMENTATION PASZKE 2016 A ENETADEEPNEURALNETWORKARCHITECTUREFORREALTIMESEMANTICSEGMENTATION ZHAO 2016 H PYRAMIDSCENEPARSINGNETWORK CHEN 2016 L DEEPLABSEMANTICIMAGESEGMENTATIONDEEPCONVOLUTIONALNETSATROUSCONVOLUTIONFULLYCONNECTEDCRFS ZHAO 2017 H ICNETFORREALTIMESEMANTICSEGMENTATIONHIGHRESOLUTIONIMAGES POUDEL 2018 R CONTEXTNETEXPLORINGCONTEXTDETAILFORSEMANTICSEGMENTATIONINREALTIME POUDEL 2019 R FASTSCNNFASTSEMANTICSEGMENTATIONNETWORK WANG 2019 Y ESNETEFFICIENTSYMMETRICNETWORKFORREALTIMESEMANTICSEGMENTATION CHEN 2019 P DSNETEFFICIENTCNNFORROADSCENESEGMENTATION LO 2019 S MULTICLASSLANESEMANTICSEGMENTATIONUSINGEFFICIENTCONVOLUTIONALNETWORKS LYU 2018 Y ROADSEGMENTATIONUSINGCNNGRU LYU 2018 Y ROADNETV2A10MSROADSEGMENTATIONUSINGSPATIALSEQUENCELAYER NEVEN 2018 D TOWARDSENDTOENDLANEDETECTIONINSTANCESEGMENTATIONAPPROACH HOU 2019 Y LEARNINGLIGHTWEIGHTLANEDETECTIONCNNSBYSELFATTENTIONDISTILLATION YU 2020 Z DETECTINGLANEROADMARKINGSADISTANCEPERSPECTIVETRANSFORMERLAYERS TASCI 2018 1 4 E 201826THSIGNALPROCESSINGCOMMUNICATIONSAPPLICATIONSCONFSIU IMAGECLASSIFICATIONUSINGENSEMBLEALGORITHMSDEEPLEARNINGHANDCRAFTEDFEATURES POUYANFAR 2016 203 208 S 2016IEEEINTERNATIONALSYMPOSIUMMULTIMEDIAISM SEMANTICEVENTDETECTIONUSINGENSEMBLEDEEPLEARNING ALMEIDA 2020 242 254 T ROBOT2019FOURTHIBERIANROBOTICSCONFERENCE SCALABLEROSBASEDARCHITECTUREMERGEMULTISOURCELANEDETECTIONALGORITHMS NOH 2015 H LEARNINGDECONVOLUTIONNETWORKFORSEMANTICSEGMENTATION JIANG 2018 J REDNETRESIDUALENCODERDECODERNETWORKFORINDOORRGBDSEMANTICSEGMENTATION CORDTS 2016 M CITYSCAPESDATASETFORSEMANTICURBANSCENEUNDERSTANDING SUDRE 2017 C GENERALISEDDICEOVERLAPADEEPLEARNINGLOSSFUNCTIONFORHIGHLYUNBALANCEDSEGMENTATIONS BRABANDERE 2017 B SEMANTICINSTANCESEGMENTATIONADISCRIMINATIVELOSSFUNCTION 2020 TUSIMPLECOMPETITIONSFORCVPR2017 ALMEIDAX2020X103605 ALMEIDAX2020X103605XT 2022-08-26T00:00:00.000Z 2022-08-26T00:00:00.000Z © 2020 Elsevier B.V. All rights reserved. 2020-07-22T14:25:16.763Z item S0921-8890(20)30445-0 S0921889020304450 10.1016/j.robot.2020.103605 271599 2020-10-09T16:35:14.36346Z 2020-11-01 2020-11-30 true 2425843 MAIN 10 61020 849 656 IMAGE-WEB-PDF 1 gr3 32924 218 678 gr9 36027 280 723 pic2 6417 132 113 pic3 5096 131 112 gr1 15497 327 489 gr14 13965 184 376 gr12 23348 280 723 gr11 34631 280 723 gr7 39868 301 320 gr4 16540 177 712 pic1 6241 131 112 gr15 20603 254 339 gr6 18881 379 565 gr5 16062 256 678 gr2 30901 203 361 gr13 26279 226 508 gr8 33251 280 723 gr10 28930 280 723 fx1 true 9087 202 301 gr3 5831 71 219 gr9 6662 85 219 pic2 18686 163 140 pic3 18277 164 140 gr1 5501 147 219 gr14 3299 107 219 gr12 5558 85 219 gr11 6446 85 219 gr7 23581 164 174 gr4 2552 55 219 pic1 21014 164 140 gr15 22960 164 218 gr6 4034 147 219 gr5 2459 83 219 gr2 20836 123 219 gr13 4205 97 219 gr8 6436 85 219 gr10 5653 85 219 fx1 true 5700 147 219 gr3 268522 966 3000 gr9 373552 1241 3203 pic2 62098 583 500 pic3 39723 583 497 gr1 129553 1450 2167 gr14 89181 815 1667 gr12 200336 1241 3203 gr11 338586 1241 3203 gr7 418213 1332 1417 gr4 114749 786 3154 pic1 53463 583 498 gr15 183026 1126 1500 gr6 142107 1679 2501 gr5 107739 1133 3003 gr2 340734 902 1601 gr13 176977 1002 2251 gr8 323699 1241 3203 gr10 327811 1241 3203 fx1 true 65718 893 1333 si7 13139 si11 3408 si5 5423 si2 2353 si9 1780 si10 1579 si6 5541 si14 1589 si1 2561 si15 2862 si4 2691 si3 4495 si13 8324 am 7569868 ROBOT 103605 103605 S0921-8890(20)30445-0 10.1016/j.robot.2020.103605 Elsevier B.V. Fig. 1 From left to right, the input image (left) is processed in parallel by the two models: the semantic segmentation (top image) and the line segmentation models (bottom left image). After that, the LaneNet output is transformed into a polygon (bottom right image) to, then, be combined with the region obtained by the ENet model to form the final confidence map (right), through the combination function ( f c ). This is any function that classifies each segmented image (bottom right image and upper image) and gives as result a scalar that expresses the confidence of each of those images, which will be used for the final weighted combination. For this work, this function will be explained further on. Fig. 2 An example of the output of the semantic segmentation model. The segmented road is shown as the red overlay in the image. Fig. 3 The LaneNet Model is composed of two branches. The segmentation branch (represented at the lower part of the image) is trained to produce binary masks of the input image (left). Besides being a binary mask producer, this architecture also provides embeddings, through its top branch: the embedding branch. It yields the pixel embeddings of the input image. These two branches, together, cluster the lane embeddings (blue dots), by repelling groups of pixels (clusters) from different lines and attracting embeddings of the same lines to the respective cluster center (red points). After the cluster post-processing stage, the output is an image with one channel, with the respective lines indexed [21]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Example of an output post-processing. The input image (left) provided by the LaneNet model is binarized and its top is cleaned (center). Finally, the small blobs in the image are removed (right). Fig. 5 Example of a polygon creation to an output based on the road lane lines. The minimum and maximum of each lane line are found (left) and after the polygons are closed, they are filled (right). Fig. 6 The raw ENet model is a region P 1 that weighs 1 N × s 1 on the final confidence map ( C M ) and the output that is given by the LaneNet model could be represented with more than one polygon. Thus, in this case those polygons are separated and weigh 1 N × s 2 and 1 N × s 3 , respectively, on the final confidence map. Fig. 7 The ATLASCAR2 vehicle. This is one of the camera setups (car roof) used for this work. The other one is inside the vehicle in the windshield. Fig. 8 The first example of the experiments consists of a roundabout entrance. This is a challenging road scenario for the models since there is no road lane line on the right side. First, the input (left) is processed in parallel by the two models. Both model outputs are post-processed (middle), and then these regions are combined, yielding the final confidence map (right). Here, it is clear the contribution of the solidity combined with the proportionality between the number of algorithms that return an area as road and the pixel values of the final confidence map. Therefore, if more algorithms return a certain area detected as road and the solidity value of that zone is high, then the pixel values of that zone (confidence) are high on the final confidence map. Moreover, if only one of the algorithms was used, the result would either be limited in terms of road space detection (for the LaneNet model alone) or it would have less information in terms of road lane (for the ENet model alone). Fig. 9 The second example of the experiments consists of highway scenario detection. Here, the road lane lines are well represented in the road scenario, thus, the models do not have problems with each feature detection (road and lines). First, the input (left) is processed in parallel providing two regions (middle). After post-processing each model’s output, these regions are combined, taking into account the respective solidity (right). Fig. 10 The third example of the experiments consists of a scenario inside a roundabout. On this scenario, the road lane lines are difficult to detect. First, the input (left) is processed in parallel by the two models contributing with two regions (middle). The road lane lines are not detected by the LaneNet model as can be seen in the bottom image. The final confidence map (right) is solely based on the ENet model contribution. Fig. 11 The fourth example of the experiments consists of another challenging road scenario because of the overshadowed road lanes. First, the input (left) is processed in parallel by the two models outputting the respective road features. After that, these outputs are post-processed (middle), they give rise to the final confidence map (right). Once again, if just one of the models was used, either there would be no distinction between the ego lane and parallel lanes (exclusive usage of ENet model) or there would be no access to information from parallel lanes (exclusive usage of LaneNet model). Fig. 12 The fifth experiment consist of a defying road scenario (left), since there is a road lane line occlusion. The post-processed outputs (middle) show the robustness of both models. Finally, the confidence map (right) is composed by the two road representations with higher confidence values since both solidity values are higher than in the other four cases. Fig. 13 Number of regions detected by each model for the acquired set. As can be seen, on average, the number of regions detected by the LaneNet is two, which matches the real number of lanes. There are, however, some outliers, on a set of frames that were taken on a roundabout, where the LaneNet struggles to detect valid regions. The segmentation model detects, as expected, a single region on all frames. Fig. 14 Histogram of the number of lanes detected by the LaneNet model. The false detections occur when the number is greater than 3 since in the used set the maximum number of lanes is also 3. Fig. 15 An example that illustrates the detection of only one road lane, when there are two road lanes. When this occurs in the tests made, the ego lane is always detected, which does not adversely affect the final confidence map. In fact, the system is just not providing all the existing information, but the one provided is reliable and, for most circumstances, enough for navigation purposes. Road detection based on simultaneous deep learning approaches Tiago Almeida a b ⁎ Bernardo Lourenço a Vitor Santos a b a Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal Department of Mechanical Engineering, University of Aveiro b Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro ⁎ Corresponding author at: Department of Mechanical Engineering, University of Aveiro, Portugal. Department of Mechanical Engineering, University of Aveiro Portugal One of the most important challenges for Autonomous Driving and Driving Assistance systems is the detection of the road to perform or monitor navigation. Many works can be found in the literature to perform road and lane detection, using both algorithmic processing and learning based techniques. However, no single solution is mentioned to be applicable in any circumstance of mixed scenarios of structured, unstructured, lane based, line based or curb based limits, and other sorts of boundaries. So, one way to embrace this challenge is to have multiple techniques, each specialized on a different approach, and combine them to obtain the best solution from individual contributions. That is the central concern of this paper. By improving a previously developed architecture to combine multiple data sources, a solution is proposed to merge the outputs of two Deep Learning based techniques for road detection. A new representation for the road is proposed along with a workflow of procedures for the combination of two simultaneous Deep Learning models, based on two adaptations of the ENet model. The results show that the overall solution copes with the alternate failures or under-performances of each model, producing a road detection result that is more reliable than the one given by each approach individually. Graphical abstract Keywords Visual perception Data combination Deep learning Computer vision Road map detection Road lane lines detection Road segmentation Driving assistance 1 Introduction The fields of Autonomous Driving (AD) and Advanced Driver Assistance Systems (ADAS) have carried out a wide range of studies that can shape the future of transportation by cars and other vehicles. One of the propelling topics fueling this changes is the field of Deep Learning (DL). Deep Learning algorithms are known to be superior in generalizing for several hard conditions that are the source of error in the classical algorithms, mainly changing lighting conditions, road occlusions, shadows, and different camera setups. Because the inclusion of sensors in cars is a current possibility, these algorithms could be used to understand the road scenario. However, most algorithms perform poorly under some conditions, because the models are not able to learn to perform well in all situations, and datasets still do not reflect all the road scenarios possible. For example, some datasets use the same camera setup for all images, and do not generalize very well for other camera setups. Similarly, other datasets have images with low diversity of information, for example, datasets with only highway road scenarios do not generalize very well for other scenarios. Also, models trained for different tasks often perform differently for different conditions. A better solution would be to use multiple deep learning algorithms trained for different tasks and datasets, and have an algorithm to combine the outputs of these models into an unique road representation. This work describes a novel road space representation method that combines two deep learning models trained to perform two different tasks: road segmentation and road lines detection. This method was found suitable and performant for both unstructured and structured roads. 2 Related work There are several algorithms in the literature for road segmentation and detection of the road lanes. Most classical algorithms are composed of several phases, such as pre-processing, feature extraction and model fitting [1–3]. Some of these classical algorithms even perform both tasks with the same pipeline [4–6]. Recently, Deep Learning has proven to be effective in training neural networks in a end-to-end fashion. That is, all the steps mentioned are performed by the neural network and are optimized together. These models have now outperformed the classical methods and are expected to improve over time. The most well-known and used architectures for object segmentation problems were based on [7–9]. Then, new more accurate and reliable approaches appeared in [10–12], in which one of them would give rise to one of the first real-time segmentation methods — ICNet [13]. Recently, the investigation in this Computer Vision task (segmentation) goes towards obtaining the best trade-off between real time and accuracy, so new methods were presented in [14–16]. Besides the existing general architectures for object segmentation, several authors develop more restrict works for road segmentation in [17,18] only based on Convolutional Neural Networks (CNNs). Moreover, more complex methods were also developed with the usage of CNNs and Recurrent Neural Networks (RNNs) techniques in order to increase the robustness of each architecture by Lyu [19,20]. In terms of road lane lines segmentation, there are also several works, one example is LaneNet [21]. Also, in [22], the authors propose applying self attention distillation (SAD) to increase the robustness of existing architectures in the detection of road lane lines. This would be achieved by training models that output attention maps, which encode contextual information about the road scenario and that are then incorporated in an architecture as a supervision method. Finally, in [23], the authors use layers to perform the perspective transformation that makes the road lane lines detection easier. The combination of different types of algorithms for road and line detection is not very common in the literature. However, it has been applied successfully in other fields, such as in machine learning, where, for example, there is a technique that combines the predictions from multiple trained models to reduce variance and improve prediction performance [24,25]. The objective is similar to the one presented in this work: combine different results in order to obtain a more accurate one. 3 Proposed approach In this work, we use the overall architecture conceived in [26] but with major changes regarding the processor algorithms as well as the combination method. Thus, we propose the usage of two deep learning convolutional models trained to perform different tasks, and try to merge their outputs to create a confidence map. This final output, the confidence map, is a grayscale image that represents the road scenario, where each pixel value corresponds to the respective confidence: 1 (white pixels) is the maximum confidence possible and 0 (black pixels) is the minimum confidence. The modifications performed in this work allow to claim that, now, the system consistently produces road representations due to the combination of two DL algorithms instead of classical algorithms. As far as the combination method is concerned, which is detailed later, in this improved approach, not only it is possible to give more confidence to pixels that belong to a zone returned by more than one algorithm as being a road area (method presented in [26]), but also if those zones have a more similar shape with a polygon (shape closer to a road). The two deep learning models used to perform the road/lanes segmentation tasks are based on the same neural network architecture, the ENet [10], which can achieve low-latency performance on semantic segmentation, which is a common application in the field of autonomous driving. The ENet model was trained to perform semantic segmentation, and it was modified to also perform road lines segmentation, based on the work of [21]. In the combination stage, a confidence map is obtained that has information about all the main features of the road: the main lane (also called ego lane), where the car is located, and the alternative lanes, where the car could drive in. In Fig. 1, an overview of the stages and algorithms can be seen. In the following sections, the DL-based architecture, as well as the modifications performed to each of its applications are described. 3.1 The ENet architecture The ENet [10] is a convolutional neural network for semantic segmentation that was optimized for real-time inference and high accuracy. This model proved to be effective in this task with the big advantage of being easy to train from scratch, not requiring any pre-training. This is due to its low capacity (a small number of parameters), which is also the reason for its fast inference performance. Its design was shaped by several choices and influences from other works developed for this task. The most important choices are describes as follows: Encoder–Decoder architecture The basic shape of this model shows semblance to other semantic segmentation models such as the SegNet [9], DeconvNet [27] and RedNet [28]. These models are composed of two stages, the encoder, and the decoder. The former downsamples the input image and transforms the raw pixels into high-level features. The latter does the opposite, upsampling the output of the encoder and using the encoded features to classify each pixel. In these networks, the outputs of the pooling layers, such as the pooling indices are preserved and used in the decoder to preserve spatial information. Bottleneck Blocks The bottleneck is the building block of this network. The idea behind a bottleneck block is to reduce the number of channels by a certain rate using a cheap depth-wise convolution so that the middle convolution has fewer parameters. In the end, the network is widened again with another depth-wise convolution. This strategy also applies some regularization to the model. Dilated Convolutions Dilated Convolutions are commonly used in the task of semantic segmentation [12], as it allows the convolutions to have a wider receptive field. This enables the model to capture more context, without downsampling the feature maps. An equivalent transformation would be a pooling operation, which has the disadvantage of reducing the spatial information and implies an extra upsampling operation. 3.2 ENet for semantic segmentation For the semantic segmentation task, a network was trained as described in [10]. The model was trained with the Cityscapes Dataset [29], which is composed of 5000 images, of which 3475 have pixel-wise annotations of 19 classes. For this task, the final convolution of the ENet model has 19 output channels. An example of the output of this model can be seen in Fig. 2. For the training, the procedure in [10] was followed, with slight modifications: • an additional data augmentation was performed, such as random rotation, random scaling, and color-jitter, in order for the model to generalize better in our road scenario; • the model was trained using progressive size cropping, starting with crops from size 256 × 256 up to 784 × 784, to train with fewer epochs. 3.3 ENet for road lane lines segmentation The LaneNet [21] is a modified ENet model for the task of road lane lines segmentation. This model has two branches (Fig. 3), sharing the first two stages of the ENet and the remaining stages are used as the backbone of each separate branch. The first branch is responsible for producing binary masks of the input image and the second generates dense embedding outputs. Embeddings are a mapping of the input pixels to a higher dimensional space, where the distances between points is a meaningful property for the instance segmentation. The segmentation part produces binary masks of the input image, where the background of the image is black pixels and the road lines are white pixels. Since these two classes are highly unbalanced, as the majority of the pixels are background, alternatively to the original LaneNet approach (where the cross-entropy loss function is used), the dice loss [30] is used to solve this issue. The embedding branch learns to map the lane pixels into a high-dimensional space, where the pixels of the same lane are closer together and pixels of different lines are farther apart. The dimension of the embedding space is a hyperparameter that was determined by a grid-search, and in this case, it was set to 4. This model learns this representation mapping through the discriminative loss function [31]. This network was trained with the TuSimple Dataset [32], which consists of 3626 video frames as the training set and 2782 video frames as the testing set. The annotations used for the dataset labeling are polylines (for the lane markings). Finally, some data augmentation techniques were performed in the training set images such as: image resizing, dimming, and rotation. During inference, the clustering is made by applying the MeanShift algorithm, which is a suitable function for finding dense cluster centers. 3.4 Output post-processing After each of the networks returns its output, post-processing is done to combine the two outputs in a further step (Section 3.5). This procedure consists of applying simple techniques that enable the suppression of small groups of pixels (blobs) as well as the lane lines sorting of the LaneNet output. Therefore, the initial phase of this procedure consists of cleaning the top of the image, as there are cases where the road lines intersect each other. This would affect the polygon creation, which is a procedure mentioned in [26] and also applied in this work. Moreover, the top of the image is mostly composed of irrelevant information such as the sky. After that, small blobs (less than 4500 pixels) are removed from the image, since the outputs of both models have quite considerable areas, so it was possible to mitigate some small false detections. These post-processing steps (Fig. 4) are common to both models. In the case of the LaneNet output, it is necessary to sort the lines in order to proceed to the correct construction of the polygons. For this purpose, the coordinates of the centroids of each detected line are ordered horizontally, from left to right. In short, the image returned by each model is cleaned. Furthermore, the lines in the LaneNet output are ordered to later built a polygon that will be combined with the road zone provided by the semantic segmentation model. 3.5 The combination approach As described in [26], the outputs that return road lane lines pass through an intermediate phase of the combination architecture, to build a polygon based on those lines. This method consists of finding the extreme points of each line (after these lines are sorted) and then the drawing of lines connecting those points. Finally, the polygon is filled to yield the final representation of Fig. 5. After the outputs of each model are in the same data type (polygons or closed zones), the problem is what is the best approach to combine these polygons. It is necessary to highlight some syntactic notation that will be used from now on: regions are closed zones of white pixels (e.g. in Fig. 5 there are 2 regions) and road representation means the two types of road representation that are combined at this stage (the two central images in Fig. 1). The method now introduced is an alternative to the one presented in [26]. Here, the road representations are weighted summed based on an image descriptor: the solidity. Solidity is a measurement of the ratio between the regions and the area of its convex hull. Therefore, more confidence is given to zones whose shape is more similar to a road. Hence, this combination method allows having a road representation — confidence map — whose lighter pixels were returned from a higher number of algorithms and belong to a zone with a high solidity value. Concretely, the confidence map is given by: (1) C M = ∑ i = 1 n ( w i × P i ) , where C M is the final confidence map, based on n polygons P , and w i is the weight of each polygon. This is the output of the combination function that is presented in Fig. 1 as f c . Each weight is given by: (2) w i = 1 N × s i , where N is the number of road representations (or number of processor algorithms) and s i is the solidity of each region. The weights distribution to the final confidence map construction are explained visually in Fig. 6. 4 Experimental infrastructure This work was developed under the ATLASCAR2 project, 1 1 which is a Mitsubishi i-MiEV equipped with cameras, LiDARs, and other sensors. In this work, we just used one sensor — one PointGrey Flea3 camera — installed mainly on the roof-top of the car, as can be seen in Fig. 7. This camera acquired the images used in the experiments described in Section 5. Additionally, it is also important to highlight that the two sets of images used to evaluate this work were acquired from two different camera setups: the main setup (shown in Fig. 7) and a second one located near the windshield, inside the vehicle. Both sets of images were used to test and assess the method. The used software architecture and framework for the development is ROS (Robot Operating System). The training of the deep learning models was done in a single NVidia RTX2080ti and with the PyTorch framework. 5 Experiments and results Regarding the experiments, five examples of different road scenarios and the behavior of each model as well as their combination are shown. These experiments assess qualitatively the confidence map creation as well as each model’s performance in challenging environments. After that, we show some statistic results for a full-set of 5000 frames in a usual environment for the ATLASCAR2 vehicle. Here, the number of detected regions provided by each model is presented. The first experiment is a roundabout entrance (Fig. 8), which is a challenging environment for the LaneNet model since the input image does not contain the right road lane line, which could affect the model performance. Nevertheless, the model detects the difference between the right road lane line and the sidewalk as a road line which is an acceptable approximation to the road lane line. On the final confidence map, the small blobs do not appear due to the post-processing done on the models’ outputs. The second experiment consists of a highway environment (Fig. 9), which is a less-challenging environment because the road lane lines are well represented in the input image. Both models perform adequately, therefore the final confidence map represents the lanes with high accuracy. The third experiment (Fig. 10) represents a roundabout; the ENet performs accurately, but there are no regions provided by the LaneNet model since the two road lane lines needed to build a polygon do not appear in the input image. Hence, the output of the LaneNet is, as could be expected, an empty set of information. In this case, the representation of the road is made exclusively with the semantic segmentation component of the method. The more challenging the road scenario, the more this combination technique makes sense, because in this case, it is so difficult to detect road/lanes that the final confidence map is a gray area (one of the algorithms could not return anything). Thus, the certainty/confidence of the final road representation depends also on the difficulty of the assessed road scenario. This implies that if one of the models is not performant in one type of environment then it will not have a major impact on the final confidence map (due to either its low solidity value or an empty set of information). The fourth example (Fig. 11) shows another challenging situation to create a reliable confidence map. Both models perform well in an overshadowed environment, which demonstrates the generalization and precision of these DL-based architectures. The last experiment consists of an occlusion of a road line (Fig. 12). This could have had a negative impact on the detection of the road line. However, the contribution to the final confidence map was not affected, which demonstrates the effectiveness of LaneNet. Likewise, ENet architecture also shows no problems in distinguishing these two classes (vehicles and road). Finally, the confidence map illustrates accurately the real road scenario. Finally, a study was performed to evaluate the quality of each method versus the joint performance of the combination method. As mentioned before, this study was performed for a full set of 5000 frames which contains several types of road scenarios (highways, roundabouts, regular roads). One of the most interesting highlights of the study was the creation of a confidence map for all image frames, which means that every frame generated a confidence map based on at least one of the outputs of both models. This study also allows to compare the number of regions that each model contributed to the final confidence map (Fig. 13). By observing this figure, we can conclude that the ENet model always contributes at most with one region, which is expected because every frame is a road representation. On the other hand, this does not happen on the LaneNet architecture (especially in the range between frames 3500 and 4500), since in some frames there are no road lines, namely when the vehicle is driving in roundabouts. It is also possible to observe that, sometimes, the LaneNet model returns more lines than expected since the maximum number of lanes of this test set is 3. This could be overcome by training this model with a more generalized dataset since the one that was used is limited. Another analysis concerns the number of detected lanes on each frame, as shown in Fig. 14; as can be seen in this figure, in 38% of the frames the LaneNet model contributed with one lane to the final confidence map, while in 51% of the times it contributed with two lanes. Compared to the real values, i.e. the real values of road lanes in each frame, there is a higher detection of one road lane versus two road lanes. Nonetheless, in all observed cases, when only one lane was detected, it was the ego lane, that is, the lane where the vehicle is navigating; this allows to conclude that this disparity of the detection versus the real number of lanes does not compromise the solution presented for most navigation purposes. One example of these cases is shown in Fig. 15. In summary, if the situations of one or two lanes are added up together, a total of 82% frames with one or two lanes in the real road is obtained, and the proposed method detected a total of 89% frames in those circumstances. This difference of 7% in the total number of frames where no lanes are detected is small enough to be compensated by a continuous road perception system that can integrate and obviate potential isolate road detection failures without consequences in a normal human timescale during the driving action. 6 Conclusion This work presents a new method for the combination of the output of multiple Deep Learning networks for road detection. This combination produces a confidence map that encodes the probability of each pixel in the image to be part of a road. This confidence map is paramount for further procedures, such as the navigation planners. By its intrinsic definition, this approach is naturally able to achieve a better overall performance than each individual algorithm because the best of each technique is always favored. When one technique fails or produces less reliable or less consistent results, the other technique that runs in parallel prevails. Another advantage of the approach is that the number of simultaneous algorithms can be increased, and each algorithm can be updated or replaced with more confidence, because the final detection does not depend entirely on the new algorithm. This could be an effective test-bed for experimentation in autonomous driving. The underlying ROS based architecture that supports the implementation of multiple sources and multiple algorithms simultaneously provides a strong framework for concurrent and complementary techniques that allow scalability and redundancy, hence, robustness and safety in autonomous driving or driving assistance. The Deep Leaning approaches used in the work have proved to perform better than most classic techniques, but it is not yet definitive that one single DL network is able to perform universally and this work will continue to exploit future developments for robust road detection. This work used a simple descriptor (region solidity) to weigh the importance of each detected region for the creation of the final confidence map. Other descriptors can be explored and other weighting techniques can be developed to reach a robust later fusion technique. Future perspectives are then wide open, both in the tuning of other Networks and detection algorithms, and also in the weighting technique to merge or fuse the results of those algorithms. Perhaps a specialized network to perform the fusion will be the next major challenge or the usage of an RNN (Recurrent Neural Network) to compute a time-based fusion instead of a frame-based combination. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was partially supported by Project SeaAI-FA_02_2017_011 and Project PRODUTECH II SIF- POCI-01-0247-FEDER-024541. References [1] Assidiq A.A. Khalifa O.O. Islam M.R. Khan S. Real time lane detection for autonomous vehicles 2008 Int. Conf. on Computer and Communication Engineering 2008 82 88 Assidiq, A.A., Khalifa, O.O., Islam, M.R., Khan, S., 2008. Real time lane detection for autonomous vehicles, in: 2008 Int. Conf. on Computer and Communication Engineering, 82–88,. [2] Aly M. Real time detection of lane markers in urban streets Intelligent Vehicles Symposium, 2008 IEEE 2008 IEEE 7 12 Aly, M., 2008. Real time detection of lane markers in urban streets, in: Intelligent Vehicles Symposium, 2008 IEEE, IEEE. 7–12,. [3] Kluge K. Lakshmanan S. A Deformable-Template Approach to Lane Detection 1995 IEEE 54 59 Kluge, K., Lakshmanan, S., 1995. A deformable-template approach to lane detection. IEEE, 54–59,. [4] Bounini F. Gingras D. Lapointe V. Pollart H. Autonomous vehicle and real time road lanes detection and tracking 2015 IEEE Vehicle Power and Propulsion Conference (VPPC) 2015 1 6 10.1109/VPPC.2015.7352903 Bounini, F., Gingras, D., Lapointe, V., Pollart, H., 2015. Autonomous vehicle and real time road lanes detection and tracking, in: 2015 IEEE Vehicle Power and Propulsion Conference (VPPC), 1–6,. 10.1109/VPPC.2015.7352903. [5] Liu S. Lu L. Zhong X. Zeng J. Effective road lane detection and tracking method using line segment detector 2018 37th Chinese Control Conf. (CCC) 2018 5222 5227 Liu, S., Lu, L., Zhong, X., Zeng, J., 2018. Effective road lane detection and tracking method using line segment detector, in: 2018 37th Chinese Control Conf. (CCC), 5222–5227,. [6] Hou C. Hou J. Yu C. An efficient lane markings detection and tracking method based on vanishing point constraints 2016 35th Chinese Control Conf. (CCC) 2016 6999 7004 Hou, C., Hou, J., Yu, C., 2016. An efficient lane markings detection and tracking method based on vanishing point constraints, in: 2016 35th Chinese Control Conf. (CCC), 6999–7004,. [7] Long J. Shelhamer E. Darrell T. Fully convolutional networks for semantic segmentation 2014 CoRR abs/1411.4038. URL: arXiv:1411.4038 Long, J., Shelhamer, E., Darrell, T., 2014. Fully convolutional networks for semantic segmentation. CoRR abs/1411.4038. arXiv:1411.4038]. [8] Ronneberger O. Fischer P. Brox T. U-net: Convolutional networks for biomedical image segmentation 2015 CoRR abs/1505.04597. URL: arXiv:1505.04597 Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation. CoRR abs/1505.04597. arXiv:1505.04597]. [9] Badrinarayanan V. Kendall A. Cipolla R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation 2015 CoRR abs/1511.00561. URL: arXiv:1511.00561 Badrinarayanan, V., Kendall, A., Cipolla, R., 2015. Segnet: A deep convolutional encoder–decoder architecture for image segmentation. CoRR abs/1511.00561. arXiv:1511.00561]. [10] Paszke A. Chaurasia A. Kim S. Culurciello E. Enet: A deep neural network architecture for real-time semantic segmentation 2016 CoRR abs/1606.02147. URL: arXiv:1606.02147 Paszke, A., Chaurasia, A., Kim, S., Culurciello, E., 2016. Enet: A deep neural network architecture for real-time semantic segmentation. CoRR abs/1606.02147. arXiv:1606.02147]. [11] Zhao H. Shi J. Qi X. Wang X. Jia J. Pyramid scene parsing network 2016 CoRR abs/1612.01105. URL: arXiv:1612.01105 Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2016. Pyramid scene parsing network. CoRR abs/1612.01105. arXiv:1612.01105]. [12] Chen L. Papandreou G. Kokkinos I. Murphy K. Yuille A.L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs 2016 CoRR abs/1606.00915. URL: arXiv:1606.00915 Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2016. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. CoRR abs/1606.00915. arXiv:1606.00915]. [13] Zhao H. Qi X. Shen X. Shi J. Jia J. Icnet for real-time semantic segmentation on high-resolution images 2017 CoRR abs/1704.08545. URL: arXiv:1704.08545 Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J., 2017. Icnet for real-time semantic segmentation on high-resolution images. CoRR abs/1704.08545. arXiv:1704.08545]. [14] Poudel R.P.K. Bonde U. Liwicki S. Zach C. Contextnet: Exploring context and detail for semantic segmentation in real-time 2018 CoRR abs/1805.04554. URL: arXiv:1805.04554 Poudel, R.P.K., Bonde, U., Liwicki, S., Zach, C., 2018. Contextnet: Exploring context and detail for semantic segmentation in real-time. CoRR abs/1805.04554. arXiv:1805.04554]. [15] Poudel R.P.K. Liwicki S. Cipolla R. Fast-SCNN: Fast semantic segmentation network 2019 CoRR abs/1902.04502. URL: arXiv:1902.04502 Poudel, R.P.K., Liwicki, S., Cipolla, R., 2019. Fast-scnn: Fast semantic segmentation network. CoRR abs/1902.04502. arXiv:1902.04502]. [16] Wang Y. Zhou Q. Wu X. Esnet: An efficient symmetric network for real-time semantic segmentation 2019 CoRR abs/1906.09826, URL: arXiv:1906.09826 Wang, Y., Zhou, Q., Wu, X., 2019. Esnet: An efficient symmetric network for real-time semantic segmentation. CoRR abs/1906.09826. arXiv:1906.09826]. [17] Chen P. Hang H. Chan S. Lin J. Dsnet: An efficient CNN for road scene segmentation 2019 CoRR abs/1904.05022. URL: arXiv:1904.05022 Chen, P., Hang, H., Chan, S., Lin, J., 2019. Dsnet: An efficient CNN for road scene segmentation. CoRR abs/1904.05022. arXiv:1904.05022]. [18] Lo S. Hang H. Chan S. Lin J. Multi-class lane semantic segmentation using efficient convolutional networks 2019 CoRR abs/1907.09438. URL: arXiv:1907.09438 Lo, S., Hang, H., Chan, S., Lin, J., 2019. Multi-class lane semantic segmentation using efficient convolutional networks. CoRR abs/1907.09438. arXiv:1907.09438]. [19] Lyu Y. Huang X. Road segmentation using CNN with GRU 2018 CoRR abs/1804.05164. URL: arXiv:1804.05164 Lyu, Y., Huang, X., 2018a. Road segmentation using CNN with GRU. CoRR abs/1804.05164. arXiv:1804.05164]. [20] Lyu Y. Huang X. Roadnet-v2: A 10 ms road segmentation using spatial sequence layer 2018 CoRR abs/1808.04450. URL: arXiv:1808.04450 Lyu, Y., Huang, X., 2018b. Roadnet-v2: A 10 ms road segmentation using spatial sequence layer. CoRR abs/1808.04450. arXiv:1808.04450]. [21] Neven D. Brabandere B.D. Georgoulis S. Proesmans M. Gool L.V. Towards end-to-end lane detection: an instance segmentation approach 2018 CoRR abs/1802.05591. URL: arXiv:1802.05591 Neven, D., Brabandere, B.D., Georgoulis, S., Proesmans, M., Gool, L.V., 2018. Towards end-to-end lane detection: an instance segmentation approach. CoRR abs/1802.05591. arXiv:1802.05591]. [22] Hou Y. Ma Z. Liu C. Loy C.C. Learning lightweight lane detection CNNs by self attention distillation 2019 arXiv:1908.00821 Hou, Y., Ma, Z., Liu, C., Loy, C.C., 2019. Learning lightweight lane detection cnns by self attention distillation. arXiv:1908.00821]. [23] Yu Z. Ren X. Huang Y. Tian W. Zhao J. Detecting lane and road markings at a distance with perspective transformer layers 2020 arXiv:2003.08550 Yu, Z., Ren, X., Huang, Y., Tian, W., Zhao, J., 2020. Detecting lane and road markings at a distance with perspective transformer layers. arXiv:2003.08550]. [24] TaSci E. Ugur A. Image classification using ensemble algorithms with deep learning and hand-crafted features 2018 26th Signal Processing and Communications Applications Conf. (SIU) 2018 1 4 TaSci, E., Ugur, A., 2018. Image classification using ensemble algorithms with deep learning and hand-crafted features, in: 2018 26th Signal Processing and Communications Applications Conf. (SIU), 1–4,. [25] Pouyanfar S. Chen S. Semantic event detection using ensemble deep learning 2016 IEEE International Symposium on Multimedia (ISM) 2016 203 208 Pouyanfar, S., Chen, S., 2016. Semantic event detection using ensemble deep learning, in: 2016 IEEE International Symposium on Multimedia (ISM), 203–208,. [26] Almeida T. Santos V. Lourenço B. Scalable ros-based architecture to merge multi-source lane detection algorithms Silva M.F. Luís Lima J. Reis L.P. Sanfeliu A. Tardioli D. Robot 2019: Fourth Iberian Robotics Conference 2020 Springer International Publishing Cham 242 254 Almeida, T., Santos, V., Lourenco, B., 2020. Scalable ros-based architecture to merge multi-source lane detection algorithms, in: Silva, M.F., Luís Lima, J., Reis, L.P., Sanfeliu, A., Tardioli, D. (Eds.), Robot 2019: Fourth Iberian Robotics Conference, Springer International Publishing, Cham. 242–254,. [27] Noh H. Hong S. Han B. Learning deconvolution network for semantic segmentation 2015 CoRR abs/1505.04366. URL: arXiv:1505.04366 Noh, H., Hong, S., Han, B., 2015. Learning deconvolution network for semantic segmentation. CoRR abs/1505.04366. arXiv:1505.04366]. [28] Jiang J. Zheng L. Luo F. Zhang Z. Rednet: Residual encoder-decoder network for indoor RGB-d semantic segmentation 2018 CoRR abs/1806.01054. URL: arXiv:1806.01054 Jiang, J., Zheng, L., Luo, F., Zhang, Z., 2018. Rednet: Residual encoder–decoder network for indoor RGB-D semantic segmentation. CoRR abs/1806.01054. arXiv:1806.01054]. [29] Cordts M. Omran M. Ramos S. Rehfeld T. Enzweiler M. Benenson R. Franke U. Roth S. Schiele B. The cityscapes dataset for semantic urban scene understanding 2016 CoRR abs/1604.01685. URL: arXiv:1604.01685 Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B., 2016. The cityscapes dataset for semantic urban scene understanding. CoRR abs/1604.01685. arXiv:1604.01685]. [30] Sudre C.H. Li W. Vercauteren T. Ourselin S. Cardoso M.J. Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations 2017 CoRR abs/1707.03237. URL: arXiv:1707.03237 Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Cardoso, M.J., 2017. Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. CoRR abs/1707.03237. arXiv:1707.03237]. [31] Brabandere B.D. Neven D. Gool L.V. Semantic instance segmentation with a discriminative loss function 2017 CoRR abs/1708.02551. URL: arXiv:1708.02551 Brabandere, B.D., Neven, D., Gool, L.V., 2017. Semantic instance segmentation with a discriminative loss function. CoRR abs/1708.02551. arXiv:1708.02551]. [32] Tusimple competitions for cvpr2017 2020 accessed: 2020-05-02 ,. Tusimple competitions for cvpr2017. Accessed: 2020-05-02. Tiago Almeida graduated in Mechanical Engineering, obtaining a Master Degree in 2019. Currently, he is a research fellow at IEETA, University of Aveiro, in the fields of Computer Vision and Robotic Systems. His Master Degree allowed the writing of his first article and the respective presentation at the 4th Iberian Conference (ROBOT2019). He is now working on one of the tasks of the Produtech project, whose aim is to build a low-cost autonomous guided vehicle that is capable of knowing its localization in a factory installation, through Artificial Vision. His research interests include Deep Learning and Machine Learning applied to Computer Vision. Bernardo Lourenço graduated in Msc. of Mechanical Engineering in 2018 at the University of Aveiro. Currently, he is a research fellow in the fields of Computer Vision and Robotic Systems at the Department of Mechanical Engineering at the University of Aveiro. He has participated in two conferences: the 2019 ICARSC Conference and the ROBOT2019 Conference. His research interests are deep learning, computer vision, programming and robotic systems. Vítor Santos graduated in Electronics Engineering and Telecommunications in 1989 and obtained a Ph.D. in Electrical Engineering in 1995. He was a researcher in mobile robotics at the Joint Research Center, Italy. Currently, he is an Associate Professor at the Department of Mechanical Engineering, lecturing courses related to advanced perception and robotics. He has managed research activity on mobile robotics, advanced perception, and humanoid robotics, with the supervision or co-supervision of more than 100 graduate and post-graduate students, and more than 140 publications. He has been in the program committee of several national and international conferences and acts regularly as a reviewer for several international conferences and journals. At the University of Aveiro, he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and the development of ATLASCAR and ATLASCAR2, the first car with autonomous navigation capabilities in Portugal that won the first prize in the Freebots competition in 2011. He is one of the founders of Portuguese Robotics Open and co-founder of the Portuguese Society of Robotics and a researcher at IEETA, in the Intelligent Robotics and Systems Group, focused on autonomous driving and driver assistance. "
    },
    {
        "doc_title": "A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach",
        "doc_scopus_id": "85085374137",
        "doc_doi": "10.1016/j.robot.2020.103558",
        "doc_eid": "2-s2.0-85085374137",
        "doc_date": "2020-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Extrinsic calibration",
            "Multi-modal approach",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization",
            "State-of-the-art approach"
        ],
        "doc_abstract": "© 2020 Elsevier B.V.This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2020-05-19 2020-05-19 2020-05-28 2020-05-28 2020-08-03T02:56:00 S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 S300 S300.1 FULL-TEXT 2020-08-03T02:04:37.606251Z 0 0 20200901 20200930 2020 2020-05-19T15:45:05.788726Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref vitae 0921-8890 09218890 true 131 131 C Volume 131 7 103558 103558 103558 202009 September 2020 2020-09-01 2020-09-30 2020 article fla © 2020 Elsevier B.V. All rights reserved. AROSFRAMEWORKFOREXTRINSICCALIBRATIONINTELLIGENTVEHICLESAMULTISENSORMULTIMODALAPPROACH OLIVEIRA M 1 Introduction 2 ROS based calibration setup 2.1 Configuration of the calibration procedure 2.2 Interactive positioning of sensors 2.3 Interactive data labelling 2.4 Collecting data 2.5 Sensor poses from partial transformations 3 Calibration procedure 3.1 Optimization parameters 3.2 Objective function 3.2.1 Camera sub-function 3.2.2 Laser sub-function 3.3 Sensors pose calibration: Optimization 4 Results 4.1 Camera to camera 4.2 Complete system calibration 5 Conclusions and future work Acknowledgements References MUELLER 2017 1 6 G 2017IEEE20THINTCONFINTELLIGENTTRANSPORTATIONSYSTEMSITSC CONTINUOUSSTEREOCAMERACALIBRATIONINURBANSCENARIOS WU 2015 2638 2642 L 2015IEEEINTCONFMECHATRONICSAUTOMATIONICMA BINOCULARSTEREOVISIONCAMERACALIBRATION ROUSU 2016 896 900 L 2016IEEEADVANCEDINFORMATIONMANAGEMENTCOMMUNICATESELECTRONICAUTOMATIONCONTROLCONFIMCEC AUTOMATICCALIBRATIONSYSTEMFORBINOCULARSTEREOIMAGING LING 2016 1771 1778 Y 2016IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROS HIGHPRECISIONONLINEMARKERLESSSTEREOEXTRINSICCALIBRATION DINH 2019 815 826 V VASCONCELOS 2012 2097 2107 F PEREIRA 2016 326 337 M ALMEIDA 2012 312 319 M IMAGEANALYSISRECOGNITION 3D2DLASERRANGEFINDERCALIBRATIONUSINGACONICBASEDGEOMETRYSHAPE GUINDEL 2017 1 6 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS KWON 2018 1451 1454 Y 201818THINTCONFCONTROLAUTOMATIONSYSTEMSICCAS AUTOMATICSPHEREDETECTIONFOREXTRINSICCALIBRATIONMULTIPLERGBDCAMERAS KHAN 2016 1960 1965 A 2016IEEEINTCONFROBOTICSBIOMIMETICSROBIO CALIBRATIONACTIVEBINOCULARRGBDVISIONSYSTEMSFORDUALARMROBOTS BASSO 2018 1315 1332 F QIAO 2013 253 256 Y 2013INTCONFCOMPUTATIONALPROBLEMSOLVINGICCP ANEWAPPROACHSELFCALIBRATIONHANDEYEVISIONSYSTEMS ZHANG 2011 1 6 C 2011IEEEINTCONFMULTIMEDIAEXPO CALIBRATIONBETWEENDEPTHCOLORSENSORSFORCOMMODITYDEPTHCAMERAS CHEN 2019 2685 2694 G QILONGZHANG 2004 2301 2306 G 2004IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROSIEEECATNO04CH37566VOL3 EXTRINSICCALIBRATIONACAMERALASERRANGEFINDERIMPROVESCAMERACALIBRATION HASELICH 2012 25 28 M 2012IEEEINTCONFEMERGINGSIGNALPROCESSINGAPPLICATIONS CALIBRATIONMULTIPLECAMERASA3DLASERRANGEFINDER CHEN 2016 448 453 Z 20169THINTCONGRESSIMAGESIGNALPROCESSINGBIOMEDICALENGINEERINGINFORMATICSCISPBMEI EXTRINSICCALIBRATIONALASERRANGEFINDERACAMERABASEDAUTOMATICDETECTIONLINEFEATURE VELAS 2014 M CALIBRATIONRGBCAMERAVELODYNELIDAR LEE 2017 64 69 G 2017IEEEINTCONFMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI CALIBRATIONVLP16LIDARMULTIVIEWCAMERASUSINGABALLFOR360DEGREE3DCOLORMAPACQUISITION LEVINSON 2013 J ROBOTICSSCIENCESYSTEMS AUTOMATICONLINECALIBRATIONCAMERASLASERS DEZHIGAO 2010 6211 6215 J 20108THWORLDCONGRESSINTELLIGENTCONTROLAUTOMATION AMETHODSPATIALCALIBRATIONFORCAMERARADAR SANTOS 2010 1421 1427 V 13THINTIEEECONFINTELLIGENTTRANSPORTATIONSYSTEMS ATLASCARTECHNOLOGIESFORACOMPUTERASSISTEDDRIVINGSYSTEMBOARDACOMMONAUTOMOBILE LIAO 2017 305 310 Y 20172NDINTCONFADVANCEDROBOTICSMECHATRONICSICARM JOINTKINECTMULTIPLEEXTERNALCAMERASSIMULTANEOUSCALIBRATION REHDER 2016 383 398 J PRADEEP 2014 211 225 V EXPERIMENTALROBOTICS12THINTSYMPOSIUMEXPERIMENTALROBOTICS CALIBRATINGAMULTIARMMULTISENSORROBOTABUNDLEADJUSTMENTAPPROACH OLIVEIRA 2020 203 215 M ROBOT2019FOURTHIBERIANROBOTICSCONFERENCE AGENERALAPPROACHEXTRINSICCALIBRATIONINTELLIGENTVEHICLESUSINGROS BRADSKI 2000 G QUIGLEY 2009 M ICRAWORKSHOPOPENSOURCESOFTWARE ROSOPENSOURCEROBOTOPERATINGSYSTEM FOOTE 2013 1 6 T 2013IEEECONFERENCETECHNOLOGIESFORPRACTICALROBOTAPPLICATIONSTEPRA TFTRANSFORMLIBRARY HORNEGGER 1999 640 647 J PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISIONVOL1 REPRESENTATIONISSUESINMLESTIMATIONCAMERAMOTION AGARWAL 2010 29 42 S BUNDLEADJUSTMENTINLARGE OLIVEIRAX2020X103558 OLIVEIRAX2020X103558XM 2022-05-28T00:00:00.000Z 2022-05-28T00:00:00.000Z © 2020 Elsevier B.V. All rights reserved. 2020-05-03T22:04:14.426Z FCT Fundação para a Ciência e a Tecnologia CYTED CYTED Ciencia y Tecnología para el Desarrollo item S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 271599 2020-08-03T02:04:37.606251Z 2020-09-01 2020-09-30 true 2168899 MAIN 11 62586 849 656 IMAGE-WEB-PDF 1 gr3 35928 345 552 gr9 19670 275 339 pic2 6890 131 112 gr1 33542 289 490 gr12 23770 202 495 gr11 36248 266 489 gr7 37441 309 371 fx1002 8338 79 360 gr4 28117 314 378 gr6 18379 143 385 fx1001 8242 73 367 gr5 45234 242 376 gr2 42926 236 553 gr8 52449 600 489 gr10 49550 264 489 fx1003 6813 77 283 gr3 8344 137 219 gr9 6226 164 202 pic2 19499 164 140 gr1 5132 129 219 gr12 5757 89 219 gr11 7781 119 219 gr7 20274 163 196 fx1002 2821 48 219 gr4 6100 163 197 gr6 10154 81 219 fx1001 2829 43 219 gr5 25359 141 219 gr2 10115 94 219 gr8 4814 163 133 gr10 11178 118 219 fx1003 3134 60 219 gr3 277021 1527 2446 gr9 136675 1219 1500 pic2 61746 583 499 gr1 243828 1282 2171 gr12 231425 895 2191 gr11 268102 1178 2168 gr7 362638 1370 1643 fx1002 30951 210 958 gr4 207176 1390 1675 gr6 147160 632 1706 fx1001 30843 193 975 gr5 416481 1072 1667 gr2 344316 1047 2451 gr8 390098 2658 2167 gr10 424428 1169 2168 fx1003 24651 206 753 si115 6585 si97 32421 si36 1420 si95 6165 si111 9455 si44 6967 si7 7552 si94 1102 si42 7085 si121 31424 si5 7072 si74 5953 si89 2139 si119 9249 si71 1806 si10 1146 si30 8639 si12 235 si35 7139 si8 12045 si37 8297 si54 6126 si16 25662 si41 1679 si117 27686 si80 14084 si88 1722 si51 1990 si91 10025 si85 7503 si6 5529 si110 14513 si21 22352 si26 1589 si32 5724 si45 21948 si47 2672 si24 1842 si27 1665 si40 5395 si112 8789 si118 8959 si18 5806 si55 7274 si59 21073 si1 1359 si87 1814 si70 2168 si38 3203 si52 22498 si83 5598 si53 5544 si77 23977 si4 916 si49 2727 si17 4071 si90 10327 si3 3897 si50 3168 si29 1813 si39 21584 si22 37437 si48 1268 si114 16710 si34 1274 si23 2199 si28 1675 si92 3705 si25 1781 si75 36143 si31 1407 am false 3012923 ROBOT 103558 103558 S0921-8890(20)30398-5 10.1016/j.robot.2020.103558 Elsevier B.V. Fig. 1 Two methodologies for solving the calibration of complex systems using pair-wise approaches: (a) sequential pairwise; (b) one level pyramid using a reference sensor. The estimated transformations use the arrangements shown in solid colour arrows. Other possible arrangements are presented in dashed grey lines. Note that in both cases only a subset of the available transformations is used. Fig. 2 The proposed calibration procedure: (a) initialization from xacro files and interactive first guess; (b) data labelling and collecting. Fig. 3 Interactive labelling of 2D LiDAR data: (a) creation of interactive marker on the sensor body, (b) dragging and dropping the marker on top of the data cluster containing the chessboard plane, (c) and (d) subsequent automatic tracking of the chessboard plane. Fig. 4 Conceptual transformation graph for a complex robotic system. Each sensor has a respective calibration partial transformation, denoted by the solid edges. Dashed edges contain transformations which are not optimized (they may be static or dynamic). Each sensor has a corresponding link to which the data it collects is attached, denoted in the figure by the solid thin ellipses. Very few approaches in the literature are capable of calibrating such a system while preserving the initial structure of the graph of transformations. Fig. 5 Example of reprojection of chessboard corners during the optimization procedure: squares denote the position of the detected chessboard corners (ground truth points); crosses denote the initial position of each projected corner; points denote the current position of the projected corners. Fig. 6 Chessboard: graphics visualization of the created grid and boundary correspondent to the real chessboard (a); image of the real chessboard (b). Fig. 7 ATLASCAR2: Autonomous vehicle from the Department of Mechanical Engineering of the University of Aveiro; the sensors are indicated by the Ellipses. Fig. 8 Pixel coordinates errors between projected (expected) chessboard corners and the ground truth indexes, from top left camera to top right camera, for each collection, before the optimization procedure (a) and after the optimization procedure (b). Fig. 9 Flowchart representing the results comparison structure. Each ellipse represents a JSON file and each rectangle identifies a programmed application. Fig. 10 Pixel coordinate errors between projected and expected chessboard corners. The kalibr results are not visible because of the selection of the axes range. Fig. 11 Average error per sensor during a full system calibration procedure. Errors for cameras in pixels, for LiDARs in metres. Fig. 12 Left laser (dots surrounded by red circles) and right laser (dots surrounded by green circles) data overlaid onto a representation of the chessboard, taking in consideration the pose of the chessboard and the LiDARs as estimated by the calibration for one particular collection: (a) and (b) the start of the optimization (initial guess); (c) and (d) the end of the optimization (calibration results). Table 1 Average errors and standard deviations along both directions, before and after the optimization. Values in pixels. Values Average error Standard deviation Initial Final Initial Final x error 2.25 1.64 2.68 1.69 y error 17.09 0.53 3.32 0.62 Both 17.34 1.83 8.71 1.51 Table 2 Average errors and standard deviations, in pixels, for the distances in x axis and y axis, for the proposed approach, the OpenCV stereo calibration and the kalibr calibration method. For kalibr, two datasets were used for training: the train dataset, which was also used to train all other approaches, and the test dataset, which was used to evaluate all approaches. Values in pixels. Calibration method Average error Standard deviation x y x y Proposed approach (left) 2.218 1.633 1.223 0.584 Proposed approach (right) 2.080 1.797 1.253 0.608 OpenCV stereo calibrate 1.251 0.903 1.509 0.767 Kalibr (train) [26] 67.383 8.887 0.832 1.722 Kalibr (test) [26] 1.187 17.999 1.369 2.225 A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Miguel Oliveira a b Afonso Castro b ⁎ Tiago Madeira a Eurico Pedrosa a Paulo Dias a c Vítor Santos a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal, Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro b Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal Department of Mechanical Engineering, University of Aveiro c Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro ⁎ Corresponding author. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups. Keywords Extrinsic calibration ROS Optimization Bundle adjustment Intelligent vehicles OpenCV 1 Introduction Intelligent vehicles require a considerable amount of on-board sensors, often of multiple modalities (e.g. camera, Light Detection And Ranging (LiDAR), etc.) in order to operate consistently. The combination of the data collected by these sensors requires a transformation or projection of data from one sensor coordinate frame to another. The process of estimating these transformations between sensor coordinate systems is called extrinsic calibration. An extrinsic calibration between two sensors requires an association of data from one sensor to the data of another. By knowing these data associations, an optimization procedure can be formulated to estimate the parameters of the transformation between those sensors that minimizes the distance between associations. Most calibration approaches make use of calibration patterns, i.e., objects that are robustly and accurately detected by distinct sensor modalities. Although there have been many solutions available in the literature, on the topic of calibration, there is no straightforward solution for the calibration of multiple sensors in intelligent vehicles, or robots in general. There are multiple factors that contribute to this lack of solutions. The majority of works on calibration focus on sensor to sensor pairwise calibrations: between only cameras [1–5] or between cameras and LiDARs [6–10]. When considering pairwise combinations of sensors, there are several possibilities, according to the modality of each of the sensors in the pair. Most of them have been addressed in the literature: RGB to RGB camera calibration [1–5,7]; RGB to depth camera (RGB-D cameras) calibration [11–16]; camera to 2D LiDAR [6,7,10,15,17–20]; 2D LiDAR to 3D LiDAR [8]; camera to 3D LiDAR [10,21,22]; and camera to radar [23]. Nonetheless, all these approaches have the obvious shortcoming of operating only with a single pair of sensors, which is not directly applicable to the case of intelligent vehicles, or more complex robotic systems in general. To be applicable in those cases, pairwise approaches must be arranged in a graph-like sequential procedure, in which one sensor calibrates with another, that then relates to a third sensor, and so forth. Another option is to establish one sensor as the reference sensor and link all other sensors to it. In this case, the graph of transformations between sensors results in a one level pyramid, which contains the reference sensor on top and all other sensors at the base. One example is [7], in which a methodology for calibrating the ATLASCAR2 autonomous vehicle [24] is proposed, wherein all sensors are paired with a reference sensor. Sequential pairwise approaches have three major shortcomings: (i) transformations are estimated using only data provided from the selected sensor tandem, despite the fact that data from additional sensors could be available and prove relevant to the overall accuracy of the calibration procedure; (ii) sensitivity to cumulative errors, since the transformations are computed in a sequence (in fact, this does not occur in [7], since the pose of a specific sensor only depends on the reference sensor pose); (iii) structure of transformation graph is enforced by the nature of the calibration procedure, rather than being defined by the preference of the programmer, which could compromise some robot functionalities. Fig. 1 shows a conceptual example in which these problems are visible. There are a few works which address the problem of calibration from a multi-sensor, simultaneous optimization, perspective. In [25], a joint objective function is proposed to simultaneously calibrate three RGB cameras with respect to an RGB-D camera. Authors report a significant improvement in the accuracy of the calibration. In [26], an approach for joint estimation of both temporal offsets and spatial transformations between sensors is presented. This approach is one of few that is not designed for a particular set of sensors, since its methodology does not rely on unique properties of specific sensors. It is able to calibrate systems containing both cameras and LiDARs. Moreover, the approach does not require the usage of calibration patterns for the LiDARs, using the planes present in the scene for that purpose. In [27], a joint calibration of the joint offsets and the sensors locations for a PR2 robot is proposed. This method takes sensor uncertainty into account and is modelled in a similar way to the bundle adjustment problem. Our approach is similar to [27], in the sense that we also employ a bundle adjustment-like optimization procedure. However, our approach is not focused on a single robotic platform, rather it is a general approach that is applicable to any robotic system, which also relates it with [26]. This paper is an extension of [28], where the general approach was originally proposed. This extension focuses on the comparison of this work against state of the art calibration approaches, i.e. methodologies provided by Open Source Computer Vision Library (OpenCV) [29] as well as the calibration method from [26]. Robot Operating System ROS [30] based architectures are the standard when developing robots. There are several ROS based calibration packages available. 1 1 2 2 3 3 In addition, some approaches are well integrated with ROS since the input data for the calibration is provided as a rosbag file. Despite this, no approach provides a complete solution for the calibration of intelligent vehicles. Thus, the seamless integration with ROS became a core component of the proposed approach. To that end, the proposed calibration procedure is self-configured using the standard ROS robot description files, the Unified Robot Description Format (URDF), and provide several tools for sensor positioning and data labelling based on RVIZ interactive markers. The remainder of this paper is organized as follows: Section 2 describes the methodologies used to set up an optimization procedure which calibrates the system. In this section, several auxiliary tools for labelling data and positioning sensors are described; Section 3 details the optimization procedure and how it is cast as a bundle adjustment problem; Section 4 provides comparisons with established OpenCV calibration methodologies; finally, Section 5 provides conclusions and future work. 2 ROS based calibration setup A schematic of the proposed calibration procedure is displayed in Fig. 2. It consists of five components: configuration; interactive positioning of sensors; interactive labelling of data; collection of data; and finally, the optimization procedure. Each component will be described in detail in the following subsections. 2.1 Configuration of the calibration procedure Robotic platforms are described in ROS using a xml file called URDF. We propose to extend the URDF description files of a robot in order to provide information necessary for configuring how the calibration should be carried out. A new URDF element, named calibration, is introduced specifically for the purpose of calibrating. Each calibration element describes a sensor to be calibrated. The element contains information about the calibration parent and child links, which define the partial transformation that is optimized. 2.2 Interactive positioning of sensors Optimization procedures suffer from the known problem of local minima. This problem tends to occur when the initial solution is far from the optimal parameter configuration. Thus, it is expected that, by ensuring an accurate first guess for the sensor poses, there is less likelihood of falling into local minima. We propose to solve this problem in an interactive fashion: the system parses the URDF robot description and creates an rviz interactive marker associated with each sensor. It is then possible to move and rotate the interactive markers. This provides a simple, interactive method to manually calibrate the system or, alternatively, to easily generate plausible first guesses for the poses of the sensors. Real time visual feedback is provided by the observation of the bodies of the robot model (e.g. where a LiDAR is placed w.r.t. the vehicle), and also by the data measured by the sensors (e.g. how well the measurements from two LiDARs match). An example of this procedure can be watched at 2.3 Interactive data labelling Since the goal is to propose a calibration procedure that operates on multi-modal data, a calibration pattern adequate to all available sensor modalities must be selected. A chessboard pattern is a common calibration pattern, in particular for RGB and RGB-D cameras. To label image data, one of the many available image-based chessboards detectors is used (Find Chessboard Corners OpenCV function 4 4 ). In the case of 2D LiDAR data, it is not possible to robustly detect the chessboard, since there are often multiple planes in the scene derived from other structures, such as walls and doors. To solve this, we propose an interactive approach which requires minimal user intervention: rviz interactive markers are positioned along the LiDAR measurement planes and the user drags the marker to indicate where in the data the chessboard is observed. This is done by clustering the LiDAR data, and selecting the cluster which is closer to the marker. This interactive procedure is done only once, since it is then possible to track the chessboard robustly. Fig. 3 shows an example of the labelling of 2D LiDAR data. This interactive data labelling procedure is showcased in 2.4 Collecting data Usually, different sensors stream data at different frequencies. However, to compute the associations between the data of multiple sensors, temporal synchronization is required. While some approaches require hardware synchronization to operate [26], in the current method this is solved trivially by collecting data (and the corresponding labels) at user defined moments in which the scene has remained static for a certain period of time. In static scenes, the problem of data desynchronization is not observable, which warrants the assumption that for each captured collection the sensor data is synchronized. We refer to these snapshot recordings of multi-sensor data as data collections. This information is stored in a JSON file that will be read by the optimization procedure afterwards. The JSON file contains abstract information about the sensors, such as the sensor transformation chain, among others, and specific information about each collection, i.e., sensor data, partial transformations, and data labels. It is important to note that the set of collections should contain as many different poses as possible. As such, collections should preferably have different distances and orientations w.r.t. the chessboard so that the calibration becomes more reliable. This concern is common to the majority of calibration procedures. 2.5 Sensor poses from partial transformations The representation of a complex, multi-sensor system requires the creation of a transformation graph. For this purpose, ROS uses a graph tree referred to as tf tree [31]. One critical factor for any calibration procedure is that it should not change the structure of that existing tf tree. The reason for this is that the tf tree, derived from the URDF files by the robot state publisher, 5 5 also supports additional functionalities, such as robot visualization or collision detection. If the tf tree changes due to the calibration, those functionalities may be compromised or require some redesigning. To accomplish this, we propose to compute the pose of any particular sensor (i.e., the transformation from the Reference Link, also known as World, to that Sensor) as an aggregate transformation A , obtained after the chain of transformations for that particular sensor, extracted from the topology of the tf tree: (1) where i T i + 1 represents the partial transformation from the ith to the i + 1 link, and p a r e n t and c h i l d are the indexes of the calibration parent and calibration child links in the sensor chain, respectively. Our approach preserves the predefined structure of the tf tree, since, during optimization, only one partial transformation contained in the chain is altered (the one in blue in Eq. (1)). This computation is performed within the optimization’s cost function. Therefore, a change in one partial transformation affects the global sensor pose, and consequently, the error to minimize. The optimization may target multiple links of each chain, and is agnostic to whether the remaining links are static or dynamic, since all existing partial transformations are stored for each data collection. To the best of our knowledge, our approach is one of few which maintains the structure of the transformation graph before and after optimization. This is a feature that is often overlooked, yet it is of critical practical importance for the selection of a calibration framework. Taking the example of Fig. 4, consider that Sensor 1 is mounted on top of a pan and tilt unit, where LinkA T Link C corresponds to the pan movement, and LinkC T Sensor 1 represents the tilt motion. For this particular case, Eq. (1) becomes: (2) where I is the identity matrix (since there are no prior links), and the pan and tilt motions are coloured in red to denote that these transformations are dynamic and, as a consequence, may also change from collection to collection. Another example is the one of Sensor 2: it contains an aggregate transformation that also includes the partial transformation optimized w.r.t. Sensor 1, resulting in the following aggregate transformation: (3) which is also directly derived from Eq. (1). Such complex arrangements are seldom tackled by a single calibration approach, even less in a transparent way by the same general formalism. The proposed optimization of partial transformations achieves this goal. We consider the ability to preserve the structure of the tf tree as a key feature of the proposed framework: from a practical standpoint, since it facilitates the integration into ROS, both before and after the optimization; and, moreover, from a conceptual perspective, since this formulation is general and adequate to handle most calibration scenarios. 3 Calibration procedure The goal of general optimization procedures is to find the parameter configuration that results in the smallest function value. This function, which depends on the optimization parameters Φ is known as the objective function. For the purpose of calibrating the multi-modal sensors of a robotic platform, like the ATLASCAR2 intelligent vehicle, the objective of this optimization is to estimate the pose of each sensor relatively to a reference link (base link for ATLASCAR2 case). 3.1 Optimization parameters An extrinsic calibration translates into a pose estimation. Thus, the set of parameters to optimize, defined as Φ , must contain parameters that together define the pose of each sensor. As discussed in the beginning of Section 2, we propose to maintain the initial structure of the transformation graph, and thus only optimize one partial transformation per sensor. In the example of Fig. 4, these partial transformations are denoted by solid arrows. Since the usage of camera sensors is considered, it is also possible to introduce the intrinsic parameters of each camera in the set Φ . Our goal is to define an objective function that is able to characterize sensors of different modalities. Pairwise methodology for devising the cost function results in complex graphs of exhaustive definition of relationships. For every existing pair of sensors, these relationships must be established according to the modality of each of the sensors, and, although most cases have been addressed in literature, as discussed in Section 1, a problem of scalability remains inherent to such a solution. To address this issue, we propose to structure the cost function in a sensor to calibration pattern paradigm, similar to what is done in bundle adjustment. That is, the positions of 3D points in the scene are jointly refined with the poses of the sensors. These 3D points correspond to the corners of the calibration chessboard. What is optimized is actually the transformation that takes these corners from the frame of reference of the chessboard to the world, for every collection. All variables must have some initial value, so that the optimizer may compute the first error, and start to refine the values in order to obtain the minimum of the cost function. The first guess for each chessboard is obtained by computing the pose of a chessboard detection in one of the cameras available. The output is a transformation from the chessboard reference frame to the camera’s reference frame. Since we already have the first guess for the poses of each sensor, calculated as an aggregate transformation A (see Eq. (1)), to obtain the transformation from the chessboard reference frame to the world (an external and absolute frame), the following calculation is applied: (4) chess T world = camera A world ︷ Eq. (1) ⋅ chess T camera ︷ chess detection , where chess and camera refer to chessboard and camera coordinate frames, respectively. Thus, the set of parameters to be optimized Φ , contains the transformation represented in Eq. (4), for each collection, along with the poses of each sensor: (5) Φ = [ x m = 1 , r m = 1 , i m = 1 , d m = 1 , … , x m = M , r m = M , i m = M , d m = M , ︷ Cameras x n = 1 , r n = 1 , … , x n = N , r n = N , ︷ LiDARs … , ︷ Other modalities x k = 1 , r k = 1 , … , x k = K , r k = K ︷ Calibration object ] where m refers to the mth camera, of the set of M cameras, n refers to the nth LiDAR, of the set of N LiDARs, k refers to the chessboard detection of the kth collection, contained in the set of K collections, x is a translation vector [ t x , t y , t z ] , r is a rotation represented through the axis/angle parameterization [ r 1 , r 2 , r 3 ] (where the vector [ r 1 , r 2 , r 3 ] is used to represent the axis and its norm the angle), i is a vector of a camera’s intrinsic parameters [ f x , f y , c x , c y ], and d is a vector of camera’s distortion coefficients [ d 0 , d 1 , d 2 , d 3 , d 4 ]. The initial estimate for the intrinsic parameters is obtained using any intrinsic camera calibration tool. The axis/angle parameterization was chosen because it has 3 components and 3 degrees of freedom, making it a fair parameterization, since it does not introduce more numerical sensitivity than the one inherent to the problem itself [32]. At this point, there are six parameters per sensor, related to the pose of each one, to be enhanced. These values compose the geometric transformation that will be calibrated. The cost function will compute the residuals based on an error (in pixels for RGB cameras and in millimetres for LiDARs) between the re-projected position of the chessboard, estimated by all transformations, and the position of the calibration pattern detected by each sensor. 3.2 Objective function The cost function for this optimization, F ( Φ ) , can be thought of as the sum of several sub-functions that compose a vector function, where, for every modality of sensor added to the calibration, a new sub-function is defined accordingly, which allows for the minimization of the error associated with the pose of sensors of that modality. Thus, the optimization procedure can be defined as: (6) arg min Φ F ( Φ ) = 1 2 ∑ i f i ( Φ i 1 , … , Φ i k ) 2 where f i ( ⋅ ) is the objective sub-function for the i th sensor with the respective parameters block { Φ i 1 , … , Φ i k } , being k the parameters number of each objective sub-function. In other words, the scalar cost function of this optimization is the sum of the squares of the returned values from a vector function, divided by two. Each different sensor has an inherent sub-function, that depends on the sensor modality. The value of all these sub-functions is a vector with the errors (residuals) associated to the re-projection of the calibration pattern points. Since for the ATLASCAR2 intelligent vehicle we are considering four sensors (two cameras and two 2D LiDARs), the objective function is composed by the vector values of four sub-functions, two of each type. Each different sub-function is detailed in the next sub-sections. 3.2.1 Camera sub-function When the sensors are cameras, their calibration is performed as a bundle adjustment [33], and as such, the sub-function created is based on the average geometric error corresponding to the image distance between a projected point and a detected one. The 3D points corresponding to the corners of the calibration chessboard are captured by one or more cameras in each collection. Each camera is defined by its pose relative to a reference link and intrinsic parameters. After the desired acquisitions are completed, the 3D points are projected from the world into the images and the 2D coordinates are compared to the ones obtained by detection of the calibration pattern in the corresponding images. The positions of the 3D points in the world are obtained by applying the transformation described in Eq. (4) to the chessboard corner points defined in the chessboard detection’s reference frame. The goal of this cost sub-function is to adjust the initial estimation of the camera parameters and the position of the points, in order to minimize the average reprojection error f camera , given by: (7) f camera = ℓ 2 ( x c = 1 , x ˆ c = 1 ) ℓ 2 ( x c = 2 , x ˆ c = 2 ) ⋯ ℓ 2 ( x c = C , x ˆ c = C ) ⊺ where ℓ 2 is the Euclidean distance between two vectors, c denotes the index of the chessboard corners, x c denotes the pixels coordinates of the measured points (given by chessboard detection), and x ˆ c are the projected points, given by the relationship between a 3D point in the world and its projection on an image plane. By knowing the real size of the chessboard squares, the 3D coordinates of all corners relatively to the chess frame can be inferred. Note that the z value will be, for every point, zero, since the chessboard is in the XoY plane. After obtaining the 3D coordinates of all corners in reference to the chessboard frame, the objective function computes the coordinates of the points relatively to the camera link through multiplying by the geometric transformation between the base link (reference frame for the ATLASCAR2 example) and the calibration pattern frame and by the transform between the camera link and the base link: (8) p c a m e r a = camera T world ⋅ world T chess ⋅ p c h e s s where p c h e s s refers to the x , y , z coordinates of a chessboard corner, defined in the local chessboard coordinate frame, and p c a m e r a refers to the x , y , z coordinates of the same chessboard corner, defined in the camera link. In fact, both p c h e s s and p c a m e r a are the homogenized matrices of the coordinates so that Eq. (8) is mathematically correct. Note that the parameters to be optimized define the chessboard to world transformation, and that the world to camera transformation is computed from an aggregate of several partial transformations, one of which is defined by other parameters being optimized; furthermore, the intrinsic matrix is dependent on parameters which are accounted for in the optimization. As is expected, the re-projected points become closer to the ground truth corners during the optimization procedure. Fig. 5 shows the difference between the initial position of the chessboard corners, projected from the 3D world to the camera image, and the final position of these same projected points, after the optimization has been completed. It is possible to observe that the pixels corresponding to the projection of the final position of the points (dots in Fig. 5) almost perfectly match the ground truth points (squares in Fig. 5). 3.2.2 Laser sub-function Finally, for the case of 2D LiDARs, the sub-function only considers the two border points, among all the measurements that are related to the chessboard plane, to compute the error associated to the pose of the LiDAR and the chessboard. In order to calculate the residuals that this cost sub-function should return, the detected points’ 3D coordinates from the chessboard frame are required. During the calibration setup stage, when the information of a time stamp is saved, the ranges of all measurements that the LiDAR is detecting are stored, as well as the information about this same LiDAR and the indexes of the ranges that correspond to the plane where the chessboard is. With the optimization parameters of the chessboard pose and the LiDAR pose (computed accordingly to Eq. (1)), both relative to the base link, the 3D coordinates of each labelled measurement of the point cloud in the chessboard frame are known: (9) x y z 1 chess = chess T world ⋅ world T lidar ⋅ x y z 1 lidar . Finally, with the coordinates from the chessboard frame, of both the first and the last points of the cluster extracted in the labelling stage, it is possible to compute the error evaluated by this cost sub-function. The error is based on the distance between each one of the limit points (the first and the last index) of the selected ranges and the chessboard surface boundaries. There are two computed distances for each point: orthogonal and longitudinal. The orthogonal distance is the z absolute value of the coordinates, in the calibration pattern frame, of the LiDAR data measurement. In an ideal setting, the z value should be zero, since the chessboard plane is on the XoY plane. This is why any value different from zero means that the optimization parameters (sensor pose and chess pose) are not yet correct. The longitudinal distance is the Euclidean distance between the x and y coordinates, in the calibration pattern frame, of the LiDAR data measurement and the x and y coordinates of the closest point that belong to the limit of the physical board that is being detected. In order to compute this distance, it is essential to create a group of points that represent the boundaries of the chessboard. By knowing the size of the board, the size of each chess square, and that the chess frame origin matches with the first (top left) chess corner, the coordinates were calculated and the points of the board boundaries were manually defined. The size of the border between the chess corner grid and the end of the physical chessboard had to be measured so that this step could be implemented. In Fig. 6, we can see the grid of the chess corners and a line around it: that line marks the limit of the board. This solid line has some points within it, which are going to be compared to the LiDAR data measured ones. Again, the optimizer will search for the closest limit point to each one of the studied LiDAR data measurement coordinates and then compute the longitudinal distance. Thus, the LiDAR sub-function f lidar is defined as: (10) f lidar = | z 1 chess | ℓ 2 ( p 1 b o a r d l i m i t , p 1 c h e s s ) | z 2 chess | ℓ 2 ( p 2 b o a r d l i m i t , p 2 c h e s s ) ⊺ where (11) p b o a r d l i m i t = x y boardlimit , (12) p c h e s s = x y chess , and z chess is the third coordinate value of the range measurement points transformed to the chessboard’s coordinate frame. 3.3 Sensors pose calibration: Optimization The cost function F ( Φ ) from Eq. (6) is minimized using a least-squares approach. 6 6 In this work we used the least-squares solver provided by SciPy: Least-squares finds a local minimum of a scalar cost function, with bounds on the variables, by having an m-dimensional real residual function of n real variables. As such, we choose this minimization approach as it is the best fit for our problem. 4 Results To assess the performance of the proposed calibration approach, we used an intelligent vehicle as test bed. The ATLASCAR2 [24] is an electric vehicle (Mitsubishi i-MiEV) with several sensors onboard. In this work four sensors were considered: two 2D LiDARs and two RGB cameras. Thus, two different modalities of sensors are used. The sensors are designated as follows: left laser, right laser, top left camera and top right camera. Fig. 7 shows the ATLASCAR2 vehicle. The proposed approach is used to calibrate the four selected sensors simultaneously. Nonetheless, as discussed above, there are no approaches which provide an off-the-shelf multi-sensor multi-modal calibration. As such, in order to evaluate this approach, we provide comparisons against other pairwise methodologies, which are abundant in the field, as was mentioned in Section 1. Note that, in the following comparisons, the results given by the proposed approach for a particular pair of sensors are obtained using a complete system calibration. On the other hand, the alternative methodologies calibrate a single pair of sensors. In this sense, the comparison methodology is not favourable to the proposed approach, since the other approaches are specialized in the case being evaluated. In the following lines, two tests are detailed: the first is a camera-to-camera evaluation which compares several calibration methods in a pairwise fashion, while the second characterizes the proposed joint optimization over time providing global metrics. 4.1 Camera to camera The methodology used to compute the error of the calibrated poses of the top right camera and the top left camera is based on the distance between pixel coordinates. These coordinates are, on the one hand, the detected chessboard corners (ground truth) of the top right camera and, on other hand, the coordinates of the projections of those corners, to the top left camera, using the transformation between the cameras which is the output of the calibration. To transform pixels from one camera to the other, we start from the projection of the 3D world coordinates to the image of a camera: (13) p = K ⋅ R t ⋅ P where P refers to the 3D homogeneous coordinates of the corners as viewed in the chessboard frame; p is a vector composed by the u , v and w values, in which: x pixel = u ∕ w and y pixel = v ∕ w , allowing for the direct extraction of image coordinates from this vector; R t is the non-homogeneous geometric transformation matrix from the camera frame to the chessboard frame, K represents the camera’s intrinsic matrix. Eq. (13) can be applied to each camera separately. Since the 3D chessboard corner coordinates are defined in the chessboard frame, the value of Z will be 0 for all corners, because they all lie on the XoY plane: Z chess = 0 . As a result, Eq. (13) may be simplified as follows: (14) u v w = f x 0 c x 0 f y c y 0 0 1 ⋅ r 11 r 12 t x r 21 r 22 t y r 31 r 32 t z ⋅ X Y 1 chess corners , which is equivalent to: (15) p camera = K ⋅ camera T chess ′ ⋅ P chess , where the geometric transformation matrix camera T chess ′ is a portion of the camera T chess matrix, as detailed in Eq. (15). We use (15) for both cameras, and relate both expressions by the 3D coordinates of the chessboard corners (which are the same for both cameras), resulting in: (16) p cam2 = K cam2 ⋅ cam2 T chess ′ ⋅ cam1 T chess ′ -1 ⋅ K cam1 -1 ⋅ p cam1 where cam1 and cam2 refer to the top left and top right cameras, respectively. This formulation provides the relation between image coordinates of the chessboard corners for both camera images of each collection. Notice, however, that calibration methods output the transformation between sensors, in this case between cameras, while Eq. (16) requires transformations from the cameras to the chessboard. Some approaches, as for example the proposed approach, also estimate the pose of the chessboards (see parameters of the calibration objects in Eq. (5)). Thus, at first glance, one could think of using these transformations directly in Eq. (16). However, these chessboard poses are estimated for a given training dataset, and cannot be accurately used for other datasets. Moreover, as said before, not all calibration approaches output the pose of the chessboards (e.g. OpenCV stereo calibrate). Instead, calibration approaches provide the transformation between cameras. By arbitrarily selecting one camera from which the chessboard pose is determined through the solvePNP function (we have used cam1, but tests have shown that the alternative provided similar results) and using the transformation cam1 T cam2 estimated by the calibration approaches, it is possible to determine the transformation of the other camera to the chess, as follows: (17) cam2 T chess = cam1 T cam2 -1 ︷ calibration ⋅ cam1 T chess ︷ s o l v e P n P . From this expression the partial matrices cam1 T chess ′ and cam2 T chess ′ are derived. Then, we apply Eq. (16) to compute the corner coordinates on the top right camera image, as projected from the detection of the top left camera image. The error is computed by measuring the difference between expected and projected corner coordinates on the top right camera image: (18) x error y error top right camera = x y projected − x y e x p e c t e d Fig. 8 shows the errors related to the projection of the chessboard corners from the top left camera to the top right camera before and after the optimization of the position and orientation parameters of the cameras. These results can be better evaluated through the calculated mean error and standard deviation values, as shown in Table 1: Next, the proposed approach was compared with other calibration methodologies: stereo calibrate function 7 7 provided by OpenCV and the kalibr calibration method [26]. The kalibr method requires hardware synchronization and receives a bag file as input, unlike the other approaches, which make use of the datasets collected as described in Section 2. Because of this, two different calibrations are provided for kalibr: the first in which the training dataset is used, and a second which uses the test dataset, i.e. the dataset which is used to evaluate all approaches. The results for the proposed approach are presented for two different scenarios, taking into account the camera (top left or top right) which was used for creating the initial values of the chessboard poses (see Eq. (4)). These two variants are used to assess the impact of the selection of the camera for providing initial estimates on the final calibration estimates. In this experiment, calibration of a pair of sensors composed by the top left camera (cam1) and the top right camera (cam2) is evaluated. The dataset used for running the calibration procedures, i.e. the training dataset, is composed of 27 collections (27 images per camera). The test dataset to be used to evaluate the estimated sensor-to-sensor transformations has 15 collections. Images from the train and test datasets are similar. In order to make this comparison fair, the three distinct calibration procedures are given the exact same information. Moreover, the procedures were implemented in such a way that the returned estimated parameters, and remaining data, are organized similarly to the proposed approach. This means that each distinct approach will output a final JSON file with the estimated position and orientation of the sensors. Taking all this into account, a specific tool was created for visualizing the results of the different calibration procedures named Results Visualization, which imports the JSON files outputted by each one of the several approaches. Fig. 9 shows a flowchart of this framework, built specifically to compare the proposed methodology with standard pairwise approaches. Fig. 10 shows the pixel errors of the three distinct calibration approaches. Note that the methodology described above is computed separately for each collection. The performance of the kalibr method is clearly below the other two. We suspect there are several factors contributing to this. The first is that this method requires hardware synchronization, not ensured in the used datasets. Another is that the kalibr method reads data from a bag file and thus we have no control over the images which are selected to run the calibration. In an attempt to address the problem, we ran a kalibr calibration using the test dataset as input (kalibr test in Table 2). It may be that the selection of images is not working well, which in turn causes a poor calibration performance. Also, due to the limited duration of the bag file of the experiment, only around 20 to 30 images have been selected, a total similar to the datasets we have used. It could be that kalibr requires a larger number of images. In any case, we believe these results are not representative of kalibr. The proposed approach and the stereo calibration display similar errors, which means that the proposed approach is on par with a state of the art calibration approach. Moreover, the largest error of each of the compared methodologies occurs for the same collection (in this case, for collection 4, the dark green). This also shows a high degree of consistency between the proposed approach and the stereo calibration. Table 2 shows the average error and the standard deviation of all tested calibration approaches. These results exhibit reprojection errors in the order of some pixels, which is the normal range of values for these methods and experimental setups. Moreover, the obtained values are very similar between the proposed approach and the stereo calibrate. As such, results show that the proposed approach is able to calibrate all sensors on-board the ATLASCAR2 using a single optimization procedure. Furthermore, the accuracy of this joint calibration framework we propose is the same as when using state of the art pairwise calibration methods. 4.2 Complete system calibration This section will provide results concerning a full system calibration. Note that, in Section 4.1, the results focus only on the evaluation of the camera sensors, despite the fact that the complete system was also calibrated. In this section, the goal is to characterize all the sensors and not just the cameras. Because of this, it is not possible to compare the full system calibration (taking into account all the sensors) with other approaches since, as described in Section 1, there is no calibration framework available, in particular a multi-sensor and multi-modal one. Fig. 11 shows the average error per sensor over the cost function evaluations, for a full system calibration test. The average error per sensor is estimated after the several error measurements computed for each particular sensor. For example, a camera cost sub-function returns as many residuals as chessboard corners (see Eq. (7)), while the LiDAR sub sub-function returns four measurements (see Eq. (10)). The average error for camera sensors is provided in pixels, while for LiDAR sensors the error is in metres. The first takeaway is that the optimization is working as intended, since the minimization of the errors of all sensors can be observed. This shows that the multi-sensor, multi-modal optimization (the joint minimization of all the sensor’s parameters) is in fact possible. Furthermore, the final errors values (after the optimization is finished) are around a few pixels for camera sensors (2.8 and 3.3 pixels for the top left camera and the top right camera, respectively), and around a few centimetres for the LiDARs (0.017 and 0.033 metres for the left laser and right laser, respectively). These values are on par with the state of the art, even when considering calibration results for pairwise approaches. Another important insight is the reason why the top left camera residual starts with a low error: Section 3.1, in particular Eq. (4), described how the initial poses of the chessboards were estimated using one camera sensor, which is arbitrarily selected. In this test, the top left camera was selected to produce the initial chessboard pose estimates. Thus, since the corner detection in the top left camera images are used to compute the initial chessboard poses, the reverse procedure of projecting the chessboard corners back to the image results in corner coordinates that are naturally very close to the detections at the beginning of the optimization. Fig. 12 shows the data from the LiDARs along with a representation of the chessboard. For a better visualization, a single collection is displayed. The four images correspond to different stages of the optimization process. It is possible to see an improvement during the calibration (i.e. from Fig. 12(a) and (b), the beginning of the optimization, to (c) and (d), the end of the optimization, since the data from both LiDARs is much closer to the chessboard plane (c) and (d) when compared to (a) and (b). This shows that the proposed approach is also capable of calibrating LiDARs within a joint optimization framework. 5 Conclusions and future work This paper proposes an extrinsic calibration methodology that is general, in the sense that the number of sensors and their modalities are not restricted. The approach is compliant with the ROS framework, having also the advantage of not altering the tf tree. To accomplish this, the problem is formalized as an optimization procedure of a set of partial transformations, which accounts for specific links in the transformation chains of the sensors. Additionally, the work contributes with a set of interactive tools for the positioning of the sensors and labelling of data, which facilitate the creation of a first guess and significantly ease the calibration procedure. Results show that the proposed approach is able to achieve similar accuracy when compared to state of the art methodologies, implemented in OpenCV. Moreover, these results are obtained by performing a complete calibration of the system, rather than one of a single pair of sensors. In other words, the proposed approach calibrates all sensors at once, with similar performance as the pairwise approaches. This confirms that the proposed approach is adequate for the calibration of complex robotic systems, as are most intelligent vehicles. Future work will focus on the extension to additional sensor modalities, e.g., 3D LiDARs, RGB-D cameras, Radio Detection And Ranging (RaDAR), etc. Given the scalability of the proposed framework, it is expected that this should be more or less straightforward. Finally, the ultimate goal is to produce a multi-sensor, multi-modal calibration package that may be released to the community. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This Research was funded by National Funds through the FCT — Foundation for Science and Technology, in the context of the project UIDB/00127/2020, as well as CYTED/TICs4CI — Aplicaciones TICS para Ciudades Inteligentes . References [1] Mueller G.R. Wuensche H. Continuous stereo camera calibration in urban scenarios 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC) 2017 1 6 10.1109/ITSC.2017.8317675 G. R. Mueller, H. Wuensche, Continuous stereo camera calibration in urban scenarios, in: 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC), 2017, 1–6. DOI: 10.1109/ITSC.2017.8317675. [2] Wu L. Zhu B. Binocular stereovision camera calibration 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA) 2015 2638 2642 10.1109/ICMA.2015.7237903 L. Wu, B. Zhu, Binocular stereovision camera calibration, in: 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA), 2015, 2638–2642. DOI: 10.1109/ICMA.2015.7237903. [3] Rou Su L. JingLiang Zhong B. QiaoLiang Li SuWen Qi HuiSheng Zhang TianFu Wang An automatic calibration system for binocular stereo imaging 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC) 2016 896 900 10.1109/IMCEC.2016.7867340 Rou Su, JingLiang Zhong, QiaoLiang Li, SuWen Qi, HuiSheng Zhang, TianFu Wang, An automatic calibration system for binocular stereo imaging, in: 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC), 2016, 896–900. DOI: 10.1109/IMCEC.2016.7867340. [4] Ling Y. Shen S. High-precision online markerless stereo extrinsic calibration 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) 2016 1771 1778 10.1109/IROS.2016.7759283 Y. Ling, S. Shen, High-precision online markerless stereo extrinsic calibration, in: 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2016, 1771–1778. DOI: 10.1109/IROS.2016.7759283. [5] Dinh V.Q. Nguyen T.P. Jeon J.W. Rectification using different types of cameras attached to a vehicle IEEE Trans. Image Process. 28 2 2019 815 826 10.1109/TIP.2018.2870930 V. Q. Dinh, T. P. Nguyen, J. W. Jeon, Rectification using different types of cameras attached to a vehicle, IEEE Trans. on Image Processing 28 (2) (2019) 815–826. DOI: 10.1109/TIP.2018.2870930. [6] Vasconcelos F. Barreto J.P. Nunes U. A minimal solution for the extrinsic Calibration of a Camera and a laser-rangefinder IEEE Trans. Pattern Anal. Mach. Intell. 34 11 2012 2097 2107 F. Vasconcelos, J. P. Barreto, U. Nunes, A minimal solution for the extrinsic calibration of a camera and a laser-rangefinder, IEEE Trans. on Pattern Analysis and Machine Intelligence 34 (11) (2012) 2097–2107. [7] Pereira M. Silva D. Santos V. Dias P. Self calibration of multiple lidars and cameras on autonomous vehicles Robot. Auton. Syst. 83 2016 326 337 M. Pereira, D. Silva, V. Santos, P. Dias, Self calibration of multiple lidars and cameras on autonomous vehicles, Robotics and Autonomous Systems 83 (2016) 326–337. [8] Almeida M. Dias P. Oliveira M. Santos V. 3d-2d laser range finder calibration using a conic based geometry shape Image Analysis and Recognition 2012 312 319 M. Almeida, P. Dias, M. Oliveira, V. Santos, 3d-2d laser range finder calibration using a conic based geometry shape, in: Image Analysis and Recognition, 2012, 312–319. [9] A. Geiger, F. Moosmann, O. Car, B. Schuster, Automatic camera and range sensor calibration using a single shot, in: Proc. IEEE Int. Conf. Robot. Autom., 2012, pp. 3936–3943. [10] Guindel C. Beltrán J. Martín D. García F. Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 1 6 C. Guindel, J. Beltrán, D. Martín, F. García, Automatic extrinsic calibration for lidar-stereo vehicle sensor setups, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) (2017) 1–6. [11] Kwon Y.C. Jang J.W. Choi O. Automatic sphere detection for extrinsic calibration of multiple rgbd cameras 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS) 2018 1451 1454 Y. C. Kwon, J. W. Jang, O. Choi, Automatic sphere detection for extrinsic calibration of multiple rgbd cameras, in: 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS), 2018, 1451–1454. [12] Khan A. Aragon-Camarasa G. Sun L. Siebert J.P. On the calibration of active binocular and rgbd vision systems for dual-arm robots 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO) 2016 1960 1965 10.1109/ROBIO.2016.7866616 A. Khan, G. Aragon-Camarasa, L. Sun, J. P. Siebert, On the calibration of active binocular and rgbd vision systems for dual-arm robots, in: 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), 2016, 1960–1965. DOI: 10.1109/ROBIO.2016.7866616. [13] Basso F. Menegatti E. Pretto A. Robust intrinsic and extrinsic calibration of rgb-d cameras IEEE Trans. Robot. 34 5 2018 1315 1332 10.1109/TRO.2018.2853742 F. Basso, E. Menegatti, A. Pretto, Robust intrinsic and extrinsic calibration of rgb-d cameras, IEEE Trans. on Robotics 34 (5) (2018) 1315–1332. DOI: 10.1109/TRO.2018.2853742. [14] Qiao Y. Tang B. Wang Y. Peng L. A new approach to self-calibration of hand-eye vision systems 2013 Int. Conf. on Computational Problem-Solving (ICCP) 2013 253 256 10.1109/ICCPS.2013.6893596 Y. Qiao, B. Tang, Y. Wang, L. Peng, A new approach to self-calibration of hand-eye vision systems, in: 2013 Int. Conf. on Computational Problem-Solving (ICCP), 2013, 253–256. DOI: 10.1109/ICCPS.2013.6893596. [15] Zhang C. Zhang Z. Calibration between depth and color sensors for commodity depth cameras 2011 IEEE Int. Conf. on Multimedia and Expo 2011 1 6 10.1109/ICME.2011.6012191 C. Zhang, Z. Zhang, Calibration between depth and color sensors for commodity depth cameras, in: 2011 IEEE Int. Conf. on Multimedia and Expo, 2011, 1–6. DOI: 10.1109/ICME.2011.6012191. [16] Chen G. Cui G. Jin Z. Wu F. Chen X. Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction IEEE Sens. J. 19 7 2019 2685 2694 10.1109/JSEN.2018.2889805 G. Chen, G. Cui, Z. Jin, F. Wu, X. Chen, Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction, IEEE Sensors Journal 19 (7) (2019) 2685–2694. DOI: 10.1109/JSEN.2018.2889805. [17] Qilong Zhang G. Pless R. Extrinsic calibration of a camera and laser range finder (improves camera calibration) 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), Vol. 3 2004 2301 2306 10.1109/IROS.2004.1389752 Qilong Zhang, R. Pless, Extrinsic calibration of a camera and laser range finder (improves camera calibration), in: 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), 3, 2004, 2301–2306 3. DOI: 10.1109/IROS.2004.1389752. [18] Häselich M. Bing R. Paulus D. Calibration of multiple cameras to a 3d laser range finder 2012 IEEE Int. Conf. on Emerging Signal Processing Applications 2012 25 28 M. Häselich, R. Bing, D. Paulus, Calibration of multiple cameras to a 3d laser range finder, in: 2012 IEEE Int. Conf. on Emerging Signal Processing Applications, 2012, 25–28. [19] Chen Z. Yang X. Zhang C. Jiang S. Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) 2016 448 453 Z. Chen, X. Yang, C. Zhang, S. Jiang, Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature, in: 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2016, 448–453. [20] Velas M. Spanel M. Materna Z. Herout A. Calibration of rgb camera with velodyne lidar 2014 M. Velas, M. Spanel, Z. Materna, A. Herout, Calibration of rgb camera with velodyne lidar, 2014. [21] Lee G. Lee J. Park S. Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 64 69 10.1109/MFI.2017.8170408 G. Lee, J. Lee, S. Park, Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition, in: 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI), 2017, 64–69. DOI: 10.1109/MFI.2017.8170408. [22] Levinson J. Thrun S. Automatic online calibration of cameras and lasers Robotics: Science and Systems 2013 J. Levinson, S. Thrun, Automatic online calibration of cameras and lasers, in: Robotics: Science and Systems, 2013. [23] Dezhi Gao J. Duan J. Xining Yang S. Zheng B. A method of spatial calibration for camera and radar 2010 8th World Congress on Intelligent Control and Automation 2010 6211 6215 10.1109/WCICA.2010.5554411 Dezhi Gao, J. Duan, Xining Yang, B. Zheng, A method of spatial calibration for camera and radar, in: 2010 8th World Congress on Intelligent Control and Automation, 2010, 6211–6215. DOI: 10.1109/WCICA.2010.5554411. [24] Santos V. Almeida J. Ávila E. Gameiro D. Oliveira M. Pascoal R. Sabino R. Stein P. Atlascar - technologies for a computer assisted driving system, on board a common automobile 13th Int. IEEE Conf. on Intelligent Transpor Tation Systems 2010 1421 1427 10.1109/ITSC.2010.5625031 V. Santos, J. Almeida, E. vila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, Atlascar - technologies for a computer assisted driving system, on board a common automobile, in: 13th Int. IEEE Conf. on Intelligent Transpor tation Systems, 2010, 1421–1427. DOI: 10.1109/ITSC.2010.5625031. [25] Liao Y. Li G. Ju Z. Liu H. Jiang D. Joint kinect and multiple external cameras simultaneous calibration 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM) 2017 305 310 10.1109/ICARM.2017.8273179 Y. Liao, G. Li, Z. Ju, H. Liu, D. Jiang, Joint kinect and multiple external cameras simultaneous calibration, in: 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM), 2017, 305–310. DOI: 10.1109/ICARM.2017.8273179. [26] Rehder J. Siegwart R. Furgale P. A general approach to spatiotemporal calibration in multisensor systems IEEE Trans. Robot. 32 2 2016 383 398 10.1109/TRO.2016.2529645 J. Rehder, R. Siegwart, P. Furgale, A general approach to spatiotemporal calibration in multisensor systems, IEEE Trans. on Robotics 32 (2) (2016) 383–398. DOI: 10.1109/TRO.2016.2529645. [27] Pradeep V. Konolige K. Berger E. Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach Experimental Robotics: The 12th Int. Symposium on Experimental Robotics 2014 Springer Berlin Heidelberg Berlin, Heidelberg 211 225 10.1007/978-3-642-28572-1˙15 V. Pradeep, K. Konolige, E. Berger, Calibrating a Multi-arm Multi-sensor Robot: A Bundle Adjustment Approach, Springer Berlin Heidelberg, Berlin, Heidelberg, 2014, 211–225. DOI: 10.1007/978-3-642-28572-1˙15. [28] Oliveira M. Castro A. Madeira T. Dias P. Santos V. A general approach to the extrinsic calibration of intelligent vehicles using ros Robot 2019: Fourth Iberian Robotics Conference 2020 Springer International Publishing Cham 203 215 M. Oliveira, A. Castro, T. Madeira, P. Dias, V. Santos, A general approach to the extrinsic calibration of intelligent vehicles using ros, in: Robot 2019: Fourth Iberian Robotics Conference, Springer International Publishing, Cham, 2020, 203–215. [29] Bradski G. The OpenCV Library, Dr. Dobb’s J. Softw. Tools 2000 G. Bradski, The OpenCV Library, Dr. Dobb’s Journal of Software Tools. [30] Quigley M. Conley K. Gerkey B.P. Faust J. Foote T. Leibs J. Wheeler R. Ng A.Y. Ros: an open-source robot operating system ICRA Workshop on Open Source Software 2009 M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, A. Y. Ng, Ros: an open-source robot operating system, in: ICRA Workshop on Open Source Software, 2009. [31] Foote T. Tf: The transform library 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA) 2013 1 6 10.1109/TePRA.2013.6556373 T. Foote, tf: The transform library, in: 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA), 2013, 1–6. DOI: 10.1109/TePRA.2013.6556373. [32] Hornegger J. Tomasi C. Representation issues in the ML estimation of camera motion Proceedings of the Seventh IEEE International Conference on Computer Vision, Vol. 1 1999 640 647 10.1109/ICCV.1999.791285 J. Hornegger, C. Tomasi, Representation issues in the ml estimation of camera motion, 1, 1999, 640–647 1. DOI: 10.1109/ICCV.1999.791285. [33] Agarwal S. Snavely N. M. Seitz S. Szeliski R. Bundle Adjustment in the Large 2010 29 42 10.1007/978-3-642-15552-9˙3 S. Agarwal, N. Snavely, S. M. Seitz, R. Szeliski, Bundle adjustment in the large, 2010, 29–42. DOI: 10.1007/978-3-642-15552-9˙3. Afonso Castro is a junior web developer. He has an M.Sc. Degree in Mechanical Engineering from the Department of Mechanical Engineering of the University of Aveiro (2019). His master specialization was in robotics and, more precisely, sensor calibration. During his M.Sc. Dissertation development, Afonso Castro has published a research article entitled “A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS” for ROBOT2019: Fourth Iberian Robotics Conference, where he has participated and presented the mentioned work. "
    },
    {
        "doc_title": "2D lidar to kinematic chain calibration using planar features of indoor scenes",
        "doc_scopus_id": "85085949613",
        "doc_doi": "10.1108/IR-09-2019-0201",
        "doc_eid": "2-s2.0-85085949613",
        "doc_date": "2020-08-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Calibration techniques",
            "Design/methodology/approach",
            "Geometric accuracy",
            "High angular resolutions",
            "Quantitative result",
            "Range measurements",
            "Reconstruction procedure"
        ],
        "doc_abstract": "© 2020, Emerald Publishing Limited.Purpose: 2D laser rangefinders (LRFs) are commonly used sensors in the field of robotics, as they provide accurate range measurements with high angular resolution. These sensors can be coupled with mechanical units which, by granting an additional degree of freedom to the movement of the LRF, enable the 3D perception of a scene. To be successful, this reconstruction procedure requires to evaluate with high accuracy the extrinsic transformation between the LRF and the motorized system. Design/methodology/approach: In this work, a calibration procedure is proposed to evaluate this transformation. The method does not require a predefined marker (commonly used despite its numerous disadvantages), as it uses planar features in the point acquired clouds. Findings: Qualitative inspections show that the proposed method reduces artifacts significantly, which typically appear in point clouds because of inaccurate calibrations. Furthermore, quantitative results and comparisons with a high-resolution 3D scanner demonstrate that the calibrated point cloud represents the geometries present in the scene with much higher accuracy than with the un-calibrated point cloud. Practical implications: The last key point of this work is the comparison of two laser scanners: the lemonbot (authors’) and a commercial FARO scanner. Despite being almost ten times cheaper, the laser scanner was able to achieve similar results in terms of geometric accuracy. Originality/value: This work describes a novel calibration technique that is easy to implement and is able to achieve accurate results. One of its key features is the use of planes to calibrate the extrinsic transformation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning-Based Analysis of a New Wearable 3D Force System Data to Classify the Underlying Surface of a Walking Robot",
        "doc_scopus_id": "85079146718",
        "doc_doi": "10.1142/S0219843620500115",
        "doc_eid": "2-s2.0-85079146718",
        "doc_date": "2020-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Biped humanoid robot",
            "Classification accuracy",
            "Coefficient of frictions",
            "Computational intelligence techniques",
            "Extreme learning machine",
            "Humanoid robot locomotion",
            "Instrumented systems",
            "Real world environments"
        ],
        "doc_abstract": "© 2020 World Scientific Publishing Company.Biped humanoid robots that operate in real-world environments need to be able to physically recognize different floors to best adapt their gait. In this work, we describe the preparation of a dataset of contact forces obtained with eight force tactile sensors for determining the underlying surface of a walking robot. The data is acquired for four floors with different coefficient of friction, and different robot gaits and speeds. To classify the different floors, the data is used as input for two common computational intelligence techniques (CITs): Artificial neural network (ANN) and extreme learning machine (ELM). After optimizing the parameters for both CITs, a good mapping between inputs and targets is achieved with classification accuracies of about 99%.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Performance analysis on deep learning semantic segmentation with multivariate training procedures",
        "doc_scopus_id": "85085956029",
        "doc_doi": "10.1109/ICARSC49921.2020.9096145",
        "doc_eid": "2-s2.0-85085956029",
        "doc_date": "2020-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Learning approach",
            "Learning semantics",
            "Performance analysis",
            "Performance assessment",
            "Semantic segmentation",
            "Training parameters",
            "Training procedures"
        ],
        "doc_abstract": "© 2020 IEEE.Deep Learning approaches are becoming ubiquitous in many fields of computation, especially for tasks of object detection and classification in images. However, the diversity of architectures, the number and range of training parameters, and the dimension and representativeness of datasets make it very complex to define a unified method or set of techniques to address diverse problems, and it is quite common for the literature to skip valuable implementation details. One of these fields is the Semantic Segmentation with many applications like autonomous driving. In that line, this paper carried out a study using an ENet architecture for semantic segmentation on road images for intelligent vehicles and multiple experiments of deep learning training with a wealth of combinations of training procedures, including multiple loss functions and datasets for out-of-domain performance assessment. Besides some expected outcomes, the results allow identifying a set of combinations with performances that stand out and are advisable for researchers to tackle more efficiently problems of a similar nature.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic Registration of IR and RGB Cameras using a Target detected with Deep Learning",
        "doc_scopus_id": "85085949048",
        "doc_doi": "10.1109/ICARSC49921.2020.9096168",
        "doc_eid": "2-s2.0-85085949048",
        "doc_date": "2020-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Automatic registration",
            "Degree of confidence",
            "Extrinsic calibration",
            "Geometric transformations",
            "Multi-modal approach",
            "Object detection and recognition",
            "Simultaneous perceptions",
            "Spatial registrations"
        ],
        "doc_abstract": "© 2020 IEEE.Multimodal approaches in perception bring redundancy and enhance the robustness of solutions for object detection and recognition. This is applicable in diverse situations ranging from the industrial process up to intelligent vehicles. However, to fuse or merge information from multiple sensors it is necessary to know their relative placement, also known as spatial registration, or extrinsic calibration. This work proposes a solution to register infrared and visual cameras seamlessly in an automatic way based on the simultaneous perception of a specific target, namely a large-sized ball. Images are used in grayscale for all cameras, and a Fast R-CNN deep learning-based architecture is used to detect fast and accurately the position of the target in a set of image frames. Sets of 3D ball centers generate point clouds whose matching provides the geometric transformation that registers each pair of cameras. As the detection results have a high degree of confidence (around 98%) the automatic registration of the cameras is also achieved with very good precision as demonstrated in the results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of data matrix encoded landmarks in unstructured environments using deep learning",
        "doc_scopus_id": "85085910650",
        "doc_doi": "10.1109/ICARSC49921.2020.9096211",
        "doc_eid": "2-s2.0-85085910650",
        "doc_date": "2020-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Artificial landmark",
            "Automated guided vehicles",
            "Dynamic configuration",
            "Industrial environments",
            "Partial occlusions",
            "Production facility",
            "Traditional techniques",
            "Unstructured environments"
        ],
        "doc_abstract": "© 2020 IEEE.Visual based navigation in industrial environments has been a challenge because of the complex and unstructured nature of the surroundings that Automated Guided Vehicles (AGV) and autonomous robots have to traverse in a continuous routine in many production facilities. Since natural landmarks are still a huge challenge due to the highly dynamic configuration of the environment and the varying nature of these landmarks, simple and low cost artificial landmarks appear as a potential solution. The one exploited in this work uses simple sheets of paper with Data Matrix encoded markers spread in the environment trying to create a constellation of landmarks in such a way that several of them are always visible to a set of cameras on board the robot or AGV. All codes are unique, which makes robot continuous localization a much simpler challenge whenever at least two or three landmarks are visible. When using traditional vision techniques in large images of the scene, one of the most demanding parts is to detect the landmarks to further process them. For that purpose, this paper proposes a technique based on deep learning that efficiently detects these special landmarks in images. A dedicated dataset was created and a Faster R-CNN architecture was adapted and trained for that purpose. The results show that almost all markers was detected in the images with a processing speed larger more than one order of magnitude than traditional techniques, including in demanding situations of poorer illumination or partial occlusions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Scalable ROS-Based Architecture to Merge Multi-source Lane Detection Algorithms",
        "doc_scopus_id": "85082140125",
        "doc_doi": "10.1007/978-3-030-35990-4_20",
        "doc_eid": "2-s2.0-85082140125",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous navigation",
            "Data combination",
            "Driving assistance",
            "Effective algorithms",
            "Merging techniques",
            "Multiple cameras",
            "Road detection",
            "Visual perception"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Road detection is a crucial concern in Autonomous Navigation and Driving Assistance. Despite the multiple existing algorithms to detect the road, the literature does not offer a single effective algorithm for all situations. A global more robust set-up would count on multiple distinct algorithms running in parallel, or even from multiple cameras. Then, all these algorithms’ outputs should be merged or combined to produce a more robust and informed detection of the road and lane, so that it works in more situations than each algorithm by itself. This paper proposes a ROS-based architecture to manage and combine multiple sources of lane detection algorithms ranging from the classic lane detectors up to deep-learning-based detectors. The architecture is fully scalable and has proved to be a valuable tool to test and parametrize individual algorithms. The combination of the algorithms’ results used in this paper uses a confidence based merging of individual detections, but other alternative fusion or merging techniques can be used.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of Road Limits Using Gradients of the Accumulated Point Cloud Density",
        "doc_scopus_id": "85082124110",
        "doc_doi": "10.1007/978-3-030-35990-4_22",
        "doc_eid": "2-s2.0-85082124110",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Driving assistance systems",
            "Horizontal surfaces",
            "Occupancy grids",
            "Point cloud",
            "Point density",
            "Processing speed",
            "Traditional computers",
            "Vertical surface"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Detection of road curbs and berms is a critical concern for autonomous vehicles and driving assistance systems. The approach proposed in this paper to detect them uses a 4-layer LIDAR placed near to the ground to capture measurements from the road ahead of the car. This arrangement provides a particular point of view that allows the accumulation of points on vertical surfaces on the road as the car moves. Consequently, the point density increases in vertical surfaces and stays limited in horizontal surfaces. A first analysis of the point density allows to distinguish curbs from flat roads, and a second solution based on the gradient of point density not only detects curbs as well but also detects berms due to the transitions of the gradient density. To ease and improve the processing speed, point clouds are flattened to 2D and traditional computer vision gradient and edge detection techniques are used to extract the road limits for a wide range of car velocities. The results were obtained on the ATLASCAR real system, and they show good performance when compared to a manually obtained ground truth.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS",
        "doc_scopus_id": "85082115002",
        "doc_doi": "10.1007/978-3-030-35990-4_17",
        "doc_eid": "2-s2.0-85082115002",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bundle adjustments",
            "Calibration problems",
            "Calibration process",
            "Extrinsic calibration",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Intelligent vehicles are complex systems which often accommodate several sensors of different modalities. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration process. The calibration problem is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ATLASCAR: A sample of the quests and concerns for autonomous cars",
        "doc_scopus_id": "85065463238",
        "doc_doi": "10.1007/978-3-030-11292-9_18",
        "doc_eid": "2-s2.0-85065463238",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Automatic vehicles",
            "Autonomous driving",
            "Driving assistance",
            "Portugal",
            "Research level",
            "Scientific projects"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.The ATLASCAR project started in 2010 as an engineering and scientific project at the University of Aveiro, Portugal, with the goal to develop a system to study and improve autonomous driving and driving assistance capabilities. The focus was made both at the engineering level to develop a real scale instrumented and automatic vehicle prototype, and at the research level for advanced perception and system command using a rich set of sensors and the associate computational and software architecture. Besides the expected challenges, new fronts appeared and a set of concerns has been studied which lightened up challenges that will certainly continue to drive other authors and technological players concerned with autonomous cars and the automotive industry.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Editorial: Special issue on autonomous driving and driver assistance systems",
        "doc_scopus_id": "85071537701",
        "doc_doi": "10.1016/j.robot.2019.103266",
        "doc_eid": "2-s2.0-85071537701",
        "doc_date": "2019-11-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2019-08-22 2019-08-22 2019-09-03 2019-09-03 2019-10-04T06:55:49 S0921-8890(19)30668-2 S0921889019306682 10.1016/j.robot.2019.103266 S300 S300.1 FULL-TEXT 2019-10-04T06:04:27.220993Z 0 0 20191101 20191130 2019 2019-08-22T15:12:59.971894Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav body acknowledge affil articletitle auth authfirstini authfull authlast grantnumber grantsponsor grantsponsorid ref 0921-8890 09218890 true 121 121 C Volume 121 7 103266 103266 103266 201911 November 2019 2019-11-01 2019-11-30 2019 simple-article edi © 2019 Published by Elsevier B.V. EDITORIALSPECIALISSUEAUTONOMOUSDRIVINGDRIVERASSISTANCESYSTEMS SANTOS V Acknowledgments References HIRABAYASHI 2019 62 72 M GUINDEL 2019 109 122 C PERSIC 2019 217 230 J PRAKASH 2019 172 186 C LEE 2019 178 189 E NEMEC 2019 168 177 D LI 2019 201 210 S GASPAR 2018 59 67 A OKAMOTO 2019 155 171 K AHN 2018 1 12 B AMANATIADIS 2019 282 290 A ANTUNES 2019 56 62 J WEN 2019 28 39 S SAJADIALAMDARI 2019 291 303 S ANTUNES 2019 83 89 A MARINPLAZA 2019 P YOU 2019 1 18 C TALAMINO 2019 93 105 J TONUTTI 2019 162 173 M SANTOSX2019X103266 SANTOSX2019X103266XV 2021-09-03T00:00:00.000Z 2021-09-03T00:00:00.000Z © 2019 Published by Elsevier B.V. 2019-09-07T05:20:54.793Z S0921889019306682 Foundation for Science and Technology UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia FCT FCT Fuel Cell Technologies Program Spanish Government, Spain TIN2017-89723-P Generalitat de Catalunya FIEC-09-2015 Generalitat de Catalunya Cities REF-518RT0559 Quad Cities Community Foundation This work has been supported by: the Spanish Government, Spain under Project TIN2017-89723-P , the “CERCA Programme / Generalitat de Catalunya”, Spain , the ESPOL project PRAIM (FIEC-09-2015), Ecuador and FCT — Foundation for Science and Technology, Portugal , in the context of project UID/CEC/00127/2019 . The authors gratefully acknowledge the support of the CYTED, Spain , Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” ( REF-518RT0559 ). item S0921-8890(19)30668-2 S0921889019306682 10.1016/j.robot.2019.103266 271599 2019-10-04T06:04:27.220993Z 2019-11-01 2019-11-30 true 235162 MAIN 3 67849 849 656 IMAGE-WEB-PDF 1 ROBOT 103266 103266 S0921-8890(19)30668-2 10.1016/j.robot.2019.103266 Editorial: Special issue on autonomous driving and driver assistance systems Vitor Santos IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Angel D. Sappa ⁎ Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación Campus Gustavo Galindo Km 30.5, Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain Computer Vision Center, Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain ⁎ Corresponding editor. Miguel Oliveira IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Arturo de la Escalera Universidad Carlos III de Madrid, Escuela Politécnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid), Spain Universidad Carlos III de Madrid, Escuela Politécnica Superior C/ Butarque, 15 - 28911 Leganes (Madrid) Spain Universidad Carlos III de Madrid, Escuela Politcnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid) Spain The research on Autonomous Driving and Driver Assistance Systems (ADDAS) has been increasing continuously for several decades, but the last years have exhibited unprecedented attention, both by the many dedicate books published in this field, but especially for the numerous papers in several conferences, workshops and special sessions worldwide. The contexts of these events cover a large spectrum of communities, ranging from robotics up to intelligent transportation, intelligent vehicles or vehicular technology, among others. The scope and potential of the technological applications in ADDAS is so huge that it can serve as test bed for an almost unbounded set of sciences and technologies that go from the classic navigation issues and system control, up to the more recent and daunting challenges in deep learning contexts. This special issue is not an exception to this scenario, and its 19 peer reviewed papers cover a wide range of problems that can be categorized into the following topics: 1. Scene Perception [1–5] 2. Localization, Visual Odometry and SLAM [6–8] 3. Cabin Concerns with Drivers and Passengers [9–11] 4. Vehicle Systems Control [12–15] 5. High Level Navigation [16–18] 6. Deep Learning [2,10,17,19] The previous categorization is however not very strict, since it is natural for a single paper to cover both a known specific problem along with some more generic tool applied to solve that particular problem. So, some times there is a fusion of focus and interest on the specific problem and the tool used to solve it, where the now ubiquitous Deep Learning paradigm is a clear example, or also the usage of Robot Operating System (ROS) as the supporting architecture for many implementations. The main contents of the papers published in the special issue are described next with more details in the following lines. Scene Perception Perception of vehicle’s surrounding is a topic of interest and a key component for both autonomous driving and driver assistance systems. Several papers have been focused on this topic covering problems that go from traffic lights recognition, scene awareness till obstacle detection and distance estimation. In [1] the authors propose an innovative reliable method to recognize the state of traffic lights in images using accurate 3D maps and a self-localization technique in it. Quantitative evaluations indicate that the method achieved over 97% average precision for each state and approximately 90% recall as far as 90 meters under preferable condition. In a more general way, [4] proposes a robust method for generic obstacle detection and collision warning. The proposed approach is able to detect all obstacles without prior knowledge and detect partially occluded obstacles. The approach is robust to variations in illumination and to a wide variety of vehicles and obstacles. Improvements on true positive detection in comparison with state of the art are shown. On the contrary to previous approaches, the usage of a stereovision based scene perception is proposed in [5] and [2]. In [5] the authors propose a novel framework for vehicle detection and localization with partial appearance using stereo vision and geometry. The proposed approach is based on the widely used v-disparity map representation to detect candidates vehicles. Then, a deep learning-based verification completes vehicle detection. A more general approach is presented in [2], where an efficient method to perform recognition and 3D localization of dynamic objects on images from a stereo camera is described. The proposed approach relies on a deep learning framework able to simultaneously identify a broad range of entities, such as vehicles, pedestrians or cyclists, with a frame rate compatible with the strict requirements of onboard automotive applications. The results presented in the paper show the capabilities of the perception system for a wide variety of traffic situations. Finally, and also related with scene perception, the authors in [3] tackle the challenging problem of extrinsic calibration of a radar-LiDAR-camera sensor system. The authors propose a novel calibration method that involves a special target design and a two-step optimization procedure. The proposed calibration method has been tested on a variety of sensor configuration showing that it is able to reliably estimate all the six parameters of the extrinsic calibration. Localization, Visual Odometry and SLAM The ability of autonomous vehicles to obtain their position and orientation within the environment where they are driving in is crucial to several tasks as planning and piloting. Several sensors are available for this, the GPS (Global Positioning System) and IMU (Inertial Measurement Unit) being two of the most used. Both have many advantages but have also several shortcomings for autonomous vehicles applications due to accuracy or their limitations in urban scenarios. That is why in [6] the authors propose sensor fusion for the localization (both translation and attitude) of a mobile wheeled robot. Several sensors are used: odometers, gyroscope, accelerometer, magnetometer and a camera for visual landmark localization. The algorithm is able to deal with the asynchronous nature of the sensors and the failures of many of them and runs in real-time. SLAM (Simultaneous Localization And Mapping) algorithm has been applied to robotics for some years and in [7] the authors proposed a system able to maintain the fast performance of a direct method and the high precision and loop closure capability of a feature-based method. A key-frame is used for global or local optimization and loop closure, whereas a non-key-frame is used for fast tracking and localization. Besides that, the system fuses the computer vision data with inertial measurements. Thanks to this an equilibrium between speed and accuracy is achieved. The public availability of data-sets is crucial for research teams do not have the needed sensors for start developing new ideas and for testing and comparing different approaches. In the last article of this subsection [8], the authors propose a benchmark of visual odometry and SLAM techniques. The Urban@CRAS data-set shows several scenarios presenting different conditions and urban situations: vehicle-to-vehicle and vehicle-to-human interactions, cross-sides, turn-around, roundabouts and different traffic conditions. The sensor data comes from a 3D LIDAR, color cameras, a high-precision IMU and a GPS navigation system. Besides the data, the authors propose a bench marking process for visual odometry and SLAM where qualitative and quantitative performance indicators are obtained so different approaches can be compared. Cabin Concerns with Drivers and Passengers Attention to car occupants, be it the driver or a passenger, is of course a central topic in Driving Assistance. Driver monitoring, namely the posture, can give indication of the driver focus and attention level. Paper [10] proposes a method to estimate head pose after a monocular camera performed by a deep neural network using a small gray scale image. Additionally, the authors released a new dataset of head poses for further studies, despite the fact that the solutions presented outperform current state-of-the-art techniques. Predicting human-driver reactions is a concern addressed by paper [9]. The authors perform a survey on several algorithms to predict the lateral control actions of human drivers. A comparison of the algorithms is made in terms of their suitability to develop haptic-shared ADAS, which share the control force with the human driver. The driver steering torque is considered a central point to establish a proper model, but as low-cost driver simulators only monitor steering angle and not steering torque, the authors propose a methodology to estimate the steering-wheel torque. Using the estimated steering torque, they train several machine learning driver control models and compare the performance using both simulated and real human-driving data sets. Paper [11] focuses on detecting and counting the passengers of nearby vehicles as seen from the ego-vehicle using monocular vision. The on-road Vehicle PassengEr Detection (ViPED) system is proposed and is based on the human perception model in terms of spatio-temporal reasoning, namely the slight movements of passenger shape silhouettes inside the cabin of the preceding car seen through the windshield. A Convolutional Neural Network is used to infer the number and position of passengers. Vehicle Systems Control Some of the proposed works have focused on specific systems of the vehicle control, be it the low level control or the fusion of data from multiple sensors: in [12], a dynamic test model used for the design and tuning of low level PID and LQR controllers are presented; in [15], the sidesliping of vehicles is estimated using a self-calibrating architecture which fuses several sensors such as an inertial measurement unit and a global positioning system; in [14], a nonlinear model predictive controller is proposed to produce an online estimate for a cost-effective cruising velocity; in [13], a study on the effect of medium access control protocol and the unreliable measurements on acceleration information for the cooperative control of vehicle platoons is presented. High Level Navigation The planning and execution of the movement of intelligent vehicles continues to be a relevant topic of research, in particular in complex, dynamic environments. As such, this special issue contains several works on the topics of navigation and trajectory planning: in [17], an approach is proposed that models the interaction between the autonomous vehicle and the environment, with the goal of determining the optimal driving strategy for the autonomous vehicle; in [18], an anticipatory kinodynamic motion planner that considers dynamic complex environments containing both static and dynamic obstacles is proposed. Finally, in [16], a novel software architecture for intelligent vehicles is proposed, focusing on the flexibility and scalability as a means to effectively evaluate novel algorithms. The implementation of the architecture is shown for two real platforms, the iCab (Intelligent Campus Automobile) and IvvI 1.0 vehicle (Intelligent Vehicle based on Visual Information). Deep Learning Deep learning based approaches are becoming the dominant paradigm in almost every basic and applied research topics. In the current special issue several works were based on the usage of such a framework. In this section just the most representatives are summarized. In [2] the authors propose an efficient approach to perform recognition and 3D localization of dynamic objects on images from a stereo camera, with the goal of gaining insight into traffic scenes in urban and road environments. The usage of a deep learning framework allows to identify a broad range of entities, at a frame rate compatible with the strict requirements of onboard automotive applications. Stereo information is later introduced to enrich the knowledge about the objects with geometrical information. A deep learning based framework has been also used in [17], but in this case it is intended to target the planning problem of autonomous vehicles. The system learns the driving style of an expert driver using reinforcement learning strategies. Simulated results demonstrate the system is able to reach the desired driving behaviors for an autonomous vehicle. Focusing also on the driving behavior, in [19] a deep learning based technique is proposed to accurately predict driving manoeuvres in a few seconds in advance. The authors propose a domain adaptation based technique, which is able to adapt a learned model to new drivers and different vehicles. The proposed approach has been evaluated in several datasets yielding an average increase in performance of 30% and 114% respectively compared to no adaptation based techniques. Finally, focusing on driver monitoring, [10] proposes a novel method to estimate a head pose from a monocular camera. The proposed algorithm is based on multi-task learning deep neural network that uses a small grayscale image. The network jointly detects multi-view faces and estimates head pose even under poor environment conditions such as illumination change, vibration, large pose change, and occlusion. The authors also release a new dataset for head pose estimation. The proposed framework outperforms state-of-the-art approaches quantitatively and qualitatively with an average head pose mean error of less than 4°in real-time. Conclusions Autonomous Driving and Driving Assistance Systems are now highly versatile and broad contexts which support the test and usage of both classic and modern techniques in domains ranging from perception, planning, control until classification and deep learning. Being a concern of daily routines for humans, and now also machines, it is becoming irrefutable that many quests can be tried and experimented in this attractive field of technology and science. This special issue clearly corroborates that with its wide range of topics and techniques as described in the earlier sections. But, what is now striking fiercely is the almost unavoidable imposition of the soft computation tool of the moment: deep learning. For example, we still see classic oriented trends such as Model Predictive Control (MPC) to predict and control machine motion, but often there are also works which tackle similar problems with deep learning approaches. Many authors are surrendering to this almost sinful and irresistible framework; more and more are migrating, and right now we can only guess what future editions of this special issue will bring to the community. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work has been supported by: the Spanish Government, Spain under Project TIN2017-89723-P, the “CERCA Programme / Generalitat de Catalunya”, Spain, the ESPOL project PRAIM (FIEC-09-2015), Ecuador and FCT — Foundation for Science and Technology, Portugal, in the context of project UID/CEC/00127/2019. The authors gratefully acknowledge the support of the CYTED, Spain , Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” (REF-518RT0559). References [1] Hirabayashi M. Sujiwo A. Monrroy A. Kato S. Edahiro M. Traffic light recognition using high-definition map features Robot. Auton. Syst. 111 2019 62 72 M. Hirabayashi, A. Sujiwo, A. Monrroy, S. Kato, M. Edahiro, Traffic light recognition using high-definition map features, Robotics and Autonomous Systems 111 (2019) 62 – 72 [2] Guindel C. Martín D. Armingol J.M. Traffic scene awareness for intelligent vehicles using convnets and stereo vision Robot. Auton. Syst. 112 2019 109 122 C. Guindel, D. Martn, J. M. Armingol, Traffic scene awareness for intelligent vehicles using convnets and stereo vision, Robotics and Autonomous Systems 112 (2019) 109 – 122 [3] Peršić J. Marković I. Petrović I. Extrinsic 6dof calibration of a radar–lidar–camera system enhanced by radar cross section estimates evaluation Robot. Auton. Syst. 114 2019 217 230 J. Peri, I. Markovi, I. Petrovi, Extrinsic 6dof calibration of a radarlidarcamera system enhanced by radar cross section estimates evaluation, Robotics and Autonomous Systems 114 (2019) 217 – 230 [4] Prakash C.D. Akhbari F. Karam L.J. Robust obstacle detection for advanced driver assistance systems using distortions of inverse perspective mapping of a monocular camera Robot. Auton. Syst. 114 2019 172 186 C. D. Prakash, F. Akhbari, L. J. Karam, Robust obstacle detection for advanced driver assistance systems using distortions of inverse perspective mapping of a monocular camera, Robotics and Autonomous Systems 114 (2019) 172 – 186 [5] Lee E.S. Choi W. Kum D. Bird’s eye view localization of surrounding vehicles: Longitudinal and lateral distance estimation with partial appearance Robot. Auton. Syst. 112 2019 178 189 E. S. Lee, W. Choi, D. Kum, Birds eye view localization of surrounding vehicles: Longitudinal and lateral distance estimation with partial appearance, Robotics and Autonomous Systems 112 (2019) 178 – 189 [6] Nemec D. imák V. Janota A. Hruboš M. Bubeníková E. Precise localization of the mobile wheeled robot using sensor fusion of odometry, visual artificial landmarks and inertial sensors Robot. Auton. Syst. 112 2019 168 177 D. Nemec, V. imk, A. Janota, M. Hrubo, E. Bubenkov, Precise localization of the mobile wheeled robot using sensor fusion of odometry, visual artificial landmarks and inertial sensors, Robotics and Autonomous Systems 112 (2019) 168 – 177 [7] Li S.-P. Zhang T. Gao X. Wang D. Xian Y. Semi-direct monocular visual and visual-inertial SLAM with loop closure detection Robot. Auton. Syst. 112 2019 201 210 S. peng Li, T. Zhang, X. Gao, D. Wang, Y. Xian, Semi-direct monocular visual and visual-inertial slam with loop closure detection, Robotics and Autonomous Systems 112 (2019) 201 – 210 [8] Gaspar A.R. Nunes A. Pinto A.M. Matos A. Urban@CRAS dataset: Benchmarking of visual odometry and SLAM techniques Robot. Auton. Syst. 109 2018 59 67 A. R. Gaspar, A. Nunes, A. M. Pinto, A. Matos, Urban@cras dataset: Benchmarking of visual odometry and slam techniques, Robotics and Autonomous Systems 109 (2018) 59 – 67 [9] Okamoto K. Tsiotras P. Data-driven human driver lateral control models for developing haptic-shared control advanced driver assist systems Robot. Auton. Syst. 114 2019 155 171 K. Okamoto, P. Tsiotras, Data-driven human driver lateral control models for developing haptic-shared control advanced driver assist systems, Robotics and Autonomous Systems 114 (2019) 155 – 171 [10] Ahn B. Choi D.-G. Park J. Kweon I.S. Real-time head pose estimation using multi-task deep neural network Robot. Auton. Syst. 103 2018 1 12 B. Ahn, D.-G. Choi, J. Park, I. S. Kweon, Real-time head pose estimation using multi-task deep neural network, Robotics and Autonomous Systems 103 (2018) 1 – 12 [11] Amanatiadis A. Karakasis E. Bampis L. Ploumpis S. Gasteratos A. ViPED: On-road vehicle passenger detection for autonomous vehicles Robot. Auton. Syst. 112 2019 282 290 A. Amanatiadis, E. Karakasis, L. Bampis, S. Ploumpis, A. Gasteratos, Viped: On-road vehicle passenger detection for autonomous vehicles, Robotics and Autonomous Systems 112 (2019) 282 – 290 [12] Antunes J. Antunes A. Outeiro P. Cardeira C. Oliveira P. Testing of a torque vectoring controller for a formula student prototype Robot. Auton. Syst. 113 2019 56 62 J. Antunes, A. Antunes, P. Outeiro, C. Cardeira, P. Oliveira, Testing of a torque vectoring controller for a formula student prototype, Robotics and Autonomous Systems 113 (2019) 56 – 62 [13] Wen S. Guo G. Observer-based control of vehicle platoons with random network access Robot. Auton. Syst. 115 2019 28 39 S. Wen, G. Guo, Observer-based control of vehicle platoons with random network access, Robotics and Autonomous Systems 115 (2019) 28 – 39 [14] Sajadi-Alamdari S.A. Voos H. Darouach M. Nonlinear model predictive control for ecological driver assistance systems in electric vehicles Robot. Auton. Syst. 112 2019 291 303 S. A. Sajadi-Alamdari, H. Voos, M. Darouach, Nonlinear model predictive control for ecological driver assistance systems in electric vehicles, Robotics and Autonomous Systems 112 (2019) 291 – 303 [15] Antunes A. Outeiro P. Cardeira C. Oliveira P. Implementation and testing of a sideslip estimation for a formula student prototype Robot. Auton. Syst. 115 2019 83 89 A. Antunes, P. Outeiro, C. Cardeira, P. Oliveira, Implementation and testing of a sideslip estimation for a formula student prototype, Robotics and Autonomous Systems 115 (2019) 83 – 89 [16] Marin-Plaza P. Hussein A. Martin D. de la Escalera A. Icab use case for ROS-based architecture Robot. Auton. Syst. 2019 P. Marin-Plaza, A. Hussein, D. Martin, A. de la Escalera, icab use case for ros-based architecture, Robotics and Autonomous Systems (2019) [17] You C. Lu J. Filev D. Tsiotras P. Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning Robot. Auton. Syst. 114 2019 1 18 C. You, J. Lu, D. Filev, P. Tsiotras, Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning, Robotics and Autonomous Systems 114 (2019) 1 – 18 [18] Talamino J.P. Sanfeliu A. Anticipatory kinodynamic motion planner for computing the best path and velocity trajectory in autonomous driving Robot. Auton. Syst. 114 2019 93 105 J. P. Talamino, A. Sanfeliu, Anticipatory kinodynamic motion planner for computing the best path and velocity trajectory in autonomous driving, Robotics and Autonomous Systems 114 (2019) 93 – 105 [19] Tonutti M. Ruffaldi E. Cattaneo A. Avizzano C.A. Robust and subject-independent driving manoeuvre anticipation through domain-adversarial recurrent neural networks Robot. Auton. Syst. 115 2019 162 173 M. Tonutti, E. Ruffaldi, A. Cattaneo, C. A. Avizzano, Robust and subject-independent driving manoeuvre anticipation through domain-adversarial recurrent neural networks, Robotics and Autonomous Systems 115 (2019) 162 – 173 "
    },
    {
        "doc_title": "Improved humanoid gait using learning-based analysis of a new wearable 3D force system: Work programme",
        "doc_scopus_id": "85070066549",
        "doc_doi": "10.23919/CISTI.2019.8760604",
        "doc_eid": "2-s2.0-85070066549",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Adaptative gait",
            "Ground reaction forces",
            "Humanoid robot",
            "Instrumented systems",
            "Low friction coefficients",
            "Machine learning techniques",
            "Real time measurements",
            "Robot controls"
        ],
        "doc_abstract": "© 2019 AISTI.The aim of this work is to develop a new framework for exploiting humanoid adaptive gait on different grounds, especially slippery floors with a low friction coefficient. To achieve this, the idea is to (1) enhance and extend a system for real-time measurement of the Ground Reaction Forces (GRFs) of a walking humanoid robot, (2) to develop a robot controller, and (3) to use machine learning techniques to interpret the GRFs and thus adapt the robot controller in order to achieve a robot able to walk on different grounds. The overall system contribution is the improvement and diversity of humanoids locomotion capacities, leading to a more easily integration of these robots into different areas where they can collaborate with humans.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Short-term Path Planning with Multiple Moving Obstacle Avoidance based on Adaptive MPC",
        "doc_scopus_id": "85068457282",
        "doc_doi": "10.1109/ICARSC.2019.8733653",
        "doc_eid": "2-s2.0-85068457282",
        "doc_date": "2019-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cost minimization",
            "Main tasks",
            "Moving obstacles",
            "Multiple moving obstacles",
            "Optimization problems",
            "Self drivings",
            "Short term"
        ],
        "doc_abstract": "© 2019 IEEE.This paper presents a different strategy for a self-driving car short-term path planning among multiple moving obstacles. The main task is to study and implement a motion planning and execution framework in order to make ATLASCAR2 coexist with other moving obstacle vehicles by avoiding collision and overtake them when necessary and possible. The proposed technique, based on the Model Predictive Control paradigm, solves an optimization problem formulated in terms of cost minimization under constraints. Simulation results demonstrate and verify the feasibility and the usefulness of the method considering different scenarios, opening space for real scenario implementation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A novel wireless instrumented shoe for ground reaction forces analysis in humanoids",
        "doc_scopus_id": "85048865627",
        "doc_doi": "10.1109/ICARSC.2018.8374157",
        "doc_eid": "2-s2.0-85048865627",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.The measurement of ground reaction forces (GRFs) is crucial in the biomechanical analysis of gait and other motor activities. Current humanoid robots mimic some of the biological adaptation methods found in humans. Therefore, studying the GRFs in these robots allows not only to improve their overall performance, but also to extrapolate possible techniques for human walking rehabilitation. The balance of a humanoid robot may be compromised when it walks and, even more, when it walks across multiple grounds with different frictions. This paper presents a system to be seamlessly installed on a walking humanoid robot to measure normal and tangential ground reaction forces. The proposed solution is a cost-effective, lightweight and wirelessly instrumented shoe (ITshoe) for real-time measuring of the GRFs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Special Issue on Autonomous Driving and Driver Assistance Systems",
        "doc_scopus_id": "85015102047",
        "doc_doi": "10.1016/j.robot.2017.01.011",
        "doc_eid": "2-s2.0-85015102047",
        "doc_date": "2017-05-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2017-01-27 2017-01-27 2017-02-09 2017-02-09 2017-12-23T23:01:50 S0921-8890(17)30056-8 S0921889017300568 10.1016/j.robot.2017.01.011 S300 S300.3 FULL-TEXT 2017-12-23T23:43:56.219361Z 0 0 20170501 20170531 2017 2017-01-27T16:56:31.823447Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav body acknowledge affil articletitle auth authfirstini authfull authlast pubtype 0921-8890 09218890 true 91 91 C Volume 91 18 208 209 208 209 201705 May 2017 2017-05-01 2017-05-31 2017 simple-article edi © 2017 Elsevier B.V. All rights reserved. SPECIALISSUEAUTONOMOUSDRIVINGDRIVERASSISTANCESYSTEMS SANTOS V Acknowledgments SANTOSX2017X208 SANTOSX2017X208X209 SANTOSX2017X208XV SANTOSX2017X208X209XV 2019-02-09T00:00:00.000Z UnderEmbargo © 2017 Elsevier B.V. All rights reserved. item S0921-8890(17)30056-8 S0921889017300568 10.1016/j.robot.2017.01.011 271599 2017-12-23T23:43:56.219361Z 2017-05-01 2017-05-31 true 294479 MAIN 2 67248 849 656 IMAGE-WEB-PDF 1 ROBOT 2786 S0921-8890(17)30056-8 10.1016/j.robot.2017.01.011 Elsevier B.V. Editorial Special Issue on Autonomous Driving and Driver Assistance Systems Vitor Santos IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal Angel D. Sappa ⁎ Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain Computer Vision Center, Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador ⁎ Corresponding editor. Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador Miguel Oliveira INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal Research on Autonomous Driving and Driver Assistance Systems has increased and became an active research field during the last decades. This increase in research is motivated by the need to reduce road accidents, which represent the 6th cause of dead in high-income countries according to the World Health Organization, and 11th worldwide. Vehicles are becoming more and more intelligent, promising fully autonomous vehicles for the near future. Such a big challenge opens a wide range of research topics that need to be tackled and efficiently solved by the community. This Special Issue on Autonomous Driving and Advanced Driver Assistance Systems (ADDAS) published by the Robotics and Autonomous Systems Journal contains extended versions of a set of selected papers from a special session on the same topic, which took place in the framework of the Second Iberian Robotics Conference, November 19th–21st 2015, in Lisbon, Portugal. The papers included in this special issue address several subjects of relevance to the ADDAS community, from scene representations using multi-modal data to infrared imaging or multiple sensor calibration algorithms. From the papers accepted for publication, two or three main interest lines were observed, which could be interpreted as current dominant trends for this community. They cover mainly 3D perception, multimodal sensors and sensor fusion. The importance of 3D perception is now undeniable in the ADDAS context, be it using lidar or stereo based sensors. As such, several papers in this special issue address the problem in several fronts, ranging from the automatic calibration in lidar and cameras multisensorial setups (Pereira et al.), up to efficient modelling and detection of features for navigation, namely on the ground, as shown both in 3D lidar approaches (Asvadi et al.) and stereo approaches (de la Escalera et al.). The ultimate challenge in processing 3D data is naturally environment and scene reconstruction, and the works by (Oliveira et al.) include relevant contributions in that field as well. Multimodality also appears as a relevant issue, and it is no longer uncommon to see on the same setups crossed combinations of lidar, vision, infrared and even radar; this richness of exteroceptive modality naturally implies the need for sensorial calibration and the subsequent data fusion procedures. The selected papers include contributions on the following topics: Evaluations on the usage of images resulting from the fusion of cross-spectral inputs, applied to the monocular visual odometry estimation, have been reported (Sappa et al.); such a fused image based approach has shown advantages when compared with the state of the art. The classical traffic signs detection and classification have been tackled with a highly optimized and accurate convolutional neural network architecture in Aghdam et al.; the stability of proposed approach has been evaluated on noisy images. In de la Escalera et al. a stereo odometry algorithm that detects and tracks features on the surface of the ground is proposed. In order to ensure a uniform distribution of feature keypoints, an inverse perspective image is used. Results show that this visual odometry is accurately estimated. In order to overcome the fact that particle filters are computationally demanding, a hardware implementation on FPGA (field programmable gate arrays) has been proposed and validated to estimate the localization and mapping (SLAM) of a robot (Sileshi et al.). In Ćesić et al., a multisensor setup consisting of a radar and a stereo camera mounted on top of a vehicle is used. Authors model the sensors uncertainty on the product of two special Euclidean groups, and show that the model is more accurate than classical approaches. Autonomous vehicles and ADAS systems often resort to multi-modal setups in order to improve the efficiency of the systems. This, however, raise the problem of geometric registration between the sensors. The work described in Pereira et al. presents a solution for the automatic calibration of multiple sensors of different natures, e.g. LIDARs and cameras, using a simple spherical calibration object. In Asvadi et al. a framework for the detection of static and moving obstacles is proposed. The algorithm is based on a piecewise planar surface fitting and a 3D voxel representation. Results have been tested on diversified driving environments. The online reconstruction of scenarios observed by autonomous vehicles is not trivial. The work presented in Oliveira et al. (a) proposes an algorithm designed to create a geometric representation of such a scenario. Furthermore, this work proposes a way to update these representations online. Results show that the approach is capable of producing accurate descriptions of the scene. Finally, the work presented in Oliveira et al. (b) focuses on the photometric reconstruction of the geometric scenario reconstructions produced in Oliveira et al. (a). The algorithm uses a constrained Delaunay triangulation to produce a mesh that is updated using a specially devised sequence of operations that ensure the quality of the final texture. Results show that the approach is capable of producing fine quality textures. We hope that these works may contribute to the development of novel and more efficient ADDAS algorithms and systems. Acknowledgments This Special Issue has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R; and by the “Fundação para a Ciência e Tecnologia” (Portuguese Foundation for Science and Technology) under grant agreement SFRH/BPD/109651/2015 and National Funds within project UID/EEA/50014/2013. This work was also financed by the ERDF - European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020 Programme within project “POCI-01-0145-FEDER-006961”. "
    },
    {
        "doc_title": "Motion Primitives for Human-to-Humanoid Skill Transfer under Balance Constraint",
        "doc_scopus_id": "85010458341",
        "doc_doi": "10.1109/ICARSC.2016.53",
        "doc_eid": "2-s2.0-85010458341",
        "doc_date": "2016-12-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Adaptive behavior",
            "Cartesian Space",
            "Dynamic movement primitives",
            "Imitation learning",
            "Modular generation",
            "Motion primitives",
            "Movement primitives",
            "Real world environments"
        ],
        "doc_abstract": "© 2016 IEEE.In order to properly function in real-world environments, the gait of a humanoid robot must be able to adapt to new situations as well as to deal with unexpected perturbations. A promising research direction is the modular generation of movements that results from the combination of a set of basic primitives. Humans seem to be able to adapt their motion in an effortless way and efficient way. In this work we present a methodology to extract a single demonstration signal from a human teacher that can be used in a biped robot to walk in an efficient way and quickly adapt to new situations. Instead of using a normal human gait, the demonstrator walks on a gait we called \"robot-like\". It is our belief that the retargeting task will be more efficient and easier by using this methodology. The acquired signal is then used to train a set of Dynamic Movement Primitives (DMP) that directly retrieves the trajectories of the end effector in the Cartesian space. In previous works we have used this methodology with efficiency in several tasks, e.g., walking over irregularities, overcoming an obstacle. Now, by combining rhythmic DMP with discrete DMP, we show that the adaptability performance to new situations that require precise foot placement is increased.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Incremental texture mapping for autonomous driving",
        "doc_scopus_id": "84991738411",
        "doc_doi": "10.1016/j.robot.2016.06.009",
        "doc_eid": "2-s2.0-84991738411",
        "doc_date": "2016-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Autonomous Vehicles",
            "Constrained Delaunay triangulation",
            "Geometric description",
            "Partial configuration",
            "Scene reconstruction",
            "Texture mapping",
            "Vision-based sensors"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Autonomous vehicles have a large number of on-board sensors, not only for providing coverage all around the vehicle, but also to ensure multi-modality in the observation of the scene. Because of this, it is not trivial to come up with a single, unique representation that feeds from the data given by all these sensors. We propose an algorithm which is capable of mapping texture collected from vision based sensors onto a geometric description of the scenario constructed from data provided by 3D sensors. The algorithm uses a constrained Delaunay triangulation to produce a mesh which is updated using a specially devised sequence of operations. These enforce a partial configuration of the mesh that avoids bad quality textures and ensures that there are no gaps in the texture. Results show that this algorithm is capable of producing fine quality textures.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-07-05 2016-07-05 2016-08-26 2016-08-26 2016-08-26T17:58:13 S0921-8890(16)30081-1 S0921889016300811 10.1016/j.robot.2016.06.009 S300 S300.1 FULL-TEXT 2016-08-26T13:55:25.23236-04:00 0 0 20161001 20161031 2016 2016-07-05T15:19:52.486451Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 84 84 C Volume 84 10 113 128 113 128 201610 October 2016 2016-10-01 2016-10-31 2016 article fla © 2016 Elsevier B.V. All rights reserved. INCREMENTALTEXTUREMAPPINGFORAUTONOMOUSDRIVING OLIVEIRA M 1 Introduction 2 Related work 3 Proposed approach 3.1 One-shot texture mapping using data dependent triangulation 3.2 Incremental texture mapping 4 Results 4.1 One-shot texture mapping 4.2 Incremental texture mapping 4.3 Projection of a single camera onto the ground plane 4.4 Projection of multiple cameras onto the ground plane 5 Conclusions Acknowledgments References OLIVEIRA 2016 503 515 M ROBOT2015SECONDIBERIANROBOTICSCONFERENCEADVANCESINROBOTICSVOLUME1 SCENEREPRESENTATIONSFORAUTONOMOUSDRIVINGAPPROACHBASEDPOLYGONALPRIMITIVES OLIVEIRA 2016 M HUANG 2011 1595 1601 A SEGAL 1992 249 252 M DEBEVEC 1998 P EFFICIENTVIEWDEPENDENTIMAGEBASEDRENDERINGPROJECTIVETEXTUREMAPPINGTECHREP RIPPA 1992 257 270 S GARLAND 1995 M FASTPOLYGONALAPPROXIMATIONTERRAINSHEIGHTFIELDSTECHREPCMUCS95181 SCHATZL 2001 309 321 R GEOMETRICMODELLING DATADEPENDENTTRIANGULATIONINPLANEADAPTIVEKNOTPLACEMENT DEMARET 2006 1604 1616 L SAPPA 2007 23010 A DYN 1992 179 192 N SCHUMAKER 1993 329 345 L OPENGL 2005 OPENGLRPROGRAMMINGGUIDEOFFICIALGUIDELEARNINGOPENGLRVERSION2 BLYTHE 2006 724 734 D LEHNER 2007 178 187 B GILECTURENOTESININFORMATICSVISUALIZATIONLARGEUNSTRUCTUREDDATASETS SURVEYTECHNIQUESFORDATADEPENDENTTRIANGULATIONS CERVENANSKSY 2010 125 135 M SVALBE 1989 941 950 I YVINEC 2012 M CGALUSERREFERENCEMANUAL40EDITION 2DTRIANGULATIONS SHEWCHUK 2008 580 637 J MOLLER 1997 25 30 T CHANG 2009 235 240 J FOGEL 2012 E CGALUSERREFERENCEMANUAL40EDITION 2DREGULARIZEDBOOLEANSETOPERATIONS OLIVEIRAX2016X113 OLIVEIRAX2016X113X128 OLIVEIRAX2016X113XM OLIVEIRAX2016X113X128XM 2018-08-26T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0921-8890(16)30081-1 S0921889016300811 10.1016/j.robot.2016.06.009 271599 2016-08-26T13:11:12.895259-04:00 2016-10-01 2016-10-31 true 8641462 MAIN 16 49999 849 656 IMAGE-WEB-PDF 1 gr4 26663 164 127 gr7 30032 163 206 gr8 29537 164 203 pic5 33051 163 140 gr2 30967 164 169 pic2 25133 163 140 gr12 33037 164 197 gr1 29767 164 183 gr5 20010 163 140 gr13 41383 163 154 pic3 22237 163 140 gr6 26630 164 214 gr9 23491 52 219 pic4 22871 164 141 gr11 23784 163 122 pic1 23187 164 141 gr3 25322 113 219 gr10 38213 164 168 gr4 152384 649 502 gr7 192344 600 756 gr8 183020 594 736 pic5 39234 132 113 gr2 149230 587 606 pic2 38908 132 113 gr12 193022 529 637 gr1 129052 533 596 gr5 164313 882 756 gr13 293812 664 626 pic3 35322 132 113 gr6 155924 579 755 gr9 111172 156 656 pic4 35061 131 113 gr11 208963 834 624 pic1 35749 131 113 gr3 64157 312 606 gr10 203603 571 585 si123 161 13 16 si56 159 12 17 si21 144 13 13 si96 4249 133 353 si71 222 15 36 si107 724 39 134 si121 2455 64 382 si166 300 19 45 si81 578 15 150 si53 159 13 17 si17 188 13 42 si184 390 15 87 si75 112 11 6 si46 401 15 87 si147 201 14 29 si37 117 12 6 si168 265 19 37 si203 401 15 87 si134 149 13 14 si39 264 15 39 si68 1486 36 284 si88 399 15 64 si78 136 10 16 si115 127 10 11 si114 405 15 80 si201 400 15 87 si163 276 19 37 si10 178 12 23 si124 151 13 16 si181 346 19 51 si202 407 15 87 si152 332 21 61 si122 154 13 16 si63 170 11 16 si66 229 13 39 si199 391 15 87 si11 195 12 29 si126 115 7 10 si119 187 14 27 si8 134 10 11 si127 1424 16 390 si110 277 16 40 si99 1388 41 292 si196 168 12 35 si132 470 15 148 si9 160 12 17 si57 922 16 247 si73 151 14 14 si54 169 13 18 si159 183 11 41 si69 170 14 19 si83 330 15 62 si205 397 15 87 si2 126 11 11 si125 713 15 145 si61 566 15 183 si80 142 10 17 si148 200 14 29 si16 178 13 40 si197 388 15 87 si108 147 14 11 si145 230 13 41 si178 287 19 36 si183 496 15 136 si207 398 15 87 si176 324 19 45 si34 145 13 13 si18 188 13 41 si170 356 19 57 si1 202 12 40 si29 136 11 12 si174 324 19 45 si140 233 13 39 si30 131 11 10 si206 403 15 87 si164 256 19 36 si135 944 17 283 si133 794 15 171 si74 699 21 173 si62 1004 51 189 si175 330 19 45 si43 273 15 52 si67 1107 15 328 si154 358 23 60 si72 143 14 14 si100 1647 41 383 si200 183 12 36 si161 318 19 45 si120 255 14 43 si60 243 17 36 si153 351 23 61 si90 140 11 10 si59 286 11 72 si91 1560 31 389 si47 392 15 87 si41 187 12 36 si70 351 15 76 si65 155 10 20 si77 326 24 62 si40 192 12 36 si195 212 12 40 si20 134 13 12 si117 972 16 249 si187 395 15 87 si105 216 15 27 si186 386 15 87 si38 114 10 8 si162 272 19 36 si112 135 12 10 si3 141 11 12 si130 167 14 16 si44 326 15 65 si50 189 13 41 si129 163 14 16 si182 381 15 87 si137 223 13 40 si131 234 13 39 si104 210 13 41 si64 160 10 19 si102 113 10 10 si146 193 14 28 si22 141 13 13 si177 283 19 37 si103 2739 70 382 si55 149 12 16 si85 160 21 14 si98 577 17 148 si58 131 8 11 si45 379 15 94 si76 123 14 8 si128 248 14 39 si158 1670 44 346 si179 287 19 37 si82 146 11 13 si106 285 15 51 si97 500 15 141 si79 145 10 17 si139 221 13 41 si167 231 13 40 si198 395 15 87 ROBOT 2656 S0921-8890(16)30081-1 10.1016/j.robot.2016.06.009 Elsevier B.V. Fig. 1 An example from the MIT data-set: three projections are collected over a period of time and mapped to a wall panel (GPP k = 4 , in blue): (a) positions of the vehicle at the time each projection is collected; (b) image from front camera, at location C ; (c), front camera, intermediate location; (d) front camera, location D ; (e) left camera, location D . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Triangulated meshes (top) and textures (bottom) created separately for each of the projections shown in the example of Fig. 1: (a) front camera, location C ; (b) front camera, intermediate location; (c) front camera, location D ; (d) left camera, location D . Fig. 3 Textures obtained using different fusion strategies: (a) option (1), average textures from local meshes; (b) option (2), insert vertices from all local meshes; (c) option (3), insert triangles of better quality, removing overlapping triangles. Fig. 4 A diagram showing the main components of the proposed system. Fig. 5 Triangle overlap test: (a) intersection returns points, overlap true; (b) intersection returns line segments, overlap true; (c) intersection returns polygons, overlap true; (d) intersection returns empty, overlap false; (e) intersection returns points, overlap false; (f) intersection returns line segments, overlap false. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 The insertion operation: (a) candidate triangle and initial mesh; (b) insertion of candidate triangle’s vertices; (c) insertion of the candidate triangle’s vertices and constraints; (d) preparation of the mesh followed by the insertion of the candidate triangle’s vertices and constraints. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 The insertion operation, example 1: (a) candidate triangles and initial mesh M ; (b) first insertion, mesh M ∗ ; (c) second insertion, mesh M ∗ ∗ ; (d) third insertion, mesh M ∗ ∗ ∗ . Fig. 8 The projection parent status of each triangle (same example as in Fig. 7): (a) candidate triangles and initial mesh M ; (b) first insertion, mesh M ∗ ; (c) second insertion, mesh M ∗ ∗ ; (d) third insertion, mesh M ∗ ∗ ∗ . Fig. 9 One-shot texture mapping: (a) image with line segments detected (red lines); (b) arbitrary Delaunay triangulation; (c) proposed approach, using a constrained Delaunay triangulation. Constrained edges marked in red. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 The evolution of the global primitive mesh (top) as well as the texture (bottom): (a) time t = t 1 ; (b) time t = t 2 ; (c) time t = t 3 , insertion of front center camera; (d) time t = t 3 , insertion of front left camera. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Mapping of a single camera to the ground plane: (first row) front center camera; (second row) rear center camera; (third row) front left camera; (left column) global primitive meshes; (right column) contribution of each projection to the total number of triangles in the global mesh. Colors denote each of the projections, i.e., black is time t 1 , red is time t 2 and yellow is time t 3 . Blue triangles are orphan triangles. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Distribution of triangles according to projection, for an example with five cameras. Three time instants (15 projections in total) are considered; (a) t 1 ; (b) t 2 ; (c) t 3 ; (d) percentage of triangles by parent projection. Projections are colored with a black to yellow color coding, denoting oldest to newest projections. Blue color denotes orphan triangles. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Images and local triangulated meshes for all projections shown in Fig. 12: First row front center teleobjective camera; Second row front center camera; Third row, front right camera; Fourth row, front left camera; Fifth row, rear center camera; Left column: time t 1 , (Fig. 11(a)); Middle column: time t 2 , (Fig. 11(b)); Right column: time t 3 , (Fig. 11(c)). Incremental texture mapping for autonomous driving Miguel Oliveira a b ⁎ Vitor Santos b Angel D. Sappa c d Paulo Dias b A. Paulo Moreira a e a INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal b IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal c Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Campus Gustavo Galindo, Km 30.5 Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador d Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center, Campus UAB Bellaterra Barcelona 08193 Spain e FEUP - Faculty of Engineering, University of Porto, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal FEUP - Faculty of Engineering, University of Porto R. Dr. Roberto Frias s/n Porto 4200-465 Portugal ⁎ Corresponding author at: INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal. INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal Autonomous vehicles have a large number of on-board sensors, not only for providing coverage all around the vehicle, but also to ensure multi-modality in the observation of the scene. Because of this, it is not trivial to come up with a single, unique representation that feeds from the data given by all these sensors. We propose an algorithm which is capable of mapping texture collected from vision based sensors onto a geometric description of the scenario constructed from data provided by 3D sensors. The algorithm uses a constrained Delaunay triangulation to produce a mesh which is updated using a specially devised sequence of operations. These enforce a partial configuration of the mesh that avoids bad quality textures and ensures that there are no gaps in the texture. Results show that this algorithm is capable of producing fine quality textures. Keywords Scene reconstruction Autonomous driving Texture mapping 1 Introduction Autonomous vehicles often have a very large number of sensors mounted on-board. This is due to the need to observe the environment all around the vehicle, but also because vehicles must observe the scene with sensors of different nature. Mainly, sensors are divided into two groups: range sensors and vision based sensors. Sensors of the first group provide 3D measurements of the scene. On the other hand, vision based sensors collect photometric information of the scene. Due to the large number of sensors on-board these vehicles, it is not trivial to combine data from these sensors into a unique representation of the scene. Given that these sensors provide a continuous stream of data over time, and that they are displaced by the movement of the vehicle, then it follows that the representation of the scene must also be dynamic, in the sense that it must evolve to represent novel information collected at later stages of the mission. Note that, given a continuous throughput of images, the most recent image is not necessarily the best image to be used for texture mapping. For example, if the vehicle is moving away from an object, a camera on the rear side of the vehicle will produce images with decreasing quality. Rather, what is required is an algorithm that produces a scene representation at the early stages of a mission (because this might be immediately required for other tasks such as navigation, planning, etc.), but the later on is also capable of evaluating newly acquired images to assess whether or not these images are better than the previously used for mapping the texture. We refer to this as incremental texture mapping. In [1], an algorithm for creating and incrementally updating a geometrical representation of the scenario is presented. This work was later extended in [2]. The approach is based on Geometric Polygonal Primitives (GPP), and is shown to be capable of providing an accurate geometric description of the scenario. It uses data from range sensors only, and the geometric description changes to accommodate novel sensor data. In this paper we use the results given by the approach described in [2]. This means that we consider that there is, at all times, a geometric description of the scenario which is constantly evolving. In this paper, we focus on how the vision based sensors can be used to enrich the description of the scenario. In other words, we propose to use the images from the cameras on-board the vehicle to produce texture, which may be added to the 3D description of the environment. Note that, as in the case of the range sensors, the vision based sensors also produce a continuous stream of information which must be integrated in order to create a unique photometric description of the scenario. In this paper, we propose an approach which is capable of incrementally updating texture mapped onto GPPs. The following lines show an example in which the need for incremental texture mapping becomes clear. For testing and evaluation purposes, we use a data-set from the Massachusetts Institute of Technology (MIT) Team, taken from their participation in the DARPA Urban Challenge [3]. A small 40 s sequence was cropped from the MIT data-set. This sequence is referred to as the MIT sequence, and five key locations ( A through E where marked in the sequence (see [2] for details). The approach described in [2] produces a description of the geometric structure of the environment observed by the vehicle’s sensors. This description is given in the form of Geometric Polygonal Primitives (GPP), i.e., a list of polygons. Note that, as pointed out in [2] the geometric description of the scene is dynamic, since it may change whenever novel sensor information is collected. An example is presented in Fig. 1 where the vehicle travels from location C to location D of the MIT sequence. Images are collected at three locations: location C at mission time t 0 , location D at mission time t 2 and an intermediate location between those two at mission time t 1 (Fig. 1(a) shows the vehicle at each location). Consider a camera of index l , that produces an image which may virtually be projected to any GPP (i.e., to one of the polygons that constitute the geometric description of the scene), at any given mission time t . The term projection is defined as an image captured from a camera that can be used to map some texture to one of the polygons contained in the geometric description of the scene, and is denoted as C { k , l , t } . The data-set contains five color cameras (see [2,4] for details). Without loss of generality, in this example only images from two cameras are used: front center ( l = 0 ) and front left ( l = 3 ), and only a single GPP (index k = 4 ) is employed, which corresponds to the wall panel in front of the vehicle (in blue, left side of Fig. 1(a)). Note that, under the constraints defined above, i.e., k = { 4 } , l = { 0 , 3 } and t = { t 0 , t 1 , t 2 } , there are a total of six possible projections. However, two of these projections are empty, namely C { k = 4 , l = 3 , t = t 0 } and C { k = 4 , l = 3 , t = t 1 } . This is because the left camera ( l = 3 ) does not see the wall panel ( k = 4 ) in the first two locations ( t = t 0 and t = t 1 ). This can be observed in Fig. 1(a), which shows that the vehicle turns right at location D , and only then the left side camera is pointed in the direction of the wall panel. The images from the remaining four projections are shown in Fig. 1(b)–(e). As the vehicle approaches the wall panel, it collects images with higher resolution and better quality of that surface. Our goal is to study how a low resolution texture created when the vehicle was distant from the surface may evolve to a higher resolution texture once the vehicle comes closer to the panel. In other words, how can the texture of a surface be incrementally refined. Note that we assume that an accurate localization is available at all times. In the case of the MIT dataset, localization is provided by an Applanix POS-LV 220 system, 1 1 which includes a GPS, an inertial measurement unit and a wheel encoder. This is a very accurate system which publishes the 6 DOF pose of the vehicle at high frequencies (100 Hz). Thus, it is possible to gather the pose of each of the onboard cameras at any point in time. Obviously, a less accurate ego motion estimation should influence the mapping of texture. However, a detailed analysis of the impact of other ego motion estimation systems is out of the scope of the current paper. The remainder of the paper is organized as follows: related work is presented in Section 2; the proposed approach is described in Section 3 and, finally, results and conclusions are given in Sections 4 and 5. 2 Related work Texture mapping is a technique for mapping a 2D image onto a 3D surface by transforming color data so that it conforms to the surface plot. It allows the application of texture such as tiles or wood grain, to a surface without performing the geometric modeling necessary to create a surface with these features, or, in other words, without computing the projection of every pixel in the image onto the surface. The color data can also be any image, such as a picture taken by a camera. Texture mapping is performed over convex polygons, most commonly on triangles. Let X 1 , X 2 be the coordinates of the vertices 1 and 2 in 3D space. The coordinates u 1 , u 2 of the pixels that correspond to those vertices in the image plane can be obtained using direct projection: (1) u i = projection ( X i ) , ∀ i ∈ { 1 , 2 } . Let α be a parameter 0 < α < 1 , that indicates how a given vertex is positioned along the X 1 X 2 ¯ line segment. Texture mapping interpolates the color value for any vertices along the line segment as follows: (2) u α = ( 1 − α ) ⋅ u 0 + α ⋅ u 1 , which is of course a linear interpolation. When this kind of linear interpolation is used, the texture mapping is referred to as affine texture mapping. A linear interpolation works fine when the image plane and the projection plane are parallel. However, when this does not occur, the projection shows some artifacts that derive from the assumption that a linear interpolation can be used. This is a well documented problem, and is discussed in several works [5,6]. The solution to this problem is called view dependent texture mapping, and it consists of making texture mapping account for the position of the vertexes in 3D space, rather than simply interpolating a 2D triangle. This achieves the correct visual effect, but it is slower to calculate. Instead of interpolating the texture coordinates directly, the coordinates are divided by their depth (relative to the viewer), and the reciprocal of the depth value is also interpolated and used to recover the perspective corrected coordinate. This correction operates so that in parts of the polygon that are closer to the viewer, the difference from pixel to pixel between texture coordinates is smaller (stretching the texture wider), and in parts that are farther away this difference is larger (compressing the texture). View dependent texture mapping can be formulated as: (3) u α = ( 1 − α ) ⋅ u 0 w 0 + α ⋅ u 1 w 1 ( 1 − α ) ⋅ 1 w 0 + α ⋅ 1 w 1 . The solution proposed in Eq. (3) is capable of producing accurate mapping for texture. View dependent texture mapping is significantly slower when compared to affine texture mapping. Since triangles are the atomic entities of texture mapping, triangulation methodologies are an important part of the process. In this scope, Data Dependent Triangulation (DDT) algorithms are of particular interest since they can produce triangulated meshes which are ideal for texture mapping. The goal of a DDT is, on the one hand, to obtain the best approximation possible, and on the other to reduce the number of triangles and in turn the memory load. Consequently, the number of vertices should be kept as small as possible to speed up processing and reduce memory load. The two variables that should be tuned to achieve a good approximation are then the position of the vertices and the connections between them. Even if we decide to fix the number of triangles and vertices, the possible combinations of the connections between vertices are usually very large. Hence, an exhaustive search of all possible combinations is not possible. Also, no assumptions should be made on the optimal shape or size of the triangles. One might tend to assume long, thin triangles are not adequate but in fact that depends on the nature of the image [7]. If the image contains high gradient long feature such as poles or trees, such triangles could be well suited to represent these regions. DDT algorithms can be divided into refinement, decimation, or modification approaches. In refinement approaches, the starting point for the algorithm is a very coarse triangular mesh that is then refined. The mesh is refined by inserting new vertices. Since the number of possible positions where vertices can be inserted is very high, authors make use of heuristics to limit the number of options. The greedy refinement algorithm proposed in [8] works by inserting vertices into a triangulated mesh. In every step, a new vertex is inserted at the position of the largest distance between the approximation and the data provided in the image. In [9], the choice of which are the triangles to decimate is based on the high curvature of the data, and the positions where new vertices are to be inserted are locations with high proximity to the data. These methods have the drawback of tending to a local optima. Decimation approaches are the opposite of refinement meshes. The algorithms start from a very fine mesh and try to remove vertices and collapse triangles as they iterate. In [10] the initial triangulation is a full triangulation where each pixel is a vertex in the mesh. The algorithm then decimates the mesh by collapsing one of the edges of the mesh. The edge to collapse is the one that implicates less increase in the approximation error. Similar approaches were proposed in [11,12]. Finally, modification strategies start from a random arbitrary mesh and try to improve it by performing modification operations. These modification operations usually are edge swaps and the number of vertices in the initial mesh remains the same. It is the case of the algorithms proposed in [13,14]. Both propose different criteria for the selection of which are the edges that should be swapped. Given a triangulated mesh of a surface and an image registered to that surface, it is possible to map the texture from the image onto the surface using classic texture mapping approaches. Fig. 2 shows the triangulated meshes and textures produced using classical texture mapping, for each of the four projections displayed in Fig. 1. These mappings are computed independently for each location. As expected, textures that derive from projections taken closer to the surface (e.g., Fig. 2(c) and (d)) have better quality when compared to projections taken far away from the surface (e.g., Fig. 2(a) and (b)). The question is how to create an unique triangulated mesh and texture and how it should be updated with local triangulated meshes from novel projections. Let us assume that there is a way of assessing the quality of each projection and in particular of each triangle in each mesh, so that it is possible to rank the triangles with respect to their quality. At first sight, several strategies can be used to fuse the textures, namely: (1) average the textures produced by local meshes; (2) insert vertices from new triangles into the existing mesh and re triangulate, provided that these triangles yield better quality; (3) remove the triangles of the existing mesh that overlap the triangle (of better quality) to be inserted, and then insert the triangle. Option (1) consists of averaging the textures provided by each local mesh. This could be achieved by setting the alpha channel of all local meshes so that they average out. Additionally, the average could be weighted by the quality of the triangles, although this was not tested in this work. The primitive would have several layers, each with a given local triangulated mesh belonging to each projection. Fig. 3 (a) shows the results obtained using this strategy. Visually, results are not appealing. Another disadvantage concerns the need to store all local meshes, which is highly inefficient in terms of memory. As seen in Fig. 2, there are textures with much better quality than others. To average good textures with bad textures does not seem to make sense. Option (2) proposes to address the problem by considering the vertices of the meshes only (rather than the triangles). Each vertex in the new mesh is added to the current mesh. This results in a super mesh containing all the vertices of the two previous meshes in which the triangles (the configuration of the mesh) are defined arbitrarily. The idea is to fuse using an additive strategy. Fig. 3(b) shows the results obtained using this approach. Again, results are not visually appealing. In option (3), an alternative to averaging is considered: a winner takes all strategy. The idea is to select, for each region in the surface, a single triangle which will provide the texture. This selection can be done using the quality of each triangle. Note that, whenever a triangle to be inserted overlaps some triangles in the existing mesh, these triangles must first be removed so that the mesh preserves its configuration and thus the quality of the texture mapping. Results obtained from this method are shown in Fig. 3(c). Textures are visually appealing. Artifacts present in the average and the additive strategies are not visible. There is one problem however: removed triangles often overlap triangles to be inserted in just a portion of their area (partial overlap). When deleted, these triangles leave empty spaces where no texture is defined. This is visible in Fig. 3(c). None of the strategies discussed provides textures of sufficient quality. Thus the problem of incrementally updating the texture is not trivial. In the following sections, an approach is presented which is capable of generating higher quality textures. 3 Proposed approach Fig. 4 shows a diagram which describes the functioning of the system. The following sections will describe these components in detail. First, a one-shot texture mapping based in DDT is presented in Section 3.1. Here, we propose an algorithm based on the extraction of edges in the image and the construction of a constrained Delaunay triangulation, which is operated as a DDT and is very efficient. Then, the incremental texture mapping approach is presented in Section 3.2. In this case, we propose a sequence of atomic operations to conduct the insertion of a new triangle in a triangulated mesh, which minimizes the changes in the configuration of the mesh. 3.1 One-shot texture mapping using data dependent triangulation In this paper, we propose an alternative solution to view dependent texture mapping. One reason for this is that the objective of this work is to develop a mechanism for mapping texture from images onto GPP (see [2]). Those geometric primitives consist of polygons, instead of the traditional triangles. The mapping of photometric properties can be performed by mapping triangles in image space to 3D space. These procedures are executed in Graphical Processing Units (GPUs), and programmed using OpenGL [15], Direct3D [16] or other graphics libraries. These libraries also have the functionalities of mapping convex polygons, but in fact these are mere high level functions that decompose the polygons in an arbitrary way into sets of triangles and then map texture onto those triangles. We argue that, if we control the process of triangulation in such a way that the edges in the images used in the projection are aligned with the edges of the triangles in 3D space, the distortion produced by linear texture mapping is not visible, and thus, linear texture mapping may be used instead of view dependent triangulation, which is much slower. In other words, if the triangles are especially defined so that their faces represent smooth regions with constant color then, a linear texture mapping over these could in fact provide accurate projections. This procedure of creating a triangulated mesh which accommodates some input data is called DDT [17], and the mapping of images using this technique will be referred to as DDT mapping as opposed to texture mapping. Unlike in standard texture mapping approaches, where the triangulation is executed in the 3D space, DDT triangulation operates in the image space, and only after those 2D triangles are mapped onto the 3D space. Although there are many approaches in the literature to the data dependent triangulation problem, most of them are focused on the fact that such a triangulated mesh is capable of producing very good data compression ratios with respect to the real image, while still maintaining low approximation errors. Real time performance of the algorithms has seldom been debated, with authors reporting processing times of over three seconds for 512×512 images. The exception was the study conducted in [18], where DDT was parallelized, resulting in a significant speed up. We propose a simple procedure similar to [12]: edges are detected using a Hough lines detector [19] extended to obtain a description of line segments instead of lines (e.g., see [20,21]). The triangulation is a Delaunay triangulation [22]: let the image be described by M line segments with starting points s m and endpoints e m , where each detected line segment is defined as s m e m ¯ . The Delaunay triangulation (Delaunay) receives the starting and endpoints as input to define the vertices of the triangulated mesh: (4) t = Delaunay ( { s 0 , e 0 , s 1 , e 1 , … , s M − 1 , e M − 1 } ) , where t is the resulting triangulated mesh. A constrained Delaunay triangulation is a generalization of the Delaunay triangulation where line segments may be imposed as belonging to the triangulated mesh (initially proposed by [23] for 2D spaces, later generalized to N dimensional spaces by [24]). A constrained Delaunay triangulation (cDelaunay) requires two inputs, a list of points and a list of line segments (also called constraints): (5) t = cDelaunay ( { s 0 , e 0 , … , s M − 1 , e M − 1 } , { s 0 e 0 ¯ , … , s M − 1 e M − 1 ¯ } ) . In brief, what we propose is a technique in which a constrained Delaunay triangulation is executed on the image space, having as input the line segments given by a line segment detection algorithm based on hough lines. 3.2 Incremental texture mapping Section 3.1 described how a constrained Delaunay triangulation may be used to produce a data dependent triangulated mesh that conforms with edges previously detected in the image. Note that this is a one-camera, one-shot approach, since it does not consider how to map more than one image. In reality, there is always a large set of images available to use for texture mapping, either from multiple cameras or from a unique camera at different times. This section addresses this problem of merging multiple projections into a single representation. As described in Section 3.1, a DDT triangulation is executed for each image used in a projection. Thus, there will be a triangulated mesh (a list of triangles) for each image (for each projection), to which we refer as local triangulated mesh. Local triangulated meshes for the example of Fig. 1 are shown in Fig. 2. Let M be the global triangulated mesh, defined in R 2 , so that only one global mesh exists per each primitive. This global mesh should be updated when new projections are collected or, in other words, when novel local meshes are received, i.e. it should contain the result of the fusion of the several textures. A local triangulated mesh from projection index j = { k , l , t } (i.e., form a given combination of k , l , t ) is denoted as T j . Local triangulated meshes contain T j number of triangles. Individual triangles are denoted as T i j , ∀ i ∈ { 0 , 1 , … , T { k , l , t } } , when indicating the i th triangle of the local mesh j , or as T { v 1 , v 2 , v 3 } j , in the case the vertices v 1 , v 2 and v 3 are specified. Likewise, triangles in the global mesh are notated as M n , ∀ n ∈ { 0 , 1 , … , N } , where N is the number of triangles in the global mesh. When the vertices of the triangles are specified, then the notation M { V 1 , V 2 , V 3 } is used. To continuously fuse local triangulated meshes from new projections onto the global mesh, we propose a mechanism which iterates all the triangles in the local projection mesh and decides whether they should be inserted in the global mesh by computing the benefit of this operation to the overall quality of the global mesh. At iteration i , triangle T i j from the local projection mesh is referred to as candidate triangle. First, the algorithm assesses if there is overlap between ( T i j ) and any of the existing triangles in the global mesh M n , ∀ n ∈ { 0 , 1 , … , N } . Let intr ( A , B ) be a function that tests intersection between triangles A and B . The test can be written as: (6) do_intersect = intr ( T { V 1 , V 2 , V 3 } j , M n ) , ∀ n ∈ { 0 , 1 , … , N } , where N corresponds to the total number of triangles in the global mesh M . The intersection of two triangles can result in an empty set, whenever there is no intersection, in a point, a line segment, or a polygon. There are several approaches to triangle triangle intersection tests, that provide fast and efficient algorithms [25–27]. Note that there is a distinction between overlap and intersection: what must be assessed is whether or not an insertion of the candidate triangle onto the global mesh will change its configuration. Thus, an overlap test is not the same as an intersection test, since there are some cases where the triangles do intersect but the mesh configuration is not altered. The overlap test is based on a set of rules that analyse the return of the intersection function (intr, implementation from [28]), between candidate triangle T i j and global mesh triangle M { V 1 , V 2 , V 3 } . It returns yes if the triangles overlap or no otherwise. The algorithm is detailed in Eqs. (7)–(9): (7) { no ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = ∅ yes ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = list of polygons Goto (8) ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = points X Goto (9) ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = line segment L where X = { X 0 , X 1 , … , X N } and L = { S 0 E 0 ¯ , … , S N E N ¯ } ; (8) { no ⇐ ∃ V g : V g = X o , ∀ V g ∈ { V 1 , V 2 , V 3 } , ∀ o ∈ { 0 , 1 , … , N } yes ⇐ otherwise (9) { no ⇐ ∃ V g : V g = S o ∧ ∃ V h : V h = E o , ∀ V g , V h ∈ { V 1 , V 2 , V 3 } , ∀ o ∈ { 0 , 1 , … , N } yes ⇐ otherwise . Fig. 5 (a) shows a global primitive mesh M that contains a single triangle (in blue), and a candidate triangle (in red). In each case, the geometries returned by the intersection function are as follows: an empty set (d), points (a) and (e), line segments (b) and (f), and polygons (c). If we consider the cases where the insertion of the candidate triangle (in red) does not change the configuration of the already existing global mesh (in this case, the initial global mesh is composed of a single triangle, in blue), we can say that in case (a), (b) and (c) the mesh would be altered, and that, in cases (d), (e) and (f) the mesh would remain unaltered. The overlap test returns a list L of indices of triangles from the global mesh which overlap with the candidate triangle. From this, the benefit of inserting the candidate triangle in the global mesh is assessed. In this context, benefit is defined as an improvement in the quality of texture and estimated as follows: (10) { beneficial ⇐ L is empty otherwise { beneficial , ⇐ q ( T i j ) > α ⋅ max ( q ( M g ) ) , ∀ g ∈ L not beneficial , ⇐ otherwise where α ≥ 1 is a user defined cost parameter, which defines how much better the quality of candidate triangle must be to any other triangle it overlaps, in order for the insertion to be considered beneficial, and q ( ⋅ ) is an arbitrary function that returns the estimated quality of each triangle. In this work we define quality as directly proportional to the resolution of the texture. An image provides a texture of higher resolution when it is closer to the GPP. In addition to this, the focal distance of the camera should be taken into account. Thus, the quality of a triangle T j = { k , l , t } is proposed as follows: (11) q ( T { k , l , t } ) = f l D { k , l , t } , where f l is the focal distance of camera index l and D { k , l , t } is the distance between the camera l and the GPP k computed at time t . When the insertion of a candidate triangle is considered to be beneficial, the next step is to execute the insertion in the global mesh. The global primitive mesh is built as a constrained Delaunay triangulation. Hence, a description of the mesh contains a set of vertices, edges and constraints (implementation from [22] is used). After insertion, the candidate triangle should be preserved on the updated mesh (since it had a larger quality than any global mesh triangle it overlapped). The configuration of all other non overlapped global mesh triangles should also be preserved. In order to comply with those objectives, the insertion of a candidate triangle is composed of a set of atomic operations executed in sequence. Fig. 6 will be used to demonstrate why the proposed set of operations is required, by comparing it to other possibilities. Fig. 6(a) shows the initial situation: an existing mesh in blue must be altered by the insertion of a candidate triangle in red. The blue triangle has all three edges constrained (blue squares, in Fig. 6(a)). Let insert ( V , M ) be a function that inserts vertex V into mesh M . (12) M ∗ = insert ( V , M ) , ∀ V ∈ { V a , V b , V c } , where M ∗ is the updated mesh. Fig. 6(b) shows the updated mesh after the insertion of the three vertices, indices 4 , 5 and 6 (see vertices indices in the Fig. 6). The updated mesh does not preserve the configuration of the candidate triangle. In other words, there is no triangle with vertices 4 , 5 , 6 in the updated mesh. The expression that asserts if the configuration of the candidate triangle is preserved can be stated as follows: (13) { preserve T { V a , V b , V c } j , if ( ∃ M { V d , V e , V f } ∗ ∈ M ∗ ) : V d = V a ∧ V e = V b ∧ V f = V c do not preserve T { V a , V b , V c } j , otherwise . One of the reasons why the simple insertion of the vertices does not work is that the existing mesh had some constrained edges. After the mesh is updated, these constraints continue to exist (see squares on edges 1–2, 2–3, and 1–3 in Fig. 6(b)). The configuration of the candidate triangle is not kept because no constraints over the edges of that triangle are set. Hence, the second alternative is to execute an additional operation on top of the insertion of vertices V a , V b and V c . Let add_constraint ( e , M ) be a function that adds a constraint on edge e . The operation can be expressed as follows: (14) M ∗ = add_constraint ( e , M ) , ∀ e ∈ { V a – V b , V b – V c , V a – V c } where V a – V b denotes the edge defined between vertices V a and V b . Fig. 6(c) shows the updated mesh after this procedure is executed. Also in this case the configuration of the candidate triangle is not preserved. The reason is that there are conflicting constraints inserted in the mesh. For example, initially, the global mesh had a constraint over edge 1–2 (see indices in Fig. 6). At the same time the constraint V a – V b is inserted into the mesh. Since these two constraints intersect, a new vertex is created at the intersection point (vertex 7). Since a vertex is created at the intersection of the two initial constrained edges, four new edges are created (edges 4–7, 7–8, 1–7 and 7–10). All of these edges are constrained. From Fig. 6(c), one can see that the overall result of this approach is that neither the candidate triangle nor the existing mesh is preserved. The reason is that contradictory (intersecting) constraints are inserted in the mesh. The solution is to remove the constraints from edges in the global mesh that intersect edges from the candidate triangle, prior to inserting the vertices and constraints of the candidate triangle. Let E = { e 0 , e 1 , … , e N , } be the list of the global mesh constrained edges that intersect any of the candidate triangle’s edges, and remove_constraint ( e , M ) a function that removes the constraint from edge e in the mesh M. The prepared mesh M ′ is obtained as follows: (15) M ′ = remove_constraint ( e , M ) , ∀ e ∈ E , and after this, the operations described in Eqs. (12) and (14) are executed. Fig. 6(d) shows the results of this approach. The mesh preparation stage detected the following intersections (indices in Fig. 6(a) and (d)): V a – V b intersects with V 1 – V 2 , V a – V b intersects with V 1 – V 3 , V b – V c intersects with V 1 – V 2 and V b – V c intersects with V 1 – V 3 . As a result, the constraints of edges V 1 – V 2 and V 2 – V 3 are removed. Note that in Fig. 6(d), the prepared mesh (not the initial global mesh) is shown in blue, and those constraints no longer appear. More important, the candidate triangle’s configuration is preserved (triangle 4–5–6). In this particular case, the initial configuration of the mesh is lost, since there was overlap between the candidate triangle and the initial mesh triangle. We now show an example of continuous update of the global mesh: three new projections ( C j = 1 , C j = 2 , C j = 3 ) are available to update to the initial mesh M . The projections are mapped sequentially, generating updated meshes M ∗ , M ∗ ∗ , etc. Each projection contains a single triangle to map to the global mesh. Triangles T { V a , V b , V c } 1 , T { V d , V e , V f } 2 and T { V g , V h , V i } 3 , correspond to projections C j = 1 , C j = 2 , C j = 3 , respectively. The quality of the triangles is such that the following holds: (16) q ( M n ) < q ( T { V a , V b , V c } 1 ) < q ( T { V d , V e , V f } 2 ) < q ( T { V g , V h , V i } 3 ) ∀ M n ∈ M , and the mesh update cost parameter is α = 1 , which means that there is no cost associated to the updating of the mesh (see Eq. (10)). In other words, the insertion of all three candidate triangles is considered beneficial. The initial mesh is shown in Fig. 7 (a), along with the three candidate triangles. Fig. 7(b) shows the mesh after the insertion of the first candidate triangle, i.e., M ∗ . Since there is no overlap, the candidate triangle is added to the mesh M { 4 , 5 , 8 } ∗ , and edges M { 4 – 5 } ∗ , M { 5 – 8 } ∗ , and M { 4 – 8 } ∗ are constrained. Also, since there was no overlap detected, the initial configuration of the mesh is preserved. The result of the second insertion is shown in Fig. 7(c). In this case, there is overlap between candidate triangle T { V d , V e , V f } 2 (seen in Fig. 7(a)) and triangle M { 2 , 5 , 7 } ∗ (seen in Fig. 7(b)). An intersection between edges V d – V e and edge M { 5 – 7 } ∗ (seen in Fig. 7(b)), is detected. As a result, the constraint from edge M { 5 – 7 } ∗ is removed. The insertion results in a new triangle M { 9 , 10 , 11 } ∗ ∗ . Note also that the overlapping triangle M { 2 , 5 , 7 } ∗ was not preserved, i.e., it does not exist in the new mesh M ∗ ∗ . Finally, the third insertion detects that triangle T { V g , V h , V i } 3 overlaps triangles M { 3 , 4 , 5 } ∗ ∗ , M { 4 , 5 , 8 } ∗ ∗ and M { 2 , 3 , 5 } ∗ ∗ . Edges M { 3 – 4 } ∗ ∗ , M { 4 – 5 } ∗ ∗ and M { 3 – 5 } ∗ ∗ intersect the edges of T { V g , V h , V i } 3 which is why their constraints are removed (actually, in this case they disappear after the candidate triangle is inserted). The insertion of candidate triangles sometimes creates not only the candidate triangle itself, but also some additional triangles on the mesh. It is the case, for example, of triangle M { 7 , 9 , 10 } ∗ ∗ . We refer to this type of triangles as orphan triangles, meaning they have no parent projection. These are shown in grey color in Fig. 8 . Unlike triangles with parent projections, these triangles do not belong to any projection and thus they do not derive from the DDT triangulation executed over an image of some projection. Because of this, there is no guarantee that these orphan triangles are compliant with edges in the projection images. For this reason, we propose that orphan triangles are set to have the quality 0. In summary, this approach for the update of a global primitive mesh consists of a set of procedures that are capable of updating the mesh whenever new, better quality triangles are available for insertion, but at the same time the mechanism is capable of filling the gaps left empty using orphan triangles. 4 Results This section shows results both from one-shot texture mapping using DDTs, as well as results from the algorithm proposed to conduct incremental texture mapping. 4.1 One-shot texture mapping Fig. 9 (a) shows the detection of line segments in an image. Fig. 9(b) displays the result of a Delaunay triangulation with arbitrary configuration, e.g. computed by giving only the vertices as input (green dots in Fig. 9(a)). Because the triangulated mesh has an arbitrary configuration, triangle often contain areas with multiple textures. This would cause problems when using affine texture mapping. Notice the large triangle that covers part of the roof of the building, as well as a portion of the sky. This triangle contains a significant change in color and thus its affine texture mapping would result inaccurate. Fig. 9(c) shows the result of the proposed DDT approach, where a constrained Delaunay triangulation is used. This triangulation is computed using as input the vertices as in the previous case but also the detected line segments (red lines in Fig. 9(a), constrained edges also shown in (c) with red lines). In this case, the large triangle described above does not exist. In fact, there are no triangles which contain both sky and roof. Thus, we can argue that the proposed approach creates a mesh in which triangles contain smooth color transitions. The next section addresses the incremental update of these triangulation meshes. 4.2 Incremental texture mapping To show the results of incremental texture mapping, we recover the example of Section 1 (see Fig. 1): the vehicle approaches a wall panel, which has the word START written on it and collects four images in sequence (color coded black-red-orange-yellow in Fig. 10 ). The global mesh is created with the first image and then updated three times. The global triangulated mesh at each iteration is shown in Fig. 10 (top). Textures for each of these cases are show in Fig. 10 (bottom). Projection C { k = 4 , l = 1 , t = t 1 } is used to create the global mesh. Thus, the global mesh is composed only of triangles with parent projection C { k = 4 , l = front center , t = t 1 } (black triangles in Fig. 10(a)). Then, a new projection C { k = 4 , l = 1 , t = t 2 } becomes available. The global mesh is updated (Fig. 10(b)), and now contains a majority of triangles from C { k = 4 , l = 1 , t = t 2 } (red triangles). Then projection C { k = 4 , l = 1 , t = t 3 } is mapped. Since only a right side portion of the primitive is seen, orange triangles can be observed on the right side of Fig. 10(c)), while the left side retains red colored triangles from previous projections. Orphan triangles (in blue) are generated to fill the gaps that appear between the triangles with parent projections. Finally, projection C { k = 4 , l = 3 , t = t 4 } is used. This image views only the left portion of the wall panel. As such, we can see yellow triangles on the left side of (Fig. 10(d)). This example shows how the proposed mechanism is capable of creating and maintaining a global triangulated mesh which is used for enhancing the texture mapped onto the GPPs whenever new (and better) images are collected. 4.3 Projection of a single camera onto the ground plane This section shows three examples of how the global primitive mesh evolves when using a single camera to map a single primitive. We consider a similar scene to the one presented in Fig. 1. Throughout the three time instants t = t 1 , t = t 2 and t = t 3 , the vehicle is moving forward. From t 1 to t 2 the vehicle drives straight, and from t 2 to t 3 the vehicle turns slightly to the right. In this case the primitive that represents the ground plane is used for texture mapping ( k = 0 ). As a consequence, there is always a portion of the images from the projections that view the ground. In other words, at all instants any of the cameras view a portion of the ground, since they are pointed downwards. We will consider three different cases, each generating a unique scene representation: In the first case only the front center camera ( l = 1 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 1 , t = t 1 } , C { k = 0 , l = 1 , t = t 2 } and C { k = 0 , l = 1 , t = t 3 } ; In the second case only the rear center camera ( l = 4 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 4 , t = t 1 } , C { k = 0 , l = 4 , t = t 2 } and C { k = 0 , l = 4 , t = t 3 } ; In the third case only the front left camera ( l = 3 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 3 , t = t 1 } , C { k = 0 , l = 3 , t = t 2 } and C { k = 0 , l = 3 , t = t 3 } . The final global primitive meshes (those obtained after inserting projections at times t 1 , t 2 and t 3 ) for each case are displayed in Fig. 11 (left column). Fig. 11 (right column) shows the percentage of triangles of the global mesh that belong to each projection, as a function of the mission time. Note that the final position of the vehicle (which is the same for all cases) is depicted in the images, and bear in mind that, during this sequence, the vehicle moves forward from the right to the left. The triangles of the global primitive mesh are shown in colors, where each color corresponds to a particular projection. Fig. 11 (first row, left) shows the distribution of triangles according to the parent projection. In this case, the images are provided by the front center camera. As the vehicle moves forward, the ground in front of the vehicle that has been previously mapped by previous projections is now visible in images at a closer range. This leads to the effect that more recent projections tend to override older projections, i.e., red color ( t 2 ) overrides black color ( t 1 ), and yellow color ( t 3 ) overrides the other two. Fig. 11 (first row, right) shows that at time t 1 , only triangles from the first projection (black) and orphan triangles (blue) exist. Then, at time t 2 , the triangles from the second projection (red) are added to the global mesh. As a consequence, the percentage of triangles from the first projection (black) decreases. At time t 3 , the third projection again takes the major slice of percentage with respect to the previous two projections. In front facing cameras, when the vehicle is moving forward, more recent projections tend to contribute with a larger portion of the total triangles in the global mesh. The second case is shown in 11 (second row, left). Here, since the camera is facing the rear side of the vehicle, the opposite phenomena occurs: the vehicle is moving away from the ground behind it, and thus older projections were taken at closer distances to the ground. As a consequence, the red color (projection at t 2 ) overrides the yellow color (projection at t 3 ), and the black color (projection at t 1 , the oldest one) overrides all others. This is observable in Fig. 11 (second row, right), where the first projection (black) is, at all times, the one with the largest percentage of the triangles. Fig. 11 (third row, left) shows the third case. Here, since the camera is facing the left side of the vehicle, a hybrid phenomena takes place. For each projection, there is always a portion of the triangles, i.e., those that map the ground directly in front of the camera for that particular instant, that have a higher projection quality when compared to others. Fig. 11 (third row, right) also shows this tendency: the percentage of projections tends to be the same for all projections, which is why the second projection (red) when first mapped at time t 2 achieves approximately the same percentage of triangles as the first projection (black). They continue to have similar percentages also at time t 3 . At time t 3 , the third projection (yellow) obtained a higher value of percentage because the vehicle as turned slightly to the right and the left camera faced an area of the ground that was not previously mapped by any of the previous projections. 4.4 Projection of multiple cameras onto the ground plane The examples given in Section 4.3 have shown that the proposed algorithm is capable of handling multiple projections, correctly determining which are the best quality projections to map onto the global mesh. Nonetheless, those examples were simplified since only one camera was considered to provide projections in each case. In this section, the five cameras onboard the Talos are used to provide projection to be mapped onto the ground plane. The same sequence is used: the vehicle is moving forward and three time instants are used to generate projections. Each time instant t 1 , t 2 and t 3 generates five projections, one for each camera. Fig. 12 (a) shows the state of the global mesh after time t 1 . Five projections are contained in the mesh. At time t 2 , the global mesh incorporates many of the projections that are computed at this time (Fig. 12(b)). The same occurs at time t 3 (Fig. 12(c)). Note that these images are not exactly the same as those in Fig. 11, because in that case only the final global projection mesh was shown for three different examples. Here, we show the state of a single global projection mesh at times t 1 , t 2 and t 3 . Thus, in this case it is possible to see how the mesh evolved as more projections became available. The resulting mesh is an intricate mosaic of triangles coming from several projections. At time t 1 , the area of projection from the rear center camera was not connected to the areas of projection of the other cameras. Note that the red triangles in Fig. 12(a) are not connected to any triangle with a parent projection, only to orphan triangles. This unmapped region corresponds to the ground that was below the vehicle at time t 1 . Obviously, there is no coverage from the cameras for that area, so the system handles this by defining orphan triangles (blue) to cover that area. At time t 2 , the vehicle has moved forward, and the uncovered ground is now visible from the rear camera. Hence, the areas mapped by the rear cameras connect to the areas mapped by the other cameras, as seen in Fig. 12(b). At time t 3 , since the vehicle has turned to the right, the rear camera now views a different portion of the ground that had not been captured by any other camera. Note, in Fig. 12(c), how the triangles of the rear camera (the brightest yellow at the bottom right side) map a region that was not seen before and was previously covered only by orphan (blue) triangles. Fig. 12(d) shows the percentage of triangles of each projection as a function of the mission time. As each time instant, only newly acquired projections are used to update the mesh. Hence, triangles from previous iterations, if removed, will not be retested for insertion. That means each triangles is tested for insertion a single time. If a triangle is removed, it will never again be reinserted. This can be observed in the Figure, since none of the projections increases the percentage of triangles it contains. Fig. 13 shows the fifteen images used to compute these representations. As the vehicle moves and turns around, more and more of the ground that had not been viewed before is covered by new projections. This is a clear example of why integrating several projections over time is advantageous. A composite photometric description of the environment can be obtained that was impossible to compute without the capability of integrating multiple projections over time in an incremental fashion. The incremental texture mapping of an entire scenario can be observed in The scenario is composed of the entire (MIT) sequence. All five cameras onboard the Talos vehicle are used as input to the texture mapping. Geometric primitives are represented in the environment by the blue–green polygons. A blue to green colormap is used to color the primitives according to their index, the more recently detected the primitive, the closer to green it is. Photometry is represented by the texture mapped onto the primitives. Note that at each of the time instants new projections will update the global meshes of the detected polygonal primitives. Hence the scenario representation will evolve photometrically over time. Furthermore, also the geometric representation will evolve over time (see [2] for details). For a better visualization, the primitive that represents the ground plane is not textured in the video. 5 Conclusions This paper addressed the problem of how to create and update a triangulated mesh. These meshes are used for texture mapping surfaces in 3D, and the input are images collected from cameras mounted on-board a vehicle. The geometric structure onto which texture is mapped is described in detail in [2] and given as a list of polygons. Because the atomic entities of the 3D structure are defined as polygons (rather than triangles), it is possible to perform a triangulation of the convex hull of that polygon, as opposed to having an arbitrary triangulation. This triangulation is computed in the image space, and is defined as a constrained Delaunay triangulation. This makes it possible to impose line segments as constrained edges in the triangulation, which creates triangles with smooth color transitions. This, in turn, makes it possible to use affine texture mapping. Incremental texture mapping is done by creating and updating a global triangulated mesh per geometric primitive. The update of this mesh is done using meshes created from projections. In this paper, we have proposed a sequence of operations which are used for inserting triangles from the projection mesh into the global triangulation mesh. This procedure ensures that the inserted triangles maintain their configuration as well as the existing triangles which do not overlap the inserted triangles. Furthermore, the proposed algorithm fills the gaps in the mesh where there are to triangles with parent projections with orphan triangles. Using this mechanism, the holes that could exist between textures of different projections are replaced by orphan triangles where texture is interpolated, resulting in a better overall quality of the texture. To the best of our knowledge, this is the first approach in this field capable of fusing images continuously and in an incremental fashion in order to generate a single texture of good quality. Acknowledgments This work has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”, reference CEB-02502014. References [1] M. Oliveira V. Santos A.D. Sappa P. Dias Scene representations for autonomous driving: An approach based on polygonal primitives Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 1 2016 Springer International Publishing Cham 503 515 (Chapter) [2] M. Oliveira V. Santos A. Sappa P. Dias A.P. Moreira Incremental scenario representations for autonomous driving using geometric polygonal primitives Robot. Auton. Syst. 2016 10.1016/j.robot.2016.05.011 in press [3] A.S. Huang M. Antone E. Olson L. Fletcher D. Moore S. Teller J. Leonard A High-rate, Heterogeneous data set from the DARPA urban challenge Int. J. Robot. Res. 29 13 2011 1595 1601 [4] A. Huang, E. Olson, D. Moore, LCM: Lightweight communications and marshalling, in: 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, 2010, pp. 4057–4062. [5] M. Segal C. Korobkin R. van Widenfelt J. Foran P. Haeberli Fast shadows and lighting effects using texture mapping SIGGRAPH Comput. Graph. 26 2 1992 249 252 [6] P. Debevec Y. Yu G. Boshokov Efficient view-dependent image-based rendering with projective texture-mapping, Tech. Rep. 1998 [7] S. Rippa Long and thin triangles can be good for linear interpolation SIAM J. Numer. Anal. 29 1 1992 257 270 [8] M. Garland P.S. Heckbert Fast polygonal approximation of terrains and height fields, Tech. Rep. CMU-CS-95-181 1995 [9] R. Schätzl H. Hagen J.C. Barnes B. Hamann K.I. Joy Data-dependent triangulation in the plane with adaptive knot placement Geometric Modelling 2001 309 321 [10] H. Hoppe, Progressive meshes, in: Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96, 1996, pp. 99–108. [11] L. Demaret N. Dyn A. Iske Image compression by linear splines over adaptive triangulations Signal Process. 86 7 2006 1604 1616 [12] A.D. Sappa M.A. García Coarse-to-fine approximation of range images with bounded error adaptive triangular meshes J. Electron. Imaging 16 2 2007 23010 [13] N. Dyn D. Levin S. Rippa Boundary correction for piecewise linear interpolation defined over data-dependent triangulations J. Comput. Appl. Math. 39 2 1992 179 192 [14] L.L. Schumaker Computing optimal triangulations using simulated annealing Comput. Aided Geom. Design 10 3–4 1993 329 345 [15] Opengl D. Shreiner M. Woo J. Neider T. Davis OpenGL(R) Programming Guide: The Official Guide to Learning OpenGL(R), Version 2 fifth ed. 2005 Addison-Wesley Professional [16] D. Blythe The direct3d 10 system ACM Trans. Graph. 25 3 2006 724 734 [17] B. Lehner G. Umlauf B. Hamann Survey of techniques for data-dependent triangulations H. Hagen M. Hering-Bertram C. Garth GI Lecture Notes in Informatics, Visualization of Large and Unstructured Data Sets 2007 178 187 [18] M. Cervenansksy Z. Toth J. Starinsky A. Ferko M. Sramek Parallel gpu-based data-dependent triangulations Comput. Graph. 34 2 2010 125 135 [19] I. Svalbe Natural representations for straight lines and the hough transform on discrete arrays IEEE Trans. Pattern Anal. Mach. Intell. 11 9 1989 941 950 [20] V. Kamat, S. Ganesan, A robust hough transform technique for description of multiple line segments in an image, in: 1998 International Conference on Image Processing, 1998. ICIP 98. Proceedings. Vol. 1, 1998, vol. 1, pp. 216–220. [21] R. Guerreiro, P. Aguiar, Incremental local hough transform for line segment extraction, in: 2011 18th IEEE International Conference on Image Processing, ICIP, 2011, pp. 2841–2844. [22] M. Yvinec 2D triangulations CGAL User and Reference Manual, 4.0 Edition 2012 CGAL Editorial Board [23] L.P. Chew, Constrained delaunay triangulations, in: Proceedings of the Third Annual Symposium on Computational Geometry, SCG ’87, 1987, pp. 215–222. [24] J.R. Shewchuk General-dimensional constrained delaunay and constrained regular triangulations i: Combinatorial properties Discrete Comput. Geom. 39 1 2008 580 637 [25] T. Moller A fast triangle-triangle intersection test J. Graph. Tools 2 1997 25 30 [26] J.-W. Chang M.-S. Kim Efficient triangle–triangle intersection test for OBB-based collision detection Comput. Graph. 33 3 2009 235 240 [27] A.D. Sappa, M.A. García, Incremental multiview integration of range images, in: ICPR, 2000, pp. 1546–1549. [28] E. Fogel R. Wein B. Zukerman D. Halperin 2D regularized Boolean set-operations CGAL User and Reference Manual, 4.0 Edition 2012 CGAL Editorial Board Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Vítor Santos obtained a 5 year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990 1994 at the Joint Research Center, Italy. He is currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or cosupervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He is also cofounder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Angel Domingo Sappa (S’1994–M’00–SM’12) received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, where he is currently a Senior Researcher. He is a member of the Advanced Driver Assistance Systems Group. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereoimage processing and analysis, 3D modelling, and dense optical flow estimation. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. António Paulo Moreira graduated with a degree in electrical engineering at the University of Oporto, in 1986. He then pursued graduate studies at University of Porto, obtaining a M.Sc. degree in electrical engineering—systems in 1991 and a Ph.D. degree in electrical engineering in 1998. Presently, he is an Associate Professor at the Faculty of Engineering of the University of Porto and researcher and manager of the Robotics and Intelligent Systems Centre at INESC TEC. His main research interests are process control and robotics. "
    },
    {
        "doc_title": "Incremental scenario representations for autonomous driving using geometric polygonal primitives",
        "doc_scopus_id": "85006515693",
        "doc_doi": "10.1016/j.robot.2016.05.011",
        "doc_eid": "2-s2.0-85006515693",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3d representations",
            "Autonomous Vehicles",
            "Geometric structure",
            "Point cloud",
            "Polygonal primitives",
            "Range measurements",
            "Reconstruction techniques",
            "Scene reconstruction"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.When an autonomous vehicle is traveling through some scenario it receives a continuous stream of sensor data. This sensor data arrives in an asynchronous fashion and often contains overlapping or redundant information. Thus, it is not trivial how a representation of the environment observed by the vehicle can be created and updated over time. This paper presents a novel methodology to compute an incremental 3D representation of a scenario from 3D range measurements. We propose to use macro scale polygonal primitives to model the scenario. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Furthermore, we propose mechanisms designed to update the geometric polygonal primitives over time whenever fresh sensor data is collected. Results show that the approach is capable of producing accurate descriptions of the scene, and that it is computationally very efficient when compared to other reconstruction techniques.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-06-07 2016-06-07 2016-08-04 2016-08-04 2016-08-04T18:32:25 S0921-8890(16)30060-4 S0921889016300604 10.1016/j.robot.2016.05.011 S300 S300.1 FULL-TEXT 2020-11-22T06:55:28.343358Z 0 0 20160901 20160930 2016 2016-06-07T15:16:25.691999Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid orcid primabst ref vitae 0921-8890 09218890 true 83 83 C Volume 83 27 312 325 312 325 201609 September 2016 2016-09-01 2016-09-30 2016 article fla © 2016 Elsevier B.V. All rights reserved. INCREMENTALSCENARIOREPRESENTATIONSFORAUTONOMOUSDRIVINGUSINGGEOMETRICPOLYGONALPRIMITIVES OLIVEIRA M 1 Introduction 2 Related work 3 Proposed approach 3.1 One-shot scene reconstruction 3.2 Incremental scenario reconstruction 4 Results 4.1 One-shot reconstruction 4.2 Incremental reconstruction 4.3 Comparison of approaches with and without expansion 5 Conclusions Acknowledgments References BIRK 2009 53 60 A BURGARD 2009 757 758 W OLIVEIRA 2016 M HUANG 2011 1595 1601 A ONIGA 2010 1172 1182 F ZHOU 2011 669 681 K THRUN 2006 661 692 S URMSON 2006 467 508 C URMSON 2008 425 466 C MONTEMERLO 2008 M BACHA 2008 467 492 A CHEN 2011 762 775 Y DEMEDEIROSBRITO 2008 1130 1140 A BERNARDINI 1999 349 359 F BYKAT 1978 296 298 A BARBER 1996 469 483 C AICHHOLZER 1995 752 761 O OLIVEIRAX2016X312 OLIVEIRAX2016X312X325 OLIVEIRAX2016X312XM OLIVEIRAX2016X312X325XM 2018-08-04T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. 2020-11-19T10:33:14.232Z S0921889016300604 Portuguese Foundation for Science and Technology FCT Fundação para a Ciência e a Tecnologia ERDF European Regional Development Fund FEDER European Regional Development Fund Spanish Government TIN2014-56919-C3-2-R Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación CEB-02502014 SENESCYT Secretaría de Educación Superior, Ciencia, Tecnología e Innovación Operational Programme for Competitiveness and Internationalisation Fundação para a Ciência e Tecnologia POCI-01-0145-FEDER-006961 SFRH/BD/43203/2008 UID/CEC/00127/2013 SFRH/BPD/109651/2015 INCT-EN Instituto Nacional de Ciência e Tecnologia para Excitotoxicidade e Neuroproteção COMPETE POFC Programa Operacional Temático Factores de Competitividade This work has been supported by the Portuguese Foundation for Science and Technology “ Fundação para a Ciência e Tecnologia ” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador ”, reference CEB-02502014 . item S0921-8890(16)30060-4 S0921889016300604 10.1016/j.robot.2016.05.011 271599 2016-08-04T15:40:51.774542-04:00 2016-09-01 2016-09-30 true 5888810 MAIN 14 58041 849 656 IMAGE-WEB-PDF 1 fx5 12830 69 83 gr4 17555 76 219 gr2 28101 119 219 fx1 23862 150 219 gr14 15759 58 219 fx6 24677 164 125 fx3 12077 56 49 pic5 33838 164 140 pic2 25222 164 141 gr15 17370 87 219 gr8 14416 27 219 fx4 12819 69 83 gr12 27038 127 219 gr1 21146 87 219 fx2 12077 56 49 gr5 22864 164 134 gr13 16887 164 205 pic3 23197 163 140 gr7 30380 136 219 gr6 53566 152 219 gr9 16879 87 219 pic4 25191 164 141 gr11 29235 115 219 pic1 24035 163 140 gr16 24620 131 219 gr3 18915 65 219 gr10 22449 111 219 fx5 29274 15 18 gr4 68423 200 580 gr2 68835 205 376 fx1 97662 388 565 gr14 73492 200 757 fx6 187395 742 566 fx3 28364 13 11 pic5 40387 131 112 pic2 38463 131 113 gr15 65788 226 572 gr8 45793 78 631 fx4 29234 15 18 gr12 149219 356 616 gr1 83494 246 619 fx2 28364 13 11 gr5 160412 724 591 gr13 51934 475 593 pic3 36162 132 113 gr7 132506 412 662 gr6 89241 236 339 gr9 61854 228 573 pic4 34967 131 113 gr11 148501 300 572 pic1 37492 132 113 gr16 101169 373 625 gr3 81281 161 545 gr10 75175 289 570 si25 624 18 180 si21 159 15 10 si46 131 11 12 si4 139 12 10 si14 135 12 10 si10 112 11 6 si26 165 17 13 si15 182 15 22 si11 574 21 138 si12 152 11 15 si9 180 18 14 si51 661 21 142 si54 574 15 150 si2 165 13 17 si48 1945 41 389 si16 225 15 35 si34 133 11 11 si18 735 31 184 si1 309 18 50 si47 344 15 58 si41 117 10 11 si6 397 18 68 si27 143 13 12 si13 166 15 18 si36 159 15 11 si42 131 15 11 si5 318 18 49 si49 207 12 25 si3 114 10 8 si50 165 11 21 si24 174 14 22 si35 170 14 19 si22 195 14 27 si55 568 15 149 si45 139 11 13 si33 123 8 10 si32 120 8 10 si19 844 12 234 ROBOT 2641 S0921-8890(16)30060-4 10.1016/j.robot.2016.05.011 Elsevier B.V. Fig. 1 Plane detection examples using RANSAC: (a) five best RANSAC candidates for the input point cloud in grey; (b) a detail of (a); (c) without using clustering; (d) using clustering. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Scenario reconstruction approach (1): BPA surface reconstruction over the accumulated point cloud of three scenes. Fig. 3 Scenario reconstruction approach (2): Accumulation of scene reconstructions over multiple scenes, each marked with a different color; (a) BPA; (b) GPP 2, shown without the ground plane for an easier visualization. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Orthogonal component of the expansion operation: (a) points are tested for their orthogonal distance to the support plane; (b) included points are projected to the support plane. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Successive iterations of the longitudinal expansion of GPPs. Solid lines in color represent the convex hull at the start of a given iteration, dashed lines the expanded polygon. Crosses over points mean they were added to the polygon in a given expansion: (a) initial situation; (b), (c), (d), (e) and (f) iterations 0–4, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 MIT sequence. Fig. 7 Location C of the MIT sequence: (a) 3d view; (b) top view; (c) satellite view of the location; (d) front 6 mm camera; (e) front; (f) rear; (g) left (h) right. Fig. 8 Detection of geometric polygonal primitives in the data-sets of the MIT sequence: (a) location C; (b) location D. Fig. 9 Cascade processing analysis for MIT sequence locations A through E: (a) the number of points left to process for a given input point cloud, as a function of the index of the detected primitive; (b) the time it takes to perform the detection of each of the geometric polygonal primitives as a function of the primitives index. Fig. 10 Reconstruction of location E of MIT sequence: (a) BPA; (b) GT; (c) POIS; (d) GPP2. The observation of Fig. 16(e) may help the reader better understand the viewpoints in these images. Fig. 11 Qualitative analysis of the one sided Hausdorff distance in location C sequence 1: (a) GT; (b) POIS; (c) GPP 1; (d) GPP 2; A Red–Green–Blue color map is used to code the distance. Red represents zero distance and blue maximum distance. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Results from the Hausdorff distance obtained when using alternatives for the GPP 2 method for location E, sequence 1: (a) the standard GPP 2, with ground plane and convex hull; (b) discarded ground plane, convex hull; (c) with ground plane, concave hull; (d) discarded ground plane, concave hull. A Red–Green–Blue color map is used to code the distance. Red represents zero distance and blue maximum distance. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 GPP reconstruction for sequence 1: (a) The reconstructed scenario after the input point cloud of location A is received; (b) after location B; (c) after location C; (d) after location D; (e) after location E. Fig. 14 (a) Comparison between the number of polygons generated by the algorithm using expansion and not using expansion through sequence 1; (b) number of points to use as input to the detection in both cases; (c) Total area of the detected polygons. Fig. 15 Analysis of the evolution of each of the polygons through sequence 1. Only pair index polygons are shown to simplify the graphs: (a) number of support points per polygon; (b) total area of the primitives. Fig. 16 Evolution of polygonal primitive 0 (the ground plane) through sequence 1: (a) location A; (b) location B; (c) location C; (d) location D; (e) location E. Table 1 LIDAR sensor systems mounted on some autonomous vehicle systems of recent years. Institution Vehicle Ref. 3D sensor type Total a 2D laser 3D laser Stanford U. Stanley b [11] 5 × Sick LMS 291 – 67.5 CMU Sandstorm b [12] 3 × Sick LMS 291 – 50.5 Highlander b Riegl Q140i – CMU Boss b [13] 6 × Sick LMS 291 Vel. HDL-64 2305.0 2 × Continental ISF 172 – 2 × IBEO Alasca XT – Stanford U. Junior c [14] 4 × SICK LMS 291 Vel. HDL-64 2278.0 2 × IBEO Alasca XT – Virginia Tech Odin c [15] 4 × Sick LMS 291 – 90.0 2 × IBEO Alasca XT – IBEO Alasca AO – MIT Talos c [6] 12 × Sick LMS 291 Vel. HDL-64 2361.2 U. Munich MuCar-3 d [16] – Vel. HDL-64 2200.0 Google Driverless Car [17] – Vel. HDL-64 2200.0 a Estimation of total 3D data throughput of all LIDAR sensors mounted on the vehicle, given as points × 10 3 / s . b These vehicles participated in the Defense Advanced Research Projects Agency (DARPA) Grand Challenge 2006. c These vehicles participated in the DARPA Urban Challenge 2007. d This vehicle participated in the Civilian European Land Robot Trial (ELROB) Trial 2009. Table 2 Information on each of the locations defined in the MIT sequence. Columns description: pt , number of points; size, memory size in mega bytes; t , mission time in seconds; d , traveled distance in meters. Location name Location snapshot Sequence accumulated pt ( × 10 6 ) Size (MB) a pt ( × 10 6 ) Size (MB) a t (s) d (m) A 1.3 15.6 1.3 15.6 1 0 B 1.3 15.6 13.0 156.0 11 75 C 1.3 15.6 26.0 312.0 21 125 D 1.3 15.6 39.0 468.0 31 140 E 1.3 15.6 52.0 624.0 41 190 a Computed from the number of points times the three xyz dimensions times the four bytes for each dimension (type float32). It is an approximate value since additional data is present in the message, such as the time stamp, the coordinate frame identification, etc. Table 3 Comparison of the computation time of several approaches for surface reconstruction on the MIT sequence. Results were obtained with an i7-860 2.8 GHz quad core processor, Ubuntu operating system. Sequence/Location Processing time (s) BPA GT POIS GPP 1 GPP 2 A 659.0 154.0 63.2 16.3 27.3 B 752.9 157.5 61.6 25.3 17.4 C 488.2 156.3 56.3 13.5 49.4 D 480.4 142.4 52.6 25.2 25.2 E 558.8 149.0 57.9 47.4 58.1 Average 585.9 151.8 58.3 25.5 35.5 Table 4 Comparison of the accuracy of the several approaches using BPA results as ground truth and Hausdorff distance as metric. Location Hausdorff distance (m) GT POIS GPP 1 GPP 2 max mean max mean max mean max mean A 11.7 0.15 14.0 1.39 7.6 1.02 7.6 0.87 B 11.8 0.12 14.1 1.39 12.7 0.94 12.6 0.81 C 12.7 0.18 13.9 1.06 8.9 0.87 8.9 0.69 D 13.8 0.10 13.9 1.90 7.6 0.86 7.6 0.69 E 12.5 0.14 14.0 1.42 14.0 1.25 14.0 1.11 Average 12.5 0.14 13.9 1.43 10.2 0.99 10.1 0.83 Table 5 Comparison of the Hausdorff distance accuracy of the GPP 2 approach using: the standard approach, convex hull and ground plane included (also in Table 4); the convex hull with no ground plane included; the concave hull with ground plane; and the concave hull without ground plane. GPP 2 Hausdorff distance (m) Hull Convex Convex Concave Concave Ground plane Included Not included Included Not included Location max mean max mean max mean max mean A 7.6 0.87 1.8 0.15 6.8 0.71 1.2 0.13 B 12.6 0.81 1.5 0.11 12.6 0.53 1.1 0.08 C 8.9 0.69 1.9 0.16 6.6 0.52 1.9 0.12 D 7.6 0.69 2.2 0.14 7.3 0.59 2.1 0.11 E 14.0 1.11 1.7 0.10 8.8 0.32 1.4 0.08 Average 10.1 0.83 1.8 0.13 8.4 0.53 1.5 0.10 Table 6 Online videos showing the system running for the MIT sequence. The non incremental approach corresponds to the continuous processing without using the expansion mechanism. It is possible to observe that because there is no expansion, the primitives are duplicated. URL Description Input data from MIT sequence Incremental approach Non incremental approach Incremental scenario representations for autonomous driving using geometric polygonal primitives Miguel Oliveira a b ⁎ Vitor Santos b Angel D. Sappa c d Paulo Dias b A. Paulo Moreira a e a INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal b IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal c Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica del Litoral, ESPOL, Campus Gustavo Galindo, Km 30.5 vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica del Litoral, ESPOL, Campus Gustavo Galindo Km 30.5 vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador d Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center, Campus UAB Bellaterra Barcelona 08193 Spain e FEUP - Faculty of Engineering, University of Porto, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal FEUP - Faculty of Engineering, University of Porto R. Dr. Roberto Frias s/n Porto 4200-465 Portugal ⁎ Corresponding author at: INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal. INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal When an autonomous vehicle is traveling through some scenario it receives a continuous stream of sensor data. This sensor data arrives in an asynchronous fashion and often contains overlapping or redundant information. Thus, it is not trivial how a representation of the environment observed by the vehicle can be created and updated over time. This paper presents a novel methodology to compute an incremental 3D representation of a scenario from 3D range measurements. We propose to use macro scale polygonal primitives to model the scenario. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Furthermore, we propose mechanisms designed to update the geometric polygonal primitives over time whenever fresh sensor data is collected. Results show that the approach is capable of producing accurate descriptions of the scene, and that it is computationally very efficient when compared to other reconstruction techniques. Keywords Incremental scene reconstruction Point clouds Autonomous vehicles Polygonal primitives 1 Introduction Recent research in the fields of pattern recognition suggest that the usage of 3D sensors improves the effectiveness of perception, “since it supports good situation awareness for motion level tele-operation as well as higher level intelligent autonomous functions” [1]. Nowadays, autonomous robotic systems have at their disposal a new generation of 3D sensors, which provide 3D data of unprecedented quality [2]. In robotic systems, 3D data is used to compute some form of internal representation of the environment. In this paper, we refer to this as 3D scene representation or simply 3D representation. The improvement of 3D data available to robotic systems should pave the road for more comprehensive 3D representations. In turn, advanced 3D representations of the scenes are expected to play a major role in future robotic applications since they support a wide variety of tasks, including navigation, localization, and perception [3]. In summary, the improvement in the quality of 3D data clearly opens the possibility of building more complex scene representations. In turn, more advanced scene representations will surely have a positive impact on the overall performance of robotic systems. Despite this, complex scene representations have not yet been substantiated into robotic applications. The problem is how to process the large amounts of 3D data. In this context, classical computer graphics algorithms are not optimized to operate in real time, which is a non-negotiable requirement of the majority of robotic applications. Unless novel and efficient methodologies that produce compact, yet elaborate scene representations, are introduced by the research community, robotic systems are limited to mapping the scenes in classical 2D or 2.5D representations or are restricted to off-line applications. Very frequently, the scenarios where autonomous systems operate are urban locations or buildings. Such scenes are often characterized for having a large number of well defined geometric structures. In outdoor scenarios, these geometric structures could be road surfaces or buildings, while in indoor scenarios they may be furniture, walls, stairs, etc. We refer to the scale of these structures as a macro scale, meaning that 3D sensor may collect thousands of measurements of those structures in a single scan. A scene representation is defined by the surface primitive that is employed. For example, triangulation approaches make use of triangle primitives, while other approaches such as Poisson surface reconstruction resort to implicit surfaces. Triangulation approaches generate surface primitives that are considered to have a micro scale, since a geometric structure of the scene could contain hundreds or thousands of triangles. Micro scale primitives are inadequate to model large scale environments because they are not compact enough. In this paper, we present a novel methodology to compute a 3D scene representation. The algorithm uses macro scale polygonal primitives to model the scene. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. The proposed representation addresses the problems that were raised in the previous lines: the representation is compact and can be computed much faster than most others, while at the same time providing a sufficiently accurate geometric representation of the scene from the point of view of the tasks required by an autonomous system. The second problem addressed in this paper is the reconstruction of large scale scenarios from a continuous throughput of massive amounts of 3D data. We will use the distinction between the terms scene and scenario. Let scenario refer to a particular location that should be reconstructed, e.g., a city, a road or a building. By scene, we refer to the portion of the scenario that is viewed by the vehicle at a particular time. Thus, the scenario is an integration of scenes over time. In the case of large scale scenarios, the compactness of a given scene representation is even more important. In this paper, we focus also on how the representation may evolve by integrating 3D data from multiple measurements over time. This is an extended version of [4]. The new material covers mostly the incremental part of the geometric reconstruction. There is also the possibility of adding texture to the geometric scene description. For further details on this see [5]. For testing and evaluation purposes, we use a data-set from the Massachusetts Institute of Technology (MIT) Team, taken from their participation in the DARPA Urban Challenge [6]. From this data-set we have extracted a 40 s sequence which will be used to assess the proposed algorithms. For the remainder of the paper, this sequence is referred to as MIT sequence. Using this data-set, we aim at reconstructing large portions of the urban environment in which the competition took place. The remainder of this paper is organized as follows: Section 2 reviews the state of the art, Section 3 presents the proposed approach. Results are given in Section 4 and conclusions in Section 5. 2 Related work At first glance, it would seem plain to translate the improvement on the quality of the 3D data into an enhancement of the 3D representations. However, the fact is that the majority of the robotic systems, namely autonomous vehicles, continue to rely on classic 2D or 2.5D scene representations [7], such as occupancy grids [8] or elevation maps [9], or use discretized grid-like approaches as in octrees [10]. The reason for that is that autonomous vehicles commonly require a large array of sensors installed on-board and, as a consequence, collect large amounts of range measurements every second. Table 1 shows an estimate of the amount of 3D data (measured by LIDAR systems alone) generated by several autonomous vehicles. Simplified 2D or 2.5D representations are used so that they can be computed in real time using large amounts of data. More advanced 3D representations have not been introduced in robotics because they fail to abide to the requirements of real time processing using the 3D data produced by new generation LIDAR sensors. One example of this is the methodologies used in the computer graphics research field: traditional algorithms such as building of triangular meshes are unable to operate in real time with the throughput of data provided by new generation 3D sensors. Some authors have tried to optimize triangulation algorithms (e.g., [2,7]), and they report near real time performances. Note that these results were obtained using point clouds from a Microsoft Kinect 3D camera. 1 1 On the other hand, the results provided towards the end of this paper are obtained using point clouds from a Velodyne HDL-64E Lidar, 2 2 and therefore results are not directly comparable. Scene reconstruction is defined as the computation of a geometric 3D model from multiple measurements. These measurements could be obtained from stereo systems, range sensors, etc. Scene reconstruction may also include the texturing of the generated 3D model. Scene reconstruction methodologies are grouped into two different approaches: surface based representations or volumetric occupancy representations. In the first, the underlying surfaces of the scene that generated the range measurements are estimated, while in the second, the range measurements are grouped into cells of a grid, and are then labeled free or occupied. Traditional surface based representations include several 3D triangulations methodologies, such as 3D Delaunay triangulation [18], or Ball Pivoting Algorithm (BPA) [19]. There are also some alternative higher order surface representation methods such as Poisson surface reconstruction [20], Orientation Inference Framework [21] or learning approaches [22]. However, most of these methods do not tackle well noisy range measurements and, above all, since these methods involve a large number of nearest neighbor queries, they are very slow to compute. One attempt to accelerate the triangulation of point clouds was done in [7], but authors report they have only achieved near real time. Volumetric occupancy representations include occupancy grids [8], elevation maps [9], multi-level surface maps [23] or octrees [10]. While these representations are easier to compute, they do not provide accurate information about the geometry of the scene. The BPA triangulation was proposed in [24]. The BPA computes a triangle mesh interpolating a given point cloud. The principle of the BPA is very simple: Three points form a triangle if a ball of a user-specified radius touches them without containing any other point. Starting with a seed triangle, the ball pivots around an edge (i.e., it revolves around the edge while keeping in contact with the edge’s endpoints) until it touches another point, forming another triangle. The process continues until all reachable edges have been tried, and then starts from another seed triangle, until all points have been considered. Although all range points are considered in the computation of the mesh, which accounts for the accuracy of the methodology, this fact also hampers the computational performance of the algorithm. The Greedy Triangulation (GT) is an approach designed for fast surface reconstruction from large noisy data sets [7]. Given an unorganized 3D point cloud, the algorithm recreates the underlying surface’s geometrical properties using data resampling and a robust triangulation algorithm, the authors claim to achieve near real time. For resulting smooth surfaces, the data is resampled with variable densities according to previously estimated surface curvatures. One of the advantages of this method is that, since a greedy search is executed, it is expected to be faster than other standard triangulation approaches. Poisson surface reconstruction was initially proposed in [25]. In this approach, surface reconstruction of a point cloud with estimated normals is viewed as a spatial Poisson problem. The Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, a Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. The work from [26] proposes an expectation maximization based method for producing a scene representation based on planes, rather than polygons. Also, this method is not intended for real time applications, since creating a scene representation may take up to twenty minutes. Finally, this work does not discuss how new sensor measurements could be integrated in the existing representation, therefore not focusing on the incremental part of scenario reconstruction. 3 Proposed approach In this section we will explain in detail the methodologies of our approach. First, we describe the scene reconstruction algorithm (see Section 3.1) and then, in Section 3.2, we describe how a scenario is created over time from a continuous throughput of 3D data. 3.1 One-shot scene reconstruction This work proposes to explore the usage of Geometric Polygonal Primitives (GPP) to perform scene reconstruction. In other words, the idea is to describe a scene by a list of macro scale polygons. The detection of geometric polygonal primitives is simple when compared to the detection of other more complex primitives. Furthermore, given that road environments are often geometrically structured, it seems feasible to represent the 3D structure with a set of planar polygons. In addition to that, polygons are compact representations: geometric polygonal primitives are described by a support plane and a bounding polygon. Let G i represent the i th polygonal geometric primitive of a given scene, with the support plane Hessian coefficients denoted by G p i = [ a i b i c i d i ] . The search for the support plane is done on a given input point cloud P using a Random Sample Consensus (RANSAC) procedure [27]. Note that there are other algorithms which are more efficient for detecting planes in 3D data [28]. However, an analysis of the impact of these alternative approaches is out of the scope of the current paper. RANSAC is an iterative method to estimate parameters of a mathematical model from a set of observed data points. The assumption is that data consists of inliers, i.e., data whose distribution can be explained by some set of model parameters, and outliers, data that does not fit the model. The input to the RANSAC algorithm is a set of observed data values, a parameterized model which is fitted to the observations, and the output are the model parameters, i.e., in the case of detecting the support plane of polygonal primitives, the Hessian coefficients. Fig. 1 (a) shows in different colors the inlier points of the five best candidates of a RANSAC search. Wall like structures are correctly detected. Fig. 1(c) shows the inliers (signaled in green) of a RANSAC plane detection. In this case, range measurements from two separate walls have been signaled as inliers of a single support plane. To address this issue, the set of inliers of each support plane hypothesis is used as input to a clustering procedure. By using the proposed clustering algorithm, it is possible to separate the two walls into separate polygons, as shown in Fig. 1(d). Polygons are computed using a 2D convex hull operation on the (clustered) RANSAC inliers. In this work, the implementation provided in [29] is used to compute the 2D convex hull, based on a non recursive version of [30,31]. To increase the efficiency of the algorithm, we propose to conduct the detection of polygonal primitives in a cascade like processing configuration. This should be more efficient and fast to process. The input point cloud contains a large amount of 3D points: we assume each 3D point can only belong to a single polygonal primitive. Let S k be the point cloud containing the support points of primitive k , and P k be the input point cloud in which the primitive was searched. The input point cloud for the search of the next primitive, P k + 1 , is obtained by removing the support points of primitive k : (1) P k + 1 = { ∈ P k ∣ ∉ S k } . Since every iteration of primitive detection will conduct a search on a smaller point cloud, it is expected that the cascade configuration is capable of reducing the processing time. Algorithm 1 details the complete procedure for the detection of a set of polygonal primitives given a point cloud. 3.2 Incremental scenario reconstruction We use the term scenario reconstruction to designate an incremental process of reconstruction of scenes from a continuous stream of sensor data. At first sight, there are three alternatives for performing scenario refinement: (1) store raw measurements and, in the end, reconstruct using the entire data; (2) reconstruct periodically and fuse partial scene reconstructions; (3) reconstruct with the first input data then make the representation evolve as new data arrives. Approach (1) is the most immediate, since there are well known surface reconstruction algorithms which reconstruct a surface from a single point cloud. This approach merely merges all input point clouds into a single point cloud, the accumulated point cloud, and then, standard surface reconstruction algorithms may be applied using the accumulated point cloud as input. One downside of this method is that the process of reconstruction can only begin after all point clouds have been collected. This is not suited for usage in real time applications. Furthermore, the amount of data that results from the accumulation of point clouds should be very large, which in turn might cause problems for reconstruction algorithms. Fig. 2 shows an example of a scenario reconstruction (with BPA) using as input the accumulated point clouds of three scenes. This reconstruction took over 2 h to complete. Alternative (2) proposes a fusion of (partially) reconstructed scenes for each of the locations. Taking the example of Fig. 2: if the three scenes are reconstructed independently using BPA, the overall process takes 488.2 + 480.4 + 558.8 = 1487.4 s (this is without considering the overhead attached to the process of fusion). Fig. 3 shows the reconstructed scenarios obtained using this alternative. The downsides of this alternative are the need to define a strategy to fuse the reconstructed scenes in order to obtain a global scenario representation. In addition to this, note that the reconstructed scenes overlap. In overlapping regions, reconstruction is carried out multiple times without originating an improved description, therefore waisting computational resources. Alternative (3) proposes an incremental scenario reconstruction. Unlike in the other two approaches, in this case, the reconstruction of a given scenario receives as input not only the current sensor data (a point cloud), but also a description of the scenario as seen by preceding reconstructions. This method appears to be more interesting than the others, since a scenario representation is updated or refined when new sensor data arrives. An illustrative example: a vehicle is moving on a road and there is a long wall on the side of the road. At the beginning of the road, sensors see only a portion of the wall and the reconstructed surface corresponds only to the visible part of the wall. As the vehicle moves forward, additional areas of the wall become visible to the sensors. These additional range measurements of the wall should be used to update the already existing shape primitive that represents the wall, rather than to create a new shape primitive which represents the new visible portion of the wall, and that overlaps, to some extent, the first wall primitive. In the following lines, we present mechanisms that enable a scene representation based on GPPs to be incrementally refined from novel point cloud data. To update the representation, an operation called expansion is executed for each of the existing GPPs. The expansion receives as input a list of points (the 3D point cloud) as well as the definition of the polygon that is to be expanded. It is composed of two parts, a perpendicular and a longitudinal expansion, and is defined as follows (see Fig. 4 ): Let P represent an input point cloud that is received at a given time and that contains several points (triangles and diamonds in Fig. 4(a)), and the scenario representation that was previously computed, being composed of a single primitive G (black solid line polygon in Fig. 4(a)). The primitive has a support plane, as well as a local coordinate system represented by red–green–blue lines. The first step is to compute a new point cloud P ort that is given by the points of P whose distance to the plane is smaller than the perpendicular expansion threshold T ort : (2) P ort = { ∈ P ∣ d j < T ort } , where d j is the distance of point to the support plane of G . Only points that lie close to the primitives support plane are stored in P ort and used in the next steps. In Fig. 4(a), some points are included in P ort (triangles) and others discarded (diamonds). Then, the points in P ort are projected into the primitives support plane and their coordinates transformed into the primitives local coordinate frame. In this local reference frame, the projected points always have z value equal to zero, which is why only the x and y coordinates are stored, i.e., points are defined in R 2 . Let J be the point cloud that contains the x and y coordinates of the projected points viewed from the primitive local coordinate system. Fig. 4(b) shows the projections of the triangles of Fig. 4(a) to the support plane (circles). This process is called the orthogonal part of the expansion. From here onward, all computations are performed in R 2 , which significantly speeds up the computation. The second part of the expansion is referred to as longitudinal expansion. Fig. 5 (a) shows an example point cloud J that contains several points. Let us consider that some of these points actually belong to the same object that the primitive represents (circles in Fig. 5(a)), and others do not (squares in Fig. 5(a)). Since these points are obtained from new data, not all of them are contained inside the bounding polygon of the corresponding primitives. Therefore, the primitive should expand to accommodate these new points. To do this, an iterative process is proposed. The first step is to offset the existing bounding polygon of the primitive. The algorithm used was introduced in [32] and the implementation is from [33]. The offsetting operation generates a new larger polygon, and in every iteration, the polygon grows as detailed next. The bounding polygon of the primitive is referred to as P , and the grown or extended polygon is referred to P ˆ . Then, all points in J are tested to see if they are inside P ˆ . The final stage is to compute a new convex hull. This new convex hull is computed from the point set that contains both the points of the previous convex hull and the points to which the polygon expanded to. The process is repeated using the newly computed convex hull as the starting hull. The iterative expansion stops when the extended polygon does not contain points inside it. Fig. 5 shows an example of an iterative longitudinal expansion. Fig. 5(a) shows the initial situation; Fig. 5(b) shows the start of the iterative process. The expanded polygon (dashed line), is offset from the initial bounding polygon (solid blue line). Points are tested to see whether they are inside the offset polygon (annotated with crosses). The process repeats until, in the fifth iteration (Fig. 5(f)), no new points are found inside the offset polygon. This causes the iterative search to finish. The expansion operation changes the polygon from its initial state Fig. 5(a) to a new shape, solid magenta line in Fig. 5(f). We propose to conduct the expansion of the current primitives using also a cascade configuration, based on the following reasoning. Each range measurement is obtained from a laser beam reflection of a single physical object. Thus, we can assume that each 3D point is explained by a GPP. Under this assumption, the points that have been assigned to a given primitive by an expansion operation, can only belong to that primitive and no other. Because of this, expanded points are removed from the input point cloud and are not a part of subsequent expansions (of other primitives) nor part of detections of new primitives. Since all the points that are taken by the expansion of a primitive are removed from the input point cloud, the subsequent expansions or searches are accelerated since that less points need to be analyzed. In terms of configuration, a cascade processing recommends that the faster stages are computed first. The expansion of primitives is faster than the detection. Because of this, when a new input point cloud is received, all existing primitives are first expanded and only then the remainder non expanded points are used for searching new primitives. Algorithm 2 describes the architecture of the complete algorithm for the geometric polygonal primitives representation computation, including both the detection and expansion operations. 4 Results In order to evaluate the proposed 3D processing techniques, a complete data-set with 3D laser data, cameras and accurate egomotion is required. The MIT autonomous vehicle Talos competed in the DARPA Urban Challenge and achieved fourth overall place. The data logged by the robot is publicly available [6]. In total, the MIT logs sum up to 315 GB of data. We have cropped a small sequence of 40 s (200 m of vehicle movement) at the start of the race (see Fig. 6 ). The sequence contains a continuous stream of sensor data, but in addition we have marked five locations (A through E) which are used to facilitate the analysis of the results. Additional information on each location is given in Table 2 . Fig. 7 shows images from all cameras, isometric and top views of the 3D data, and a satellite photograph of location C. The proposed approach is evaluated by analyzing how the scenario contained in this MIT sequence is reconstructed. Fig. 8 shows the polygonal primitives detected at locations C and D. It is possible to observe that most of the relevant planes are picked up by the algorithm. 4.1 One-shot reconstruction The detection of polygonal primitives is operated in a cascade-like configuration. In other words, the algorithm will search for polygonal primitives on a given input point cloud. After the first primitive is found, all the range measurements that are explained by that primitive are removed from the input point cloud. The second primitive is then searched in a smaller point cloud and so on. Since the search for a primitive is done over a decreasing size point cloud, it is expected that the search becomes faster as the primitives are extracted. In Fig. 9 an analysis of the computation time of each primitive is displayed. Primitives with higher numbers are detected in posterior phases. Fig. 9(a) shows the number of points remaining in the input point cloud as a function of the polygon number. Results are shown for all locations in the MIT sequence. The number of detected primitives varies from location to location. It is also possible to observe that, as expected, the number of remaining points decreases with the increase in the number of detected primitives. Also, the reduction in the number of points is larger for primitives detected earlier in the process. Hence, since the algorithm tends to remove the largest portion of points at the early stages of the cascade processing, this means that the latter stages will also be more efficient to compute. The reason for this behavior is the RANSAC algorithm. Because RANSAC will search for the larger consensus, it will most likely select planes that are supported by a greater number of points. In this way, RANSAC tends to select first polygons with the largest amount of primitive support points. As a consequence, the largest decreases in the input point cloud occur early in the cascade, which in turn accelerates the subsequent detection stages of the cascade. The detection time per primitive is shown in Fig. 9(b). The detection time tends to decrease with the increase in polygon number, for the reasons that were previously reported. We compare the proposed approach with three surface reconstruction methodologies: Ball Pivoting Algorithm (BPA) [24], Greedy Triangulation (GT) [7] and Poisson Surface Reconstruction (POIS) [25]. Two different parameter configurations for the proposed approach are used. In the first GPP 1, parameters are set so that only very large polygons are detected. Processing time is faster, since a lot of polygons are discarded but, on the other hand, the accuracy or completeness of the scene representation is not very good. The second alternative, GPP 2, is configured so that even small polygons are detected, which should provide a more accurate scene description at the cost of a higher computation time. Fig. 10 (a) shows that the BPA method Fig. 10(d) shows results from the GPP. Since our approach uses primitives to define macro size structures, the number of polygons used to represent the scene is small. Even though, it can be said that the most relevant polygons are part of the representation. Table 3 shows the computation times that each algorithm took to reconstruct each of the locations in the sequence. The GPP methodology is the fastest one. This efficiency is related to the simplicity of the computed representation, and to the fact that RANSAC analyzes only a small sample of points in the input point cloud, which means that not all input points are visited in order to reconstruct the scene, as is the case with the slower triangulation approaches. To measure the accuracy of each reconstruction approach, the results obtained by BPA (the most accurate algorithm) are used as reference. Let X and Y be two meshes. The Hausdorff distance between those meshes d H ( X , Y ) is computed as: (3) d H ( X , Y ) = max ( sup x ∈ X ( inf y ∈ Y d ( x , y ) ) , sup y ∈ Y ( inf x ∈ X d ( x , y ) ) ) , where sup and inf are the supremum and infimum, respectively. In this particular case, a variation of the Hausdorff distance, called the one sided Hausdorff distance is used where only the sup x ∈ X ( inf y ∈ Y d ( x , y ) ) part is computed, because we only wish to measure how distant is each approach to the ground truth and not the other way around. In this case, the X meshes are given by each of the algorithms and the Y mesh is supplied by the ground truth mesh BPA. Table 4 shows the Hausdorff distance values obtained by GT, POIS, GPP 1 and GPP 2 using BPA meshes as ground truth. The algorithm that obtains the best mean results is GT with an average error of 0.14 m. The accuracy exhibited by the GPP 1 and GPP 2 approaches is about 0.99 and 0.83 m, respectively. Fig. 11 shows a graphical representation of the error for all of the approaches. For each approach, the output mesh has been sampled and the points are shown with color associated to the computed one sided Hausdorff distance of each point. A Red–Green–Blue colormap is used to code the distance. Red represents zero distance and blue maximum distance. In Fig. 11(a), corresponding to the GT approach, almost all points have red color, resulting in low mean error. The POIS approach, represented in Fig. 11(b), shows many points in blue and green color, e.g., points whose minimum distance to the ground truth sampled points was very large. This is why POIS shows low accuracy values. In the case of the GPP approaches, 11(c) and (d), some regions of the sampled points are more prone to have large error distances, while those in red seem to perfectly fit the ground truth mesh. The reason for this is that the BPA methodology, that was selected to serve as ground truth, does not perform interpolation over occluded areas, as the GPP approaches do. Most of the errors appear in the polygonal primitive that represents the ground plane; that occurs because this is the one that suffers more from occlusion from other planes. Errors may also result from the usage of convex hulls to compute the boundary polygons. We have investigated this by using alternatives to the proposed approach where the ground plane polygon is suppressed, and where concave hulls are used. Results are shown in Table 5 . We can observe that, with the option of ground plane suppression and concave hull, the mean accuracy of GPP 2 improves to 0.1 m. Fig. 12 shows a visual analysis of the Hausdorff distance errors for these variations of GPP 2. It is possible to observe that regions with error, e.g., in blue and green, decrease considerably when the concave hull is used, but in particular when the ground plane polygon is discarded. 4.2 Incremental reconstruction This section presents several results and analysis of the expansion mechanism of the geometric polygonal primitives. The polygonal primitives algorithm without the expansion mechanism is compared against the same algorithm using the expansion mechanism. These two algorithms will be referred to as “with expansion” and “without expansion”. By using this evaluation, it is possible to assess what are the benefits or disadvantages of the expansion mechanism. All five locations of sequence 1 are used to obtain results. Parameters used in the detection are similar to the GPP 1 set. Fig. 13 shows a reconstruction of the scene using the expansion mechanism. The state of the reconstructed scenario at each location is shown. One clear advantage of this representation is that there are no overlapping primitives. A qualitative analysis of the results present in Fig. 13 shows that the most important features of the scenario are contained in the representation, especially if the task in mind is navigation. To evaluate the computational complexity of the proposed algorithms, we measure the amount of detail of the representation that they produce when given a fixed amount of time to reconstruct a scene. Fig. 14 (a) shows the number of polygons created in both algorithms. Although at the beginning of the sequence both algorithms generate a similar number of primitives, after some locations the algorithm without expansion shows a greater number of primitives. The explanation is that since the algorithm without expansion does not compare the stored primitives with the new data, it ends up duplicating primitives. On the contrary, when using the expansion mechanism, the duplication of primitives is avoided which leads to a smaller number of primitives. Note that just because a representation has more primitives it is not necessarily better. In fact, if there are duplicated primitives, the representation may be worse than another with a smaller number of non duplicated primitives. Fig. 14(b) shows the number of points that are given as input for the detection mechanism. In the case of the algorithm without expansion, all of the input points are passed on to the detection component, i.e., approximately 400000 points per scan. When the expansion is active, a large number of points are explained by the expansion component and are not fed into the detection. In conclusion, the expansion mechanism takes a small amount of time when compared to the detection and other processes, and is able to quickly explain a large portion of the input points, thus filtering out many points which are not sent to other slower components. Finally, in Fig. 14(c), the total accumulated area of the primitives is shown. There is a clear difference between the with and without expansion approaches. This difference is due to the duplication of primitives in the case of the without expansion approach. 4.3 Comparison of approaches with and without expansion Next, we focus on characterizing how the polygonal primitives evolve. Only the algorithm with expansion is portrayed, since in the without expansion approach primitives are static. Fig. 15 (a) shows the number of support points assigned to each primitive. Only primitives of even index are shown. We can see that primitives are initialized in different locations in the sequence: at location A primitive 0 is created, while primitives 2, 4 and 6 are created at location B. These results show that most primitives significantly increase their number of support points throughout the sequence: Primitive 0 was detected at location A with 0.4 × 50 × 10 4 = 200 kpoints, and at location E it already supported 1.4 × 50 × 10 4 = 700 kpoints. In other words, it increased the number of support points by 350%. Another example, primitive 10, detected at location D with 3 kpoints, has at location E around 7 kpoints. A 230% increase between consecutive locations. The same analysis holds when considering the area of the primitives (see Fig. 15(b)): significant increases in the area of the primitives bounding polygons are also observed. All these observations, both in number of support points as well as in terms of area, show that polygons grow considerably after being detected. If these primitives were not expanded, the additional support points and area would have to be handled by a detection mechanism. Fig. 16 shows how primitive 0, i.e., the ground plane primitive, evolves over sequence 1. The primitive expands at every iteration to accommodate newly observed data points that belong to the ground plane. Table 6 provides the links for some videos that show how the system processes the data stream from the MIT sequence. It is possible to see the difference between an incremental versus a non incremental (without expansion) approach. 5 Conclusions This paper proposes a novel approach to produce scene representations using the array of sensors on-board autonomous vehicles. Since roads are semi structured environments with a great deal of macro size geometric structures, we argue that the use of polygonal primitives is well suited to describe these scenes. Furthermore, we propose mechanisms designed to update the polygonal primitives as new sensor data is collected. Results have shown that the proposed approach is capable of producing accurate descriptions of the scene, and that it is considerably faster than all the approaches used in this evaluation. The proposed expansion mechanism updates previous descriptions of the scene, therefore not creating duplicate representations of the same objects. In addition to this, the expansion mechanism is capable of efficiently filtering out data points that otherwise would be handled by (slower) detection mechanisms. Future work will include the addition of texture on the polygons generated by the proposed algorithm. In this way, we expect to have the means to produce scene representations that can be used not only for standard task such as obstacle detection and motion planning, but also for more complex endeavors such as recognizing patterns in the scene. Acknowledgments This work has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”, reference CEB-02502014. References [1] A. Birk N. Vaskevicius K. Pathak S. Schwertfeger J. Poppinga H. Buelow 3-d perception and modeling IEEE Robot. Autom. Mag. 16 4 2009 53 60 [2] R.B. Rusu, S. Cousins, 3D is here: Point Cloud Library (PCL), in: IEEE International Conference on Robotics and Automation, ICRA, Shanghai, China, 2011. [3] W. Burgard P. Pfaff Editorial: Three-dimensional mapping, part 1 J. Field Robot. 26 10 2009 757 758 [4] M. Oliveira, V. Santos, A.D. Sappa, P. Dias, Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 1, Springer International Publishing, Cham, 2016, Ch. Scene Representations for Autonomous Driving: An Approach Based on Polygonal Primitives, pp. 503–515. [5] M. Oliveira V. Santos A. Sappa P. Dias A.P. Moreira Incremental texture mapping for autonomous driving Robot. Auton. Syst. 2016 (submitted January 2016) [6] A.S. Huang M. Antone E. Olson L. Fletcher D. Moore S. Teller J. Leonard A High-rate, Heterogeneous Data Set from the DARPA Urban Challenge Int. J. Robot. Res. 29 13 2011 1595 1601 [7] Z.C. Marton, R.B. Rusu, M. Beetz, On fast surface reconstruction methods for large and noisy datasets, in: Proceedings of the IEEE International Conference on Robotics and Automation, ICRA, Kobe, Japan, 2009. [8] T. Weiss, B. Schiele, K. Dietmayer, Robust driving path detection in urban and highway scenarios using a laser scanner and online occupancy grids, in: Intelligent Vehicles Symposium, 2007 IEEE, 2007, pp. 184–189. [9] F. Oniga S. Nedevschi Processing dense stereo data using elevation maps: Road surface, traffic isle, and obstacle detection IEEE Trans. Veh. Technol. 59 3 2010 1172 1182 [10] K. Zhou M. Gong X. Huang B. Guo Data-parallel octrees for surface reconstruction IEEE Trans. Vis. Comput. Graphics 17 5 2011 669 681 [11] S. Thrun M. Montemerlo H. Dahlkamp D. Stavens A. Aron J. Diebel P. Fong J. Gale M. Halpenny G. Hoffmann K. Lau C.M. Oakley M. Palatucci V. Pratt P. Stang S. Strohband C. Dupont L.-E. Jendrossek C. Koelen C. Markey C. Rummel J. van Niekerk E. Jensen P. Alessandrini G.R. Bradski B. Davies S. Ettinger A. Kaehler A.V. Nefian P. Mahoney Stanley: The robot that won the darpa grand challenge J. Field Robot. 23 9 2006 661 692 [12] C. Urmson J. Anhalt D. Bartz M. Clark T. Galatali A. Gutierrez S. Harbaugh J. Johnston H. Kato P.L. Koon W. Messner N. Miller A. Mosher K. Peterson C. Ragusa D. Ray B.K. Smith J.M. Snider S. Spiker J.C. Struble J. Ziglar W.R.L. Whittaker A robust approach to high-speed navigation for unrehearsed desert terrain J. Field Robot. 23 8 2006 467 508 [13] C. Urmson J. Anhalt H. Bae J.A.D. Bagnell C.R. Baker R.E. Bittner T. Brown M.N. Clark M. Darms D. Demitrish J.M. Dolan D. Duggins D. Ferguson T. Galatali C.M. Geyer M. Gittleman S. Harbaugh M. Hebert T. Howard S. Kolski M. Likhachev B. Litkouhi A. Kelly M. McNaughton N. Miller J. Nickolaou K. Peterson B. Pilnick R. Rajkumar P. Rybski V. Sadekar B. Salesky Y.-W. Seo S. Singh J.M. Snider J.C. Struble A.T. Stentz M. Taylor W.R.L. Whittaker Z. Wolkowicki W. Zhang J. Ziglar Autonomous driving in urban environments: Boss and the urban challenge J. Field Robot. 25 8 2008 425 466 Special Issue on the 2007 DARPA Urban Challenge, Part I [14] M. Montemerlo J. Becker S. Bhat H. Dahlkamp D. Dolgov S. Ettinger D. Haehnel T. Hilden G. Hoffmann B. Huhnke D. Johnston S. Klumpp D. Langer A. Levandowski J. Levinson J. Marcil D. Orenstein J. Paefgen I. Penny A. Petrovskaya M. Pflueger G. Stanek D. Stavens A. Vogt S. Thrun Junior: The stanford entry in the urban challenge J. Field Robotics 2008 [15] A. Bacha C. Bauman R. Faruque M. Fleming C. Terwelp C. Reinholtz D. Hong A. Wicks T. Alberi D. Anderson S. Cacciola P. Currier A. Dalton J. Farmer J. Hurdus S. Kimmel P. King A. Taylor D.V. Covern M. Webster Odin: Team victortango’s entry in the darpa urban challenge J. Field Robot. 25 8 2008 467 492 [16] T. Luettel, M. Himmelsbach, F. von Hundelshausen, M. Manz, A. Mueller, H.-J. Wuensche, Autonomous Offroad Navigation Under Poor GPS Conditions, in: Proceedings of 3rd Workshop On Planning, Perception and Navigation for Intelligent Vehicles, PPNIV, IEEE/RSJ International Conference on Intelligent Robots and Systems, St. Louis, MO, USA, 2009. [17] Wikipedia, Google driverless car—Wikipedia, the free encyclopedia, [Online; accessed November 2015], 2015. [18] R. Jovanovic, R. Lorentz, Compression of volumetric data using 3d delaunay triangulation, in: 2011 4th International Conference on Modeling, Simulation and Applied Optimization, ICMSAO, 2011, pp. 1–5. [19] A. Specht, M. Devy, Surface segmentation using a modified ball-pivoting algorithm, in: 2004 International Conference on Image Processing, 2004. ICIP’04. Vol. 3, 2004, pp. 1931–1934. [20] C. Yin, D. Gang, C. Zhi-quan, L. Hong-hua, L. Jun, J. Shi-yao, An algorithm of cuda-based poisson surface reconstruction, in: 2010 International Conference on Audio Language and Image Processing, ICALIP, 2010, pp. 203–207. [21] Y.-L. Chen S.-H. Lai An orientation inference framework for surface reconstruction from unorganized point clouds IEEE Trans. Image Process. 20 3 2011 762 775 [22] A. de Medeiros Brito A. Doria Neto J. Dantas de Melo L. Garcia Goncalves An adaptive learning approach for 3-d surface reconstruction from point clouds IEEE Trans. Neural Netw. 19 6 2008 1130 1140 [23] C. Rivadeneyra, I. Miller, J. Schoenberg, M. Campbell, Probabilistic estimation of multi-level terrain maps, in: IEEE International Conference on Robotics and Automation, 2009. ICRA’09. 2009, pp. 1643–1648. [24] F. Bernardini J. Mittleman H. Rushmeier C. Silva G. Taubin The ball-pivoting algorithm for surface reconstruction IEEE Trans. Vis. Comput. Graphics 5 4 1999 349 359 [25] M. Kazhdan, M. Bolitho, H. Hoppe, Poisson surface reconstruction, in: SGP’06: Proceedings of the Fourth Eurographics Symposium on Geometry Processing, Eurographics Association, 2006, pp. 61–70. [26] R. Triebel, W. Burgard, F. Dellaert, Using hierarchical em to extract planes from 3d range scans, in: Proceedings of the 2005 IEEE International Conference on Robotics and Automation, 2005, pp. 4437–4442. [27] M.A. Fischler, R.C. Bolles, Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography, in: ACM, Los Angeles, California, 1981. [28] A. Nurunnabi, D. Belton, G. West, Robust segmentation for multiple planar surface extraction in laser scanning 3d point cloud data, in: 2012 21st International Conference on Pattern Recognition, ICPR, 2012, pp. 1367–1370. [29] S. Hert, S. Schirra, 2D convex hulls and extreme points, in: CGAL User and Reference Manual, 4.0 Edition, CGAL Editorial Board, 2012. [30] A. Bykat Convex hull of a finite set of points in two dimensions Inform. Process. Lett. 7 1978 296 298 [31] C.B. Barber D.P. Dobkin H. Huhdanpaa The quickhull algorithm for convex hulls ACM Trans. Math. Software 22 4 1996 469 483 [32] O. Aichholzer F. Aurenhammer D. Alberts B. Gartner A novel type of skeleton for polygons J.UCS 1 12 1995 752 761 [33] F. Cacciola, 2D straight skeleton and polygon offsetting, in: CGAL User and Reference Manual, 4.0 Edition, CGAL Editorial Board, 2012. Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Vítor Santos obtained a 5 year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990–1994 at the Joint Research Center, Italy. He his currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or cosupervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He his also cofounder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Angel Domingo Sappa (S’94-M’00-SM’12) received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, where he is currently a Senior Researcher. He is a member of the Advanced Driver Assistance Systems Group. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereoimage processing and analysis, 3D modeling, and dense optical flow estimation. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. António Paulo Moreira graduated with a degree in electrical engineering at the University of Oporto, in 1986. He then pursued graduate studies at University of Porto, obtaining an M.Sc. degree in electrical engineering-systems in 1991 and a Ph.D. degree in electrical engineering in 1998. Presently, he is an Associate Professor at the Faculty of Engineering of the University of Porto and researcher and manager of the Robotics and Intelligent Systems Centre at INESC TEC. His main research interests are process control and robotics. "
    },
    {
        "doc_title": "Self calibration of multiple LIDARs and cameras on autonomous vehicles",
        "doc_scopus_id": "84977663236",
        "doc_doi": "10.1016/j.robot.2016.05.010",
        "doc_eid": "2-s2.0-84977663236",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D data",
            "Automatic calibration",
            "Autonomous navigation",
            "Autonomous Vehicles",
            "Extrinsic calibration",
            "Point cloud",
            "Real world environments",
            "Rigid body transformation"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-05-31 2016-05-31 2016-08-04 2016-08-04 2016-08-04T18:32:25 S0921-8890(16)30017-3 S0921889016300173 10.1016/j.robot.2016.05.010 S300 S300.1 FULL-TEXT 2016-08-04T16:53:09.271769-04:00 0 0 20160901 20160930 2016 2016-05-31T15:48:20.983401Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 83 83 C Volume 83 28 326 337 326 337 201609 September 2016 2016-09-01 2016-09-30 2016 article fla © 2016 Elsevier B.V. All rights reserved. SELFCALIBRATIONMULTIPLELIDARSCAMERASAUTONOMOUSVEHICLES PEREIRA M 1 Introduction 1.1 Context of the work 1.2 Related work 2 Proposed solution 3 Ball detection algorithms 3.1 Sphere center detection in 3D data: SwissRanger 3.2 Sphere center detection on LIDAR lasers 3.2.1 Segmentation 3.2.2 Circle detection 3.2.3 Calculation of the circle properties 3.2.4 Calculation of the center of the ball 3.3 Ball detection on cameras 4 Calibration 4.1 3D rigid transformation 4.2 External camera calibration 5 Results 5.1 Consistency of ball detection 5.2 Consistency of the geometric transformation 5.3 Validation of the method 5.3.1 Grid pattern calibration 5.3.2 Calibration with random acquisition spots 6 Conclusions and future work References DIAS 2009 J DISTANCIOMETRO3DBASEADONUMAUNIDADELASER2DEMMOVIMENTOCONTINUO LEVINSON 2013 J ROBOTICSSCIENCESYSTEMSRSS AUTOMATICONLINECALIBRATIONCAMERASLASERS ALMEIDA 2012 312 319 M IMAGEANALYSISRECOGNITION 3D2DLASERRANGEFINDERCALIBRATIONUSINGACONICBASEDGEOMETRYSHAPE FERNANDEZMORAL 2015 E BRADSKI 2000 120 125 G FISCHLER 1981 381 395 M COIMBRA 2013 D LIDARTARGETDETECTIONSEGMENTATIONINROADENVIRONMENT RAMER 1972 244 256 U DOUGLAS 1973 112 122 D ARUN 1987 698 700 K HORN 1987 629 642 B PEREIRAX2016X326 PEREIRAX2016X326X337 PEREIRAX2016X326XM PEREIRAX2016X326X337XM 2018-08-04T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0921-8890(16)30017-3 S0921889016300173 10.1016/j.robot.2016.05.010 271599 2016-08-04T15:40:51.784284-04:00 2016-09-01 2016-09-30 true 3811601 MAIN 12 55474 849 656 IMAGE-WEB-PDF 1 gr4 16067 110 219 gr2 18808 143 219 pic2 29748 163 140 gr18a 15625 164 215 gr20c 20835 164 108 gr17b 20252 119 219 gr14a 26766 78 219 gr20a 24661 132 219 gr20b 23129 114 219 gr18b 36240 164 218 gr12 21003 69 219 gr1 27257 87 219 gr5 13945 164 171 gr13 39139 134 219 gr14c 19998 164 192 gr17a 21932 129 219 pic3 32172 163 140 gr7 14362 80 219 gr15a 15531 156 219 gr14b 14733 148 219 gr8b 35489 164 218 gr17c 17546 163 119 gr6 24107 163 164 gr9 16887 90 219 gr19 38732 164 218 pic4 34270 163 140 gr11 17847 69 219 pic1 34474 164 140 gr15b 36304 164 218 gr16 36306 164 218 gr8a 37620 164 218 gr3 14494 163 174 gr10 16804 87 219 gr4 39043 107 213 gr2 50725 205 313 pic2 40944 132 113 gr18a 46094 258 339 gr20c 64979 460 304 gr17b 47136 180 333 gr14a 71656 181 507 gr20a 50139 181 299 gr20b 48819 154 296 gr18b 53590 255 339 gr12 70597 195 621 gr1 75208 193 489 gr5 39533 272 284 gr13 70288 208 339 gr14c 78393 372 435 gr17a 49542 194 329 pic3 40110 132 113 gr7 39013 110 303 gr15a 44676 242 339 gr14b 40662 228 338 gr8b 51430 255 339 gr17c 58851 448 327 gr6 38078 174 175 gr9 51276 154 376 gr19 52883 229 305 pic4 42468 132 113 gr11 64615 150 480 pic1 40842 132 113 gr15b 53183 255 339 gr16 53502 255 339 gr8a 55724 255 339 gr3 39806 265 282 gr10 51316 149 376 si25 222 11 30 si56 835 35 212 si21 213 11 28 si96 159 11 17 si111 224 11 30 si95 126 11 11 si121 152 14 15 si31 124 8 10 si81 655 41 125 si53 245 14 48 si17 514 14 127 si75 398 19 70 si86 131 8 11 si46 248 15 37 si37 595 15 128 si7 580 15 124 si39 397 21 76 si4 231 11 31 si88 389 16 78 si14 162 14 16 si78 253 16 36 si115 167 12 34 si114 112 11 6 si10 197 14 22 si26 226 11 30 si15 162 14 16 si66 412 17 100 si8 401 15 72 si110 252 12 33 si99 830 45 164 si9 159 11 19 si87 141 11 12 si73 1203 21 303 si51 479 19 112 si54 390 15 76 si69 671 15 170 si83 321 15 60 si2 200 11 28 si61 665 15 174 si80 699 41 127 si48 151 11 15 si16 159 14 16 si108 957 17 195 si116 139 11 13 si89 154 14 16 si34 666 22 148 si18 303 14 48 si1 139 12 10 si30 126 11 10 si94 1153 63 150 si74 426 17 87 si62 586 21 147 si43 335 17 70 si72 715 17 204 si100 303 14 61 si120 162 14 17 si60 155 11 16 si90 458 39 80 si59 146 11 15 si47 258 15 36 si41 284 14 74 si70 175 11 22 si6 204 11 30 si77 254 12 39 si27 184 11 22 si40 271 14 74 si20 130 9 12 si13 155 14 15 si117 131 11 12 si36 170 14 19 si42 313 13 69 si5 653 15 135 si105 173 16 15 si38 350 21 74 si112 147 11 15 si49 143 11 14 si3 201 11 23 si44 388 19 78 si50 141 11 11 si24 152 11 15 si35 961 31 216 si104 166 14 15 si64 154 13 13 si52 788 17 245 si22 205 11 23 si55 821 35 206 si85 164 13 16 si98 146 11 13 si45 421 17 88 si33 480 15 134 si76 243 15 40 si32 135 11 11 si82 210 16 24 si106 164 14 15 si97 160 12 16 si109 224 12 26 si19 153 11 14 si84 159 11 16 si79 517 23 119 si23 219 11 30 si93 372 15 71 ROBOT 2640 S0921-8890(16)30017-3 10.1016/j.robot.2016.05.010 Elsevier B.V. Fig. 1 ATLASCAR, a Ford Escort SW 98 adapted for autonomous driving capabilities on the left. Sensors used in this work—Sick LMS151 and LD-MRS40001, Point Grey camera and SwissRanger—on the right. Fig. 2 Detection in a SwissRanger cloud of points. Fig. 3 Result of the segmentation in a scan of the sensor Sick LMS151. Fig. 4 Congruent angles of points on an arc in respect to the extremes. Fig. 5 Result of the circle detection in real data from the sensor Sick LMS151. Fig. 6 Example of the cross-section of the sphere at a distance d from the sphere’s center. Fig. 7 Detection of the ball in a 2D scan from the Sick LMS151. Fig. 8 Outdoors ball detection with a Point Grey camera. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 8(a) (a) Initial camera image. Fig. 8(b) (b) Circle is the ball contour; plus sign is the ball center. Fig. 9 Standard deviation of ball center detection for several distances. Fig. 10 Standard deviation of the translation between the three sensor pairs. Fig. 11 Standard deviation of the Euler angles between the three sensor pairs. Fig. 12 Real layout of the sensors on positions detected by the calibration process. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Setup used on the validation test and the sensor positions. Fig. 14 Real world and Sick LMS151(A) grid pattern. Fig. 14(a) (a) Real world grid points used for calibration. Fig. 14(b) (b) Sick LMS151(A) ball centers point cloud. Fig. 14(c) (c) Vertical and horizontal distance between grid points. Fig. 15 Uncalibrated point clouds in their respective coordinate systems. Fig. 15(a) (a) Point clouds to be used with the 3D rigid transform method. Fig. 15(b) (b) Ball centers to be used with the external camera calibration method. Fig. 16 Ball centers in the camera coordinate system (green plus sign) and projected ball centers from Sick LMS151(A) (red points). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 17 Sensor placement in a Ford Escort SW 98 3D model after calibration. Fig. 17(a) (a) 3D view. Fig. 17(b) (b) Front view. Fig. 17(c) (c) Top view with calibrated point clouds. Fig. 18 Uncalibrated point clouds in their respective coordinate systems. Fig. 18(a) (a) Point clouds to be used with the 3D rigid transform method. Fig. 18(b) (b) Ball centers to be used with the external camera calibration method. Fig. 19 Ball centers in the camera coordinate system (green plus sign) and projected ball centers from Sick LMS151(A) (red points). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Sensor placement in a Ford Escort SW 98 3D model after calibration. Fig. 20(a) (a) 3D view. Fig. 20(b) (b) Front view. Fig. 20(c) (c) Top view with calibrated point clouds. Table 1 Properties of the main 3D range sensors onboard the ATLASCAR. Sick LD-MRS400001 Sick LMS151 SwissRanger sr4000 Scan planes 4, with full vertical aperture of 3.2 ° 1 Field of view 2 scan planes: 85 ° 270 ° 43.6 ° ( h ) × 34.6 ° ( v ) 2 scan planes: 110 ° Scanning frequency 12.5 Hz/25 Hz/50 Hz 25 Hz/50 Hz Angular resolution 0.125 ° / 0.25 ° / 0.5 ° 0.25 ° / 0.5 ° Operating range 0.5–250 m 0.5–50 m 0.1–5.0 m Statistical error ( 1 σ ) ±100 mm ±12 mm ±10 mm Table 2 Standard deviation and absolute mean error for each sensor calibration, using the 3D rigid transform method, against Sick LMS151(A). Absolute mean error (cm) Standard deviation (cm) Sick LMS151 5.9 3.0 Sick LD-MRS 13.4 10.1 Point Grey camera 18.4 8.3 Table 3 Standard deviation and absolute mean error of each sensor calibration, using the 3D rigid transform method, against Sick LMS151(A). Absolute mean error (cm) Standard deviation (cm) Sick LMS151 6.8 3.4 Sick LD-MRS 14.8 8.2 Point Grey camera 16.8 9.8 Self calibration of multiple LIDARs and cameras on autonomous vehicles Marcelo Pereira a David Silva a Vitor Santos a c ⁎ Paulo Dias b c a Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro Portugal c IEETA-Institute for Electronics Engineering and Informatics of Aveiro, University of Aveiro, Portugal IEETA-Institute for Electronics Engineering and Informatics of Aveiro, University of Aveiro Portugal ⁎ Corresponding author. Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity. Keywords Extrinsic calibration Point cloud 3D data fitting 1 Introduction Many vehicles with autonomous navigation capabilities, and also many advanced drivers assistance systems, rely on LIDAR (Light Detection And Ranging) and vision based sensors. Moreover, most of the developed systems use multiple sensors simultaneously. Thus, when there is more than one sensor in the same setup, a calibration procedure must take place to combine data from different sensors in a common reference frame. In order to solve that necessity, this work presents a new extrinsic calibration method. Differently from the majority of existing methods, which are manual or semi-automatic, the proposed method is automatic, with no requirement of manual measurements or manual correspondences between sensors data. Instead of using those approaches, a ball is used as a calibration target allowing the detection of its center by all sensors and then perform the several registration steps. To estimate the full transformation between the sensors, at least three 3D point correspondences between the sensors are needed, but more may be used for precision purposes. These points can be estimated during the ball movement, which furthermore increases the accuracy of the method as a significant number of points can be obtained along that path. 1.1 Context of the work This work is part of the ATLASCAR project [1], carried out at the University of Aveiro, whose main purpose is the research and development of solutions on autonomous driving and Advanced Driver Assistance System (ADAS). For that goal, a common Ford Escort car is equipped with a rich set of sensors dedicated mainly to the perception of the surrounding environment (Fig. 1 ). The car is equipped with several exteroceptive sensors, namely a stereo camera, a 3D LIDAR, a foveated vision unit and two planar laser range finders. The planar lasers already installed on the car are two Sick LMS151, and a custom made 3D LIDAR using a Sick LMS200 in a rotating configuration adapted from [2]. Additionally, a new multi-layer LIDAR (sick LD-MRS 400001) and two Point Grey cameras are available as well as a SwissRanger 3D TOF used occasionally in some experiments and contexts. This paper will focus on the sensors illustrated on the right side of Fig. 1, and Table 1 presents some of the 3D range sensors main properties. 1.2 Related work Over the past several years, a number of proposed solutions for the calibration between a camera and a laser were introduced, including some automatic on-line calibration solutions as presented in [3]. In [4] a method to estimate the motion of a camera–laser fusion system was developed that projects the laser points onto the images. Features are then selected using the Kanade–Lucas–Tomasi feature tracker [5] and tracked between frames to be used as 3D–2D correspondences for the motion estimation using a three-point method based in the algorithm developed by Bock et al. [6]. In [7] a plane with a printed black ring and a circular perforation is used to solve the extrinsic calibration between a camera and a multi-layer LIDAR; the method consists of estimating different poses of the calibration target detected simultaneously by the camera and the multi-layer LIDAR, resulting in a set of point correspondences between frames (circle centers of each pose), that are used to compute the extrinsic calibration by using the singular value decomposition (SVD) along with the Iterative Closest Point (ICP) algorithm to refine the resulting transformation. A similar approach is used in [8] to calibrate the same set of sensors; a planar triangle plane is used as target to extract correspondences between sensors, and the extrinsic calibration between sensors is solved using the Levenberg–Marquardt algorithm that projects the laser points into the image. There are also some other studies for the specific problem of calibration between LIDAR sensors, however, there is still room for improvement since no fully automatic method using perception sensor information, and adapted to any configuration, is available. Previous works on the ATLASCAR project used a technique that also uses a calibration target, but it requires manual input from the user [9]. Other authors developed algorithms to calibrate one [10] or two [11] single-beam LIDARs within the body frame; both methods use approaches that rely on establishing feature correspondences between the individual observations by preparing the environment with laser-reflective tape, which additionally requires an intensity threshold for correspondences and some initial parameters. A more recent method [12] is based on the observation of perpendicular planes; this calibration process is constrained by imposing co-planarity and perpendicularity constrains on the line segments extracted by the different laser scanners; despite being an automatic calibration method, it does not provide the versatility of the method presented in this paper. Finally an approach using a sphere target to perform extrinsic calibration of multiple 3D cameras was presented in [13], which presents some similarities with the current proposal since the user also moves the target to different positions and heights within a shared viewing area. In that process, the algorithm automatically detects the center of the ball in the data from each camera, and then uses those centers as corresponding points to estimate the relative positions of the 3D sensors. However, this approach [13] is only used with Kinect sensors with a smaller ball and a reduced working range. 2 Proposed solution The proposed solution in this paper is to estimate the rigid body transform between different sensors (SICK LMS151 and LD-MRS400001, Point Grey camera and SwissRanger) using a ball as calibration target. The only restriction of the calibration target is about its size (diameter). The size of the calibration target is related to the angular resolution of the sensors used; after some empirical experiments, it was concluded that the ball must have a diameter large enough for the sensors to have at least 8 measurements of the target at 5 m. The approach used to obtain the calibration among all the devices is achieved in three stages. First, each sensor must detect the ball; then, the ball is placed in motion in front of the sensors allowing them to detect its center along successive positions, creating a synchronized point cloud of detected centers for each of the sensors. The condition to consider a new reference point (ball center) for each point cloud is that each new point is separated from the previous one by a minimum distance pre-defined to 15 cm, but can be defined by the user. Finally, a sensor is chosen as reference and the remainder are calibrated relatively to it, one at a time, by using our own developed algorithms or ones available on the Point Cloud Library (PCL) [14] or on OpenCV [15]. 3 Ball detection algorithms The method to find the center of the ball depends on the type of data. In the following sections the methods used for the ball detection in the different sensors are described. 3.1 Sphere center detection in 3D data: SwissRanger The ball recognition using the SwissRanger is achieved using the PCL segmentation capabilities, namely the sample consensus module. The Random Sample Consensus (RANSAC) [16], which is an iterative method used to estimate parameters of a mathematical model from a set of data containing outliers is the algorithm applied to the point cloud measured with the SwissRanger. In this case, the model is a sphere, and so the resulting parameters are the coordinates of the center of the ball and its radius. Fig. 2 shows the detected ball computed after applying the RANSAC algorithm to the point cloud. 3.2 Sphere center detection on LIDAR lasers The method for 2D data is based on finding circular arcs, taking advantage of the particularity that any planar section of the ball is a circle. Thus, the process to detect the center of the ball is divided in the following sequence: segmentation of the laser scan, circle detection and calculation of its properties (center coordinates and radius), and calculation of the center of the ball given its diameter. It is important to mention that in 2D scans, due to the symmetry of the ball, there is an ambiguity relatively to which of the hemispheres belongs the detected circle, since every section of the ball has a symmetric section relatively to the hemisphere. This generates two solutions for the center of the ball (one above and another below the detected section) and, consequently, a priori information about the position of the sensor relatively to the ball is required. For the Sick LD-MRS, the idea was to solve this ambiguity problem taking advantage of its multi-layer technology, however, it was verified that when all the laser scans were on the lower ball hemisphere, the lowest scan did not measure the smallest circle diameter as expected, due to its own intrinsic error. Thus, it is required to give a priori information about is the sensor position relatively to the ball—below or above the ball’s equator. 3.2.1 Segmentation Segmentation is a very important part before the calibration process. The main goal is to cluster the point cloud in subsets of points which have high probability to belong to the same object through detected discontinuities in the laser data sequence, which are called break-points. Several methods are available to perform 2D point cloud segmentation. Based on the work of Coimbra [17], the Spatial Nearest Neighbor (SNN) is used since it has the most consistent results over different tested scenes. The SNN is a recursive algorithm where the distance between a point and all the other points that are not yet assigned to a cluster is computed. If that distance is smaller than a certain threshold the points are assigned to that cluster. Fig. 3 shows the result of the segmentation in a scan and the cluster related to the ball. The only variable in this algorithm is the threshold value D th , so it is expected that for a higher D th the result will be larger clusters, and for a smaller D th the result will be smaller clusters. 3.2.2 Circle detection The method used for circle detection is inspired on a work developed for line, arc/circle and leg detection from laser scan data [18]. The circle detection makes use of a technique named Internal Angle Variance (IAV), which is based on the trigonometric property that every point in an arc has congruent angles (angles that have the same amplitude) in respect to the extremes. This property can be verified in Fig. 4 . Let P 1 and P 4 be the extremes of the arc, and P 2 and P 3 random points belonging to the same arc. Then ∠ P 1 P 2 P 4 = ∠ P 1 P 3 P 4 because both angles measure one-half of ∠ P 1 O P 4 . The detection of circles involves calculating the mean of the aperture angle ( m ̄ ) between the extreme points and the remainder points of a cluster, as well as its standard deviation ( σ ). In an earlier approach, and considering that the scan covers approximately half a circle, values of standard deviation smaller than 8.6 ° and values of mean aperture between 90 ° and 135 ° are used to define a positive detection. However, those values depend on two factors: the error associated to the sensors, and how much of the circle is covered by the scan. Thus, after analyzing the results empirically, the values were adjusted to standard deviations under 5 ° , and values of mean aperture between 105 ° and 138 ° for the sick LMS151, and 10 ° and between 110 ° and 135 ° , respectively for the sick LD-MRS400001. These adjustments allowed to obtain the best results avoiding false circle detections. Considering that a segment S has n points ( P ), say S = { P 1 , P 2 , … , P n } , the mean and standard deviation of the angle are calculated as follows: m ̄ = 1 n − 2 ∑ i = 2 n − 1 ∠ P 1 P i P n and σ = 1 n − 2 ∑ i = 2 n − 1 ( ∠ P 1 P i P n − m ̄ ) 2 . 3.2.3 Calculation of the circle properties The calculation of the center and radius of the circle uses the method of least squares to find the circle that best fits the points. Given a finite set of points in R 2 , say { ( x i , y i ) | 0 ≤ i < N } , first calculate their mean values by x ̄ = 1 N ∑ i x i and y ̄ = 1 N ∑ i y i . Let u i = x i − x ̄ , v i = y i − y ̄ for 0 ≤ i < N , and define S u = ∑ i u i , S u u = ∑ i u i 2 , S u v = ∑ i u i v i , etc. The problem is then to solve first in ( u , v ) coordinates, and then transform back to ( x , y ) . Considering that the circle has center ( u c , v c ) and radius R , the main goal is to minimize S = ∑ i g ( u i , v i ) 2 , where g ( u , v ) = ( u − u c ) 2 + ( v − v c ) 2 − α , and α = R 2 . To do that, it is necessary to differentiate S ( α , u c , v c ) in order to the two variables, resulting in the following expressions: (1) u c S u u + v c S u v = 1 2 ( S u u u + S u v v ) and (2) u c S u v + v c S v v = 1 2 ( S v v v + S v u u ) . Solving expressions (1) and (2) simultaneously allows to obtain ( u c , v c ). Then, the center ( x c , y c ) of the circle in the original coordinate system can be obtained as: ( x c , y c ) = ( u c , v c ) + ( x ̄ , y ̄ ) , being α = u c 2 + v c 2 + S u u + S v v N . The result of the combination of circle detection with the properties of the circle is illustrated in Fig. 5 . 3.2.4 Calculation of the center of the ball After knowing all the properties of the circle, it is possible to calculate the center of the ball through trigonometric relations as shown in Fig. 6 , where R is the radius of the ball, R ′ the radius of the circle and d the distance between the center of the circle and the center of the ball, with d = R 2 − R ′ 2 . Taking into account the ambiguity for the 2D lasers mentioned in Section 3.2, and considering that the center of the circle is at ( x c , y c ), the coordinates of the center of the ball for this laser are defined as follows: ( X c , Y c , Z c ) = ( x c , y c , ± d ) . Fig. 7 illustrates an example of the ball detection in a 2D scan. A similar methodology is applied to each of the four layers from the Sick LD-MRS laser sensor and a circle has to be successfully detected in each layer. Thus, in order to simplify the calculations in the 3D multi-layer laser, the coordinates of the circle center are transformed into the X Y plane for each layer; then for each layer, the center of the ball is calculated in the same way as in the 2D laser. After this, the centers of the ball are transformed back into the respective original plane. At this point, for each layer, the center of the ball was calculated, which means that there are as many centers as layers. Thus, statistically, the center of the ball is obtained by calculating the mean of all the centers. Considering that there are n layers and the different circles have ( x c 1 , y c 1 , z c 1 ) , … , ( x c n , y c n , z c n ) coordinates, the center mean coordinates are defined as follows: ( X c , Y c , Z c ) = ( 1 n ∑ i = 1 n x c i , 1 n ∑ i = 1 n y c i , 1 n ∑ i = 1 n z c i ) . 3.3 Ball detection on cameras Giving continuity to the work presented in [19], the integration of the cameras on the calibration process was also addressed, initially with a non-uniform color ball (see Fig. 12), which complicated its detection, therefore the ball was replaced by a uniform color (red) ball. The first approach to detect a single color ball in 2D images was based on the Hough Transform implementation available in the OpenCV library. The Hough Transform is a method used to extract features of specific shapes in images. Since the calibration target is a ball, the Hough Circle Transform was used to find points in the image that best fit the ball, returning its parameters (apparent radius in pixels R pix = D pix / 2 , and center in pixels ( x c pix , y c pix ) ). In order to detect the ball with Hough Circle Transform a total of five parameters had to be specified: 1. Ball color in HSV; 2. Canny threshold for edge detection; 3. Accumulator threshold for center detection; 4. Minimum circle radius in pixels; 5. Maximum circle radius in pixels. In practice, the method proved to be unreliable, since constant adjustments to minimum and maximum circle radius were required to detect the ball when its position changed. Therefore a second algorithm was implemented. This second approach is based on the Ramer–Douglas–Peucker algorithm [20,21], implemented in the OpenCV approxPolyDP function. When given a set of points that defines a curve, the algorithm finds a similar curve with fewer points, an approximated curve. The initial set of points that define a curve are obtained from the ball’s contour. Approximated contours with 6 or more vertices are considered potential circles. The area limited by the original contour is calculated ( A r . cont ) and an up-right bounding rectangle is computed in order to know its width ( w b . rect ) and height ( h b . rect ). A circle is positively detected when: (i) the bounding rectangle has similar height and width (3); and (ii) the area limited by the contour is similar to the area of a circle with radius R pix = w b . rect + h b . rect 4 (4). Thresholds on Eqs. (3) and (4) were obtained through empirical experiments. (3) | 1 − w b . rect h b . rect | ⩽ 0.2 . (4) | 1 − A r . cont π r 2 | ⩽ 0.2 . Fig. 8 (b) shows the result of applying this method to the image in Fig. 8(a), successfully detecting the red ball and calculating its radius ( R pix ) and center ( x c pix , y c pix ). It is also much more reliable than Hough Transforms since only the ball’s color needs to be specified at the start of the calibration process. Knowing the intrinsic parameters of the camera (obtained previously with a traditional chessboard calibration method like the one available in OpenCV) with focal length α x and α y approximately equal to α , the real world ball diameter ( D ) and the ball diameter in pixels ( D pix = 2 R pix ), it is possible to determine the distance from the camera to the ball ( Z c ) through the following equation, (5) Z c = α D D pix . Knowing Z c , the center of the ball coordinates on the image ( x c pix , y c pix ), and once again using the camera intrinsic parameters, it is possible to solve Eq. (6) for the ball center coordinates in the camera coordinate frame ( X c , Y c , Z c ) . (6) Z c [ x c pix y c pix 1 ] = K [ X c Y c Z c 1 ] . A Stereo configuration with two Point Grey cameras was also implemented however, tests revealed large calibration errors and as such this paper will focus on monocular cameras. 4 Calibration 4.1 3D rigid transformation The purpose of 3D transformation estimation is to align the point clouds obtained by each sensor. This alignment results on the relative pose (position and orientation) between sensors in a global coordinate frame, such that the overlapping areas between the point clouds match as well as possible. To compute the transformation between sensors an Absolute Orientation algorithm is used. A variant of the Iterative Closest Point (ICP) algorithm is used to estimate the 3D translation and rotation between a pair of point clouds, available on PCL. The objective is to solve the rigid transformation T that minimizes the error of the point pairs. For that purpose, the ICP algorithm provided by PCL has two error minimization metrics: point-to-point and point-to-plane. For this work the point-to-point (7) metric was applied, where p n and q n are 3D points of N pair correspondences from the source and target cloud. (7) E ( T ) = ∑ n = 1 N ‖ T p n − q n ‖ 2 . Minimizing the sum of the Euclidean distances between corresponding points is a least-square problem, which can be solved by using a closed-form solution, based on the Singular Value Decomposition (SVD) method proposed in [22,23]. The method is used to estimate the lasers and camera 6 DOF pose (roll, pitch, yaw, X, Y and Z) from their 3D point clouds consisting of ball center position ( X c , Y c , Z c ) in their respective coordinate systems. 4.2 External camera calibration External camera calibration can be used as an alternative to the 3D rigid transformation for cameras. Similarly to the 3D rigid transformation method, a 3D target cloud is used, however instead of using the ball center position in the camera coordinate system ( X c , Y c , Z c ) as the source cloud, this method uses the ball coordinates in a reference 3D sensor (a LMS for example) and the pixel center coordinates in the image coordinate system ( x c pix , y c pix ) to evaluate the extrinsic parameters of the camera. Pose estimation of a calibrated camera with a set of 3D points and their corresponding 2D image projections is a Perspective-n-Point problem. The problem is solved resorting to an iterative method based on Levenberg–Marquardt optimization algorithm, finding a pose that minimizes reprojection error. The implementation used was the one available in the solvePnP function of OpenCV. Given the frequent existence of outliers in the source and target pointclouds, more stable results were obtained using the available RANSAC version of the same algorithm (solvePnPRansac). 5 Results Several experiments were carried out and results are focused in the evaluation of the following three fronts: (i) consistency of the ball detection for each laser sensor and SwissRanger; (ii) consistency in the 3D transformation estimation depending on the number of points used for each laser and SwissRanger; and (iii) global validation of the method for laser sensors and PointGrey cameras. The experiments carried out do not provide an absolute validation of the resulting estimated transformations from the calibration procedure, nevertheless, the obtained results demonstrate the validity of the method. 5.1 Consistency of ball detection In the ball detection consistency test, and in similar conditions, the ball was placed statically at different distances from the 3D sensors and, for each position, 500 samples of coordinates of the ball center were acquired. From these samples, a mean and a standard deviation were calculated. Fig. 9 shows the standard deviations of the measurements of the three sensors used on this test. Since the camera is not returning directly 3D information, it was not considered in this test. The results show that the variation is consistent with the error associated to each sensor as shown in Table 1 (below 10 cm for the Sick LD-MRS, and about 1 cm for the remaining sensors). In the case of the SwissRanger a greater variation is verified for the closest and furthest distances; it may be due to the proximity of the ball and a lower density of points for further distances, but a more detailed study for confirmation is needed. Nonetheless, the standard deviation in the detection of the ball is not larger than the error associated to the standalone sensors, indicating that the method does not introduce new measurement errors. For the Sick LD-MRS, even with results in accordance with its associated error, it is difficult to evaluate the interference of the distance of the ball on its detection, which can be possibly explained by the fluctuations in the data provided by the sensor; for example, it is possible to have a set of 500 measurements of a static object at 10 m, where 80% of the measurements have a deviation of ±0.01 m and the remainder 20% have a deviation of ±0.08 m. On the other hand, for the exact same conditions, if a new set of 500 measurements is obtained, a ratio of 50%/50% may be found instead. 5.2 Consistency of the geometric transformation In the second set of tests, a calibration among all the 3D sensors was performed using always the same setup; this was done by using different sizes of point clouds (number of points), where each point of the cloud corresponds to a different position of the ball along its motion in front of the sensors. Thus, for each size of the point clouds, a set of 20 calibrations was performed with the respective matrix of the estimated rigid transformation. The analysis of results compares the translation and rotation; the translation is obtained directly from the matrix of the associated rigid geometric transformation; considering the rotation matrix ( R ) from calibration and R x , R y and R z the generic rotations around each axis, R is defined as R = R x ( Roll ) R y ( Pitch ) R z ( Y aw ) , which allows to be solved and obtain the Euler angles ( Roll , Pitch , Y aw ). Then, as in the first test, a mean and a standard deviation of the translation and Euler angles are calculated, with the difference that the Euler angles are analyzed individually. Fig. 10 shows the results for the translation, and Fig. 11 presents the results for the Euler angles. Fig. 12 shows the setup used for the calibration and the estimated positions of the sensors after being calibrated, it can noted that in this setup the ball does not need to be in a single color since all the data is acquired directly with 3D sensors. As expected, the standard deviation decreases with the number of points, and stabilizes at around 20 points; for this test the minimum distance between consecutive points was 10 cm. The mean variation from point clouds of 20 points is lower than 10 cm and about 4 ° or 5 ° , which, once again, is in the range of the standalone sensors errors. 5.3 Validation of the method The difficulty of having a ground-truth for the sensors relative position complicates the absolute validation of the method, which would allow the comparison of the estimated transformation with the exact transformation among sensors. It is very difficult to guarantee with precision if the obtained calibration is correct, given the difficulty to measure exactly the correct pose between pairs of sensors. This problem is particularly difficult with rotations (it is possible to have a fairly reasonable evaluation of the translation with simple measurements using a measuring tape), which is critical, since small errors in rotation may result in large error in the final registered data. With this problem in mind, our goal has been to perform a calibration of all the devices with respect to the sensor with better accuracy. Then, the point cloud of the reference sensor is used as ground-truth, and on the remainder sensors the resulting transformation from the calibration is applied. This allows to evaluate how well the point clouds fit in the reference point cloud by calculating the absolute mean error and the standard deviation of the corresponding points. This test was performed with a full size vehicle (ATLASCAR) to evaluate the method on real conditions. And since these tests took place outdoors, the devices used are the ones that will have some functionality on the ATLASCAR project, which means that the set of sensors does not include the SwissRanger, since sunlight interferes with the infrared light it projects. The sensors used in this experiment are: the Sick LD-MRS, the two Sick LMS151 and a Point Grey camera. One of the Sick LMS151 was chosen as reference (henceforth named Sick LMS151(A)) and the remainder are calibrated relatively to it. The experience has the following steps: 1. Choose one of the sensors as reference (in this case the Sick LMS151(A) was chosen given its higher accuracy); 2. Point cloud acquisition for each sensor and calibrate them with respect to the reference sensor; 3. Apply the resulting transformations to the respective point cloud of each sensor; 4. Results evaluation: (a) For transformations calculated with the 3D rigid transformation method, described in Section 4.1, the mean euclidean distance and respective standard deviation between corresponding points of the source point cloud and target point cloud (LMS151(A)) is used to evaluate its results; (b) For the LMS151(A) to camera transformation, calculated by the external camera calibration method as described in Section 4.2, the mean reprojection error is used to evaluate its results; (c) Visual assessment. The acquisition of the point clouds was performed with two different approaches: (i) following a grid-pattern and, (ii) acquiring random points during the motion of the ball in front of the sensors. The idea of acquiring a pattern is to indirectly establish a better ground-truth. The acquisition of random points is intended to evaluate the calibration method in a real world scenario. Fig. 13 shows the setup mounted on ATLASCAR, where the Sick LD-MRS is placed on the top of a box since there is not yet a support on the car for it; nonetheless, its final location will not be much different from where it is shown in Fig. 13. 5.3.1 Grid pattern calibration To perform the calibration with a grid pattern, 15 markers were positioned on the ground, in a 3×5 grid arrangement, as can be seen in Fig. 14 (a). The ball was then placed on top of each marker and a point cloud was acquired for each sensor. In order to evaluate the grid pattern, Fig. 14(b) shows the acquired point cloud for Sick LMS151(A) (reference sensor) overlayed by a virtual grid. With a perfect grid and laser, the distance between a ball center on grid point i and a ball center on grid point i + 1 , would be 1 m. To evaluate this condition, the vertical and horizontal euclidean distance between ball centers and ball centers was calculated and is represented on Fig. 14(c), which shows that the mean overall distance between ball centers is 1.027 m, only 2.7 cm above what would be expected from a perfect laser and grid. The acquired point clouds for each sensor are presented on Fig. 15 (a) with respect to their own coordinate system. The 3D rigid transformation method is applied to estimate the geometric transformations between the Sick LMS151(A) and the other sensors, whose point clouds are displayed on Fig. 15(a). Regarding the camera, as discussed in Section 4, there is an alternative method, based on the external camera calibration (Section 4.2), which resorts to a point cloud of ball centers in the image coordinate system (see Fig. 15(b)) and the 3D point cloud from Sick LMS151(A). Resulting transformations from both methods are then applied to the point cloud and pose of each sensor. To evaluate the quality of the point cloud fitting obtained through the 3D rigid transformation method, we present on Table 2 , the mean absolute error and standard deviation, based on the euclidean distance between corresponding points of a sensor point cloud and the Sick LMS151(A) point cloud. For both Sick laser sensors the results from Table 2 are expected since they do not differ much from results obtained in Section 5.1 and their own intrinsic errors. Concerning the Point Grey camera, Table 2 shows an error larger than expected, indicating that the point cloud fitting is not as good as with the lasers. Concerning the external camera calibration method, the same metric cannot be applied since the camera’s point cloud is in the image coordinate system. However, it is possible to project the Sick LMS151(A) point cloud to the image coordinate system, using the geometric transformation obtained by this method. Thus, making it also possible to compute the mean reprojection error of such transformation—in this case the reprojection error is 3.7 pixels. Fig. 16 shows the initial ball centers detected by the camera and the projected 3D Sick LMS151(A) points. To visually assess the lasers and camera pose relative to the car as well as the point cloud alignment, Fig. 17 is presented. To ease the visual assessment a Ford Escort SW 98 3D model (same as the ATLASCAR) was placed in the scene. However, the model does not include any of the extra equipment used on the actual car, like the supports for the sensors. In Fig. 17 sensors are represented by arrows; for lasers the arrow points in the X axis direction of their own frame and for cameras it points in the their Z axis direction. A comparison between Figs. 13 and 17 shows that the calibration results for the Sick LD-MRS is very close to its real position, however the Sick LMS151 is a few centimeters above its real position. Regarding the two methods used to estimate the camera pose, comparing Figs. 13 and 17, we conclude that the external camera calibration method yielded better results. The source for such a significant error with the 3D rigid transform method is probably related to the fact that the ball center coordinates in the camera coordinate system, ( X c , Y c , Z c ) , are calculated based on the detected diameter. Since Z c is inversely proportional to the detected diameter, Eq. (5) and is then used to calculate X c and Y c , Eq. (6). Even a slight variation of one or two pixels in the detected ball diameter during calibration, causes errors in the range of centimeters in all X c , Y c and Z c coordinates. It was observed that even though the diameter frequently suffers small variations, the detected ball center was much more stable, which explains the better results with the external camera calibration method. 5.3.2 Calibration with random acquisition spots The calibration method does not need to follow any particular pattern with the corresponding points. Thus, this new test is intended to evaluate a calibration using points in random positions, allowing for a higher point density. To take advantage of this, we chose to increase the number of points per point cloud to 25. The acquired point clouds for each sensor with respect to their own frame are shown in Fig. 18 (a) for point clouds that use the 3D transform method; Fig. 18(b) shows the detected ball centers in the image coordinate system to be used with the external camera calibration method. Again, we chose Sick LMS151(A) as the reference sensor. After the calibration process, the estimated transformation was applied on the corresponding point cloud. Then, as in the previous test, the point cloud fitting mean absolute errors and standard deviation for the 3D rigid transform method were computed and are presented on Table 3 . Regarding the external camera calibration method, similarly to the grid pattern test, Sick’s LMS151(A) 3D point cloud was projected to the image plane and a mean reprojection error of 3.3 pixels was obtained; Fig. 19 shows the reprojection error. The final arrangement of the sensor after calibration (with the Ford Escort SW 98 3D model) is shown on Fig. 20 . Comparing Figs. 13, 17 and 20, Sick’s LMS151 position and the Point Grey camera position, obtained from the 3D rigid transform method, have improved on this test, while Sick’s LD-MRS position is identical on both tests. The overall improvement on 3D rigid transform results is likely due to the increase in points per point cloud. The external camera calibration method results appear to be slightly worse than the 3D rigid transform method. However, since the 3D model is not an exact replica of ATLASCAR, no conclusion can be drawn from Fig. 20 about which method is more accurate. 6 Conclusions and future work This paper proposes a new automatic calibration methodology to seamlessly calibrate different 2D/3D sensors and cameras by using a ball as calibration target. In the current version, the only a priori parameter required is the relative position between laser scanners and the ball—whether it is placed above or below its equator. The good results on the consistency tests for the ball detection and transformation estimation were confirmed with the validation tests by the small errors that were obtained on the calibration of the LIDAR lasers. Regarding the camera, tests showed that both 3D rigid transformation method and external camera calibration method were capable of yielding good results. Although the external camera calibration was the most consistent of the two. Future work includes improving the camera calibration results and designing an absolute ground-truth experiment to evaluate the estimated transformations. It is also planned to impose full 3D motion paths on the ball, possibly hopping or bouncing in the sensors fields, to solve all ambiguities for any positions of the sensors, and thus requiring no a priori information on hemisphere position relative to lasers. Also, it would be interesting to use external high precision devices (for example a motion capture setup) for a better validation of the method. Finally, in order to simplify the use of the method, an interface is to be developed to be a more user friendly method, including possibly a software package to be released for the ROS community. References [1] V. Santos, J. Almeida, E. Avila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, ATLASCAR—Technologies for a computer assisted driving system on board a common automobile, in: 13th International IEEE Conference on Intelligent Transportation Systems, ITSC2010, Madeira Island, Portugal, 2010. [2] J. Dias Distanciómetro 3D baseado numa unidade laser 2D em movimento contínuo (Masters’ thesis) 2009 Universidade de Aveiro [3] J. Levinson S. Thrun Automatic online calibration of cameras and lasers Robotics: Science and Systems (RSS) 2013 [4] Y. Bok, D. Choi, I. Kweon, Generalized laser three-point algorithm for motion estimation of camera-laser fusion system, in: IEEE International Conference on Robotics and Automation, ICRA, 2013, pp. 2880–2887. [5] J. Shi, C. Tomasi, Good features to track, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1994, pp. 593–600. [6] Y. Bok, D.-G. Choi, Y. Jeong, I.S. Kweon, Capturing citylevel scenes with a synchronized camera-laser fusion sensor, in: Proceedings of the IEEE/RSf International Conference on Intelligent Robots and Systems, 2011, pp. 4436–4441. [7] S.A. Rodriguez F., V. Fremont, P. Bonnifait, Extrinsic calibration between a multi-layer lidar and a camera, in: IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, 2008, pp. 214–219. [8] S. Debattisti, L. Mazzei, M. Panciroli, Automated extrinsic laser and camera inter-calibration using triangular targets, in: IEEE Intelligent Vehicles Symposium, IV, 2013, pp. 696–701. [9] M. Almeida P. Dias M. Oliveira V. Santos 3D-2D Laser Range Finder calibration using a conic based geometry shape Aurélio Campilho Mohamed Kamel. Image Analysis and Recognition 2012 Springer Verlag 312 319 [10] J. Underwood, A. Hill, S. Scheding, Calibration of range sensor pose on mobile platforms, in: IEEE/RSJ International Conference on Intelligent Robots and Systems, 2007, pp. 3866–3871. [11] G. Chao, J. Spletzer, On-line calibration of multiple LIDARs on a mobile vehicle platform, in: IEEE International Conference on Robotics and Automation, ICRA, 2010, pp. 279–284. [12] E. Fernandez-Moral J. Gonzalez-Jiménez V. Arévalo Extrinsic calibration of 2D laser rangefinders from perpendicular plane observations Int. J. Robot. Res. 2015 [13] M. Ruan, D. Huber, Extrinsic calibration of 3D sensors using a spherical target, in: Second International Conference on 3D Vision, 3DV, 2014, pp. 187–193. [14] R.B. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: IEEE International Conference on Robotics and Automation, ICRA, 2011, pp. 1–4. [15] G. Bradski The OpenCV Library Dr Dobbs J. Softw. Tools 25 2000 120 125 [16] M.A. Fischler R.C. Bolles Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [17] D. Coimbra LIDAR Target Detection and Segmentation in Road Environment (Masters’ thesis) 2013 Universidade de Aveiro [18] J. Xavier, M. Pacheco, D. Castro, A. Ruano, Fast line, arc/circle and leg detection from laser scan data in a Player driver, in: Proceedings of the 2005 IEEE International Conference on Robotics and Automation, ICRA, 2005, pp. 3930–3935. [19] M. Pereira, V. Santos, P. Dias, Automatic calibration of multiple LIDAR sensors using a moving sphere as target, in: Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Vol. 1, 2015, pp. 477–486. [20] U. Ramer An iterative procedure for the polygonal approximation of plane curves Comput. Graph. Image Process. 1 3 1972 244 256 [21] D.H. Douglas T.K. Peucker Algorithms for the reduction of the number of points required to represent a digitized line or its caricature Cartogr.: Int. J. Geogr. Inf. Geovisualization 10 2 1973 112 122 [22] K.S. Arun T.S. Huang S.D. Blostein Least-squares fitting of two 3-D point sets IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 9 5 1987 698 700 [23] B.K.P. Horn Closed-form solution of absolute orientation using unit quaternions J. Opt. Soc. Am. A 4 4 1987 629 642 Marcelo Pereira obtained his Master’s Degree in Mechanical Engineering in 2015 at the University of Aveiro, Portugal. During his later graduation studies he focused especially on applications in automation, computer vision robotics, and programming in the ROS framework. His first technical publication took place in the ROBOT2015 conference. He played futsal during three years in Atlético Clube do Luso, being team captain in his last year. His current interests are related to robotics applications, application of perception systems in autonomous navigation and vision systems. David Silva is currently writing his Master’s Degree Thesis in Mechanical Engineering at the University of Aveiro, Portugal, within the Laboratory for Automation and Robotics (LAR). His Thesis is entitled “Multisensory navigation based on LIDAR for ATLASCAR”. During his Master’s Degree he specialized in programming, computer vision, automation, robotics and computerized numerical control. Vitor Santos obtained a 5-year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990–1994 at the Joint Research Center, Italy. He is currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or co-supervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He is also co-founder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. "
    },
    {
        "doc_title": "Adaptive Robot Biped Locomotion with Dynamic Motion Primitives and Coupled Phase Oscillators",
        "doc_scopus_id": "84957016680",
        "doc_doi": "10.1007/s10846-016-0336-1",
        "doc_eid": "2-s2.0-84957016680",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Adaptive behavior",
            "Control architecture",
            "Coupled phase oscillators",
            "Dynamic movement primitives",
            "External disturbances",
            "Movement primitives",
            "Phase resetting",
            "Real world environments"
        ],
        "doc_abstract": "© 2016, Springer Science+Business Media Dordrecht.In order to properly function in real-world environments, the gait of a humanoid robot must be able to adapt to new situations as well as to deal with unexpected perturbations. A promising research direction is the modular generation of movements that results from the combination of a set of basic primitives. In this paper, we present a robot control framework that provides adaptive biped locomotion by combining the modulation of dynamic movement primitives (DMPs) with rhythm and phase coordination. The first objective is to explore the use of rhythmic movement primitives for generating biped locomotion from human demonstrations. The second objective is to evaluate how the proposed framework can be used to generalize and adapt the human demonstrations by adjusting a few open control parameters of the learned model. This paper contributes with a particular view into the problem of adaptive locomotion by addressing three aspects that, in the specific context of biped robots, have not received much attention. First, the demonstrations examples are extracted from human gaits in which the human stance foot will be constrained to remain in flat contact with the ground, forcing the “bent-knee” at all times in contrast with the typical straight-legged style. Second, this paper addresses the important concept of generalization from a single demonstration. Third, a clear departure is assumed from the classical control that forces the robot’s motion to follow a predefined fixed timing into a more event-based controller. The applicability of the proposed control architecture is demonstrated by numerical simulations, focusing on the adaptation of the robot’s gait pattern to irregularities on the ground surface, stepping over obstacles and, at the same time, on the tolerance to external disturbances.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Proprioceptive visual tracking of a humanoid robot head motion",
        "doc_scopus_id": "84978793013",
        "doc_doi": "10.1007/978-3-319-41501-7_56",
        "doc_eid": "2-s2.0-84978793013",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Angular displacement",
            "Angular positions",
            "Humanoid robot",
            "Illumination conditions",
            "Inertial sensor",
            "SURF",
            "Visual feature",
            "Visual Tracking"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper addresses the problem of measuring a humanoid robot head motion by fusing inertial and visual data. In this work, a model of a humanoid robot head, including a camera and inertial sensors, is moved on the tip of an industrial robot which is used as ground truth for angular position and velocity. Visual features are extracted from the camera images and used to calculate angular displacement and velocity of the camera, which is fused with angular velocities from a gyroscope and fed into a Kalman Filter. The results are quite interesting for two different scenarios and with very distinct illumination conditions. Additionally, errors are introduced artificially into the data to emulate situations of noisy sensors, and the system still performs very well.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Biped walking learning from imitation using dynamic movement primitives",
        "doc_scopus_id": "84952311012",
        "doc_doi": "10.1007/978-3-319-27149-1_15",
        "doc_eid": "2-s2.0-84952311012",
        "doc_date": "2016-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Inter-limb coordination",
            "Learning by demonstration",
            "Motion capture",
            "Movement primitives",
            "Non-linear oscillators"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.Exploring the full potential of humanoid robots requires their ability to learn, generalize and reproduce complex tasks that will be faced in dynamic environments. In recent years, significant attention has been devoted to recovering kinematic information from the human motion using a motion capture system. This paper demonstrates the use of a VICON system to capture human locomotion that is used to train a set of Dynamic Movement Primitives. These DMP can then be used to directly control a humanoid robot on the task space. The main objectives of this paper are: (1) to study the main characteristics of human natural locomotion and human “robot-like” locomotion; (2) to use the captured motion to train a DMP; (3) to use the DMP to directly control a humanoid robot in task space. Numerical simulations performed on V-REP demonstrate the effectiveness of the proposed solution.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representations for autonomous driving: An approach based on polygonal primitives",
        "doc_scopus_id": "84951868739",
        "doc_doi": "10.1007/978-3-319-27146-0_39",
        "doc_eid": "2-s2.0-84951868739",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3-D scene representation",
            "Autonomous driving",
            "Autonomous Vehicles",
            "Geometric structure",
            "Macro scale",
            "Novel methodology",
            "Point cloud",
            "Scene reconstruction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.In this paper, we present a novel methodology to compute a 3D scene representation. The algorithm uses macro scale polygonal primitives to model the scene. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Results show that the approach is capable of producing accurate descriptions of the scene. In addition, the algorithm is very efficient when compared to other techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pedestrian pose estimation using stereo perception",
        "doc_scopus_id": "84951829074",
        "doc_doi": "10.1007/978-3-319-27146-0_38",
        "doc_eid": "2-s2.0-84951829074",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Dangerous situations",
            "Hierarchical search",
            "Monte Carlo techniques",
            "Orientation information",
            "Pedestrian models",
            "Pose estimation",
            "Stereo perception",
            "Stereo vision system"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents an algorithm to perform pedestrian pose estimation using a stereo vision system in the Advanced Driver Assistance Systems (ADAS) context. The proposed approach isolates the pedestrian point cloud and extracts the pedestrian pose using a visibility based pedestrian 3D model. The model accurately predicts possible self occlusions and uses them as an integrated part of the detection. The algorithm creates multiple pose hypotheses that are scored and sorted using a scheme reminiscent of the Monte Carlo techniques. The technique performs a hierarchical search of the body pose from the head position to the lower limbs. In the context of road safety, it is important that the algorithm is able to perceive the pedestrian pose as quickly as possible to potentially avoid dangerous situations, the pedestrian pose will allow to better predict the pedestrian intentions. To this end, a single pedestrian model is used to detect all pertinent poses and the algorithm is able to extract the pedestrian pose based on a single stereo depth point cloud and minimal orientation information. The algorithm was tested against data captured with an industry standardmotion capture system. Accurate results were obtained, the algorithm is able to correctly estimate the pedestrian pose with acceptable accuracy. The use of stereo setup allows the algorithm to be used in many varied contexts ranging from the proposed ADAS context to surveillance or even human-computer interaction.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic calibration of multiple LIDAR sensors using a moving sphere as target",
        "doc_scopus_id": "84951782220",
        "doc_doi": "10.1007/978-3-319-27146-0_37",
        "doc_eid": "2-s2.0-84951782220",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D data",
            "Automatic calibration",
            "Automatic method",
            "Calibration system",
            "Fitting techniques",
            "Point cloud",
            "Rigid body transformation",
            "Sensor calibration"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.The number of LIDAR sensors installed in robotic vehicles has been increasing, which is a situation that reinforces the concern of sensor calibration. Most calibration systems rely on manual or semi-automatic interactive procedures, but fully automatic methods are still missing due to the variability of the nearby objects with the point of view. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. This is indeed feasible if a ball is placed in motion in front of the set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation between all pairs of sensors. This paper proposes and describes such a method with encouraging preliminary results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Leader following: A study on classification and selection",
        "doc_scopus_id": "84948583085",
        "doc_doi": "10.1016/j.robot.2014.09.028",
        "doc_eid": "2-s2.0-84948583085",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Dynamic environments",
            "Dynamic obstacles",
            "Dynamic scenarios",
            "Human-aware",
            "Leader following",
            "Leader selection",
            "Motion prediction",
            "Robotic navigation"
        ],
        "doc_abstract": "© 2015 Elsevier B.V. All rights reserved.This work proposes a different form of robotic navigation in dynamic environments, where the robot takes advantage of the motion of pedestrians, in order to improve its own navigation capabilities. Instead of treating persons as dynamic obstacles that should be avoided, here they are treated as special agents with an expert knowledge on navigating in dynamic scenarios. This work proposes that the robot selects and follows leaders, in order to move along optimal paths, deviate from undetected obstacles, improve navigation in densely populated areas and increase its acceptance by other humans. To accomplish this proposition, two novel approaches are developed in the area of leader selection. In the first, a motion prediction approach is used, to detect candidates that are moving to the same place that the robot is. In the second, a machine learning algorithm is trained with real examples and is used to select the best leader among several candidates. Experiments with a real robot are performed to validate the proposed approaches.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2014-10-02 2014-10-02 2015-11-27 2015-11-27 2015-11-27T21:27:11 S0921-8890(14)00213-9 S0921889014002139 10.1016/j.robot.2014.09.028 S300 S300.1 FULL-TEXT 2016-09-03T01:54:41.162154-04:00 0 0 20160101 20160131 2016 2014-10-02T16:07:39.336942Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb vol volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst primabst ref vitae 0921-8890 09218890 true 75 75 PA Volume 75, Part A 9 79 95 79 95 201601 January 2016 2016-01-01 2016-01-31 2016 Assistance and Service Robotics in a Human Environment Dr. Yacine Amirat Dr. David Daney Dr. Samer Mohammed Dr. Anne Spalanzani Dr. Abdelghani Chibani Prof. Olivier Simonin article fla Copyright © 2014 Elsevier B.V. All rights reserved. LEADERFOLLOWINGASTUDYCLASSIFICATIONSELECTION STEIN P 1 Introduction 2 Selecting and following a leader based on goal prediction 2.1 Selection among several leaders 2.2 Leader following navigation methods 2.2.1 Navigating without a leader: RiskRRT motion planner 2.3 Experiments 2.3.1 Experiments using recorded data 2.3.2 Experiments using a pedestrian simulator 2.3.3 Discussion on leader selection and following using goal similarity 3 Leader classification using machine learning 3.1 AdaBoost classifier 3.2 Features study 3.3 Leader classification using learning 3.3.1 Data acquisition 3.3.2 Extraction of features 3.3.3 Data labeling 3.3.4 Training and performance evaluation 3.3.5 Discussion of results 3.3.5.1 Feature contribution 3.4 Experiments on leader selection and following using learning 3.4.1 Platform and setup 3.4.2 Tests on switching navigation method 3.4.3 Tests on leader switching 3.4.4 Tests on leader following among crowds 4 Conclusions Acknowledgment References BENNEWITZ 2005 31 M RIOSMARTINEZ 2011 2014 2019 J 2011IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS UNDERSTANDINGHUMANINTERACTIONFORPROBABILISTICAUTONOMOUSNAVIGATIONUSINGRISKRRTAPPROACH TRAUTMAN 2010 797 803 P 2010IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS UNFREEZINGROBOTNAVIGATIONINDENSEINTERACTINGCROWDS MOMBAUR 2009 369 383 K ALTHOFF 2011 5407 5412 D 2011IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA SAFETYASSESSMENTTRAJECTORIESFORNAVIGATIONINUNCERTAINDYNAMICENVIRONMENTS HENRY 2010 981 986 P 2010IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA LEARNINGNAVIGATETHROUGHCROWDEDENVIRONMENTS HELBING 2001 361 384 D MAKRIS 2002 895 903 D JOHNSON 1996 609 615 N ELLIS 2009 1229 1234 D 2009IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS MODELLINGPEDESTRIANTRAJECTORYPATTERNSGAUSSIANPROCESSES IKEDA 2012 T ROBOTICSSCIENCESYSTEMSVIII MODELINGPREDICTIONPEDESTRIANBEHAVIORBASEDSUBGOALCONCEPT VASQUEZGOVEA 2009 1486 1506 D HALL 1966 E HIDDENDIMENSION LAVALLE 2001 378 S FOX 1997 23 33 D DIJKSTRA 1959 269 271 E VAUGHAN 2008 189 208 R HELBING 1995 4282 4286 D HELBING 2000 487 490 D TRAUTMAN 2013 2153 2160 P 2013IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA ROBOTNAVIGATIONINDENSEHUMANCROWDSCASEFORCOOPERATION TAKAYAMA 2009 5495 5502 L IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS2009 INFLUENCESPROXEMICBEHAVIORSINHUMANROBOTINTERACTION FREUND 1997 119 139 Y SUSNJAK 2009 T ACCELERATINGCLASSIFIERTRAININGUSINGADABOOSTWITHINCASCADESBOOSTEDENSEMBLESSCIENCEINCOMPUTERSCIENCES DRAUSCHKE 2008 M FEATURESUBSETSELECTIONADABOOSTADTBOOSTTECHREPTECHREPTDEPARTMENTPHOTOGRAMMETRY ALMEIDA 2010 J TARGETTRACKINGUSINGLASERRANGEFINDEROCCLUSION KOHLBRECHER 2011 S PROCIEEEINTERNATIONALSYMPOSIUMSAFETYSECURITYRESCUEROBOTICSSSRR AFLEXIBLESCALABLESLAMSYSTEMFULL3DMOTIONESTIMATION STEINX2016X79 STEINX2016X79X95 STEINX2016X79XP STEINX2016X79X95XP UnderEmbargo 2017-11-27T00:00:00Z item S0921-8890(14)00213-9 S0921889014002139 10.1016/j.robot.2014.09.028 271599 2015-11-27T17:07:34.477379-05:00 2016-01-01 2016-01-31 true 3745523 MAIN 17 54128 849 656 IMAGE-WEB-PDF 1 gr24 41666 164 204 gr4 26819 78 219 gr2 18568 103 219 gr14 20754 73 219 pic2 34020 164 140 gr15 18192 74 219 gr22 17923 77 219 gr8 17241 70 219 gr21 34310 164 123 gr12 17473 137 219 gr1 20068 68 219 gr5 20540 76 219 gr20 20439 103 219 gr13 31314 164 157 pic3 29468 164 140 gr7 25466 164 177 gr17 18391 121 219 gr6 27405 164 146 gr25 37475 163 157 gr23 16898 93 219 gr9 22479 164 190 gr19 21912 80 219 pic4 32302 164 140 gr11 13297 36 219 pic1 30550 164 140 gr18 28621 132 219 gr16 19513 102 219 gr26 41593 164 204 gr3 15836 86 219 gr10 19572 86 219 gr24 137752 472 586 gr4 64336 172 482 gr2 47404 143 306 gr14 62787 201 601 pic2 39694 132 113 gr15 45253 115 339 gr22 65911 200 570 gr8 50053 122 382 gr21 74852 326 245 gr12 49323 240 383 gr1 54324 163 525 gr5 70508 176 506 gr20 107337 331 700 gr13 51306 160 153 pic3 40021 132 113 gr7 63692 282 305 gr17 51506 202 367 gr6 96642 429 382 gr25 159034 633 608 gr23 57470 217 509 gr9 59303 243 282 gr19 53241 140 382 pic4 37511 132 113 gr11 45060 99 600 pic1 37966 132 113 gr18 121760 333 555 gr16 53006 178 382 gr26 137061 472 586 gr3 39524 111 282 gr10 95441 235 600 si56 113 8 9 si21 175 12 18 si71 146 14 11 si31 155 11 16 si81 178 17 18 si17 135 11 11 si75 175 10 24 si46 503 17 107 si37 701 22 155 si7 437 15 101 si39 327 11 73 si68 156 15 14 si14 278 15 54 si78 209 13 29 si10 167 11 21 si26 131 8 11 si63 814 31 183 si11 180 14 17 si8 556 15 143 si12 189 14 23 si9 157 11 15 si57 504 37 112 si69 133 12 9 si83 207 17 24 si2 379 12 74 si28 242 15 33 si80 193 17 19 si48 486 20 104 si16 126 11 11 si18 142 11 12 si1 376 13 74 si29 266 17 35 si30 318 19 42 si74 117 8 19 si62 1178 86 184 si43 145 11 15 si67 259 15 35 si60 152 11 15 si47 258 11 54 si41 355 17 53 si6 324 15 54 si65 484 15 127 si77 189 13 23 si27 175 19 15 si40 140 11 10 si20 192 14 21 si13 196 15 24 si36 199 19 22 si42 193 17 21 si5 133 11 11 si38 514 19 108 si49 1100 37 242 si3 130 9 12 si44 182 17 16 si24 134 11 11 si35 155 15 12 si64 231 15 31 si22 1472 45 301 si85 123 8 10 si45 716 20 159 si33 373 19 56 si76 158 13 17 si32 490 22 95 si82 207 17 22 si19 173 14 19 si79 222 13 29 si23 122 8 9 ROBOT 2361 S0921-8890(14)00213-9 10.1016/j.robot.2014.09.028 Elsevier B.V. Fig. 1 Two instants in the goal prediction of a moving person. In the left image, there are two possible destinations, but after some iterations the prediction converges to a single destination region (right image). The height of the bars is proportional to the probability of a cell to be the final goal. Fig. 2 Visualization of leader selection rules. The robot is represented by the blue square and its goal by the blue crossed square. The candidates are represented by colored circles and their goals as crossed circles, with the respective colors. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Illustration of the leader following navigation mode. Fig. 4 (a) Markers used for the tracking system and a camera fitted with wide angle lens; (b) INRIA Rhône Alpes’ entrance hall. Fig. 5 Images of the recorded training data and the GHMM structure, the large circles represent the interest points of the scenario. (a) Recorded trajectories of subject’s moving among interest points. (b) Resulting GHMM structure, showing states as nodes and transitions as edges. Fig. 6 Leader selection and following using the goal similarity criteria, predicted by a GHMM. The robot is represented by the light gray rectangle and its goal by an X. Three leader candidates are represented by circles with letters R, G and B. The predicted goals are the triangles with the corresponding letters. Fig. 7 Experiment of a leader guiding the robot to avoid two incoming persons. The robot is represented by the light gray rectangle and its goal by an X. Three leader candidates are represented by circles with letters R, G and B. The predicted goals are the triangles with the corresponding letters. Fig. 8 Workflow and description of the forces used in the pedestrian simulator. Fig. 9 Lane formation phenomena: in densely populated environments, crossing groups of people naturally arrange themselves into crossing lanes, which results in a more efficient navigation. Fig. 10 Performance comparison of three navigation techniques in a narrow corridor. Left column: A ∗ + DWA , middle: RiskRRT, right: leader following. Each row depicts the same instant during tests, which is a time step of 5 s. Agents are represented by circles, the robot by the black rectangle and its path by a sequence of red arrows, showing its orientation in each instant. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Initial conditions for the quantitative comparison of navigation techniques using three different densities of simulated agents. The size of the corridor is approximately 12 m long and 4.5 m wide. The black point in the right end of each example is the goal of the robot. Fig. 12 Quantitative comparison of three navigation methods: Leader following, DWA + A ∗ and RiskRRT. Time average of 10 trials with minimum and maximum values. Fig. 13 The robot used for data acquisition when following pedestrians. Fig. 14 (a) Image from a wide-angle camera. (b) Representation of the laser measurements, showing the three tracked subjects and their orientation and velocity as vectors, the robot is represented as a rectangle. Fig. 15 Setup of modules used for data acquisition. Fig. 16 Setup of modules used for data extraction. Fig. 17 Graphical representation of the features extracted during the said procedure. The robot is represented as the blue square while the human being followed as the red circle. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 18 Example of data labeled by three individuals, the gray area represents the bad leader labels group, and the white region before it is the good leader group. The vertical red line in all graphs is the mean transition instant, computed based on the inputs of the three participants. For the final label, all measurements after the red mark are given a bad leader label. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 19 Diagram of the data labeling process. Fig. 20 Feature contribution ratio using 24 features, number of weak learners: 548. Fig. 21 Experimental platform, showing the two LIDARs and an RGBD camera. Fig. 22 Experimental architecture, showing the different modules used for leader detection, classification, selection and following. Fig. 23 Leader score example. Any subject has a minimum score of −0.1 and a maximum of 1. Any person with a score higher than 0 is considered as a good leader and can be selected to be followed. Fig. 24 Switching navigation method between leader following and independent navigation. In image 1 the robot is engaged in leader following, while in the remaining it uses RiskRRT for the navigation. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 25 Two tests on leader switching. On the left column the leader stops and the robot switches to follow another person. On the right a better leader appears on the scene. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 26 Leader following in a crowded environment. By selecting and following a leader the robot manages to successfully navigate through a difficult situation. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Table 1 Labels from each evaluator and the final label. Test Situation and time tag (s) pro rich jor Average 1 st (27.0) st (27.0) st (27.0) st (27.0) 2 fr (9.0) fr (11.5) fr (8.5) fr (9.0) 3 as (XX) fr (12.5) fr (13.0) fr (12.75) 4 fr (XX) gd (—) gd (—) gd (—) 5 as (18.0) st (XX) as (18.0) as (18.0) Table 2 Contribution ratio of features. Features Size of feature set 24 16 12 8 σ distance 0.077 0.085 0.104 0.181 Lateral displacement 0.120 0.130 0.118 0.169 Distance 0.120 0.131 0.125 0.122 Angle to robot 0.087 0.080 0.090 0.119 Sagittal displacement 0.052 0.053 0.078 0.118 Relative heading 0.078 0.083 0.081 0.113 Target velocity 0.068 0.068 0.082 0.093 σ relative velocity in y 0.057 0.062 0.075 0.085 Table 3 Relative error in leader classification. Features Leader situations st I st II gd I gd II as I as II fr I nm I nm II od I od II Average False good 24 0.062 0.000 0.000 0.000 0.098 0.145 0.012 0.066 0.000 0.000 0.000 0.036 16 0.062 0.000 0.000 0.000 0.098 0.145 0.005 0.044 0.000 0.000 0.000 0.033 12 0.062 0.000 0.000 0.000 0.071 0.145 0.005 0.017 0.000 0.000 0.000 0.028 8 0.046 0.000 0.000 0.000 0.047 0.145 0.005 0.039 0.000 0.000 0.000 0.026 False bad 24 0.000 0.110 0.000 0.010 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.011 16 0.000 0.120 0.000 0.006 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.011 12 0.000 0.124 0.000 0.004 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.011 8 0.069 0.129 0.014 0.006 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.015 Leader following: A study on classification and selection Procópio Stein a ⁎ Anne Spalanzani a b c Vítor Santos d Christian Laugier a a Institut National de Recherche en Informatique et Automatique, INRIA, 655 Avenue de l’Europe, 38334 Saint Ismier cedex, France Institut National de Recherche en Informatique et Automatique, INRIA 655 Avenue de l’Europe Saint Ismier cedex 38334 France b Université Grenoble Alpes, 1030 Avenue Centrale, 38400 Saint-Martin-d’Hères, France Université Grenoble Alpes 1030 Avenue Centrale Saint-Martin-d’Hères 38400 France c Laboratoire d’informatique de Grenoble, Domaine Universitaire de Saint-Martin-d’Hères- BP 53 - 38041 Grenoble cedex 9, France Laboratoire d’informatique de Grenoble, Domaine Universitaire de Saint-Martin-d’Hères BP 53 Grenoble cedex 9 38041 France d Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal ⁎ Corresponding author. Tel.: +33 69537799. This work proposes a different form of robotic navigation in dynamic environments, where the robot takes advantage of the motion of pedestrians, in order to improve its own navigation capabilities. Instead of treating persons as dynamic obstacles that should be avoided, here they are treated as special agents with an expert knowledge on navigating in dynamic scenarios. This work proposes that the robot selects and follows leaders, in order to move along optimal paths, deviate from undetected obstacles, improve navigation in densely populated areas and increase its acceptance by other humans. To accomplish this proposition, two novel approaches are developed in the area of leader selection. In the first, a motion prediction approach is used, to detect candidates that are moving to the same place that the robot is. In the second, a machine learning algorithm is trained with real examples and is used to select the best leader among several candidates. Experiments with a real robot are performed to validate the proposed approaches. Keywords Leader following Leader selection Leader classification Dynamic environment Human-aware navigation Machine learning 1 Introduction Mobile robots that provide assistance or services to humans must face a series of requirements that set them apart from other types of robots. They are expected to be able to interact with humans, adapt to unpredicted situations, respect social conventions and be able to move in dynamic environments. These requirements are directly related to the success of interactions and human acceptance of service and assistance robots. To address the issue of robot motion planning in populated environments, modern techniques implement methods based on probabilistic and predictive approaches [1,2] and on models of social interactions [3]. These techniques create motion plans that allow the robot to avoid trajectories that have a risk of future collision with persons, while at the same time avoiding entering personal and social spaces and causing discomfort to the persons involved. A drawback of those approaches is that they usually do not take into account changes that people perform in their typical paths to avoid and adapt to other moving persons. This omission, allied to excessive future uncertainty and/or densely populated environments, may lead to situations where every generated path leads to collisions, resulting in frozen situations [4]. However, humans are able to seamlessly move in populated environments and also to address complex situations and interactions with other humans. The way humans move is the result of information gathering and very complex decision making processes, which is not yet completely understood, although some models have been developed [5] and incorporated in planning algorithms [6,7]. Based on these observations, it is proposed that robots can enhance their navigation capabilities by following humans in such environments, taking advantage of their navigation skills. The prevailing concept in this work is that the main problem to be addressed here is not the motion planning itself, but rather the identification of agents in the environment that can be used as leaders. To accomplish this idea, this work pursuits a study in different aspects of this problem. First it is proposed a leader selection and following method where leader candidates are evaluated based on the prediction of their destinations and a sequence of rules are used to choose among several possible leaders. Next, a learning framework for leader classification and selection is presented. It uses features extracted from laser measurements for classification and a voting system for leader selection. This framework addresses two objectives, where the first is the leader classification per se, and the other is the study on what features have the largest contribution to the classification process. Data used for learning was labeled according to subjective impressions of volunteers on what characterizes a good or a bad leader. In the last part of this section, the performance of the proposed system is evaluated in a real robot. 2 Selecting and following a leader based on goal prediction The first approach to select a leader is to chose someone whose goal is the same as the robot’s or has a path that passes close to the robot’s intended destination. This is a simple and reasonable approach, but the difficulty lies on the fact that the destination of leader candidates are not known to the robot. In some cases, the candidates may somehow broadcast their goal intentions, specially with the advent of smartphones and social networks. Examples would be applications that direct persons to a certain boarding gate at an airport or GPS navigation softwares that share position and destinations of a group of friends. In both cases, the robot could use the information available to select persons based on their chosen goals. However, in most day-to-day scenarios, the robot must itself try to identify likely goals of leader candidates. This is a well researched field, with a number of available techniques. Simpler methods rely on a model and on the current state of the candidate, and the subsequent extrapolation of that state in the future. An example is the Kalman Filter, which provides future predictions on the state of a subject, which is updated based on current available information. A drawback of techniques that rely only on the internal states of the agents is that it ignores the complexity of the environment where the movement takes place. One way of compensating it is to base prediction on the fact that humans do not move at random. Their motions are related with points of interest in the environment (doors, stairs, tables), with optimal trajectories between those points and with rules of allowed motions. This behavior results in well defined motion patterns, that can be represented by typical paths [8,9]. Usually, approaches that rely on typical paths consist in two parts: learning and prediction. In the first phase, the motion of persons is observed and modelized, which is often an offline process. Following that, the prediction phase must obtain current measurements of a person, as velocity, orientation and position and try to match that information with models of typical paths previously learned. This should yield not only the path but also the goal likely to be reached by the subject of interest. In the literature, many examples of techniques for typical paths modeling and prediction can be found. They may be organized according to the methods they implement: geometric approaches [10,11], neural networks [12], Gaussian Processes [9,13], spatio-temporal models [14], sub-goals [15] and Markov Models [16]. To validate the leader following method proposed in this section, the Growing Hidden Markov Model (GHMM) [16] algorithm has been chosen for modeling typical paths and performing goal predictions. One important distinction of this algorithm compared to the others is that it implements an approach where the learning and prediction phases are on-line concurrent processes, resulting in a learn and predict paradigm. As a result, the typical paths are modeled as Markov Models which are not static, but rather adapt to new observations sequences once they are available, changing the topology of the model, as well as the transition probabilities between states. For the goal prediction, the output of the algorithm is a probabilistic value over a discrete grid, as can be seen in Fig. 1 . In both images, the red bars represent the probability for each of the grid’s cell to be the agent’s destination, where the agent’s position is represented by the three orthogonal axes. On the left image, it is possible to see two groups of bars, which represent two regions on the map that are likely to be the agent’s goal. As the agent continues its motion, the goal prediction converges, as shown by the single region with red bars on the right image. 2.1 Selection among several leaders In many situations more than one candidate may satisfy the goal similarity criteria. Therefore, a set of rules have been defined, in order to decide which, among several leader candidates, is the best one and should be followed by the robot. These rules are illustrated in Fig. 2 , and described below: • Goal similarity: a good candidate must go to a place near the robot’s intended destination. In the image, the orange candidate is promptly discarded as its goal falls out of the goal similarity threshold. • Motion: the leader candidate must be moving, so any subject with velocity inferior to 0.5 m / s is not considered. • Proximity: only subjects close enough for the robot to profit from their motion would be chosen, which eliminates the yellow candidate in Fig. 2. • Position: the candidate must be closer to the robot’s goal than the robot itself. This guarantees that the leader is in front, or at least beside the robot. This rule is represented as the shaded semicircle centered on the robot. This results on the elimination of the red candidate, as it is behind the robot. • Distance: among the remaining candidates, the one that is closer to the robot would be chosen as leader, which is, in this case, the green one. The constant evaluation of these rules allows the robot to switch leaders and adapt to new configurations during the motion execution. 2.2 Leader following navigation methods Once a leader has been selected, a leader following strategy must be put in place. In the work of Gockley et al. [17], the authors evaluated two following methods regarding their acceptance by humans: direct leader following and leader path following. In their experiments, they noticed a preference for the former approach, as it was felt as more natural and human-like by the participants. Based on the mentioned work, the initial approach here was to implement an hybrid approach with both following methods, as it was believed that the leader path-following could also bring advantages to the robot navigation, namely when the robot was far from the leader. However, after preliminary tests, it was shown that such method increased complexity, with little or no benefit. The direct leader following was implemented using two independent proportional controllers, which controlled the distance and the heading of the robot towards a leader, as shown in Fig. 3 . The velocity commands of a differential wheeled robot are computed with the following equations: (1) v θ = h error ∗ K θ (2) v lin = ( d − d ref ) ∗ K lin where v θ is the angular velocity and v lin the linear velocity of the robot. K θ and K lin are the proportional gains of each controller. d ref is set at 2 m, which is a distance that respects the social space of the person being followed [18]. In the case that d < d ref , v lin is set to 0. This is a very simple controller, and certainly there is room for better approaches. However it was good enough to be used in tests to demonstrate the advantages of leader following for robot navigation. 2.2.1 Navigating without a leader: RiskRRT motion planner When no leader is available to the robot, the Risk Rapidly Exploring Random Tree (RiskRRT) motion planner is used [1,19]. This algorithm has been developed explicitly to be used in dynamic environments and is based on the classic Rapidly Exploring Random Tree (RRT) algorithm [20]. The difference being that it uses the notion of the risk of collisions with moving agents to compute the best motion plan. It combines a part dedicated to perception (of static objects and moving agents) with another part for planning trajectories, with navigation and planning performed in parallel. During the execution of the algorithm, the configuration-time space is searched randomly, and a tree T is grown from the initial configuration until a feasible path to the goal configuration is found. The algorithm randomly chooses a configuration P in the C -space and extends an existing node of its current search tree towards that point. The node chosen for extension is the one where the resulting partial path has the lowest risk of collision with moving agents, based on their motion prediction. The likelihood of each partial path can be expressed as the multiplication of the independent probability of collision with static ( P c s ) and dynamic components ( P c d ), where q N is the candidate node, for a partial path: (3) L π ( q N ) = ∏ n = 0 N ( 1 − P c s ( q N ) ) ⋅ ∏ n = 0 N ( 1 − P c d ( q N ) ) . After the tree expansion phase, the motion plan is computed taking into account the sequence of nodes that minimizes the risk of collision with other agents while at the same time bringing the robot closer to its goal. Here, the risk of collision can be seen as a measure of the feasibility of a path, with the maximum accepted risk specified as a threshold in this case. In more recent developments, [3] created a technique called Social Filter (SF), which consists of modeling the interactions among humans ( o -space) and also their personal spaces ( p -space) as risks of disturbances, represented by Gaussian distributions. By integrating such technique with the RiskRRT algorithm, the authors developed an approach that is able to respect social conventions. This is accomplished due to the increased risk of nodes generated close to humans and close to social interaction regions, and as a result, the robot can behave in a socially acceptable way. With this enhancement, the risk function becomes a measure of safety and also of human friendly navigation. 2.3 Experiments For the method of leader following based on goal similarity, two types of experiments were conducted. In the first, human motion is recorded in the entrance hall of a building and a GHMM is used to learn the typical paths of this environment. Later, a simulated robot selects and follows leaders in a virtual environment where the motion of persons is replicated from recorded data. In the second experiment, a pedestrian simulator is implemented, to take into account the reaction of agents to the motion and presence of the robot. Within this framework, three navigation techniques are evaluated: leader following, A ∗ + DWA [21,22] and RiskRRT. 2.3.1 Experiments using recorded data For the data acquisition, fiducial markers were worn as hats by subjects in order to provide a robust and fast deployment tracker system. An overhanging camera with wide angle lens acquired images of the markers, which were further processed by the tracker algorithm (Fig. 4 (a)), similarly to the approach implemented by [23]. Participants were then asked to move naturally among predefined interest points in the main hall of INRIA Rhône Alpes building (Fig. 4(b)). The registered trajectories were then used to train the GHMM algorithm. Fig. 5 (a) shows those trajectories, linking the interest points in the entrance hall. The final GHMM structure after training is shown in Fig. 5(b). It is important to notice that the typical paths are particular to a given environment and are unlikely to change frequently, unless some structural modification takes place on the environment. In such event, new models must be created. Once the prediction framework was in place, a scenario was created using STAGE [24], where virtual robot navigated in a simulated environment. At this same environment the recorded trajectories of participants were reproduced. Both in Figs. 6 and 7 , the robot is represented as a light gray rectangle. The obstacles are colored dark gray and encompass walls, desks and sofas. The circles represent persons and the triangles are their respective predicted goals. They have a letter associated to identify their colors (Red, Green and Blue). The robot goal is marked with a cross, located at the lower left of the test area. In the first test, shown in Fig. 6, three humans start to move just in front of the robot, each moving towards a different goal. After some iterations, as the subjects start to move in the scenario, the prediction algorithm gives a goal estimation for two of them (red and green). Based on that estimation, the leader following algorithm makes the choice to follow the red subject, as its predicted goal is close to the robot’s. The objective of the second test is to evaluate the benefits of following a leader in order to avoid agents moving in the opposite direction and is shown in Fig. 7. The way the robot selects and follows a leader occurs in the same fashion as in the previous test. The robot goal is again in the left bottom corner of the image, but here there are now two humans that move from the door to the stairs, in the opposite direction of the robot’s desired trajectory. After the leader is chosen, the robot starts to follow him/her. As the leader approaches the two humans moving in the opposite direction, they naturally give room for him/her to pass. The robot benefits from this space and is able to continue to move without the need to take evasive maneuvers to avoid the two incoming persons. 2.3.2 Experiments using a pedestrian simulator To account for the reaction of agents to the motion and presence of the robot, a simple pedestrian simulator has been developed. It is based on the Social Forces Model (SFM), that has been extensively validated in several works as a simple and efficient way of replicating pedestrian dynamics [25,26,8]. In this model, the resulting velocity and orientation of the robot are the resultant of the action of two types of forces: an attraction force that pulls the agents towards their destination and also in the direction of other points of interest, as street performers or shop windows; and repulsive forces caused by other pedestrians and by objects, as walls and obstacles. Only the attraction force towards the agent’s goal has been implemented here, together with the repulsion force exerted by other agents and by walls. This system has been developed using the Robot Operating System (ROS) framework [27], where STAGE provides information about a virtual world, and a pedestrian simulator module computes the resulting “force”. This is transformed in velocity commands which are sent back to STAGE, so the virtual agents will behave as pedestrians, as shown in Fig. 8 . Following the original notation, with minor modifications, these forces are described below: • every agent α wants to reach its destination goal r α 0 as directly and as fast as possible. The desired direction of travel is computed based on the agent’s position r α ( t ) and represented as e → α ( t ) . So a constant force F α 0 ( r α ) is applied to the agent in the direction of its desired goal and modulated according to the agent’s preferred velocity v α : (4) F → α 0 ( r α ) = v α e → α • the motion of each pedestrian is also affected by the presence of other pedestrians. In the case where two pedestrians get too close to each other, their discomfort increases, as they invade each other’s personal space. To simulate the tendency of keeping some distance from each other, a repulsion force F α β ( r → α β ) between agents α and β is implemented, where r → α β is the distance between the two agents: (5) F → α β ( r → α β ) = V α β e − r → α β / σ . Here V α β = 10 m 2 / s 2 and σ = 0.3 m . Both these values were set empirically and differ from the values from [25], to reduce the number of collision among agents. The motion of pedestrians will also be affected by the presence of the robot in scenario. As the robot is a strange entity among groups of people, it is expected that persons keep a larger distance from it than from other persons. To model this, the repulsion force produced by the robot is 50% stronger than the repulsion force among agents. • finally, the walls and static objects also influence the motion of the pedestrians, as they must adapt to the environment structure. Pedestrians tend to become uncomfortable when too close to the walls, for example, as they have their freedom restricted and also can get hurt. In this way, walls and objects’ border B produce a repulsive force F α B ( r → α B ) , with r → α B being the distance between the agent’s position r α and the closest border portion r B α , according to the equation (6) F → α B ( r → α B ) = U α B e − ‖ r → α B ‖ / R . With U α B = 10 m 2 / s 2 and R = 0.2 , which are values obtained from the original paper, which provided good results in the simulations. • to account for situations where dead-locks could happen, due to symmetrical generated forces, as agents with opposing goals facing each other, a fluctuation term ξ → ∈ [ − 0.5 , 0.5 ] was randomly generated and also incorporated into the model. The resulting equation has the form (7) F → α ( t ) = F → α 0 + ∑ β F → α β + ∑ B F → α B + ξ → . In summary, according to this model, there is a constant force that pulls the agent towards its goal and two repulsive forces that decrease exponentially, one according to the distance to the other agents, and the other according to the distance between the agent and the borders of walls and obstacles. Regarding the scope of this work, an important phenomena that arises in real human populated environments is the lane formation. This naturally occurs when groups of people cross each other in a high density environment, which is the result of a motion pattern that is more efficient, as it reduces avoidance maneuvers. This phenomena is also important for the proposed approach, as if the robot can enter one of the lanes forming, it will be able to navigate “with the flow”, efficiently moving in congested spaces. Fig. 9 illustrates this process and also validates the developed simulator in the sense this phenomena naturally arises in it. Using the developed pedestrian simulator, it is possible to qualitatively compare the performance of other motion planning approaches to the proposed method of leader following, in densely populated corridors. In the experiment shown in Fig. 10 , three techniques are compared: the A ∗ + DWA algorithm, the RiskRRT and the leader following approach. Here, two groups of nine agents each (represented as circles), must move to opposite sides in a corridor. The robot (represented as a rectangle) starts behind the group in the left and must move to the right side of the corridor. Each row of images represents the same time instant, which are separated by 5 s each. The A ∗ + DWA technique has been developed to work in a static environment, therefore its planning phase takes place at “snapshots” of the current environment state. The problem is that the planned path is constantly invalidated by the motion of the agents, prompting the algorithm to create a new plan. This sequence of plan/replan steps virtually bring the robot to a halt when it encounters the crowd, with no plan valid for more than a couple of seconds. Regarding the RiskRRT algorithm, which have been developed for navigation in dynamic environments, its plan is based on the prediction of future states of other agents. In this experiment it was sufficient to use a linear motion prediction, due to the simplicity of the environment. With this approach, the robot is able to navigate reasonably, until it is faced with motion predictions of incoming agents. The problem is that, due to the high density of agents in the corridor, at some point, the predicted future states of the agents cover all the region in front of the robot, which causes the algorithm to fail in finding a solution. This is known as the Freezing Robot Problem (FRP) [4], which is a recurrent problem in prediction-based dynamic motion planners. Finally, the right column shows the leader following approach. Soon after the test began, the robot chooses a leader and starts to follow it. As the opposing groups encounter each other, the phenomena of lane formation starts to occur. As the robot is following a leader, it manages to participate in the lane formation phenomena, and effectively becomes part of the group moving to the right side of the corridor. The comparison of the three approaches shows that the leader following technique results in a much smoother path traveled by the robot, and also in a smaller time to reach its objective. Using the same framework, a qualitative comparison is conducted, using a methodology close to the one implemented in [28]. In that work two evaluations are performed regarding the navigation of a robot along 6 m in dynamic environments: navigation efficiency, which measures the time spent by the robot to reach its goal, with respect to the density of persons in the scene; and safety, which measures how many times the experiment required external intervention to avoid a collision, also with respect to the density of persons. In the current work, there was no safety issues, so no tests were interrupted and the total amount of collisions were negligible. Therefore, only the first metrics, regarding efficiency, were applied. The setup was similar to the one from the previous experience, with agent crossing to opposite sides of the scenario, but in a more compact scale. It represents a small portion of a corridor, with 12 m long and about 4.5 m wide. The robot must travel a distance of 11 m until its goal, which is on the right end of the corridor. In total 90 tests were performed, with 10 runs for each of the three algorithms, and three different densities of agents (6, 12 and 18). Fig. 11 illustrates the initial conditions for different numbers of agents. The results of the efficiency test is shown in Fig. 12 . As it would be expected, the time for the robot to reach its goal grows with the number of agents present on the scenario. It also shows that the RiskRRT algorithm performs poorly, taking about 10 s longer than the other techniques, on average. Comparing the leader following method with the A ∗ + DWA , both have similar performances in sparsely populated environments, but when 18 agents are present, the advantages of the leader following technique become evident, with a shorter time to reach the destination and a higher consistency. 2.3.3 Discussion on leader selection and following using goal similarity These experiments assessed the capability of the system to predict the goal of real moving agents, as well as the ability of the designed algorithm to properly follow a chosen leader, while avoiding other dynamic agents. Results show that the prediction of a candidate’s goal allows a proper leader selection, even when the motion of candidates is not directed towards their goals. This is an important advantage of a probabilistic approach for goal detection, based on previous knowledge of the most common trajectories in the environment. The advantages of following a leader in a dynamic environment also become evident in the experiments, when compared to other navigation approaches. When the robot follows a human, it is able to become part of a group, avoiding collisions in a natural way. As a result it is able to follow a smooth trajectory to the goal. This shows the benefit of the leader following technique: the robot is able to improve its navigation as a consequence of following a leader that has a better understanding on how to behave in such situations. It is interesting to notice that regarding the efficiency evaluation, both A ∗ + DWA and the Leader Following technique had similar results for the smaller number of agents present on the simulation. However, the metric used conceals an important information, which is the discomfort the robot caused for the surrounding persons. While the leader following method resulted in a motion of the robot as part of the group of agents, the A ∗ + DWA forced its way among the crowd. Due to the use of a pedestrian simulator, the agents adapted their motion and avoided the robot. As a result, the robot managed to quickly reach its goal (efficiency measurement), but at a great cost, forcing agents out of its way. Unfortunately, although this behavior is quite evident in the experiments, it is a challenging task to create a measurement of the discomfort caused by the robot to the agents. A drawback of the presented approach is that it relies heavily on the prediction of goal when selecting leaders to be followed. This can be seen as a global approach, where only the final objective of the person plays a role, without taking into account the behavior of such leaders throughout their trajectory and the interaction of the leader with the robot. As a result, the person being followed may feel discomfort, change his/her pace or stop moving. To address these issues, another leader selection method is developed, which can be seen as a local approach. It focuses specially on the interactions that take place when a robot is following a leader, and on the classification of candidates as good or bad leaders by means of a machine learning approach, according to their behavior. This method is presented in the following section. 3 Leader classification using machine learning The idea to use this approach comes from the fact that humans often encounter themselves in the situation where they engage in leader following behaviors and group formation, usually without much thought or reasoning. In this sense, it is interesting to investigate what are the underlying criteria used by persons to engage and disengage in leader following behavior, so this can also be applied in the design of robots. It is important to have in mind that, although robots can benefit from following humans, this cannot be done at the expense of their comfort. Disturbing persons while following them violates a crucial requirement for robots existing among humans, which is to respect their social conventions. The developed method also proposes to address this issue, as the data used for training will be labeled by persons, who can easily perceive cues associated with discomfort in other persons, as looking backwards, moving faster or moving out of the way. It is expected that this approach brings benefits to the leader selection problem, as: • the robot does not need to create a plan or make predictions about the candidate’s intentions, being able to explore unknown environments while following someone; • it is possible to provide individual scores for leader candidates, allowing a better basis for the selection of whom to follow; • a bad leader can be associated with situations of discomfort experienced by the subjects being followed, so the algorithm can stop following a leader, improving its acceptance. Although numerous works have been conducted regarding the robot–human interactions and proxemics [17,29,30], they are normally focused on how the humans react and feel regarding an approaching robot. However there is a lack of study on what happens when the human is also moving and being followed by a robot. In this sense, this work also launches the basis of such study, by proposing a methodology for data acquisition in this area. Besides the objective of classifying a good or bad leader candidate, it is also important to understand what are the measurements, or features, that are relevant to that classification (distance, velocity, etc.). Because of this, the machine learning algorithm chosen is the AdaBoost, as this algorithm inherently selects the features that contribute the most to the classification process. 3.1 AdaBoost classifier Boosting is the name of a category of supervised machine learning ensemble algorithm, where several weak classifiers, or rule-of-thumbs, are combined in order to create a more accurate predictor, or strong classifier. The adaptive Boosting, or AdaBoost [31], is a variation of the basic Boosting algorithm where more emphasis is given in the correct classification of previously misclassified samples, adapting the weights of samples in the training set. In the algorithm workflow, the confidence parameter α is responsible for the adaptive capability of the algorithm. This parameter is used in the update of the weights of the training set and its value is based on the classification error ϵ : α t = 1 2 ln 1 − ϵ t ϵ t . If a training sample is correctly classified, its weight is reduced, whereas if the classification fails, the weight of this sample is increased. The new weight distribution will now affect the choice of the weak classifier on the next iteration, favoring one that manages to correctly classify the difficult cases. Among the advantages of the AdaBoost algorithm is the exponential convergence of the training error to zero and good generalization properties, besides the capacity of the algorithm to perform feature selection. 3.2 Features study It is interesting to investigate which features are the most important in a classification problem, since learning and understanding what matters in following–leading engagements are essential to the development of a more refined theory on human-following behaviors for robots. According to [32], AdaBoost has a feature selection built into it when using decision stumps as weak learners. As in each iteration, a weak classifier is associated with only one feature, the contribution parameter α and error ϵ are measurements of the importance of that feature in the classification problem. This provides a form of quantitative evaluation that can be used as a criteria for feature selection. As a result, a reduction of the number of features used can benefit the time spent for acquiring and computing those features, which is an important requirement in on-line systems. Besides that, and more relevant to this work, is the possibility to draw conclusions about why some features are more important than others, as this can help the understanding of the criteria used by humans in following behaviors. According to [33,34], the contribution of each feature in the general classification problem can be measured by summing the number of times each feature is used, weighted by the α t of the weak classifier that uses the feature in question. Different from the aforementioned works, here this sum will also be normalized by the sum of all α t , to obtain the ratio of the contribution of each feature. C ( f i ) = ∑ t = 1 T | α t | δ [ F ( h t ) − f i ] ∑ t = 1 T | α t | where δ [ n ] = { 0 , n = 0 1 , n ≠ 0 where C ( f i ) is the contribute ratio of feature f i ∈ { 1 , 2 , 3 , … , i } , α t is the confidence parameter and F ( h t ) is a function that returns the identification (number) of the feature chosen by the weak learner h t . Finally, δ is the Kronecker delta, which outputs 1 in the case the weak learner h t was created using feature f i and 0 otherwise. As a result, C ( f i ) will be the weighted sum of the number of times that the feature of interest f i has been used by a weak learner. This will be the measurement used to evaluate features contribution. 3.3 Leader classification using learning This section will present the steps for the creation of a leader classifier using learning. The first step is the data acquisition and labeling process. After that, the classification capabilities of the algorithm will be evaluated, together with the study of the most relevant features used in the classification process. All tests were performed using the modular architecture of ROS [27]. 3.3.1 Data acquisition A data collection was performed with a small car-like robot (Fig. 13 ), measuring approximately 0.4 × 0.75 m. A laser scanning range finder was installed on the robot, providing distance measurements up to 30 m and with a field of view of 270°. Also a firewire camera fitted with a wide-angle lens was present, providing a way to acquire images with a large field of view. In this way, videos could be taken during tests and then associated with the laser scans, as shown in Fig. 14 . This allowed a better understanding of the experiment situations, together with precise measurements of the surrounding area. The robot was teleoperated by a person that was hidden from the subjects being followed, in order to create the illusion that the robot was autonomous. In total 47 tests of the robot following persons or group of persons were recorded, with the mean duration of about 20 s each. Tests were conducted in an open corridor, about 3 m wide. The objective of the robot operator was to position the robot behind the pedestrians and try to maintain a constant speed and distance from the person being followed. In the case the person stopped, moved aside or was too fast, the operator should resume a stand-alone navigation, simulating the situation where an algorithm would abandon a leader. The setup used for data acquisition is sketched in Fig. 15 . There is a module that receives the laser range finder measurements and another for the camera images. Readings from both sensors are stored in a ROS bag file. Isolated from these modules, there is also a program that receives commands from a gamepad and passes them to a module responsible for a low level interface with the robot’s motors. 3.3.2 Extraction of features The laser range finder measurements were used to obtain the descriptors of each leader subject. The system setup used during the feature extraction process can be seen in Fig. 16 . Initially, the bag containing measurements from the test has its messages published to another module that clusters and tracks moving objects, using the motion tracker developed by [35]. As the laser measurements are obtained from the robot’s point of view (robot’s frame), static objects will appear as moving ones, as the robot moves. To account for that, the transformation of the robot frame with respect to the global frame (or map) must be computed, as this will allow the laser measurements to be transformed to the global frame. The task of finding the transformation global frame → robot, is accomplished using the Hector SLAM tool [36], which is able to localize the robot in an environment based only on laser measurements. With the laser measurements transformed to the global frame, the motion tracker is used to manage the identification and tracking of moving targets. This algorithm also implements a Kalman Filter (KF), to cope with temporary obstructions of tracked humans. After this, another module proceeds to the feature extraction of moving targets, which are then stored in a . t x t file that will be further used during the training process. Fig. 17 illustrates the features extracted, which are listed below: • candidate’s absolute velocity v c i ; • relative heading between the robot and the candidate α r c i ; • candidate’s relative velocity with respect to the robot reference frame v x r c i and v y r c i ; • angle between the robot heading and the candidate’s position β c i ; • distance between the candidate and the robot d c i ; • lateral and sagittal displacement of the candidate ld c i and sd c i . As the tracker implements a KF, all the measurements incorporate a temporal dimension, as the filter uses previous states to compute the current one. Besides the filtered features, their derivatives and standard deviations were also computed. The total number of features extracted was 24. 3.3.3 Data labeling In many data labeling situations, the decision to create a label on the dataset is clear and objective. Examples are faces in images or pedestrians in laser scans. However, this is not the case in this work. Here, the intention is to capture how humans decide when to start or stop following someone, or in other words, when someone is a good or bad leader. In order to do so, participants must create labels based on their feeling about someone being a good leader or not. The labels created should reflect the quality of a leader in a given instant, according to the measurements obtained. One way of doing this would be to poll a volunteer to constantly mark the data during a test, but this would be a tedious process and dependent on an empirical timestep to separate tags. To address this issue a different labeling system was proposed. Samples of the experiments were extracted where the robot was always already following someone, therefore it was clear who was the leader, and then either the robot stopped following that person or continued doing so until the end of the sample. Within this framework, the participants would only create one tag (or none) for each test extract and all the data before that tag was labeled as good leader and everything after as bad leader. It is important to emphasize that the objective is not to label the transitions from good to bad leader, but rather the transition is used to separate two groups of labels given to measurements in each instant. Fig. 18 gives an illustration of how this process works. In the labeling process, depicted in Fig. 19 , the volunteers would watch a video of the robot following a person and should press a button whenever they felt that the leader being followed should be left alone. Before the labeling began, examples of different classes of good and bad leader situations were shown to the volunteers. Later they were asked to tell which of these situations occurred in each experiment. Although each test has its own peculiarities, the experiments can roughly fit one of the following classes: • good leader (gd)—leader(s) maintained their speed and orientation, without changing their behavior while being followed; • bad leader, moved aside (as)—leaders gave room for the robot to pass, generally moving aside, while keeping their original motion direction; • bad leader, far or fast (fr)—leaders moved too fast, increasing the distance between the leader and the robot to a point where it was not advantageous to keep following them; • bad leader, stopped (st)—leaders stopped moving. Three persons participated in this process and the final label was computed as the mean time of the three labels instants. In the case that labels were not close to each other (timewise), only the two closest were used. This analysis was aided by the typical situation identified by the labelers. For example, if one had found that the transition occurred because the leader was too far, but the other two thought that the transition was due to the leader moving aside, only the label of the latter two was used, as they agreed in the situation. Table 1 shows an example of how the labeling scheme works. The first column shows the test number, the next three columns show the identified situation and the time it occurred, according to each one of the evaluators (pro, rich, jor). Note that in test 3, for example, the first evaluator identified a different situation from the others, so his time label was discarded (XX) and did not contribute to the final label. If no transition of bad/good leader was found, meaning the leader remained a good one throughout the test, no label was created (—). The last column shows the average time of the labels according to the consensus of the transition situation. Besides the four typical situations mentioned before, two other sets were recorded, which did not require manual labeling. These situations refer to cases when candidates were not moving, but rather standing close to the robot’s path (nm) and when persons were moving to the opposite direction compared to the robot’s (od). After the labeling processes, each resulting dataset was given a name to identify what situation they represented, together with a number to differentiate them. This allowed the creation of datasets for training and validation that comprehended all the situations encountered. In total, 12911 samples were obtained, with the proportion of 37% of bad leader labels and 63% of good leader labels. This is equivalent to 451 s of tests, divided in 47 experiments and distributed in the following classes: 9 good leader; 7 bad leader, leader moved aside; 5 bad leader, leader too far or fast; 9 bad leader, leader stopped; 7 bad leader, candidate not moving; 10 bad leader, candidate coming from opposite direction. 3.3.4 Training and performance evaluation The data was organized according to the situation they represented (good leader, moved aside, stopped, too far/fast, not moving, opposite direction). This approach allowed to create datasets that contained examples of all the situations, both for training and for evaluation, so the classifier could also be evaluated in terms of specific situations. To create a dataset to evaluate the performance of the classifiers, two sets of each situation (except only one far/fast) were randomly chosen, making a total of 2715 samples with 39% cases of bad leader. The remaining sets were grouped into the training dataset, totaling 8504 samples with 34% cases of bad leader. During the AdaBoost training, there was no limitation in the number of the weak learners used (iterations), and the stop criteria was the error. In all cases, the error reached 0 after different numbers of iterations. The objectives of these tests were to evaluate how different sets of features affected the training and performance of the resulting classifiers. After each test, the features that provided the weaker contribution were removed and a new test was conducted. In this way it was possible to determine if the contribution of the most important features were maintained and how that affected the classification error. This error was computed comparing the output of the classifier with the labeled ground truth. In the first test, all the 24 features were used and their contribution result is shown in Fig. 20 . In this figure it is possible to distinguish the three groups of features used in the experiments: the directly measured features, their derivatives and their standard deviation. It is clear that the derivatives group has almost no role at all in the classification, with individual contribution ratios usually smaller than 1%. Looking at the first group (directly measured features), the most important features are clearly the lateral displacement and the distance between the robot and the leader, while the less important ones being the relative velocities, specially along the y axis. Finally focusing on the last group, the most important features were the standard deviation of the distance, of the relative heading and of the target velocity. Following the investigation of the most important features for this classification problem other tests were conducted, using the same training sets, but with a smaller number of features. In each test, the less important features were removed and then the AdaBoost algorithm was retrained and evaluated again. This resulted in three other tests, using 16, 12 and 8 features. Table 2 shows the final 8 most important features, and how their contribution ratio evolved from previous tests. Parallel to the investigation of the most relevant features for the leader classification, after each training of the AdaBoost classifier, its performance was evaluated regarding its classification error, which was divided according to the distinct leader situations, for a better analysis. The results of the classifier performance are shown in Table 3 , according to the size of the feature set used. To evaluate the classification, two measurements were made. The false good leader, where the classifier labeled a sample as good leader but the ground truth has a bad leader label. This is the most critical error, because by following someone that should not be followed, the robot may be disturbing the leader or find itself in unwanted situations. The second measurement is the false bad leader, that occurred when the classifier output a bad leader and the ground truth is labeled as good leader. Although this is also a mismatch in the classification, it is much less critical than the previous measurement, because if a robot does not follow a potential leader, it only looses an opportunity. The table shows both errors with respect to the situation and the number of features used. The bold values identify which were the smallest errors for each situation. 3.3.5 Discussion of results 3.3.5.1 Feature contribution According to these results, the standard deviation of the distance between the robot and the leader and the lateral displacement of the leader are the two most important features in the leader classification. Together their contribution ratio is approximately 35% when using only the eight most important features. All are spatial features that do not take into account the dynamics of the leader. This means that the position of someone with respect to the robot plays a very important role when selecting a leader. Regarding the standard deviation of the distance between the robot and the candidate, this feature can be understood as the stability of such distance. So the importance of this feature can be associated with the robot maintaining a constant distance from the candidates. It is interesting to notice the low contribution of features associated with velocity, namely the relative velocity in both x and y axes, and the standard deviation of those velocities. This may be explained by the intrinsic relation that exists between the features related with distances (which were the most relevant) and the ones related with velocities (least relevant). For example, a constant distance between the robot and the candidate is equivalent to an absence of relative velocities. A large variation on the distances is also equivalent to the leader stopping or speeding up. As such groups of features are somehow equivalent, it is normal that the classifier will prefer one in detriment to the other. Arguably, the preference for the distances group may have been due to a larger range of values and less noise, which facilitated the class separation performed by the algorithm. The lack of importance of the derivative of the used features was evident, with them having almost no contribution to the results. It was expected that the derivatives provided a measure of the tendency of features to grow or diminish (a growing distance or an increasing velocity of a leader). A probable cause for the small contribution is that the derivatives ended up amplifying the noise presented in the measurements, making it difficult for them to be used in the classification. If this was the cause, a possible workaround would be to compute the mean derivative over a time window in order to reduce the noise amplification. Classifier performance. Regarding the performance of the classifier in each situation, the false good leader relative error is particularly large at the move aside situations (as I and as II). However, the error was much smaller in the remaining situations, and the overall performance of the classifier was remarkable, with the average relative error across all the situations in the order of 3%. Evaluating the results in the light of the size of the features set, it can be seen that although the results are very similar, the smaller feature set of size 8 had a better result in the critical classification of a good leader. Even though its performance on the misclassification of bad leaders was worse than when using larger features sets, that was not a critical error. Although promising, these results are obtained from a sum of instant classifications. This means that the output of the classifiers cannot be taken directly to be used for leader selection and following, with the risk of switching constantly between leaders. Instead of that, it should be used as the input of a higher level program that is capable of computing a leader score that takes into account previous classifications, maintaining some kind of hysteresis. 3.4 Experiments on leader selection and following using learning Although the aforementioned framework provided promising results, the classifier only operates over instantaneous measurements and the ground truth was based on subjective impressions of volunteers. Therefore, to properly validate the proposed approach, interactive experiments are required. 3.4.1 Platform and setup The experiments were performed with an autonomous robotic wheelchair, shown in Fig. 21 . This robot was equipped with two LIDAR sensors, where the one at bottom was positioned 0.1 m from the ground and another was positioned at approximately 0.55 m from the ground. The platform also incorporated an RGBD camera, that was used to register images of the experiments and an on-board computer that is responsible for the low-level hardware control tasks. Finally, a notebook computer hosts the high-level algorithms, as the localization, planning and leader selection and following. The framework of the experimental setup is illustrated in Fig. 22 : 1. People tracking: the technique used for detecting and tracking persons uses the top LIDAR measurements in an AdaBoost classifier, which was previously trained with examples of legs patterns. If two legs are found and close to each other, it is assumed that a person has been detected. 2. Leader classifier: the information about tracked persons is passed to a module that implements the leader classification system previously presented. This node provides a binary output (good/bad leader) based on a set of features extracted from a tracked subject. 3. Leader selector: the previous module provides instantaneous classification of subjects based on the state of the robot and on their state. But to select a leader, a history of classifications must be considered. This module keeps a list of tracked subjects and associates a score to each one of them, which is a result of votes cast by different classifications. A good leader classification equals a vote of 0.01 points, while a bad leader vote is −0.1 points. Only after the score of a subject passes a threshold (0), that person can be considered as a good leader to be followed. An example of this system is shown in Fig. 23 . As a result, a person is considered as a good leader if he/she is recognized as a good leader for more than 10 times successively. And only a couple of bad leader classifications is enough for his/her score to fall below the threshold and stop being considered as a candidate. Another advantage of this method is that a list with candidates scores allows for an easy switching of leaders. 4. RiskRRT navigation: if no feasible leader was found, the robot still needs to be able to navigate in dynamic environments. To accomplish this, this module uses the RiskRRT [1] algorithm to compute paths that avoid risky situations and respect social interactions. This module is always active and computing possible paths, even when the robot is engaged in a leader following behavior. In this way it can keep navigating when there is no potential leader in the scene. 5. Velocity command multiplexer: in this setup, there are two modules that generate velocity commands to the robot. To manage which command will be sent to the robot, a simple multiplexer is used, which forwards the command with the highest priority. The leader selector has the preference but once a leader is lost or abandoned, it stops sending commands and then the RiskRRT module takes over. 3.4.2 Tests on switching navigation method The first experiment tests the performance of the proposed setup in switching between the leader following mode and the independent navigation, using the RiskRRT navigation algorithm. Fig. 24 illustrates this experiment, where detected persons are represented by either a red circle (bad leader) or a green circle (good leader). The chosen leader is represented by a light blue circle overlapping a green one. The small blue nodes are the exploration nodes of RiskRRT and the black line is the best path to navigate. The first image of the sequence (1) shows the robot following a leader while, at the same time, the independent algorithm computes possible navigation routes. Once the leader stops moving (2), a sequence of bad leader classifications makes his score fall below the threshold, and the robot abandons that leader. As soon as this happens, the robot starts to follow the path computed by the independent navigation algorithm, avoiding a collision with the former leader and also with another person crossing its path (3–5), which is also classified as a bad leader. The robot later continues to navigate using only the RiskRRT algorithm (6). 3.4.3 Tests on leader switching This experiment tests the setup capacity of maintaining a list of feasible leaders and switching among them when the score of one surpasses the score of another. Fig. 25 shows two experiments (one in each column), and as in previous tests, a green circle means an instantaneous good leader classification while a red one means the opposite. The blue circle marks the person with the highest leader score. On the first examples (left column), the robot is behind two persons that are classified as good leaders and following the one on the right. As this person slows down to enter an office, he looses score points and the algorithm promptly switches to the other person, who continues to move along the corridor. The second example (right column), shows the robot following a leader along a corridor. At a given moment, the leader slows down and another person, who is moving faster, passes the robot. According to the voting setup, the person that passes by the robot receives a higher score and then the robot changes its leader and starts to follow the new subject. 3.4.4 Tests on leader following among crowds The final experiment shows that the robot is able to select and follow leader in densely populated environments. The test occurs in a narrow corridor, where even small groups of people create difficult conditions for robot navigation. Results are shown in Fig. 26 , which follows the same nomenclature as previous examples. The experiment starts with the robot behind two persons moving along a corridor and following the one on the right side. Soon, a group of persons appear coming from the opposite direction and at the same time, the current leader stops to enter an office. The proposed setup manages to correctly identify a new suitable leader among the group and successfully navigates through the crowd, taking advantage of the natural interactions among persons. 4 Conclusions This work focused on the study of leader selection and classification for the purpose of enhancing robot motion in populated environments. The leader selection was based on people’s goal prediction, and it was demonstrated the use of a probabilistic approach in order to improve that prediction, which learned from previous data what were the typical paths traveled by persons in the environment. A pedestrian simulator was created which allowed realistic experiments where agents reacted to the presence of the robot and to the conditions of the environment. Experiments in crowded environment showed the benefits of taking advantage of the motion of other agents, as the robot successfully navigated through difficult situations, where other algorithms provided suboptimal solutions. Regarding candidates classification as leaders, a learning framework was presented. A machine learning algorithm was trained using real data, that was labeled by humans. The use of the AdaBoost algorithm allowed the study of the contribution of the features used to train the classifier, giving an insight about how humans decide on who is a good or bad leader. Namely, the most important features used were the lateral displacement and the distance to the leader (and its standard deviation). Several classifiers were trained, using different numbers of features. Overall, the trained classifiers produced good results, with the total relative error smaller than 5%, and the total error for false good leader classifications, which is the most critical error, smaller than 4%, according to the training set and the number of features used. Finally, there were no significant differences among the classifiers while using different numbers of features. The final validation of the proposed technique was performed using an autonomous wheelchair. It used a leg tracking system to detect pedestrians and extract features that were fed to the leader classification system. A voting scheme was created to incorporate a temporal dimension on the leader choice. Three types of experiments were conducted to evaluate different characteristics of the proposed scheme: leader switching capability, navigation methods switching and navigation in crowded environments. All tests demonstrated the feasibility of the leader following technique and the advantages that it brought to the navigation of the robot. Future research will focus on what are the thresholds of the features studied here, for example, what is the range of acceptable leader velocities or what are the minimal and maximal distances from a person so he/she can be considered a good leader. Other developments will focus on the enrichment of the features space, using different sensors to detect higher level features as torso orientation, gaze direction, path deviation, density of pedestrians. Tests will also be extended to more complex environments and situations, to test the limits of the leader following approach. Acknowledgment Procópio Stein is funded by INRIA’s Large-scale initiative action PAL (Personally Assisted Living). References [1] C. Fulgenzi, C. Tay, A. Spalanzani, C. Laugier, Probabilistic navigation in dynamic environment using rapidly-exploring random trees and Gaussian processes, in: IEEE/RSJ 2008 International Conference on Intelligent Robots and Systems, Nice, France, 2008. [2] M. Bennewitz W. Burgard G. Cielniak S. Thrun Learning motion patterns of people for compliant robot motion Int. J. Robot. Res. 24 1 2005 31 [3] J. Rios-Martinez A. Spalanzani C. Laugier Understanding human interaction for probabilistic autonomous navigation using Risk-RRT approach 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2011 IEEE 2014 2019 10.1109/IROS.2011.6094496 [4] P. Trautman A. Krause Unfreezing the robot: Navigation in dense, interacting crowds 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2010 IEEE 797 803 10.1109/IROS.2010.5654369 [5] K. Mombaur A. Truong J. Laumond From human to humanoid locomotion—an inverse optimal control approach Auton. Robots 28 2009 369 383 10.1007/s10514-009-9170-7 [6] D. Althoff D. Wollherr M. Buss Safety assessment of trajectories for navigation in uncertain and dynamic environments 2011 IEEE International Conference on Robotics and Automation (ICRA) 2011 IEEE 5407 5412 10.1109/ICRA.2011.5980115 [7] P. Henry C. Vollmer B. Ferris D. Fox Learning to navigate through crowded environments 2010 IEEE International Conference on Robotics and Automation (ICRA) 2010 IEEE 981 986 10.1109/ROBOT.2010.5509772 [8] D. Helbing P. Molnar I. Farkas K. Bolay Self-organizing pedestrian movement Environ. Plann. B 28 3 2001 361 384 [9] C. Tay, C. Laugier, Modelling smooth paths using Gaussian processes, in: Proc. of the Int. Conf. on Field and Service Robotics, Chamonix, France, 2007. [10] D. Makris T. Ellis Path detection in video surveillance Image Vis. Comput. 20 12 2002 895 903 URL: [11] P. Baiget, E. Sommerlade, I. Reid, J. Gonzalez, Finding prototypes to estimate trajectory development in outdoor scenarios, in: Proceedings of the 1st THEMIS Workshop, 2008, pp. 27–34. [12] N. Johnson D. Hogg Learning the distribution of object trajectories for event recognition Image Vis. Comput. 14 8 1996 609 615 10.1016/0262-8856(96)01101-8 URL: [13] D. Ellis E. Sommerlade I. Reid Modelling pedestrian trajectory patterns with Gaussian processes 2009 IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops) 2009 IEEE 1229 1234 10.1109/ICCVW.2009.5457470 [14] G. Tipaldi, K. Arras, I want my coffee hot! Learning to find people under spatio-temporal constraints, in: 2011 IEEE International Conference on Robotics and Automation, ICRA, 2011, pp. 1217–1222. [15] T. Ikeda Y. Chigodo D. Rea F. Zanlungo M. Shiomi T. Kanda Modeling and prediction of pedestrian behavior based on the sub-goal concept Robotics: Science and Systems VIII 2012 [16] D.A. Vasquez Govea T. Fraichard C. Laugier Growing hidden Markov models: An incremental tool for learning and predicting human and vehicle motion Int. J. Robot. Res. 28 11–12 2009 1486 1506 [17] R. Gockley, J. Forlizzi, R. Simmons, Natural person-following behavior for social robots, in: Proceedings of the ACM/IEEE International Conference on Human–Robot Interaction, 2007, pp. 17–24. [18] E.T. Hall The Hidden Dimension 1966 Doubleday [19] C. Fulgenzi, A. Spalanzani, C. Laugier, Probabilistic motion planning among moving obstacles following typical motion patterns, in: IEEE/RSJ International Conference on Intelligent Robots and Systems, 2009. IROS 2009. 2009, pp. 4027–4033. [20] S.M. LaValle J.J. Kuffner Jr. Randomized kinodynamic planning Int. J. Robot. Res. 20 5 2001 378 [21] D. Fox W. Burgard S. Thrun The dynamic window approach to collision avoidance IEEE Robot. Autom. Mag. 4 1 1997 23 33 10.1109/100.580977 [22] E.W. Dijkstra A note on two problems in connexion with graphs Numer. Math. 1 1 1959 269 271 10.1007/BF01386390 URL: [23] J. Snape, J. van den Berg, S. Guy, D. Manocha, Independent navigation of multiple mobile robots with hybrid reciprocal velocity obstacles, in: Proceedings of the International Conference on Intelligent Robots and Systems, IROS, 2009, pp. 5917–5922. [24] R. Vaughan Massively multi-robot simulation in stage Swarm Intell. 2 2–4 2008 189 208 [25] D. Helbing P. Molnar Social force model for pedestrian dynamics Phys. Rev. E 51 1995 4282 4286 [26] D. Helbing I. Farkas T. Vicsek Simulating dynamical features of escape panic Nature 407 2000 487 490 [27] M. Quigley, B. Gerkeyy, K. Conleyy, J. Fausty, T. Footey, J. Leibsz, E. Bergery, R. Wheelery, A. Ng, ROS: an open-source robot operating system, in: ICRA Workshop on Open Source Software, 2009. [28] P. Trautman J. Ma R.M. Murray A. Krause Robot navigation in dense human crowds: the case for cooperation 2013 IEEE International Conference on Robotics and Automation (ICRA) 2013 IEEE 2153 2160 [29] L. Takayama C. Pantofaru Influences on proxemic behaviors in human–robot interaction IEEE/RSJ International Conference on Intelligent Robots and Systems, 2009 IROS 2009 2009 IEEE 5495 5502 10.1109/IROS.2009.5354145 [30] M.L. Walters, K. Dautenhahn, R. Te Boekhorst, K.L. Koay, D.S. Syrdal, C.L. Nehaniv, An empirical framework for human–robot proxemics, in: Symposium at the AISB09 Convention, 2009. [31] Y. Freund R.E. Schapire A decision–theoretic generalization of on-line learning and an application to boosting J. Comput. System Sci. 55 1 1997 119 139 10.1006/jcss.1997.1504 [32] T. Susnjak Accelerating Classifier Training Using AdaBoost Within Cascades of Boosted Ensembles, Science in Computer Sciences 2009 Massey University Auckland, New Zealand [33] M. Tsuchiya, H. Fujiyoshi, Evaluating feature importance for object classification in visual surveillance, in: 18th International Conference on Pattern Recognition, 2006. ICPR 2006. vol. 2, 2006, pp. 978–981. [34] M. Drauschke Feature subset selection with AdaBoost and ADTboost, Tech. Rep. Tech. Rept. Department of Photogrammetry 2008 University of Bonn [35] J. Almeida Target tracking using laser range finder with occlusion (Master’s thesis) 2010 Universidade de Aveiro Aveiro, Portugal [36] S. Kohlbrecher J. Meyer O. von Stryk U. Klingauf A flexible and scalable slam system with full 3D motion estimation Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR) 2011 IEEE Procópio Silveira Stein received his B.S. degree in Control and Automation Engineering from Universidade Federal de Santa Catarina, Brazil, in 2007. He received his M.S. degree in Automation Engineering in 2009 from Universidade de Aveiro, Portugal and his Ph.D. in Mechanical Engineering in 2013, from the same University. His research interests are in the areas of assistance robotics, autonomous vehicles, and navigation in dynamic environments. Currently he holds a Post-doctoral / R&D Engineering position at INRIA, France. Anne Spalanzani received her Ph.D. in Cognitive and Computer Sciences from the University Joseph Fourier of Grenoble, France, in 1999. Since 2003, she is an Associate Professor in e-Motion team of INRIA, LIG laboratory and Pierre-Mendès-France University of Grenoble (France). Her research focuses on navigation in human populated environments and assistive robotics. She works on perception and autonomous navigation integrating probabilistic prediction of risk of collision and social interactions. Vítor Manuel Ferreira dos Santos graduated in Electronics Engineering and Telecommunications in 1989, and obtained the Ph.D. in Electrical Engineering in 1995, from the University of Aveiro, Portugal, with fellowships in mobile robotics during 1990–1994 at the Joint Research Center, Italy. Currently, he is an Associate Professor at the University of Aveiro lecturing courses on robotics and advanced perception. He was the Coordinator of the ATLAS project for mobile robot competitions that achieved 7 first prizes in Portuguese robotics competitions. He was the Co-Founder of the Portuguese Robotics Open in 2001 and the Portuguese Society of Robotics in 2006. His current interests extend to humanoid robotics, ADAS and ITS. Christian Laugier received the Ph.D. and the “State Doctor” degrees in Computer Science from Grenoble University (France) in 1976 and 1987 respectively. He is a First class Research Director at INRIA and he is the Scientific Leader of the e-Motion team-project common to INRIA Rhône-Alpes and to the LIG Laboratory From 2007 to 2011 he was the Deputy Director of the LIG Laboratory involving about 500 staff; he was also the Deputy Director of the Computer Science and Artificial Intelligence Laboratory (LIFIA) from 1987 to 1992. Since 2009, he is also a Scientific Program Manager for Asia & Oceania at the International Affairs Department of INRIA. His current research interests mainly lie in the areas of Motion Autonomy, Probabilistic Reasoning, Embedded Perception, and Intelligent Vehicles. He has co-edited several books in the field of Robotics, and several special issues of scientific journals such as IJRR, Advanced Robotics, JFR, and IEEE Trans on ITS. In 1997, he was awarded the “IROS Nakamura Award” for his contributions to the field of “Intelligent Robots and Systems”, and in 2012 he received the “IEEE/RSJ Harashima award for Innovative Technologies” for his “Outstanding contributions to embedded perception and driving decision for intelligent vehicles”. He is member of several scientific committees such as the Steering/Advisory Committees of the IEEE/RSJ IROS, FSR, and ICARCV conferences. He is also Co-Chair of the IEEER AS Technical Committee on AGV & ITS. He has been General Chair, Program Chair or Co-Chair of international conferences such as IEEE/RSJ IROS’97, IROS’02, IROS’08, IROS’10, IROS’12, and FSR’07. In addition to his research and teaching activities, he co-founded four start-up companies in the fields of Robotics, Computer Vision, Computer Graphics, and Bayesian Programming tools. He also served as Scientific Consultant for the companies ITMI, Aleph Technologies, and Probayes. "
    },
    {
        "doc_title": "Experiments in leader classification and following with an autonomous wheelchair",
        "doc_scopus_id": "84948163781",
        "doc_doi": "10.1007/978-3-319-23778-7_17",
        "doc_eid": "2-s2.0-84948163781",
        "doc_date": "2016-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Autonomous wheelchair",
            "Classification system",
            "Data labeling",
            "Human-aware",
            "Leader following",
            "Leader selection",
            "Navigation algorithms",
            "Robotic platforms"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.With decreasing costs in robotic platforms, mobile robots that provide assistance to humans are becoming a reality. A key requirement for these types of robots is the ability to efficiently and safely navigate in populated environments. This work proposes to address this issue by studying how robots can select and follow human leaders, to take advantage of their motion in complex situations. To accomplish this, a machine learning framework is proposed, comprising data acquisition with a real robot, data labeling, feature extraction and the training of a leader classifier. Preliminary experiments combined the classification system with a multi-mode navigation algorithm, to validate this approach using an autonomous wheelchair.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bimanual haptics for humanoid robot teleoperation using ROS and V-REP",
        "doc_scopus_id": "84933060015",
        "doc_doi": "10.1109/ICARSC.2015.27",
        "doc_eid": "2-s2.0-84933060015",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Computational learning",
            "Degree of complexity",
            "Haptics",
            "Humanoid robot teleoperation",
            "Humanoid robotics",
            "Kinesthetic teachings",
            "Robot teleoperation",
            "Simulated model"
        ],
        "doc_abstract": "© 2015 IEEE.The high degree of complexity associated with humanoid robotic platforms significantly hinders the definition of accurate models. Robot learning based on human teleoperation provides suitable ways and means of meeting this challenge. Teleoperating the robot in different motion and balance tasks through a haptic interface, endows the user with the ability to \"feel\" the dynamics of the system and react to it, taking actions to maintain the robot's balance. The data collected during the demonstrations can then be used on computational learning algorithms to teach the robot how to move, balance, and walk on its own. However, the complexity of real platforms makes the control and data gathering during the teleoperation an intricate task. In order to enhance the user's control over the teleoperation, a dual haptic joystick configuration is introduced, which is first tested in a simulated model of the real platform in V-REP. This paper presents a setup solution to implement the bimanual teleoperation of a humanoid robot, based on a distributed ROS network that bridges the haptic devices and the simulator. Data gathering during several balancing tasks has shown to be possible, and the simulation's behavior is reliable enough to support the development of an infrastructure to operate the real robot based on this approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptation of robot locomotion patterns with dynamic movement primitives",
        "doc_scopus_id": "84933036813",
        "doc_doi": "10.1109/ICARSC.2015.9",
        "doc_eid": "2-s2.0-84933036813",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Continuous modulation",
            "Dynamic movement primitives",
            "Forward velocity",
            "Locomotion patterns",
            "Motor primitives",
            "movement adaptation",
            "On-line adaptation",
            "Walking movements"
        ],
        "doc_abstract": "© 2015 IEEE.Functional locomotion requires continuous modulation of coordination within and between legs to flexibly accommodate demands of real-word environments. In this context, dynamic movement primitives (DMP) is a powerful tool for motion planning based on demonstrations, being used as a compact policy representation well-suited for robot learning. In this work, we study on-line adaptation of robot biped locomotion patterns when employing DMP as trajectory representations. Here, the adaptation of learned walking movements is obtained from a single demonstration. The goal is to demonstrate and evaluate how new movements can be generated by simply modifying the parameters of rhythmic DMP learned in task space. The formulation in task space allows recreating new movements such that the DMP's parameters directly relate to task variables, such as step length, hip height, foot clearance and forward velocity. Several experiments are conducted using the V-REP robotics simulator, including the adaptation of the robot's gait pattern to irregularities on the ground surface and stepping over obstacles.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Simultaneous segmentation and superquadrics fitting in laser-range data",
        "doc_scopus_id": "84923111087",
        "doc_doi": "10.1109/TVT.2014.2321899",
        "doc_eid": "2-s2.0-84923111087",
        "doc_date": "2015-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Aerospace Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2202"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Autonomous Vehicles",
            "Global objective functions",
            "Object model",
            "Outdoor environment",
            "Partial occlusions",
            "Segmentation methods",
            "State of the art",
            "Superquadrics"
        ],
        "doc_abstract": "© 1967-2012 IEEE.This paper presents a method for simultaneous segmentation and modeling of objects, detected in range data gathered by a laser scanner mounted onboard ground-robotic platforms. Superquadrics are used as model for both segmentation and object shape fitting. The proposed method, which we name Simultaneous Segmentation and Superquadrics Fitting, relies on a novel global objective function that accounts for the size of the object and the distance of range points, and for partial occlusions. Results on experimental 2-D range data, which are collected from indoor and outdoor environments, are qualitatively and quantitatively analyzed. Results are compared with those from popular and state-of-the-art segmentation methods. Moreover, we present results on 3-D data obtained from an in-house setup and also from a Velodyne LIDAR. This paper finds applications in areas of mobile robotics and autonomous vehicles, namely object detection, segmentation, and modeling.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A probabilistic approach for color correction in image mosaicking applications",
        "doc_scopus_id": "84920982597",
        "doc_doi": "10.1109/TIP.2014.2375642",
        "doc_eid": "2-s2.0-84920982597",
        "doc_date": "2015-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Color correction",
            "Color transfers",
            "Connected region",
            "Evaluation metrics",
            "Image histograms",
            "Image mosaicking",
            "Mapping functions",
            "Probabilistic approaches"
        ],
        "doc_abstract": "© 2014 IEEE.Image mosaicking applications require both geometrical and photometrical registrations between the images that compose the mosaic. This paper proposes a probabilistic color correction algorithm for correcting the photometrical disparities. First, the image to be color corrected is segmented into several regions using mean shift. Then, connected regions are extracted using a region fusion algorithm. Local joint image histograms of each region are modeled as collections of truncated Gaussians using a maximum likelihood estimation procedure. Then, local color palette mapping functions are computed using these sets of Gaussians. The color correction is performed by applying those functions to all the regions of the image. An extensive comparison with ten other state of the art color correction algorithms is presented, using two different image pair data sets. Results show that the proposed approach obtains the best average scores in both data sets and evaluation metrics and is also the most robust to failures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multimodal inverse perspective mapping",
        "doc_scopus_id": "85027949320",
        "doc_doi": "10.1016/j.inffus.2014.09.003",
        "doc_eid": "2-s2.0-85027949320",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Co-ordinate system",
            "Computation time",
            "Intelligent transportation systems",
            "Inverse perspective mappings",
            "Laser range finders",
            "Multimodal sensor fusion",
            "Obstacle detection",
            "Robust solutions"
        ],
        "doc_abstract": "© 2014 Elsevier B.V.Over the past years, inverse perspective mapping has been successfully applied to several problems in the field of Intelligent Transportation Systems. In brief, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. There is, however, a significant limitation in the inverse perspective mapping: the presence of obstacles on the road disrupts the effectiveness of the mapping. The current paper proposes a robust solution based on the use of multimodal sensor fusion. Data from a laser range finder is fused with images from the cameras, so that the mapping is not computed in the regions where obstacles are present. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping. Furthermore, the proposed approach is also able to cope with several cameras with different lenses or image resolutions, as well as dynamic viewpoints.",
        "available": true,
        "clean_text": "serial JL 272144 291210 291773 291872 291874 291884 31 Information Fusion INFORMATIONFUSION 2014-09-22 2014-09-22 2015-02-05T15:31:51 S1566-2535(14)00103-1 S1566253514001031 10.1016/j.inffus.2014.09.003 S300 S300.1 FULL-TEXT 2015-05-15T06:40:15.932961-04:00 0 0 20150701 20150731 2015 2014-09-22T01:31:58.066333Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor primabst ref 1566-2535 15662535 true 24 24 C Volume 24 10 108 121 108 121 201507 July 2015 2015-07-01 2015-07-31 2015 Regular Papers article fla Copyright © 2014 Elsevier B.V. All rights reserved. MULTIMODALINVERSEPERSPECTIVEMAPPING OLIVEIRA M 1 Introduction 2 Related work 3 Problem formulation 4 Solutions for direct and inverse projections 4.1 Direct projection 4.2 Inverse projection 5 Road and vehicle reference systems 6 Mappable versus unmappable pixels 6.1 Half space of projection 6.2 Desired area of perception 6.3 Image boundaries 6.4 Laser generated polygon 6.5 Image projection polygon 7 Results 7.1 Test platforms 7.2 Computational performance 7.3 IPM accuracy 7.4 IPM accuracy using LRF 7.5 Tests in real environments 8 Conclusions Acknowledgements References ELFAOUZI 2011 4 10 N STILLER 2011 244 252 C SATHYANARAYANA 2011 293 303 A KASTRINAKI 2003 359 381 V SIMOND 2007 4283 4288 N IROS OBSTACLEDETECTIONIPMSUPERHOMOGRAPHY LUNDQUIST 2011 253 263 C MALLOT 1991 177 185 H EHLGEN 2008 657 665 T FANG 2009 463 468 H LI 2011 232 242 S DORNAIKA 2011 954 966 F BERTOZZI 1998 62 81 M TAN 2006 153 165 S THRUN 2006 21 28 S PROCEEDINGSROBOTICSSCIENCESYSTEMSCONFERENCEUNIVERSITYPENNSYLVANIA PROBABILISTICTERRAINANALYSISFORHIGHSPEEDDESERTDRIVING SAPPA 2008 476 490 A EVANS 1998 417 428 O OLIVEIRAX2015X108 OLIVEIRAX2015X108X121 OLIVEIRAX2015X108XM OLIVEIRAX2015X108X121XM item S1566-2535(14)00103-1 S1566253514001031 10.1016/j.inffus.2014.09.003 272144 2015-02-06T01:52:57.118652-05:00 2015-07-01 2015-07-31 true 4678031 MAIN 14 56693 849 656 IMAGE-WEB-PDF 1 si96 2302 42 287 si89 1119 25 221 si82 1911 88 269 si77 1032 23 210 si75 2515 72 419 si68 2998 52 505 si64 900 19 199 si61 1977 73 349 si49 778 19 160 si48 534 18 95 si43 3092 99 304 si42 3233 117 291 si41 3484 94 344 si40 3460 94 336 si39 3452 94 336 si38 795 18 194 si35 2902 88 230 si34 2560 73 295 si31 941 73 136 si30 1750 73 230 si29 456 34 59 si27 768 20 161 si19 1254 73 150 si17 625 18 133 si99 514 18 85 si98 480 14 78 si97 483 14 78 si95 298 13 31 si94 269 14 38 si93 480 14 78 si92 480 14 78 si91 483 14 78 si90 209 17 14 si9 298 13 31 si88 417 19 60 si87 313 16 36 si86 238 16 21 si85 303 19 30 si84 314 17 30 si83 313 16 36 si81 314 17 30 si80 850 20 215 si8 313 16 36 si79 238 16 21 si78 185 12 13 si76 185 12 13 si74 297 14 39 si73 202 14 12 si72 190 13 10 si71 303 19 30 si70 560 22 95 si7 238 16 21 si69 303 19 30 si67 366 23 44 si66 542 20 122 si65 597 20 141 si63 227 15 14 si62 204 12 13 si60 548 20 128 si6 303 19 30 si59 897 20 266 si58 291 15 35 si57 267 15 23 si56 380 16 57 si55 971 18 229 si54 360 17 44 si53 252 14 43 si52 632 15 168 si51 312 15 39 si50 360 17 44 si5 417 19 60 si47 239 14 25 si46 469 17 70 si45 442 17 63 si44 435 17 64 si4 307 17 31 si37 436 20 87 si36 542 20 122 si33 692 19 153 si32 726 19 157 si3 303 19 30 si28 436 20 87 si26 543 20 127 si25 542 20 122 si24 211 16 11 si23 233 13 17 si22 223 11 17 si21 233 14 17 si20 221 11 16 si2 350 15 49 si18 206 12 14 si16 596 20 145 si15 598 20 143 si14 269 14 38 si13 223 15 22 si12 281 14 39 si11 250 15 24 si100 298 13 31 si10 298 13 31 si1 350 15 50 gr1 44906 389 524 gr9 41885 428 364 gr8 44865 235 467 gr7 32038 236 466 gr6 25070 217 468 gr5 25961 284 378 gr4 34971 293 478 gr3 23671 198 466 gr20 54834 327 585 gr2 45568 455 520 gr19 58451 484 489 gr18 45786 188 556 gr17 43286 304 374 gr16 37574 285 378 gr15 95152 239 574 gr14 89711 336 757 gr13 54065 303 588 gr12 46822 272 378 gr11 60911 228 364 gr10 48950 228 638 gr1 11248 163 219 gr9 5045 163 139 gr8 6926 110 219 gr7 4310 111 219 gr6 3621 101 219 gr5 4347 164 218 gr4 4393 134 219 gr3 3696 93 219 gr20 9908 123 219 gr2 5027 164 187 gr19 13296 164 166 gr18 8433 74 219 gr17 6987 164 201 gr16 6571 164 217 gr15 12873 91 219 gr14 6111 97 219 gr13 10824 113 219 gr12 12837 158 219 gr11 19077 137 219 gr10 8740 78 219 INFFUS 667 S1566-2535(14)00103-1 10.1016/j.inffus.2014.09.003 Elsevier B.V. Fig. 1 Two input images (a) and (b) and their corresponding IPM projected images, respectively (c) and (d). In (b), the presence of a blue color vehicle on the road causes the IPM image to present blue artifacts (d). The yellow lines show the areas of the images that are used for IPM projection. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 (a) A typical road scene with a camera mounted on the host vehicle facing the road. The camera reference system is labelled X c Y c Z c and the road reference system is labelled X r Y r Z r . (b) An example of an image captured by the camera. This image is used as input to IPM. (c) The output image of IPM. Since the road is viewed from above no perspective distortion is present. Fig. 3 A typical road scene. The host vehicle has a camera mounted on the roof. Note that the figure shows the reference systems of both the vehicle and the road, since they may not coincide. Fig. 4 An example of a pixel that cannot be projected (green) since its optical ray intersects the road plane on the back of the image plane. Inversely, the pixel in red is projectable. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 The half space of projection computed after the image plane. Fig. 6 The desired area of perception, polygon ( ψ dap ) in green. All vertices of this polygon should be contained by the half space of projection, according to (19). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 The projection of image boundary pixels onto the road plane results in the image boundaries polygon ( ψ Γ ) . Fig. 8 A typical urban road scenario with several obstacles near the host vehicle. Fig. 9 A road scenario with several obstacles: isometric view (a) and top view (b). The projection polygon ( ψ projection ) is shown in red. It is obtained by the intersection of the desired area of perception ( ψ dap ) in green, the image boundaries polygon ( ψ Γ ) in blue, and the laser generated polygon ( ψ laser ) in yellow. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 Two of the test platforms used for testing the proposed approach: dual camera PTU unit (a); the ATLASMV small scale robot (c), equipped with a LRF and a multi-camera perception unit (b). Fig. 11 The entire projection obtained using Camera 0 of the ATLASMV test platform (a). The correctly projected pixels (b). Pixels that where incorrectly projected (c). Bellow each image, an enlarged region of the pixels in shown. Fig. 12 The ATLASCAR full scale robotic platform. It is equipped with an active perception unit (A), a stereo rig (B), three LRF (C, D, H), a thermal vision camera (F), GPS (G) and an inertial measurement unit (E). Fig. 13 Some key frames of the test sequence. First row: images taken from Camera 0, the blue area is the area of projected pixels, the red is the area outside the desired area of perception and the green area is the area outside the half plane of projection; Second row: a map of the projection. Projected/unprojected pixels from Camera 0 in green/red. Projected/unprojected pixels from Camera 1 in magenta/blue; Third row: the IPM resulting image after mapping both cameras. In columns, different snapshots of the test sequence: 0 (a), 2 (b), 5.5 (c) and 12 (d) seconds. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 14 (a) Time taken to perform the IPM projection for both cameras. The classic IPM time and the time of the proposed approach are shown. (b) Percentage of time saved and number of projected pixels. Proposed approach compared to the classic approach. Key frames of Fig. 13 signaled as the vertical black lines. Fig. 15 The resulting IPM projection when errors in yaw (a), (b) and in pitch (d), (e) are introduced in the calculation. The reference projection, where no errors were introduced, is shown in (c). Fig. 16 IPM accuracy ( η IPM ) scores for errors in camera pose. Results are presented for errors in yaw and pitch angles. Fig. 17 IPM accuracy ( η IPM ) for the classic IPM (dotted lines) and the proposed approach (dashed lines). Fig. 18 Some tested scenarios: (a) obstacle at 0.3m in front; (b) obstacle at 0.5m to the left; (c) obstacle at 0.75m to the right; and (d) obstacle at 1.5m in front. Fig. 19 Comparison of the classical IPM (middle column) with the proposed Multimodal IPM (right column). The input image (left column) shows the image projection polygon highlighted in yellow. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Using the proposed IPM approach in real scenarios. (a) Images of the three cameras on-board the ATLASCAR; (b) the distribution of mapping for each camera; (c) IPM using just green camera; and (d) IPM using all cameras Multimodal inverse perspective mapping Miguel Oliveira a ⁎ Vitor Santos b a Angel D. Sappa c a Institute of Electronics and Telematics Engineering of Aveiro, Campus Universitario de Santiago, 3800 Aveiro, Portugal Institute of Electronics and Telematics Engineering of Aveiro Campus Universitario de Santiago 3800 Aveiro Portugal b Department of Mechanical Engineering, University of Aveiro, Campus Universitario de Santiago, 3800 Aveiro, Portugal Department of Mechanical Engineering University of Aveiro Campus Universitario de Santiago 3800 Aveiro Portugal c Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center Campus UAB 08193 Bellaterra Barcelona Spain ⁎ Corresponding author. Over the past years, inverse perspective mapping has been successfully applied to several problems in the field of Intelligent Transportation Systems. In brief, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. There is, however, a significant limitation in the inverse perspective mapping: the presence of obstacles on the road disrupts the effectiveness of the mapping. The current paper proposes a robust solution based on the use of multimodal sensor fusion. Data from a laser range finder is fused with images from the cameras, so that the mapping is not computed in the regions where obstacles are present. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping. Furthermore, the proposed approach is also able to cope with several cameras with different lenses or image resolutions, as well as dynamic viewpoints. Keywords Inverse perspective mapping Multimodal sensor fusion Intelligent vehicles 1 Introduction Intelligent Transportation Systems offers one of the most relevant frameworks for data fusion [1]. Within this scope, Advanced Driver Assistance Systems (ADAS) are considered to have paramount importance, since they have a significant impact in the safety of both passengers and pedestrians alike [2]. One very important aspect of ADAS is road detection or lane marker detection techniques [3], i.e., the automatic estimation by the vehicle of its position with respect to the road. Robust road position awareness is one of the primary features that an intelligent vehicle should present [4]. This paper focuses on a particular technique called Inverse Perspective Mapping (IPM) which is very often used in vision-based road estimation algorithms as a pre-processing component. IPM uses information from the camera’s position and orientation towards the road to produce a bird’s eye view image where perspective effects are removed. The correction of perspective allows much more efficient and robust road detection, lane marker tracking, or pattern recognition algorithms to be implemented. In reality, IPM has been employed not only with the purpose of detecting the vehicle’s position with respect to the road, but also in many other ADAS related applications, e.g., obstacle detection [5,6], free space estimation [7], pedestrian detection [8] or ego motion estimation [9]. Therefore, IPM is of paramount importance to a large number of automated tasks that should be handled by an intelligent vehicle. If IPM provides input to many other algorithms, then special care should be given to it. In this paper, we focus on the robustness of the IPM algorithm, since it still presents some limitations when applied in the context of on-board road mapping. IPM works under three core assumptions: the road must be a flat surface, there should be a rigid body transformation from the camera to the road, and the road should be free of obstacles. We focus on the last assumption: obstacle free road. This is the least realistic assumption of all, since very often the roads are populated by other vehicles, protection walls, pedestrians, etc. In fact, the classical IPM algorithm fails to produce an accurate projected image when the input images from the cameras contain other things other than the road itself. An example of this is shown in Fig. 1 : in (a) the road is free of obstacles, which leads to an IPM image (c) which is very accurate. In Fig. 1(b), there is another vehicle on the road. As a consequence, the resulting IPM image (d) contains artifacts which might be misleading for example for pattern recognition algorithms. In this paper, we propose an extension to the classical IPM that can still compute accurate IPM images when obstacles are in front of the vehicle. To accomplish this, we make use of an additional laser range finder (LRF). These sensors are especially designed for obstacle detection purposes. In fact, nowadays, some of-the-shelf vehicles already have these sensors installed to assist collision avoidance or collision mitigation systems. 1 last accessed February 2014. 1 In comparison with alternative on-board distance measuring devices, such as stereo vision or radar, LRFs produce more accurate data at higher output frequencies. They are also robust, since they work in challenging conditions such as fog, shadows or even at night. The core idea behind the proposed approach is to fuse the laser data with the pixels in the image in an attempt to identify which pixels should be used as input for IPM. As will be shown, this multimodal sensor fusion framework is capable of producing much more accurate IPM images when compared to the classical IPM approach. In addition to this, we will show that the proposed approach is faster to compute than the classical IPM. The reason for this is that our approach employs a faster direct projection mechanism (from 3D world to image, i.e., 3D points to pixels) to discover mappable pixels, before operating the slower IPM based inverse projection (from pixels to 3D points). Classical IPM approaches make no considerations on this topic (e.g., [10–12]). The paper is organized as follows. First, Section 2 presents the state of the art on IPM techniques, then Section 3 presents the mathematical formulation of the problem to be used in Section 4; the new reference system, proposed in the current work, is introduced in Section 5. Finally, Section 6 describes the proposed multimodal inverse perspective mapping through the computation of the mappable pixels. Experimental results are given in Section 7, and conclusions are presented in Section 8. 2 Related work Over the last decades, IPM has been successfully applied to several problems, especially in the field of Intelligent Transportation Systems. Although it was some years ago that authors began to mention the advantages of IPM (e.g., [13,14]), several recent publications (e.g., [15–17]) show that this is still a topic of interest to the robotics, computer vision and Intelligent Transportation Systems communities. The core application of IPM is the determination of the vehicle’s position with respect to the road, commonly referred to as “road detection” or “lane marker detection”. There are several examples of using IPM for assisting road detection in the literature (e.g., [18,12,19,20]). The usage of IPM on-board a vehicle may also aid other automatic detection systems such as generic obstacle detection [21,22], free space estimation [23,7], pedestrian detection [8,24], or optical flow computation [25]. The IPM method receives as input the image from the camera, the 6D position of the camera with respect to the road reference system (i.e., extrinsic parameters), and a description of the properties of the lens (i.e., intrinsic parameters). Under the assumption that the road ahead of the vehicle is flat, that there is a fixed rigid body transformation from the camera to the road’s reference frame, and that there are no obstacles present, the input image pixels are mapped to the road reference system, and a new image is produced where perspective effects are removed. The image that is produced by the IPM will, henceforward, be named simply IPM image. Considering on-board road detection setups, cameras are usually mounted somewhere close to the rear view mirror inside the vehicle, facing the road in front of it. The camera’s position and orientation induces perspective associated effects to the captured road images. The IPM technique consists of transforming the images by mapping the pixels to a new reference frame where the perspective effect is corrected. This reference frame is usually defined on the road plane, so that the resulting image becomes a top view of the road. Fig. 2 (a) shows an example of a road scene; Fig. 2(b) depicts the input image captured by the camera; and Fig. 2(c) represents the image produced using IPM. One of the advantages of IPM is that the subsequent perception algorithms can be computed in the IPM resulting image, which is defined in a new reference system where the geometric properties of road painted patterns are independent from the perspective of the camera, i.e., from the position of the camera. In [14], the authors claim that the parallelization of road features is crucial for curvature determination. Another advantage is that, since the perspective effect associates different meanings to different image pixels, depending on their position in the image, after the removal of the perspective effect, each pixel represents the same portion of the road, allowing a homogeneous distribution of the information among the pixels of the resulting IPM image [22]. Other authors have also employed steerable filters for lane markings detection and sustain that filtering on the IPM image allowing a single kernel size to be used over the entire area of interest [18]. Furthermore, since images are mapped to a new reference system, several cameras may be used to produce a single IPM image mosaicking, which is a subject also present in the literature [22,20]. It should also be noted that IPM requires no explicit feature detection, which contributes to the overall robustness of the algorithm. In addition, there are also dedicated hardware systems being developed to compute the IPM images [26]. Given this, it is fair to say that IPM is a cornerstone in the development of on-board video processing systems. It assists, or is very frequently a primary step, in road modelling, obstacle and pedestrian detection, free space estimation and many other advanced drivers assistance systems. Despite the advantages of IPM, the current state of the art on this method has some limitations. These derive mostly from the fact that the classical IPM algorithm makes three assumptions: static position of the camera with respect to the road, flat road plane, and obstacle free road. Each of these assumptions and proposed solutions are described in the following lines. Since the position of the camera with respect to the road plane is considered static, pitch and roll variations from the host vehicle (and thus of the camera which is rigidly attached to it) are neglected. Pitch variations occur during demanding brake or acceleration maneuvers, while roll changes are expected to appear during hard turns. When the vehicle rolls or pitches, the position of the camera with respect to the road changes. As a consequence, the accuracy of IPM decreases during these maneuvers. This problem has been identified in [27,28,19,29]. In fact, some authors claim that even a small error in the vehicle’s roll/pitch estimation leads to a massive terrain classification error [30]. In [31] an algorithm is proposed that also fuses vision and laser data. However, in this case, the objective is to correct the laser range measurements, rather than to correct the projection of the image pixels, as is proposed in the current paper. In that paper, a stereo vision system is used to detect the road plane and thus estimate the position of the lasers with respect to the road. With this information, it is possible to compensate for roll or pitch variations continuously, which in turn is used for correcting raw laser scan data. Another assumption that is generally made, is to consider the road as a flat surface. The approximation of the road surface to a plane is acceptable. Nonetheless, in some specific cases such as a road climbs, this could also be a factor for low IPM accuracy. In [32] a solution to this problem is proposed, where the “height” of the lane markings is estimated with respect to a reference plane. Using this technique, it is possible to compute IPM images in sloped roads. The final assumption is that there are no obstacles on the road. This is often the case when other vehicles, buildings or pedestrians appear in the image. When these obstacles are present in the image, the mapping of IPM is disrupted because, in the classical IPM approach, all pixels from the input image are assumed to be on the road plane and are thus used in the projection. In real automotive applications it is unfeasible to assume an obstacle-free scenario. Nonetheless, no previous solution has been proposed. In this paper we propose a multimodal laser vision sensor fusion strategy that addresses the obstacle-free road assumption. 3 Problem formulation Let c R r be the classical 3 × 3 rotation matrix in 3D and c T r be the 3 × 1 translation vector in 3D that relates two coordinate systems. Their combination maps a point in the 3D road coordinate system Q r = [ X r Y r Z r ] T to a point in the camera’s coordinate system Q c = [ X c Y c Z c ] T : (1) Q c = c R r · Q r + c T r . Let K be the intrinsic parameters matrix of a given camera, represented as: (2) K = α x β x 0 0 α y y 0 0 0 1 , where α x and α y are the lens scaling factors in both directions, x 0 and y 0 the principal point coordinates in pixels and β the skewness factor. These parameters can be obtained by an offline calibration since they are constant for each camera-lens setup. The projection of an arbitrary 3D point Q = [ X Y Z ] T to a point q h = [ u v w ] T in the camera’s homogeneous image coordinate system, is described as: (3) q h = K ( c R r · Q + c T r ) . Finally, the coordinates of a pixel q = [ x y ] T are obtained by adjusting the homogeneous coordinates with the scaling factor w: (4) q = q h w . For simplification purposes, the current paper will use the following notation: (5) K · c R r = P = p 11 p 12 p 13 p 21 p 22 p 23 p 31 p 32 p 33 , and also: (6) K · c T r = t = t 1 t 2 t 3 . The above formulation may describe the projection of a point to a pixel in the image (direct projection), or it may be used to obtain the 3D point from the pixel coordinates (inverse projection). The direct projection (dp) may be formulated as dp : R 3 → Z 2 , Q → q . In the case of inverse perspective mapping, what is sought is the 3D coordinates of a given pixel. This is the inverse projection (ip), defined as ip : Z 2 → R 3 , q → Q . 4 Solutions for direct and inverse projections The following subsections present the general form solutions for the direct and inverse projections. 4.1 Direct projection As discussed in Section 3, the direct projection aims at obtaining the pixel coordinates of a 3D world point projected to the image. Eq. (3) may then be rewritten as: (7) u v w = p 11 p 12 p 13 p 21 p 22 p 23 p 31 p 32 p 33 X Y Z + t 1 t 2 t 3 . Using (4), we get the definition of the direct projection dp (8) x = p 11 X + p 12 Y + p 13 Z + t 1 p 31 X + p 32 Y + p 33 Z + t 3 , y = p 21 X + p 22 Y + p 23 Z + t 2 p 31 X + p 32 Y + p 33 Z + t 3 , this system of equations defines the direct projection of a point in the world reference system Q = [ X Y Z ] T to a pixel in image coordinates q = [ x y ] T . 4.2 Inverse projection The inverse projection is the problem of obtaining the real world coordinates of a point from a pixel in the image. The problem is under-defined, since the three real world coordinates are sought from only two pixel coordinates. In IPM, the system is completed by defining the plane onto which the pixel is projected. Let an arbitrary plane, defined as: (9) Π : aX + bY + cZ + d = 0 , be the plane that contains the projection of the pixel. The system of equations in (3) may be extended to include the constraint of the projection plane, defined in (9): (10) w x y 1 0 = p 11 p 12 p 13 0 p 21 p 22 p 23 0 p 31 p 32 p 33 0 a b c d X Y Z 1 + t 1 t 2 t 3 0 , rearranging this formulation, the equations for inverse perspective mapping can be obtained. First, variable d may be moved inside the translation vector: (11) w x y 1 0 = p 11 p 12 p 13 0 p 21 p 22 p 23 0 p 31 p 32 p 33 0 a b c 0 X Y Z 1 + t 1 t 2 t 3 d , then, (11) may be rearranged: (12) - t 1 - t 2 - t 3 - d = p 11 p 12 p 13 0 p 21 p 22 p 23 0 p 31 p 32 p 33 0 a b c 0 X Y Z 1 - w x y 1 0 , and finally, the vector of pixel coordinates can be embedded inside the projection matrix: (13) - t 1 - t 2 - t 3 - d = p 11 p 12 p 13 - x p 21 p 22 p 23 - y p 31 p 32 p 33 - 1 a b c 0 ︸ A X Y Z w , rearranging the system of equations results in the inverse projection (ip) of a pixel to a known plane: (14) X Y Z w = p 11 p 12 p 13 - x p 21 p 22 p 23 - y p 31 p 32 p 33 - 1 a b c 0 - 1 - t 1 - t 2 - t 3 - d , this is a valid solution whenever matrix A is invertible and not singular. In other words, the projection formulation is invalid when the projection plane and the image plane are parallel and the projection plane is behind the image plane. The term behind will be clarified in Section 6.1 with the introduction of the half space of projection. 5 Road and vehicle reference systems In the classic IPM formulation the camera and road reference systems have a known static transformation between them. The IPM projection will transform the pixels from the camera to the road reference system. In the current paper we use an additional reference system, the vehicle reference system. The vehicle reference system is fixed to the host vehicle. It is the reference system to which all sensors on the vehicle are related. Therefore, a fixed, rigid body transform is used to represent the pose of the camera with respect to the vehicle reference system. Hence, three reference systems are used: the camera system { X c Y c Z c } , the road reference system { X r Y r Z r } and the vehicle reference system { X v Y v Z v } . Fig. 3 shows the reference systems for the vehicle, road and camera. The general camera to road reference systems transformation was introduced in (1). Let the rotation and translation matrices of (1) be assembled into a global transformation matrix c H r in homogeneous format, so that: (15) Q c = c H r · Q r , the global transformation from the camera to the road is obtained as the product of a fixed camera to vehicle transformation and a dynamic (pitch, roll, therefore time dependent) vehicle to road transformation. (16) Q c = c H v · v H r ( t ) · Q r . In the general mathematical model proposed here, the classic IPM approach may still be used: v H r ( t ) is constant for all values of t, i.e., the coefficients of (9) are defined to represent the X v Y v plane Π road : a r = b r = d r = 0 and c r = 1 ; or the road plane may be actually detected, if v H r ( t ) is estimated over time using stereo or laser sensors pointed towards the road, i.e., some estimation function of the parameters in (9) is running continuously. An example of real time estimation of road to vehicle transformation is presented in [33]. 6 Mappable versus unmappable pixels IPM is the application of (14) to the pixels in the image. However, in a given image, not all pixels may be interesting or even possible to project. The current work addresses this problem by using a laser sensor to detect mappable regions, together with a set of criteria to select which pixels should be mapped. In summary, the fusion mechanism we propose is the following: using several criteria, we compute a set of polygons in 3D (defined in the road’s reference frame). Each polygon delimits the area of the road which, in accordance with the corresponding criteria, should be mapped using IPM. Then, we fuse all these criteria by computing a polygon (the projection polygon) which results from the intersection of the several criteria driven polygons. The projection polygon now encodes the region of the road that should be mapped using IPM. However, the projection polygon is defined in the road’s reference frame (in 3D). Hence, to use this information as input to an IPM projection, we first need to project the projection polygon onto the image plane (this is done using a direct projection mechanism), which we call the image projection polygon. The image projection polygon is the tool that allows the pixels to be labelled as mappable or not: pixels inside this polygon should be mapped and pixels outside the polygon are skipped. Note that in this approach we are fusing multimodal data, since that some of the criteria we use are related to the vision sensor and others to the LRF sensor data. The following subsections present the different criteria used to find which pixels in an image are possible to be projected. 6.1 Half space of projection Eq. (14) is the mathematical solution of the intersection of the optical ray of a given pixel with the road plane. Because of this, a pixel above the horizon line in the image will be projected to the back of the camera’s plane. Fig. 4 shows the projection rays of two pixels, one is projectable and the other should be discarded. Although the presented solution is a valid mathematical solution, for the proposed model, however, the unprojectable pixels must be handled in accordance. This is done by first computing the image plane. The image plane divides the three-dimensional Euclidean space into two parts. One of them is called half space of projection. It is defined as the region of the Euclidean space where all points contained by it may be virtually projected into the image plane. The image plane is defined as Π image : a i X + b i Y + c i Z + d i = 0 ; it is obtained as follows: Let M 0 , M 1 and M 2 be three non collinear points in the X r Y r plane of the road reference system. As an example M 0 = [ 0 0 0 ] T , M 1 = [ 1 0 0 ] T and M 2 = [ 0 1 0 ] T . The points are projected from the cameras reference frame by means of the transformation matrix defined in (1). In the camera’s reference system, those points are contained by the image plane and may be used to define two vectors whose cross product defines the vector normal to the image plane: (17) a i b i c i = C R V · M 0 - M 1 ⊗ C R V · M 0 - M 2 , where ⊗ denotes the cross product. The remaining image plane parameter d i is obtained by substituting in the plane equation one of the projected points: (18) d i = - ( a i X 0 + b i Y 0 + c i Z 0 ) . Having the parameters of the image plane, and a test point Q t = [ X t Y t Z t ] T that is sure to be inside the half space of projection (for example a point a couple of meters in front of the host vehicle), a test is devised to assess if a point Q = [ X Y Z ] T belongs to the half space of projection (denoted as Π image + ): (19) Q ∈ Π image + , if ( a i X t + b i Y t + c i Z t + d i ) ( a i X + b i Y + c i Z + d i ) > 0 , Q ∉ Π image + , otherwise. The half space of projection in (19) is shown in Fig. 5 . It is used to define projectable polygons in 3D, as detailed in the following sections. 6.2 Desired area of perception For an autonomous system, it is important to define the area of perception that it requires to effectively navigate. A very large perception area increases the computational cost, while a small perception area might make the system unfit to handle quick variations in the road scenario. This section addresses the desired perception limits, i.e., how the programmer can effectively set an area of interest for the host vehicle to perform the IPM operation. In the case of a vehicle travelling in urban scenarios for example, perhaps 30m of view range are sufficient. The desired area of perception is formally defined as a polygon ψ dap in the road’s projection plane. This polygon must be contained in the half space of projection ( ψ dap ⊂ Π image + ). Fig. 6 shows an example of an area of perception. Currently, ψ dap is set as a four vertices polygon, defining, in the road plane, a rectangle in front of the host vehicle. The rectangle’s side in the direction of the vehicle’s movement may dynamically increase size depending on the vehicle speed. 6.3 Image boundaries Besides the desired area of perception, other regions of the road plane must be defined in order to perform an effective IPM operation. The camera lens properties and orientation towards the road plane define a possible area of projection. Let γ be the list of pixels in the image boundaries, obtained from all image pixels q ( γ ⊆ q ) that are in accordance with: (20) q i = x i y i ∈ γ , if ( x i = 1 ∨ x i = W ∨ y i = 1 ∨ y i = H ) , q i ∉ γ , otherwise, where W and H are the image width and height respectively. The boundaries of the image are then projected onto the road plane using the inverse projection ip from (14), and the real world coordinates of the image boundary pixels Γ are obtained. The half space of projection is again used to assert the validity of 3D points: (21) Γ = ip ( γ ) , ∀ ip ( γ ) ∈ Π image + . The list of world points Γ are used to form the vertices of the polygon ψ Γ (an illustration is shown in Fig. 7 ). 6.4 Laser generated polygon The IPM technique requires that the road surface seen from the cameras is flat. This might not always be the case, particularly when other vehicles or obstacles lie on the road, as shown in the IPM resulting images published by some authors [18,22,10]. In these examples, artifacts are generated in the regions of the image where the flat road assumption fails. Vehicles are mapped as if they had been painted on the road (see Fig. 1(b) and (d)). Some authors have taken advantage of this phenomenon to detect obstacles in the road, by using the differences in two IPM images, from a pair of stereo cameras [22]. This method is called stereo IPM. Although the latest is a valid approach, the fact is that calibration issues tend to disrupt the perfect mapping of stereo images. Because of this, it may sometimes be difficult to distinguish if disparities in the IPM stereo are due to a sub-optimum calibration or to an obstacle that lies on the road surface. There is also work related to sensor integration using both vision and laser in autonomous vehicles [31], but in this case the objective was to enhance obstacle detection. Fig. 8 shows a typical urban road scenario with several obstacles near the host vehicle. Let Q laser = [ X laser Y laser Z laser ] T be the 3D points obtained by the laser range finder, referenced in the world coordinate system. Assuming that objects picked up by the laser have a vertical expression, the coordinates where obstacles touch the floor, i.e., the object baseline Q bln , is obtained by the vertical projection of laser points onto the road plane: (22) Q bln = X laser Y laser - ( a r X laser + b r Y laser + d r ) c r . The laser generated polygon ψ laser is defined by the list of vertices at generic coordinates given by Q bln . 6.5 Image projection polygon As stated before, the core of IPM is applying (14) to the pixels in the image that are known to be on the projection plane. The objective is to be able to define for the input image which pixels are possible (and desirable) to map. The proposed approach defines three polygons in the road plane: a polygon defining the desired area of perception ( ψ dap ), a polygon corresponding to the boundaries of the image ( ψ Γ ) and a polygon defining the laser scanned objects ( ψ laser ). The resultant projection polygon ( ψ projection ) is obtained by the intersection of the three other polygons: (23) ψ projection = ψ dap ⋂ ψ Γ ⋂ ψ laser , where ⋂ represents polygon intersection. The projection polygon is composed of a list of vertices, i.e., 3D points defined the road reference system (see fig. 9 ). The vertices defined in the road reference system are direct projected into the image plane using (8). The result is a list of 2D vertices that define a polygon in the image plane. This is called the image projection polygon. Inside the polygon are all pixels that should be mapped using IPM. Since perspective transformation is an affine transformation, the image projection polygon is calculated as the direct projection of the vertices of the projection polygon in the road plane. 7 Results Several experiments have been devised to obtain quantitative results of the proposed IPM methodology. First, the platforms used to obtain the results are presented: a dual camera pan and tilt unit (PTU), a small scale robot and finally, a full scale autonomous vehicle. The computational performance of the proposed approach is compared to the classic IPM using a measure of the accuracy of IPM. Results are presented for the accuracy of the proposed approach. Also, a comparative study of the classic IPM versus the laser assisted IPM shows that the accuracy of the latest is much better when obstacles appear in the area of projection. Finally, this section ends with some qualitative results, providing images of IPM from on-board cameras of a full scale vehicle. 7.1 Test platforms In order to assess the performance of the proposed methodology, the test platforms depicted in Fig. 10 where used. Fig. 10(a) shows the dual camera PTU. The servo actuated PTU controlled through RS232 serial protocol was used so that the IPM is tested when the cameras move into different positions. The cameras have different lenses and also different image resolution. Camera 0 has a wide angle lenses and a resolution of 800 × 600 pixels, while Camera 1 has a tele-lens and a resolution of 320 × 240 pixels. This platform is used to assess the computational performance of the proposed approach. The time taken to perform IPM on both cameras is measured during a test where the PTU moves the cameras to different positions. Fig. 10(c) shows the ATLASMV robotic platform. It is a small scale autonomous robot built for participating in an autonomous driving competition. It is equipped with four cameras and a LRF. The side cameras (Fig. 10(b)), used to map the road in front of the robot, have wide angle lenses and produce images with a resolution of 320 × 240 pixels. The ATLASMV is used in two tests: one for measuring the accuracy of IPM, another to assess the effects of using the LRF to assist IPM. The quantitative results obtained from the accuracy of IPM are calculated using a color 2 For interpretation of color in Figs. 10, 11 and 20, the reader is referred to the web version of this article. 2 calibrated grid (shown in Fig. 10(c), below the robot and, in Fig. 11 , viewed from the cameras). The grid is a 3 × 1 m sheet of paper marked with a special colored pattern. The grid is placed in a known position in front of the cameras. Using the position and rotation of the cameras with respect to the calibration grid, a virtual image of the grid is produced to overlap the resultant IPM image. This virtual image of the grid serves as a test mask for measuring the IPM accuracy ( η IPM ): after projection using IPM, pixels are labelled with a color that should match the color of the virtual image. The accuracy of the projection is obtained as the ratio between correctly projected pixels and the total projected pixels: (24) η IPM = number _ of _ correct _ projections total _ number _ of _ projections . Fig. 11(a) shows all the pixels of a given projection. Pixels classified as correctly and incorrectly projected are displayed in Fig. 11(b) and (c) respectively. Also, the virtual grid is overlaid onto the images. The final test platform is the ATLASCAR (Fig. 12) , a real scale robotic platform [34] used for research on autonomous driving and advanced driver assistance systems. It is equipped with cameras and lasers. Results will show IPM images using the three on-board cameras. 7.2 Computational performance The computational performance of the IPM transformation has been a concern of some authors [21,35]. Its implementation on on-board systems requires real time performance from the systems. In order to test the performance of the proposed approach, the dual camera PTU setup was used (Fig. 10(a)). In a classic IPM, all pixels in a given image are inverse projected, i.e., (14) is applied to all pixels. On the other hand, the proposed approach first computes the image projection polygon, and then applies (14) only to the pixels that should be projected. The computational demand of an IPM operation depends on the amount of projected pixels, which in turn depends on the camera’s pose towards the projection plane. For example, a camera pointing to the sky will have only a small amount of pixels viewing the road plane. To compare the performance of classic IPM with the proposed approach a 14s test sequence was devised. Since the orientation of the camera’s towards the road plane changes the amount of projectable pixels, during the 14s of the test, the PTU is ordered to go to specific positions: • State 1 (0–5.5s) the PTU is moving upwards. This causes an increasingly smaller amount of mappable pixels for both cameras. • Stage 2 (5.5–8.5s) moves the PTU down and the inverse phenomena occurs. • Stage 3 (8.5–14s) maintains a fixed tilt and the PTU pans increasingly to the left, which will make Camera 1 to have increasingly less mappable pixels. Fig. 13 shows some IPM resulting images of key points in the test sequence. Fig. 14 (a) compares the projection time of both cameras using the classic IPM and the proposed approach. Fig. 14(b) indicates the amount of projected pixels and the time saved using the proposed approach in relation to the classic IPM. From 0 to 5.5s, the PTU is moving upwards and so the pitch angle of the cameras is changing. This is observable in the difference of mapping in Fig. 13(a)–(c). Fig. 14(b) shows a reduction in the number of projected pixels for each camera. In Fig. 14(a), a reduction of IPM projection time using the proposed approach is clearly noted. Camera 0 takes more time to project than Camera 1 because the resolution of the images is different ( 800 × 600 pixels and 320 × 240 pixels, respectively). From 5.5 to 8.5s the PTU is moving downwards and the effects are the inverse. From 8.5 to 14s the change in pan angle causes Camera 1 to view increasingly less of the desired area of perception. Fig. 14(b) shows a decrease in the number of projected pixels during this period. 7.3 IPM accuracy Although many researchers have employed the IPM operation in order to ease the road recognition process [2–5], the fact is that no reporting of the accuracy of each implementation was found in the literature. Despite some insights on the topic of accuracy measurement for general projective geometry [10,11], a method had to be devised for this particular application to provide a quantitative analysis of the proposed method. For this experience the dual camera PTU setup was used (Fig. 10(a)). The calibration grid presented in Section 7.1 was employed and an accuracy of η IPM = 0.85 was achieved for the system. Because the current paper is the first to present such quantitative results, a measure of the quality of this value is not possible. The second experiment is intended to assess how important is to have accurate measures of the camera’s position and orientation with the road plane. In other words, how does the uncertainty of the camera pose estimation reflect on the final accuracy of the projection. For this purpose, errors in the yaw and pitch angles of the camera were introduced, and the IPM accuracy was calculated. Fig. 15 shows the resulting IPM of mappings with some errors (a), (b), (d) and (e) and the resulting image with no errors (c). Fig. 16 shows the decrease in IPM accuracy with the increase of error in yaw and pitch. The pitch angle is the most relevant for the projection accuracy, since a half degree error changes the accuracy from 0.85 to 0.30. Variations in yaw also drop the accuracy value to 0.30, but only after a 3.5 degree deviation. This is consistent with the concerns of several researchers worldwide that mention on-board camera’s pitch estimation to be a cumbersome problem. In [30], for example, it is stated that “a small error in the vehicle’s roll/pitch estimation leads to a massive terrain classification error forcing the vehicle off the road. Such situations occur even for roll/pitch errors below 0.5 degrees”. 7.4 IPM accuracy using LRF In order to test the usage of the LRF on the IPM projection the ATLASMV robot was used. An obstacle with 0.2m height (green box in Fig. 10(c)) was placed over the calibration grid in front of the robot at several distances and in several positions (to the left, right or in front of the robot). For each obstacle position the accuracy was computed. Fig. 17 shows the η IPM results both for the classic and the proposed IPM approach. The laser polygon introduced in Section 6.4 is able to classify pixels that view the obstacle as unmappable. Because of this, the proposed IPM approach (Fig. 17, dashed lines) consistently gets better accuracy results than the classic IPM (Fig. 17, dotted lines). When the obstacle is very close (0.3m, Fig. 18 (a)), using a classic IPM operation would be catastrophic (0.33 accuracy ratio) but the proposed approach remains accurate enough (0.75). In theory, the performance of IPM should increase when the obstacle is moved to a higher distance. This is not always observable in Fig. 17: for example the curve Obstacle Front, Proposed IPM shows a decrease from 0.3 to 0.5m. We believe that these variations in the performance measurement methodology might possibly be caused by noise in the input images. In fact, some other experiences also show a decrease in performance when the distance increases. For example, Obstacle Left, Proposed IPM, also decreases performance from 1m to 1.25m. Nonetheless, the main point is that, in the cases where an obstacle is present in the image, despite these variations in performance measurements, it is evident that the proposed IPM always shows a better performance when compared to the classic IPM. Fig. 18 shows the IPM resultant images for some of the tested scenarios. 7.5 Tests in real environments For the final validation of the proposed approach, several tests in real road scenarios with a full scale vehicle were done. The test platform used was the ATLASCAR (Fig. 12). The platform is equipped with three cameras (each with a different focal distance lens) and several lasers. Several hours of data from urban and highway roads were used for validating the algorithm. The proposed approach is less time consuming, is able to deal with pitch/roll variations due to brake/turning maneuvers, and using the LRF copes with obstacles present in the projection area. Fig. 19 provides a qualitative comparison of the classical IPM with the proposed multimodal IPM. It is possible to observe that in the presence of other vehicles or obstacles, the classical IPM produces several artifacts on the resultant image. On the contrary, the multimodal approach to IPM is able to cope with obstacles and removes them from the resultant image. Even in free road scenarios, as is the case of Fig. 19 (fourth line), the artifacts produced by the parked cars could reduce the effectiveness of a road detection approach. The flexibility of the proposed approach can also handle the usage of several input cameras. In Fig. 20 , the three cameras on-board the ATLASCAR (Fig. 12), each with different focal distance lenses, are used to obtain a more detailed mapping of the road. Fig. 20(a) shows images from the three cameras. The IPM is mapped to the road plane and the distribution of pixels supplied by each camera is shown in Fig. 20(b). Using a single camera to map the road (the green camera), shows the classical problems of lack of accuracy at long distances (the yellow traffic pattern in Fig. 20(c)). However, if multiple cameras are employed, the tele-camera (blue camera) can provide a high resolution view at long distances, which leads to a high resolution view of the yellow pattern of the road (Fig. 20(d)). Several video sequences showing results from classical and proposed IPM can be found at 8 Conclusions The current paper presents a flexible mathematical model for performing IPM. The methodology is to fuse laser data with vision data in order to improve the accuracy of the IPM projection. The fusion mechanism is based on the intersection of polygons defined according to several criteria: the algorithm computes the polygons generated from the image boundaries, the laser obstacles and the desired area of perception; then, the combination of these polygons (projection polygon) is projected back to the image plane, resulting in the image projection polygon. The image projection polygon is defined in the image coordinate system and can therefore be directly used to as a criterion to indicate which pixels are to be mapped through IPM, and which should not be mapped. Different test platforms, from a small scale robot to a full scale autonomous vehicle, were used to obtain both quantitative and qualitative results. Results show that the proposed approach is computed in less time than the classic IPM, and that the IPM image produced by the proposed approach has higher accuracy when obstacles are present in the road. A study of the influence of errors in the camera’s pose estimation to the IPM projection accuracy which corroborates previous findings is also presented. Finally, several hours of data both from urban roads as well as highways were qualitatively analyzed to evaluate the robustness and efficiency of the proposed approach. In sum, this paper proposes a novel algorithm that solves a common problem of the classical IPM: the disrupting of the IPM image when obstacles are present in the road. The proposed solution is to fuse the information from the images with the data from a LRF in order to specify in the image which pixels are viewing the ground plane and should therefore be mapped using IPM. Results show that the proposed method is more efficient than classical IPM. Acknowledgements Part of this work was developed under the grant SFRH/BD/43203/2008, from the Portuguese National Science Foundation – FCT, and supported by the Spanish Research Project reference TIN2011-25606. References [1] Federal Highway Administration, Data Fusion for Delivering Advanced Traveler Information Services, United States Department of Transportation Intelligent Transportation Systems Joint Program Office, 2003, pp. 247–254. [2] N.-E. El Faouzi H. Leung A. Kurian Data fusion in intelligent transportation systems: progress and challenges – a survey Inform. Fusion 12 1 2011 4 10 [3] C. Stiller F.P. León M. Kruse Information fusion for automotive applications – an overview Inform. Fusion 12 4 2011 244 252 [4] A. Sathyanarayana P. Boyraz J.H. Hansen Information fusion for robust ‘context and driver aware’ active vehicle safety systems Inform. Fusion 12 4 2011 293 303 [5] V. Kastrinaki M. Zervakis K. Kalaitzakis A survey of video processing techniques for traffic applications Image Vision Comput. 21 4 2003 359 381 [6] N. Simond M. Parent Obstacle detection from ipm and super-homography IROS 2007 IEEE 4283 4288 [7] P. Cerri, P. Grisleri, Free space detection on highways using time correlation between stabilized sub-pixel precision IPM images, in: Proceedings of the IEEE International Conference on Robotics and Automation, Barcelona, Spain, 2005, pp. 2223–2228. [8] M. Guanglin, P. Su-Birm, S. Miiller-Schneiders, A. Ioffe, A. Kummert, Vision-based pedestrian detection-reliable pedestrian candidate detection by combining IPM and a 1D profile, in: Proceedings of the IEEE Intelligent Transportation Systems Conference, Seattle, Washington, USA, 2007, pp. 137–142. [9] C. Lundquist T.B. Schön Joint ego-motion and road geometry estimation Inform. Fusion 12 4 2011 253 263 [10] M. Aly, Real time detection of lane markers in urban streets, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Eindhoven, Netherlands, 2008, pp. 7–12. [11] J. McCall, O. Achler, M. Trivedi, Design of an instrumented vehicle test bed for developing a human centered driver support system, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Parma, Italy, 2004, pp. 483–488. [12] A. Muad, A. Hussain, S. Samad, M. Mustaffa, B. Majlis, Implementation of inverse perspective mapping algorithm for the development of an automatic lane tracking system, in: Proceedings of the IEEE International Conference TENCON, vol. A, Chiang Mai, Tailandia, 2004, pp. 207–210. [13] H. Mallot H. Bülthoff J. Little S. Bohrer Inverse perspective mapping simplifies optical flow computation and obstacle detection Biol. Cybernet. 64 3 1991 177 185 [14] D. Pomerleau, RALPH: rapidly adapting lateral position handler, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Detroit, USA, 1995, pp. 506–511. [15] T. Ehlgen T. Pajdla D. Ammon Eliminating blind spots for assisted driving IEEE Trans. Intell. Transp. Syst. 9 4 2008 657 665 [16] H. Fang M. Yang R. Yang C. Wang Ground-texture-based localization for intelligent vehicles IEEE Trans. Intell. Transp. Syst. 10 3 2009 463 468 [17] S. Li Y. hai Easy calibration of a blind-spot-free fisheye camera system using a scene of a parking space IEEE Trans. Intell. Transp. Syst. 12 1 2011 232 242 [18] J. McCall, M. Trivedi, Performance evaluation of a vision based lane tracker designed for driver assistance systems, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Las Vegas, NV, USA, 2005, pp. 153–158. [19] F. Dornaika J. Álvarez A. Sappa A. López A new framework for stereo sensor pose through road segmentation and registration IEEE Trans. Intell. Transp. Syst. 12 4 2011 954 966 [20] C. Guo, S. Mita, D. McAllester, Stereovision-based road boundary detection for intelligent vehicles in challenging scenarios, in: Proceedings of the IEEE International Conference on Intelligent Robots and Systems, St. Louis, MO, USA, 2009, pp. 1723–1728. [21] M. Bertozzi, A. Broggi, A. Fascioli, Real-time obstacle detection on a massively parallel linear architecture, in: 3rd International Conference on Algorithms and Architectures for Parallel Processing, Melbourne, Australia, 1997, pp. 535–542. [22] M. Bertozzi A. Broggi GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection IEEE Trans. Image Proc. 7 1 1998 62 81 [23] S. Tuohy, D. O’Cualain, E. Jones, M. Glavin, Distance determination for an automobile environment using inverse perspective mapping in OpenCV, in: Proceedings of the IET Irish Signals and Systems Conference, Cork, Ireland, 2010, pp. 100–105. [24] A. Broggi, P. Cerri, S. Ghidoni, P. Grisleri, H.G. Jung, Localization and analysis of critical areas in urban scenarios, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Eindhoven, Netherlands, 2008, pp. 1074–1079. [25] S. Tan J. Dale A. Anderson A. Johnston Inverse perspective mapping and optic flow: a calibration method and a quantitative analysis Image Vision Comput. 24 2 2006 153 165 [26] L. Luo, I. Koh, S. Park, R. Ahn, J. Chong, A software-hardware cooperative implementation of bird’s-eye view system for camera-on-vehicle, in: Proceedings of the IEEE International Conference on Network Infrastructure and Digital Content, Beijing, China, 2009, pp. 963–967. [27] P. Coulombeau, C. Laurgeau, Vehicle yaw, pitch, roll and 3D lane shape recovery by vision, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Versailles, France, 2002, pp. 619–625. [28] R. Labayrade, D. Aubert, A single framework for vehicle roll, pitch, yaw estimation and obstacles detection by stereovision, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Columbus, OH, USA, 2003, pp. 31–36. [29] M. Nieto, L. Salgado, F. Jaureguizar, J. Cabrera, Stabilization of inverse perspective mapping images based on robust vanishing point estimation, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Istanbul, Turkey, 2007, pp. 315–320. [30] S. Thrun M. Montemerlo A. Aron Probabilistic terrain analysis for high-speed desert driving Proceedings of the Robotics Science and Systems Conference, University of Pennsylvania 2006 University of Pennsylvania Philadelphia, PA 21 28 [31] A. Broggi, S. Cattani, P.P. Porta, P. Zani, A laserscanner-vision fusion system implemented on the terramax autonomous vehicle, in: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Beijing, China, 2006, pp. 111–116. [32] A.F. Massimo Bertozzi, Alberto Broggi, An extension to the inverse perspective mapping to handle non-flat roads, in: IEEE International Conference on Intelligent Vehicles. Proceedings of the 1998 IEEE International Conference on Intelligent Vehicles, 1998, pp. 305–310. [33] A. Sappa F. Dornaika D. Ponsa D. Gerónimo A. López An efficient approach to on-board stereo vision system pose estimation IEEE Trans. Intell. Transp. Syst. 9 3 2008 476 490 [34] V. Santos, J. Almeida, E. Avila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, Atlascar – technologies for a computer assisted driving system on board a common automobile, in: Proceedings of the IEEE International Conference on Intelligent Transportation Systems, Madeira Island, Portugal, 2010, pp. 1421–1427. [35] O.D. Evans Y. Kim Efficient implementation of image warping on a multimedia processor Real-Time Imaging 4 6 1998 417 428 "
    },
    {
        "doc_title": "Modulation of dynamic movement primitives for biped locomotion",
        "doc_scopus_id": "84999853133",
        "doc_doi": "10.1142/9789814725248_0048",
        "doc_eid": "2-s2.0-84999853133",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Dynamic movement primitives",
            "Foot clearances",
            "Forward velocity",
            "Gait pattern",
            "Ground surfaces",
            "Locomotion patterns",
            "Motor primitives",
            "On-line adaptation"
        ],
        "doc_abstract": "© 2015, World Scientific Publishing Co. Pte Ltd. All rights reserved.In recent years, several studies have suggested that improved performance of modern robots can arise from encoding commands in terms of motor primitives. In this context, dynamic movement primitives (DMP) is a powerful tool for motion planning based on demonstrations, well-suited for robot learning. In this work, we study on-line adaptation of biped locomotion patterns when employing DMP. The goal is to demonstrate and evaluate how new movements can be generated by simply modifying the parameters of rhythmic DMP learned in task space. This formulation allows recreating new movements such that the DMP's parameters directly relate to task variables, such as step length, hip height, foot clearance and forward velocity. Several experiments are conducted using the V-REP simulator, including the adaptation of the robot's gait pattern to irregularities on the ground surface and stepping over obstacles.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptive behavior of a biped robot using dynamic movement primitives",
        "doc_scopus_id": "84945926215",
        "doc_doi": "10.1007/978-3-319-23485-4_46",
        "doc_eid": "2-s2.0-84945926215",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive behavior",
            "Coupled phase oscillators",
            "Dynamic movement primitives",
            "External perturbations",
            "Interlimb coordination",
            "Movement primitives",
            "Phase resetting",
            "Sensory information"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Over the past few years, several studies have suggested that adaptive behavior of humanoid robots can arise based on phase resetting embedded in pattern generators. In this paper, we propose a movement control approach that provides adaptive behavior by combining the modulation of dynamic movement primitives (DMP) and interlimb coordination with coupled phase oscillators. Dynamic movement primitives (DMP) represent a powerful tool for motion planning based on demonstration examples. This approach is currently used as a compact policy representation well-suited for robot learning. The main goal is to demonstrate and evaluate the role of phase resetting based on foot-contact information in order to increase the tolerance to external perturbations. In particular, we study the problem of optimal phase shift in a control system influenced by delays in both sensory information and motor actions. The study is performed using the V-REP simulator, including the adaptation of the humanoid robot’s gait pattern to irregularities on the ground surface.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On leader following and classification",
        "doc_scopus_id": "84911498541",
        "doc_doi": "10.1109/IROS.2014.6942996",
        "doc_eid": "2-s2.0-84911498541",
        "doc_date": "2014-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Adaboost classifications",
            "Classification process",
            "Dynamic environments",
            "Human environment",
            "Innovative method",
            "Leader following",
            "Learning frameworks"
        ],
        "doc_abstract": "© 2014 IEEE.Service and assistance robots that must move in human environment must address the difficult issue of navigating in dynamic environments. As it has been shown in previous works, in such situations the robots can take advantage of the motion of persons by following them, managing to move together with humans in difficult situations. In those circumstances, the problem to be solved is how to choose a human leader to be followed. This work proposes an innovative method for leader selection, based on human experience. A learning framework is developed, where data is acquired, labeled and then used to train an AdaBoost classification algorithm, to determine if a candidate is a bad or a good leader, and also to study the contribution of features to the classification process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Motion generalization with dynamic primitives",
        "doc_scopus_id": "85007286962",
        "doc_doi": "10.1142/9789814623353_0026",
        "doc_eid": "2-s2.0-85007286962",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Dynamic motions",
            "Generalization performance",
            "Human like",
            "Human motion capture data",
            "Motion generalization",
            "Reaching movements",
            "Task space"
        ],
        "doc_abstract": "© 2014 by World Scientific Publishing Co. Pte. Ltd.Although considerable research has been done on motion planning based on demonstrated examples, the ability to generalize from few examples is still an open problem. In this paper, we examine the ability of reproduction and generalization of human-like movements based on the paradigm of dynamic motion primitives (DMPs). The purpose of the current study is to accurately portray the characteristics and generalization performance of this approach, both using simulated and human motion capture data. To this aim, we consider discrete movements that require reach actions toward a target. The adaptation of learnt movements to new situations is performed in the joint and the task spaces. The generalization performance is evaluated and the feasibility of the approach is discussed for this study of reaching movements.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A kinect-based motion capture system for robotic gesture imitation",
        "doc_scopus_id": "84924557551",
        "doc_doi": "10.1007/978-3-319-03413-3_43",
        "doc_eid": "2-s2.0-84924557551",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D pose estimation",
            "Articulated structures",
            "Constraint optimizations",
            "Correction method",
            "Dynamic environments",
            "Gesture imitation",
            "Kinematic information",
            "Motion capture system"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Exploring the full potential of humanoid robots requires their ability to learn, generalize and reproduce complex tasks that will be faced in dynamic environments. In recent years, significant attention has been devoted to recovering kinematic information from the human motion using a motion capture system. This paper demonstrates and evaluates the use of a Kinectbased capture system that estimates the 3D human poses and converts them into gestures imitation in a robot. The main objectives are twofold: (1) to improve the initially estimated poses through a correction method based on constraint optimization, and (2) to present a method for computing the joint angles for the upper limbs corresponding to motion data from a human demonstrator. The feasibility of the approach is demonstrated by experimental results showing the upper-limb imitation of human actions by a robot model.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi hypotheses tracking with nonholonomic motion models using lidar measurements",
        "doc_scopus_id": "84924542281",
        "doc_doi": "10.1007/978-3-319-03413-3_20",
        "doc_eid": "2-s2.0-84924542281",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Acceleration models",
            "Constant velocities",
            "Future predictions",
            "Lidar measurements",
            "MHT",
            "Multiple hypothesis tracking",
            "Nonholonomics",
            "Real time performance"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.This paper presents an implementation of the Multiple Hypothesis Tracking (MHT) algorithm in the Advanced Driver Assistance Systems (ADAS) context.The proposed algorithmuses laser data received from two front mounted sensors on a mobile vehicle. The algorithm was tested with simulated and real world data and shown to obtain a very good performance. Nonholonomic motion models were used to model the movement of road agents instead of the more traditional constant velocity/ acceleration models. The use of the nonholonomic motion models allows to obtain not only the linear velocity, but also the steering angle of vehicles, improving this way the future prediction and handling of occlusions. The MHT algorithm possesses some well-known critical disadvantages due to its complexity and computational growth, in this work we circumvent these limitations in order to achieve real time performance in real work conditions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Increasing flexibility in footwear industrial cells",
        "doc_scopus_id": "84905041352",
        "doc_doi": "10.1109/ICARSC.2014.6849801",
        "doc_eid": "2-s2.0-84905041352",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "High-quality standards",
            "Industrial cells",
            "Industrial sector",
            "International markets",
            "Iterative closest point algorithm",
            "Position and orientations",
            "Production line",
            "Technological innovation"
        ],
        "doc_abstract": "Nowadays, entering in the highly competitive international market becomes a key strategy for the survive and sustained growth of enterprises in the Portuguese textile and footwear industrial sector. Thereby, to face new requirements, companies need to understand that technological innovation is a key issue. In this scenario, the research presented in this paper focuses on the development of a robot based conveyor line pick-and-place solution to perform an automatic collection of the shoe last. The solution developed consists of extracting the 3D model of the shoe last suport transported in the conveyor line and aligning it, using the Iterative Closest Point (ICP) algorithm, with a template model previously recorded. The Camera-Laser triangulation system was the approach selected to extract the 3D model. With the correct position and orientation estimation of the conveyor footwear, it will make possible to execute the pick-and-place task using an industrial manipulator. The practical implication of this work is that it contributes to improve the footwear production lines efficiency, in order to meet demands in shorter periods of time, and with high quality standards. This work was developed in partnership with the Portuguese company CEI by ZIPOR. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Motion generalization from a single demonstration using dynamic primitives",
        "doc_scopus_id": "84905024225",
        "doc_doi": "10.1109/ICARSC.2014.6849807",
        "doc_eid": "2-s2.0-84905024225",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Dynamic movement primitives",
            "Generalization performance",
            "Geometric invariance",
            "Human motion capture data",
            "Imitation learning",
            "Incremental process",
            "Motion generalization",
            "Motor commands"
        ],
        "doc_abstract": "In recent years, several studies have suggested that improved performance of modern robots can arise from encoding motor commands in terms of dynamic primitives. In this context, dynamic movement primitives (DMPs) have been proposed as a powerful tool for motion planning based on demonstrated examples. In this work, we focus on generalizing discrete and periodic movements from a single demonstration. Here, we argue that geometric invariance in itself may be useful to provide an initial representation of movements in an incremental process of learning from experience. The purpose of the current study is to portray the generalization performance of this approach, both using simulated and human motion capture data. The generalization performance is evaluated and the feasibility of the approach is discussed. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Object recognition and pose estimation for industrial applications: A cascade system",
        "doc_scopus_id": "84901687133",
        "doc_doi": "10.1016/j.rcim.2014.04.005",
        "doc_eid": "2-s2.0-84901687133",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Autonomous systems",
            "Co-ordinate system",
            "Conventional identification",
            "Flexible manufacturing",
            "Mechanical structures",
            "Off-the-shelf hardwares",
            "Recognition systems",
            "Spray coating"
        ],
        "doc_abstract": "The research work presented in this paper focuses on the development of a 3D object localization and recognition system to be used in robotics conveyor coating lines. These requirements were specified together with enterprises with small production series seeking a full robotic automation of their production line that is characterized by a wide range of products in simultaneous manufacturing. Their production process (for example heat or coating/painting treatments) limits the use of conventional identification systems attached to the object in hand. Furthermore, the mechanical structure of the conveyor introduces geometric inaccuracy in the object positioning. With the correct classification and localization of the object, the robot will be able to autonomously select the right program to execute and to perform coordinate system corrections. A cascade system performed with Support Vector Machine and the Perfect Match (point cloud geometric template matching) algorithms was developed for this purpose achieving 99.5% of accuracy. The entire recognition and pose estimation procedure is performed in a maximum time range of 3 s with standard off the shelf hardware. It is expected that this work contributes to the integration of industrial robots in highly dynamic and specialized production lines. © 2014 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 271446 291210 291866 291883 291885 31 Robotics and Computer-Integrated Manufacturing ROBOTICSCOMPUTERINTEGRATEDMANUFACTURING 2014-05-28 2014-05-28 2014-07-16T12:50:35 S0736-5845(14)00031-3 S0736584514000313 10.1016/j.rcim.2014.04.005 S300 S300.1 FULL-TEXT 2015-05-15T02:05:36.934174-04:00 0 0 20141201 20141231 2014 2014-05-28T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref alllist content subj ssids 0736-5845 07365845 true 30 30 6 6 Volume 30, Issue 6 4 605 621 605 621 201412 December 2014 2014-12-01 2014-12-31 2014 article fla Copyright © 2014 Elsevier Ltd. All rights reserved. OBJECTRECOGNITIONPOSEESTIMATIONFORINDUSTRIALAPPLICATIONSACASCADESYSTEM ROCHA L 1 Introduction 1.1 Motivation and proposed approach 1.2 Literature review 1.2.1 Model reconstruction sensors 1.2.2 Feature extraction, object recognition and pose estimation 1.3 Industrial application considered for validation of the proposed system 2 3D modeling solution adopted 2.1 Laboratory set-up prototype 2.2 Industrial test bed 3 Object recognition approach 3.1 The perfect match algorithm 3.1.1 2D approach – used in robot soccer field 3.1.2 3D approach 3.1.3 Matching algorithm applied to 3D objects models – classification purpose 3.1.4 Creating the 3D model, distance and gradient matrices 3.1.5 State variables 3.1.6 Phase 1 – computation of the matching error 3.1.7 Phase 2 – optimization routine Resilient Back-Propagation (RPROP) 3.1.8 Perfect match training and production 3.2 Support Vector Machine (SVM) 3.2.1 Feature extraction – Hu moments and Fourier Transformation (FT) 3.2.2 Feature selection 3.2.3 SVM model selection and assessment 3.3 Support vector machine final architecture 3.3.1 Support vector machine – training phase 3.4 Support vector machine – production phase 3.4.1 SVM results 4 Final architecture – cascade approach 4.1 Final results 4.1.1 Confusion matrix 4.1.2 Perfect match – precision of the 3 DoF pose estimation 4.2 Recognition results – cascade classifier vs viewpoint feature histogram (VFH) 5 Conclusions and future work Acknowledgments References CARPIN 2005 1 14 S FERREIRA 2012 1031 1041 M HERNANDEZLOPEZ 2012 196 204 J HINTERSTOISSER 2012 548 562 S COMPUTERVISIONACCV2012LECTURENOTESINCOMPUTERSCIENCE MODELBASEDTRAINING KHABOU 2007 141 153 M KIRKPATRICK 1983 671 680 S LZARO 1998 822 830 A METHODOLOGYTOOLSINKNOWLEDGEBASEDSYSTEMS11THINTERNATIONALCONFERENCEINDUSTRIALENGINEERINGAPPLICATIONSARTIFICIALINTELLIGENCEEXPERTSYSTEMSIEAAIE98CASTELLNSPAINJUNE141998PROCEEDINGSVOLUMEILECTURENOTESINCOMPUTERSCIENCE IDENTIFICATIONDEFECTINSPECTIONULTRASONICTECHNIQUESINFOUNDRYPIECES MINGQIANG 2008 43 90 Y MURPHY 2006 382 400 K TOWARDCATEGORYLEVELOBJECTRECOGNITIONLECTURENOTESINCOMPUTERSCIENCE OBJECTDETECTIONLOCALIZATIONUSINGLOCALGLOBALFEATURES PINTO 2013 12 22 A RAMACHANDRAM 2004 219 231 D ROCKMORE 2000 60 64 D SANSONI 2009 568 601 G ROCHAX2014X605 ROCHAX2014X605X621 ROCHAX2014X605XL ROCHAX2014X605X621XL item S0736-5845(14)00031-3 S0736584514000313 10.1016/j.rcim.2014.04.005 271446 2014-07-16T09:27:43.185625-04:00 2014-12-01 2014-12-31 true 5679339 MAIN 17 60908 849 656 IMAGE-WEB-PDF 1 si0085 299 13 72 si0084 295 15 69 si0083 820 15 191 si0082 297 13 55 si0081 334 12 65 si0080 175 11 25 si0079 175 11 25 si0078 297 13 55 si0077 334 12 65 si0076 152 13 16 si0075 156 13 16 si0074 248 14 35 si0073 520 15 128 si0072 217 15 32 si0071 240 11 39 si0070 459 15 76 si0069 506 15 128 si0068 217 15 32 si0067 240 11 39 si0066 459 15 76 si0065 816 17 224 si0064 217 15 32 si0063 754 15 168 si0062 718 19 204 si0061 217 15 32 si0060 754 15 168 si0059 459 15 76 si0058 248 14 35 si0057 223 13 31 si0056 210 12 30 si0055 3820 110 398 si0054 3029 88 284 si0053 980 32 195 si0052 964 32 194 si0051 398 15 66 si0050 498 16 107 si0049 223 13 31 si0048 210 12 30 si0047 1015 33 220 si0046 1087 33 251 si0045 1291 35 258 si0044 749 34 146 si0043 2351 43 380 si0042 398 15 66 si0041 3332 101 329 si0040 498 16 107 si0039 740 38 166 si0038 498 16 107 si0037 241 13 37 si0036 2432 57 245 si0035 300 10 47 si0034 175 11 29 si0033 325 13 58 si0032 359 13 76 si0031 2734 61 361 si0030 341 16 54 si0029 253 13 42 si0028 686 15 175 si0027 248 14 35 si0026 248 14 35 si0025 1861 49 404 si0024 1849 49 403 si0023 1042 30 226 si0022 1036 30 226 si0021 223 13 31 si0020 209 12 30 si0019 1372 55 197 si0018 561 14 104 si0017 395 15 73 si0016 789 55 134 si0015 789 55 121 si0014 223 13 31 si0013 209 12 30 si0012 223 13 31 si0011 210 12 30 si0010 1485 15 352 si0009 195 14 21 si0008 540 16 134 si0007 349 16 54 si0006 195 14 21 si0005 2011 40 338 si0004 945 40 154 si0003 358 13 68 si0002 252 14 31 si0001 127 11 10 gr9 13343 224 377 gr8 11809 211 376 gr7 46804 196 747 gr6 47149 195 742 gr5 42016 195 746 gr4 39987 196 751 gr3 91397 485 659 gr2 51383 333 528 gr16 47501 543 508 gr15 83893 864 565 gr14 21527 245 508 gr13 20132 187 565 gr12 42426 286 520 gr11 60757 488 565 gr10 73418 504 374 gr1 36590 305 559 gr9 1939 130 219 gr8 1650 123 219 gr7 6217 57 219 gr6 6021 57 219 gr5 5459 57 219 gr4 5831 57 219 gr3 12157 161 219 gr2 12866 138 219 gr16 4934 164 153 gr15 2692 164 107 gr14 2451 105 219 gr13 2930 72 219 gr12 7148 120 219 gr11 5898 164 190 gr10 6470 163 121 gr1 10032 120 219 RCM 1253 S0736-5845(14)00031-3 10.1016/j.rcim.2014.04.005 Elsevier Ltd Fig. 1 Examples of objects coated by FLUPOL. Fig. 2 (a) Overall problem concept – top view. (b) FLUPOL׳s production line prototype (part size in the figure 800×300mm) [6]. Fig. 3 FLUPOL shop-floor cell – scheme designed by CEI by ZIPOR company. Fig. 4 Example of 3D models and the respective real object (a and b). Fig. 5 Example of 3D models and the respective real object (c and d). Fig. 6 Example of 3D models and the respective real object (e and f). Fig. 7 Example of 3D models and the respective real object (g and h). Fig. 8 Sketch of the world coordinate system, the robot relative coordinate system and a vector s i pointing to a detected line point. Fig. 9 Comparison between the squared error function (dashed line) and the used one in this algorithm (solid line) [18]. Fig. 10 3D World occupancy Grid Map. Fig. 11 Final result of the distance matrix computation procedure. Fig. 12 Matching of the unknown object (red color) with a template model (green color). (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.) Fig. 13 Perfect Match architecture. Fig. 14 K-fold cross-validation scheme. Fig. 15 Support vector machine – train flow chart. Fig. 16 Cascade final classification architecture algorithm considered. Table 1 Example of two cases where it is represented the asset of using Perfect Match in the cascade system. Ground truth part class SVM classification Perfect Match final result E E:10.227%; F:75.624% E B B:36.147%; H: 24.217% B Table 2 Confusion matrix for the SVM classifier – without considering SVM output probabilities. Correct Class Label SVM classification A B C D E F G H A 25 – – – – – – – B – 25 – – – – – – C – – 25 – – – – – D – – – 25 – – – – E – – 2 – 19 1 – 1 F – – – – – 24 – 1 G – – – – – – 25 – H – – – – – – – 25 Table 3 Confusion matrix for the SVM classifier – without considering SVM output probabilities. Correct Class Label SVM classification A B C D E F G H A 25 – – – – – – – B – 24 – – – – – 1 C – – 25 – – – – – D – – – 25 – – – – E – 3 3 – 17 1 – 1 F – – 2 – – 22 – 1 G – 1 – – – – 24 – H – – – – 1 – – 24 Table 4 Confusion matrix for the Cascade classifier. Correct Class Label Cascade system classification A B C D E F G H A 25 – – – – – – – B – 25 – – – – – – C – – 25 – – – – – D – – – 25 – – – – E – – 1 – 24 – – – F – – – – – 25 – – G – – – – – – 25 – H – – – – – – – 25 Table 5 Results for the pose estimation repeatability tests. Real part displacement Perfect Match pose estimation x (m) y (m) θ (degree) x (m) y (m) θ(degree) 0 0 0 Max absolute error: 0.0030 0.0012 0.0572 Mean absolute error: 0.001 0.0011 0.0114 Standard deviation: 0.0015 0.0011 0.0229 0 0 5 Max absolute error: 0.0080 0.0027 0.9741 Mean absolute error: 0.0030 0.0024 0.6875 Standard deviation: 0.0037 0.0026 0.1776 0 0 −5 Max absolute error: 0.0060 0.0060 0.745 Mean absolute error: 0.0039 0.0026 0.687 Standard deviation: 0.0042 0.0026 0.1318 0.05 0.05 0 Max absolute error: 0.0011 0.0044 0.4755 Mean absolute error: 0.0009 0.0042 0.4297 Standard deviation: 0.0002 0.0002 0.0573 Object recognition and pose estimation for industrial applications: A cascade system L.F. Luís F. Rocha a ⁎ Marcos Ferreira a V. Santos b A. Paulo Moreira a a INESC TEC - INESC Technology and Science (formerly INESC Porto) and FEUP - Faculty of Engineering, University of Porto, Portugal INESC TEC - INESC Technology and Science (formerly INESC Porto) and FEUP - Faculty of Engineering University of Porto Portugal b Department of Mechanical Engineering, IEETA, University of Aveiro, Portugal Department of Mechanical Engineering IEETA, University of Aveiro Portugal ⁎ Corresponding author. Tel.: +351 222 094 000; fax: +351 222 094 050. The research work presented in this paper focuses on the development of a 3D object localization and recognition system to be used in robotics conveyor coating lines. These requirements were specified together with enterprises with small production series seeking a full robotic automation of their production line that is characterized by a wide range of products in simultaneous manufacturing. Their production process (for example heat or coating/painting treatments) limits the use of conventional identification systems attached to the object in hand. Furthermore, the mechanical structure of the conveyor introduces geometric inaccuracy in the object positioning. With the correct classification and localization of the object, the robot will be able to autonomously select the right program to execute and to perform coordinate system corrections. A cascade system performed with Support Vector Machine and the Perfect Match (point cloud geometric template matching) algorithms was developed for this purpose achieving 99.5% of accuracy. The entire recognition and pose estimation procedure is performed in a maximum time range of 3s with standard off the shelf hardware. It is expected that this work contributes to the integration of industrial robots in highly dynamic and specialized production lines. Keywords Pattern recognition Flexible manufacturing Autonomous systems Robotics Spray coating 1 Introduction 1.1 Motivation and proposed approach Industrial manufacturing has always taken into consideration competitive factors such as time cost and quality. However, manufacturing is more and more characterized by customization which can be accomplished by reducing lot sizes increasing variety and specific products manufactured in short periods of time. Another specificity of next-generation manufacturing is the increasing complexity of products. Instead of rigid infrastructures or inflexible approaches, adaptive manufacturing is envisioned as a new paradigm aiming for continuous and permanent adaptation of all entities inherent to manufacturing systems namely the ones related to manufacturing resources, materials and information flows. The understanding of these multidimensional challenges leads to the use of techniques and tools to improve manufacturing processes and to decrease and eliminate non-value activities (such as transportation tasks, machinery set-up times and others). Now a days and due to the constant development of technology, sophisticated machinery is increasingly available which allows manufacturing firms to achieve significant process and set-up time reductions. Therefore, and to maintain their competitiveness in the market industrial manipulators must follow this technology evolution or otherwise they will only be used for repetitive processes or mass production scenarios. One of their most limiting features accepted as such from a flexible manufacturing point of view is their programming procedure. Typically this programming is a fairly time consuming process and represents a high investment unfordable for SMEs. These limitations are imposed by the complexity of their teach pendant (human–machine interface) that needs to contain all the commands available to interact with the industrial manipulator, as well as their programming simulator which is only accessible to expert personal. However, the programming procedure is not the only obstacle of industrial manipulators that prevent its widespread use in diversified fields of industry. The lack of capacity that they demonstrate in detecting and locating three-dimensional objects, as well as the stiffness of previously defined motion paths makes it impossible to be applied in highly dynamic production environments. These characteristics are at odds with the current state of the industry. Therefore, in an industrial environment where an unstructured material flow is present (type of objects arrive randomly and/or are randomly located) vision systems are a must enabling objects visual inspection needed for robot–object interaction. The scientific developments achieved as part of this work focus on the flexibility enhancement of industrial cells where the key elements are industrial manipulators. The idea is to create a recognition and pose estimation system (robot independent) that indicates to the industrial manipulator the object Id and correspondent trajectory adjustments (varying accordingly to the objects pose). This work makes it possible to increase the flexibility associated to industrial manipulators by introducing new perception skills. 1.2 Literature review Considering the addressed research problem, this section presents a revision of 3D Model Reconstruction Techniques and Object Classification streams. The focus of this paper is object recognition and pose estimation. Therefore, any scientific contributions were made regarding 3D modeling sensors. 1.2.1 Model reconstruction sensors Over the last decade there has been a major development in 3D modeling. In [34] it is presented a reasonable set of three-dimensional image reconstruction techniques: (1) Structured light sensor, (2) Stereo Vision, Photometry, and (3) Time of flight and others. (1) In the field of Structured Light Sensors, laser beam plus camera triangulation systems are usually used. Considering industrial applications, Pinto et al. in [28] sense and measure the position and dimensions of a cork piece in a conveyor belt. In [7] the authors present the same system this time in a coating application. In these approaches the precision values are directly related with the thickness of the laser line and the camera resolution. The major disadvantage is the need of object/robot motion so the 3D model can be created. Furthermore, a well-structured light environment is also required. In more recent works, as in [16], a new approach is presented for 3D modeling that makes it possible to extract the 3D shape and color. This system is based on a three-dimensional Laser Range Finder (LRF) and a camera, which is relevant if the color feature is important to differentiate objects. Using a very similar concept, projection of an infra-red pattern (structured light), the Microsoft Kinect 1 1 now receives most of the attention for 3D modeling especially because it is an affordable tool. Although the sensor has a high potential because it is capable of extracting 3D points model adding the color feature, its resolution falls shortly comparatively to other low cost solutions. The authors in [9] use the Kinect to perform the segmentation of objects present in a scene. In their application, the Kinect RGB camera is used to perform object color segmentation and the depth information used to differentiate objects that are not in the same plane of interest. (2) In the field of stereo vision, [23] presents an object detection system that uses 3D information provided by stereo reconstruction. According to the authors the resulting system is a high-accuracy far distance obstacle detector, covering a wide range of real scenarios. In [19], Li et al. propose a 3D reconstruction approach based on stereo vision and texture mapping that it could be used in a vast application areas (such as visual navigation of robots or 3D games). (3) Finally for the time of flight approaches, Cui et al. [3] describe a method for 3D object scanning by aligning depth scans that were taken from around an object. The authors refer that their approach overcomes the sensor׳s random noise and the presence of a non-trivial systematic bias by showing good quality 3D models with a sensor that presents such low quality data. Due to the simple technology that these sensors have comparatively the authors see potential for low cost production in large volumes. References [15,1] present an interesting research work with 2D (LRF) to perform 3D scene reconstruction. However, this is usually done in mobile navigation and not with industrial systems. The disadvantages of LRF are their high price for high precision measurements and the measurement variation with the object reflective properties. Although laser based solutions are the most used in the industrial environment, laser beam sensors are the common choice. For the research work presented in this paper, the solution for 3D modeling must consider the unique characteristics that may distinguish each of the objects to recognize (such as color or/and shape) and the industrial environment limitations where the recognition system will be assembled. 1.2.2 Feature extraction, object recognition and pose estimation 3D models contain a significant amount of information that can be analyzed, which makes it possible to extract the fundamental characteristics of the scene foreground. Object recognition is coarsely composed by two steps: (1) feature extraction and (2) object classification. (1) To differentiate objects it is simply necessary to distinguish the value of the parameters/features that belong to each class [26]. In the image analysis field (and for the feature extraction purpose) one of the most used techniques for evaluating object shapes is determining the invariant moments as they do not depend on scaling, translation and rotation [8]. Although that is one of the most well known approaches, others such as Fourier descriptors, Dirichlet Laplacian eigenvalues [12] and wavelet descriptors [13] have been developed to describe the shape of different patterns. Mingqiang et al. [21] present a survey of shape feature extraction techniques. Unlike the traditional classification, in the referred paper the authors divided the shape-based feature extraction according to their processing methods. (2) So, after capturing unique features using some of the techniques referred before object classification algorithms need to be applied. In this research field the most used strategies are from pattern recognition like: machine learning (such as the k-Nearest Neighbor (kNN), Support Vector Machine (SVM), Neural Networks (NN), Hidden Markov Models and Bayesian approaches) and point cloud analysis. There is a large number of research papers available in these areas, combining feature extraction and machine learning techniques. In [5], it is presented a fingerprint matching scheme based on transform features. In order to extract the unique features, the researchers use the Discrete Cosine, the Fast Fourier and the Discrete Wavelet Transform. Then, the Euclidean distance is used to classify the fingerprint minimum and thus compare two feature vectors. The authors refer that with the combination of the Discrete Cosine and the Fast Fourier it were achieved better results (recognition rate of 87.5%) when compared with the Discrete Wavelet Transform. In [30] is used a neural network to learn the complex relationship between the robots pose displacements and the observed feature variations on the image. Their objective is to create a visual positioning system that addresses features extraction issues for a class of objects that have smooth or curved surfaces. After the NN training, the authors use visual feedback to guide the robot manipulator to the target. For its turn, Koker et al. present an industrial machine vision for object recognition [17]. This system was developed based on a CCD camera, invariant moments and neural-networks (NN), where the main focus was the recognition of objects on a conveyor line. Lzaro et al. in [20] performs objects ultrasonic-based inspection and identification system. For object classification, the authors use NN. In their approach, they achieved a 100% classification accuracy. Murphy et al. in [22] propose an approach based on the extraction of local and global “gist” features (texture features) from 2D real world images. Their dataset comprises the following objects: Screen, Keyboard, CarSide and Person. High classification rates were achieved. It is worth highlighting that valid research efforts have been made for object recognition that do not rely on Machine Learning, namely those using direct template matching. In [35] Wopfner et al. present a recognition approach based in the squared Mahalanobis distance. Note that, this last research work refers directly to the recognition of objects for direct manipulation by a 5 axis robot in a human environment. However, once again industrial application has not been considered. In [24,25], Oliveira and Santos present an interesting work which deals with the problem of tracking fully dynamic objects (for instance, for car detection), based on Harr features that are used as a single view identifier. These features are then complemented by template matching to track the object classified previously. Dalal and Triggs [4] focuses in the application of Histograms of Oriented Gradient (HOG) descriptors for human detection. The proposed approach outperforms many state-of-art methods, presenting a near-perfect detection on the original MIT pedestrian database. Although, their work focused in human detection, the authors have tested their approach in generic objects and good result were also achieved. The work originated an Object Dection and Localization Toolkit available at Considering now point cloud 3D data, the authors in [33] presents a new descriptor data that encodes geometry and viewpoint (Viewpoint Feature Histogram) together with kNN. The authors indicate that with this approach, object recognition (recognition rate of 98.52% in a set of 60 objects) and pose estimation is accurate enough for robot manipulation. The only constraints are object rigidity, relatively Lambertian, not reflective or transparent. Considering [33] work, Hinterstoisser et al. [10] propose a framework for automatic modeling, detection and 3D object tracking (texture-less 3D objects). Their approach is based on capturing virtual viewpoints of the object (templates with viewpoint information). Then, the surface normals of these templates among with color gradient features are used for object detection. The templates view points along with iterative closest point (ICP) are utilized for 6DoF pose estimation. High recognition rates were achieved in heavy cluttered scenes. Another approach that can be considered, and using the 3D point cloud models directly, is the matching algorithm that some middle size robotic teams use for accurately locating the robot in a soccer field. The approach is based on an efficient numerical approach to find the best match between the camera image and the field model. For this purpose, the authors use the resilient back propagation algorithm to minimize the match error [18]. The matching in this algorithm is made in 2D, and therefore 3D extrapolation is required. Considering only object pose estimation, Jayawardena et al. in [11] present a 6 DoF pose estimation of cars in 2D images captured by any unknown and without prior calibration camera. This 6 DoF position, that is computed relatively to the car CAD models, is estimated utilizing a linearly invariant loss function. In the end, a good pose estimation precision was achieved. Although a vast number of research work in the recognition area, few refers directly to industrial direct application. Mainly due to the difficulty of creating a reliable method that can generalize well in a dynamic environment where new objects are constantly appearing. 1.3 Industrial application considered for validation of the proposed system The development of the research work presented was followed closely by an industrial partner – FLUPOL. 2 2 This enterprise is an industrial coating applicator whose goal is to help to solve problems of surface adhesion, dry lubrication or corrosion. Their production process requires a very high degree of specialization of their coating operators, as well as great flexibility of the means of production given the vast range of different objects that are treated. Today FLUPOL is focused on developing a robotized cell that allows a specialized coating operator to teach the coating trajectory directly to the industrial robot. Moreover, at FLUPOL they expect the system to have the capacity to identify the object type that needs to be coated, allowing the robot to upload autonomously the correct program to execute. This industrial partner produces 400 parts per day with a high types variability, therefore its consideration as industrial validator is a sure plus. Fig. 1 present some examples of the objects provided by FLUPOL. As it is possible to see, these parts cannot be distinguished by their texture (texture-less objects) or color. The production line in this Portuguese enterprise is characterized by a closed low speed conveyor line (0.01m/s), where the objects are transported vertically and where the coating operations and heat treatment are applied. For this reason and during production the conveyor cannot be stopped. Furthermore, each object can go through these two operations several times without leaving the conveyor. This production procedure makes it impossible to use sensors such as RFID for object identification. For FLUPOL, it is also important that the system is immune to the objects׳ rotation and pose changes that are imposed by the conveyor mechanical constraints. In addition, CAD models are not always available. Although, a specific enterprise considered, these problems are present in many SME׳s where a conveyor based transportation system is used and where production process limits the use of conventional part identification systems. Nerveless, the object recognition and pose estimation architecture that will be presented can be used in other scenarios the feature selection was tuned to the industrial set of parts. Therefore, it is not possible to guarantee their good performance when applied to other object recognition problems. 2 3D modeling solution adopted For the 3D modeling sensor selection, and considering the work previously presented in [27], the Camera-Laser triangulation system was chosen. Moreover, for transportation systems based on conveyors and for applications with high precision requisites this 3D modeling sensor seems to be an obvious choice. For the single camera approach, and considering that future new type of 3D objects can be added to the production line without any control of object shape, and due to the environment light sensitive it was discarded [27]. In Fig. 2 (a) the overall problem concept is presented. As it is possible to see, the objective is to create a complementary system that is responsible to recognize the objects and their pose and send this information to the robot so it can select and perform the manipulation trajectory correctly. Also note that the industrial robot is only responsible for the execution of its main tasks (handling/assembly, coating, etc). To allow the execution of some preliminary tests [6,31], a laboratory set-up was built in partnership with FLUPOL, Fig. 2(b). 2.1 Laboratory set-up prototype In the proposed set-up, the laser and the CCD Camera (characteristics: grey image and 1024×768 resolution) are located in a central position relatively to the object. The object is then fixed on a support attached to the conveyor, allowing it to move. This will make it possible to produce the required motion for the Camera-Laser triangulation system to extract the 3D model. Also, measures were taken hence the support that transports the object would not suffer oscillations in the axis normal to the conveyors׳ movement. Today, this work has already been industrialized. 2.2 Industrial test bed Fig. 3 presents the automatic coating cell scheme. As it is possible to see, the idea was to directly transfer the solution built in the prototype to FLUPOL׳s shop-floor. In this robotic cell, the object firstly goes through a dark cabin where its 3D model is extracted. With this model, the object is then classified and its position is estimated. Afterwards, this information is used so the industrial robot can select the correct manipulation program and if necessary perform trajectory pose adjustments. This will allow the complete cell automation. With the system placed at FLUPOL׳s shop-floor, new 3D data models were extracted. Figs. 4–7 present the 8 different objects considered. These objects have some pattern and dimensional similarities. During the extraction of these models, a problem emerged. The reflective properties of the parts of interest change during the production process. Initially the parts have high-reflectivity (parts are metallic) and after being applied the first coating layer their reflective properties change. This problem has direct influence in the quality of the objects 3D model (imposed by the Laser-Camera Triangulation system). Therefore, this problem was solved at the level of the 3D modelling sensor system, by analyzing the thickness of the laser line. In this sense, and during the production process, the exposure value of the camera is dynamically changed so the width of the laser line is maintained between a given range. Furthermore, the construction of the dark cabin contributed to the good efficiency of the presented solution. These procedures allowed to have immunity to the variation of the parts reflective properties (imposed by the color and specific paint characteristics). 3 Object recognition approach 3.1 The perfect match algorithm As the first solution explored, and for object recognition purposes, a 3D model direct matching was executed. The idea is to compare the object 3D model (the point cloud) transported in the conveyor (with an unknown class) with previously recorded and known class models (geometric template matching). The matching with the lowest error value will be the class of the unknown object. To perform this matching, the algorithm presented in [18] and recently updated for 3D matching by Pinto et al. [29] was utilized. In both works, a mobile robotic localization problem was considered. 3.1.1 2D approach – used in robot soccer field The algorithm presented in [18] is nowadays used by most of the world soccer robotic teams for robots localization, by using an estimation of their distance to the closest lines (a CCD Camera is mounted in the robot structure). High precision, robustness and computational efficiency are some of the motives which make this algorithm so used in the robotic soccer field. In their application scenario, the objective is to estimate the position and heading of the robot with respect to the information retrieved by the processed image (field markings). This estimation if performed considering an error function that describes the fitness of a particular soccer field estimation to a map previously recorded. Therefore, letting ( p , θ ) be a pair of possible robot positions p = ( px , py ) and heading θ in a global coordinated system, and s i be the vectors list of detected line points given relative to the robot׳s pose, the robot position in the world coordinates is given by (see Fig. 8 ) (1) p + [ cos θ − sin θ sin θ cos θ ] ⁎ s i Minimizing the error between detected line points and true field marking means to solve: (2) minimize p , θ E = ∑ i = 1 err ( d ( p + ( cos θ − sin θ sin θ cos θ ) ) ⁎ s i ) ) d ( . ) gives the distance from a certain point on the field to the closest field marking. err is the error function that punishes deviations between detected lines and the field soccer lines model. In other words, punish the deviation between the computed position of robot soccer field lines, computed using the position (x,y) and orientation of the robot (θ), with the robot soccer field map recorded in the database. Therefore, minimizing the error means to change the x, y and (θ) so the soccer field viewed by the robot fits the model in database. For this purpose, the most used error function is 1 / 2 ⁎ d ( . ) 2 that is standard in many applications. However, it is not appropriate for the present task since is not robust to outliers. Therefore, the selected error function was err = 1 − c 2 / ( c 2 + d ( . ) 2 ) (see Fig. 9 and [18] for more detailed information). Due to the non-linearity of the minimization problem it is not possible to analytically calculate its solution, so a numerical minimizer is needed. Since d ( . ) is almost everywhere differential it is possible to build its gradient almost everywhere and interpolate the gradient at the non-differentiable places. Hence, the gradient descent algorithm is used to solve the minimization problem. Due to RPROP quick convergence and high robustness the authors selected this algorithm to solve the minimization task. For computational performance purposes, distance map and respective gradient along the x axis and y axis are computed off-line. Remember this feature, since it is one of the most important one for computational efficiency of the algorithm. 3.1.2 3D approach Presented the basic of Perfect Match algorithm, Pinto et al. in [29] updates the referred method for 3D matching. The main difference is that his approach is based in a point cloud retrieved by an LRF. In other words, the authors started by acquiring a 3D Map (constructed with the point cloud information) of the environment with a LRF coupled to the mobile robot. After the creation of this 3D Map and considering off-line mode, a distance map and a gradient map are created and stored. These matrices are then used as look-up tables for the 3D matching localization procedure in the on-line robot motion. For the creation of the distance matrix, the distance transform is applied in the 3D occupancy grid of the world map. Furthermore, the Sobel Filter again in 3D space, is applied to obtain the gradient matrices. 3.1.3 Matching algorithm applied to 3D objects models – classification purpose Making now the parallel to the application presented in the present paper, the 3D Map is the 3D model of each of the objects to be recognized. Therefore, for each of these models it is necessary to store the equivalent 3D distance model according to the world grid map (see Fig. 10 ) and the gradient matrices along x (width) and y (height) axis of the object. 3.1.4 Creating the 3D model, distance and gradient matrices For the computation of the 3D world occupancy grid map, where the 3D object model is recorded, it was considered a limit of y voxels along the height and a maximum height of maxheight. Resulting in a resolution value equal to res mm. Therefore, considering a maximum width maxwidth and a depth of maxdepth the world grid map is a 3D matrix of ( maxwidth / res ) × ( maxheight / res ) × ( maxdepth / res ) voxels . For the experiments presented later is this article, the size of the world grid map considered was 296×500×182 with resolution (res) equal to 2.2mm. After the construction of the 3D model, in this world grid map, the distance and the gradient matrices need to be computed. These matrices have equal dimension and resolution of the world occupancy grid map. The distance matrix, represented by DistMap, has at each coordinate position the distance to the nearest ‘occupied cell’ in the occupancy grid map. For the computation of this matrix it is necessary to initialize it with zeros in the position where the cells in the 3D world occupancy grid map are occupied, and in the others with an infinite value. Then a standard procedure is applied. Fig. 11 illustrates the final result. For more information see [2]. Having now defined the distance matrix, now it is possible to compute the gradient matrix along x ( ∇ x 3 D ) and y ( ∇ y 3 D ). This corresponds to the variation of the DistMap in each world position with the variation of x and y correspondingly. The computation of ∇ x 2 D is equal to the weighted average of DistMap using a vertical Sobel filter (Eq. (3)). In the same way, ∇ y 2 D is equal to the weighted average of DistMap but this time using a horizontal Sobel filter (Eq. (4)): (3) H = [ − 1 0 + 1 − 2 0 + 2 − 1 0 + 1 ] (4) V = [ − 1 − 2 − 1 0 0 0 + 1 + 2 + 1 ] In this way and considering that for each position p, with coordinates in the world reference frame ( x W , y W , z W ) and with distance value equal to DistMap ( p ) = dist , the neighbors are defined as (5) N ( x W , y W , z W ) = [ lo mo ro lm dist rm lu mu ru ] Therefore, the ∇ x 2 D and ∇ y 2 D (for a fixed z value) are computed from (6) ∇ x 2 D ( x W , y W , z W ) = H × N ( x W , y W , z W ) 8 (7) ∇ y 2 D ( x W , y W , z W ) = V × N ( x W , y W , z W ) 8 Remember that the problem presented is in 3D space therefore: (8) ∇ x 3 D ( x W , y W , z W ) = ∇ x 2 D ( x W , y W , z W − 1 ) + ∇ x 2 D ( x W , y W , z W ) + ∇ x 2 D ( x W , y W , z W + 1 ) 3 (9) ∇ y 3 D ( x W , y W , z W ) = ∇ y 2 D ( x W , y W , z W − 1 ) + ∇ y 2 D ( x W , y W , z W ) + ∇ y 2 D ( x W , y W , z W + 1 ) 3 3.1.5 State variables Another important parallel that is necessary to be done, is the variables present in the localization and in the matching problems. Therefore, and for mobile robotic purpose, the objective is to estimate the pose (x, y positions and θ orientation) of the robot in the 3D Map. For object recognition, beyond the problem of identifying the model with the lowest matching error, this approach estimates the displacement x Match and y Match and orientation θ Match (along z axis – depth) of the unknown model when compared with the stored ones (state X Match ). The stored models are used as templates and their industrial robot trajectory is known. In other words, having two 3D models (template point cloud and an unknown one) in a world reference frame, move the x Match and y Match pose and orientation along the z axis ( θ Match ) of the unknown so it fits the template. This procedure allows the computation of the relative displacement(X Match ): (10) X Match = [ x Match y Match θ Match ] For the stored models and for which the industrial robot has the correct trajectory, the X Match state variable is equal to [ 0 , 0 , 0 ] . Now, consider a list of 3D model points (unknown Model) with points [ x i L , y i L , z i L ] , it is then possible to write (11) [ x i Lnew y i Lnew z i Lnew ] = [ x Match y Match 0 ] + [ cos θ Match − sin θ Match 0 sin θ Match cos θ Match 0 0 0 1 ] × [ x i L y i L z i L ] The 3D Perfect Match runs in the following steps: Algorithm 1 Pseudo-code for the Perfect Match algorithm. Data: X Match ← X init ∇ E ( 0 ) ← 0 for k ← 1 to iterMax do | X Lnew ← X Match + R · X L ∇ E ( k ) ← GradientMatrix ( X Lnew , X Match ) X M atch ← RPROP ( ∇ E ( k ) , ∇ E ( k − 1 ) , X Match ) end Result: X Match and E (Matching Error) 3.1.6 Phase 1 – computation of the matching error The distance matrix stored in memory is used to compute the matching error. The matching error is computed through the cost value of the list of 3D model points changed [ x i Lnew , y i Lnew , z i Lnew ] : (12) E = ∑ i = 1 N E i , E i = 1 − L c 2 L c 2 + d i 2 where d i and E i are representative of the distance matrix (DistMap), and the respective cost value for a given 3D Model point [ x i Lnew , y i Lnew , z i Lnew ] . N is the number of points in the 3D Model. L c is an adjustable parameter that controls the contribution of outliers to the error cost function. The value tuned for the presented problem is 0.5. 3.1.7 Phase 2 – optimization routine Resilient Back-Propagation (RPROP) For the computation of the RPROP the gradient of the cost function in order to the X Match needs to be computed. Therefore, it is given by (13) ∇ E ( k ) = [ ∇ E x ∇ E y ∇ E θ ] T ⟺ ∇ E ( k ) = ∑ i = 1 N 2 · L c 2 · d i ( L c 2 + d i 2 ) 2 [ ∂ d i ∂ x Match ∂ d i ∂ y Match ∂ d i ∂ θ Match ] T ⟺ ∇ E ( k ) = ∑ i = 1 N 2 · L c 2 · d i ( L c 2 + d i 2 ) 2 ∂ d i ∂ X Match with k being the current RPROP iteration. ∂ d i / ∂ X Match can be re-written as (14) ∂ d i ∂ X Match = [ ∂ d i ∂ x i L · ∂ x i L ∂ x Match , ∂ d i ∂ y i L · ∂ y i L ∂ y Match , ∂ d i ∂ x i L · ∂ x i L ∂ θ Match + ∂ d i ∂ y i L · ∂ y i L ∂ θ Match ] T Analyzing now each partial derivative (15) ∂ x i L ∂ x Match = 1 , ∂ y i L ∂ y Match = 1 (16) ∂ d i ∂ x i L = ∇ x 3 D ( x i L , y i L , z i L ) , ∂ d i ∂ y i L = ∇ y 3 D ( x i L , y i L , z i L ) (17) ∂ x i L ∂ θ Match = − sin θ Match · x i L − cos θ Match · y i L (18) ∂ y i L ∂ θ Match = cos θ Match · x i L − sin θ m · y i L Note that, the ∇ x 3 D and ∇ y 3 D are the gradient values of the computed and stored 3D matrices at the position [ x i Lnew , y i Lnew , z i Lnew ] . In this sense, the partial derivative vector ∂ d i / ∂ X Match in Eq. (13) can be re-written as (19) ∂ d i ∂ x Match = ∇ x 3 D ( x i Lnew , y i Lnew , z i Lnew ) (20) ∂ d i ∂ y Match = ∇ y 3 D ( x i Lnew , y i Lnew , z i Lnew ) (21) ∂ d i ∂ θ Match = [ ∇ x 3 D ( x i Lnew , y i Lnew , z i Lnew ) ∇ y 3 D ( x i Lnew , y i Lnew , z i Lnew ) ] T [ − sin θ Match − cos θ Match cos θ Match − sin θ Match ] [ x i Lnew y i Lnew ] In summary, the matching error between the two object models can be computed by the equation presented in 12. The cost function gradient in order to the state of the model (X Match ) is given by (22) ∇ E ( k ) = ∑ i = 1 N 2 · L c 2 · d i ( L c 2 + d i 2 ) 2 [ ∇ x 3 D ( x i Lnew , y i Lnew , z i Lnew ) ∇ y 3 D ( x i Lnew , y i Lnew , z i Lnew ) [ ∇ x 3 D ( x i Lnew , y i Lnew , z i Lnew ) ∇ y 3 D ( x i Lnew , y i Lnew , z i Lnew ) ] T [ − sin θ Match − cos θ Match cos θ Match − sin θ Match ] [ x i Lnew y i Lnew ] ] Record that the ∇ x 3 D and ∇ y 3 D (gradient matrices) were computed and stored in the beginning of the algorithm; hence they are used as look-up tables contributing to the good computational efficiency of the procedure. After computing the matching error, RPROP is applied to each model variable. The RPROP routine can be described as the following: during a limit number of iterations (iterMax), the next steps are performed to each variable that is intended to estimate, x Match , y Match and θ Match . Next are presented the steps performed by RPROP (the steps are presented for the x state variable but are identical to the other dimensions): If the actual partial derivative ∂ E ( k ) / ∂ x Match : 1. if ∂ E ( k ) / ∂ x Match − ∂ E ( k − 1 ) / ∂ x m is greater than zero, it means that the algorithm is converging in the right direction, minimization of the matching error E. In this case, the RPROP step λ x Match will be increased accelerating in this way the convergence: (23) λ x Match = λ x Match × β x Match + , β x Match + > 1 2. if ∂ E ( k ) / ∂ x Match − ∂ E ( k − 1 ) / ∂ x m is lower than zero, it means that the algorithm as passed a local minimum and the direction of convergence needs to be inverted. In this case, the RPROP step λ x Match will be decreased to decelerate the convergence: (24) λ x Match = λ x Match × β x Match − , β x Match − ∈ ] 0 , 1 [ 3. if ∂ E ( k ) / ∂ x Match is higher than zero it means that the cost function E is increasing in the positive direction of x Match . In this sense , x Match should be decreased in the amount of the step λ x Match . (25) x Match = x Match − λ x Match 4. if ∂ E ( k ) / ∂ x Match is lower than zero it means that the cost function E is decreasing in the positive direction of x Match . In this sense , x Match should be increased in the amount of the step λ x Match . (26) x Match = x Match + λ x Match In the end, a new estimation for X Match (displacement of the actual 3D Model with the template recorded in the database) is achieved, and the algorithm runs over again from Phase 1 until the number of iteration iterMax is reached. Trying in this way to minimize the matching error and at the same time to estimate the objects pose precisely. The limitation of the number of iterations in the RPROP routine makes it possible to guarantee a maximum execution time. The number of iterations should be defined after the performance of some tests for matching error evaluation. Just to illustrate the matching step performed with Perfect Match, the matching of two different point clouds of the same type of object (green model is the template, red model is the unknown” object) is presented in Fig. 12 . The left side Figure presents the initial position of the red model relatively to the green model (template). The right image shows the adjustments performed (X Match ) to the red model (unknown” object) so the matching error is minimized. 3.1.8 Perfect match training and production The Perfect Match algorithm can be divided into two steps: teaching and production phase, see Fig. 13 . The teaching phase consists in acquiring the 3D models of the new objects that are the target of robot interaction. Therefore, the object type is inserted in the production line and the developed system will capture/store its 3D model and compute the distance map and gradient maps necessary for the matching algorithm. This procedure only needs to be performed once for each type of object. In the end, a database with the taught objects and respective name is dynamically created. One more time, note that the robot has the program to correctly manipulate the object that corresponds to this 3D model template. For the production phase the already taught object type is inserted in the production line. Then, and by using the Perfect Match algorithm presented before, the model of the unknown object will be compared with the ones in the database (used as templates), retrieving its classification. This classification information is then transferred to the industrial robot and the correct program is uploaded. If the operator inserts an object that it was not yet taught two situations may occur: by evaluating the magnitude of the perfect match error it is considered that the object is not recognizable or it is misclassified. Other than object classification, the object position – X Match is also estimated. Therefore, and assuming that the industrial robot was taught to manipulate perfectly the models saved in the database, it will be possible to send to the robot the trajectory adjustments along x (width), y (height) and rotation along z (depth) axis. Note that, both the matching error and the x Match , y Match and θ Match are computed simultaneously and are dependent from each other. Using the prototype presented in Fig. 2, preliminary tests were performed and the results were published in FAIM conference 2013 [31]. Although high object recognition rate was obtained, the processing time in each object recognition procedure increases with the number of classes recorded in database. Admitting a high number of objects this approach is incompatible with industrial production processes. For this reason, a preliminary stage was introduced in the algorithm. This stage consist of extracting a subset of the 3D models database where the unknown object (object to classify) can fit better. Then, the Perfect Match is only applied to this subset, and the object class is retrieved. With this objective and after the work presented in [27], the subset will be created using the SVM algorithm. 3.2 Support Vector Machine (SVM) Support Vector Machine is a supervised learning technique that is capable of inferring the output label of new input values through a function (modeled by a set of input examples with a known label – supervised). The output of the algorithm is a mathematical function that takes two different values at all points in space, a binary classifier. There are two main SVM-based approaches to solve the multi-class problems: One-against-one and One-against-all. • One against all: This approach is used quite often for SVM training and prediction because it is only necessary to compute N models, where each model represents the classification for each class (belongings to a class or not). One of the biggest problems with this approach is the loss of significance in the dataset. A significant discrepancy between the number of observations may mislead or jeopardize the training process of the SVM models. • One against one: This method constructs one binary classifier for every pair of distinct classes, meaning that all together exist N(N +1)/2 binary classifiers. After the vote of each of the N(N +1)/2 binary classifiers, the final result can be the class with the largest number of votes. Other approach is the iterative exclusion of the class that loses; that is, all SVM models belonging to this class versus those that can be excluded from the classification process, where only the models of the winning class should be considered. Finally, there is a tree type-like approach where the classification result is the strongest class. The LibSVM was used to implement the Support Vector Machine algorithm. For the multi-classification purpose, this library used the one against all approach. For the application of this algorithm the extraction of features that characterize well each class must be performed. And how much better is the quality of these features better will be the generalization error of the SVM model. As it is possible to see in Fig. 1, the objects type are not differentiated by their color or texture, therefore using shape based features was the approach selected. 3.2.1 Feature extraction – Hu moments and Fourier Transformation (FT) Considering features extraction purpose, it was added to our previously approach presented in article [27] the FT component. The Hu׳s invariant moments were selected due to their invariance to object rotation, scale and translation. With the introduction of FT it is expected that the horizontal and vertical pattern of the objects could be analyzed. Therefore, to perform this analysis, it is necessary to make horizontal and vertical cuts in the 3D Model. For the implementation of the 1D Fourier Transformation, it was considered that the Fast Fourier Transformation algorithm, requires a low computational effort to execute the discrete Fourier Transform. However, it presents a constraint as the number of samples of the input waveform need to be an integer power of two ( 2 N ). In addition the signal leakage problem was also treated. Therefore, a window function was multiplied to the original object horizontal or vertical cut. In the end, 72 features will be extracted for each of the model. These 72 features are • width and height of the object after support removed; • area; • eccentricity; • Hu 8 moments; • 60 features corresponding to 15 cuts each made horizontally and vertically to the 3D model, with the corresponding FFT maximum magnitude and spatial frequency value; where Area, Eccentricity and 8 Hu moments are presented more in detail in [27] and FFT in [32]. 3.2.2 Feature selection Feature selection is a process often used in data mining applications. The main idea is that a subset of features is created from all available features according to certain criteria. By reducing the number of features and aspects such as redundant and irrelevant, or noisy data, it is possible to speed up the learning algorithms and improve their performance. The best features subset is the one that, with the minimum number, contributes to maximize the classification accuracy. With the elimination of the irrelevant features the probability of the learning model to over-fit is reduced. This is an important preprocessing stage where the final goal is to reduce the dimension of the data by finding a small set of important features that offer good classification performance and reduce the complexity of the classification models. There are two main approaches to reduce the number of features: feature selection and feature transformation. In feature transformation methods, the data is turned from the original high dimensional feature space to a lower dimensional one. Feature selection consists of selecting a subset of features directly from the original features. In this approach, the original meaning of the features is preserved. The selection criteria usually involve minimizing of a specific predictive error for models to fit different subsets. Feature Selection can be organized into three main families: filter, wrapper and embedded methods. The wrapper method, the method selected for this work, consists of searching for the subset of features that fit for the chosen learning algorithm better (evaluate the feature subsets based on the performance of the classifier). They can be significantly slower than filter methods if the learning algorithm takes a long time to run. Since this method evaluates the feature subset with the performance of the classifier, the resulting feature subset for a specific classifier is not necessarily the best one for all the classification methods if more used. In wrapper methods, the selection of a good subset of features could be performed in three ways: exhaustive search ( 2 D possible combinations, where D is the total number of features), random search methods and heuristic search strategies. In the exhaustive search, all combinations of the feature subsets are tested. Although this method is able to find the optimal feature subset, it is in some cases computationally impractical due to a larger search space. Random search methods add randomness to the searching process, but it is not guaranteed that the best feature subset is found. Feature selection is an NP-hard optimization problem and heuristic methods are commonly used for those kinds of problems (where a certain guideline is used for the selection). There is one problem for the heuristic approach, as there may be a high order combination of relevant feature subset and for that an optimal solution is also not guaranteed. Having this in mind, the Simulated Annealing heuristic was the selected approach. The main idea behind this heuristic is: when optimizing a very large system (a system with many degrees of freedom), instead of “always” going downhill (minimization problem) or uphill (maximization problem), try to go downhill “most of the time” [14]. This way, it is possible to accept a worst solution (the solution in this case is the feature subset) based on the temperature value (specific variable of the heuristic) or the difference for the best known solution. The main advantages of this procedure are as follows: it tries to escape a local minimum/maximum and it is simple and robust. The disadvantages have to do with parameter tuning. One quick way to decide the best feature subset is defining an Evaluation Criterion (normally the Classifier Error Rate, that is, the number of misclassified observations divided by the number of observations). 3.2.3 SVM model selection and assessment To overcome the problem of over-fitting the data in the model choice and parameters adjustments, some strategies are usually applied for selecting and assessing classifiers models. • Model Selection: estimating the performance of different models in order to choose the best one. • Model Assessment: after a final model is chosen, its generalization error is estimated on new data. To accomplish the model selection and assessment, the dataset is divided into two parts. While the training set is used to fit the models, the test set is used to assess the generalization error for the final model chosen. The main methods for model selection in classification problems are Leave-one-out, cross-validation with random sub-sampling and K-fold cross-validation. In K-fold cross-validation, the training data is divided into K subsets, example in Fig. 14 . At each time K−1 subsets are put together to form the training set, and the remaining subset is used as the validation set in order to compute the misclassification error in the training phase. However, the disadvantage of this method is that the training algorithm has to be rerun from scratch k times, which means that it takes k times and as much computational cost to make a single evaluation. 3.3 Support vector machine final architecture Once again, the classification algorithm can be divided into two phases: the training phase and the production phase. 3.3.1 Support vector machine – training phase During the training phase, all the procedures presented, from Section 3.2 until now are used. The flow chart presented in Fig. 15 summarizes the approach followed. In the beginning, 45 models (value optimized for preliminary tests) of each type of objects (8 classes) were recorded. These are the inputs of the flow chart. Features are extracted from these 360 models (previously presented in this paper). These are organized into two subsets, F _ train n × m and F _ test i × m , where n and i denote the number of models in each subset and m is the number of features extracted. There exists also a vector, s 1 × k that indicates the features that are active or not (k equal to the number of features). The simulated annealing in each iteration will make the active features vary, training the SVM model and test their performance in K-fold cross-validation. This procedure will allow the elimination of the features that do not contribute to or even jeopardize the SVM model. At the end, a subset of features will be driven that maximize the generalization error (percentage of correct classification objects). With these features it is possible to create and store an SVM model, as well as s 1 × k . 3.4 Support vector machine – production phase For the production phase it is necessary to • Extract the 3D Model of the unknown object that goes through the production line. • Feature Extraction – considering only the features selected in the simulated annealing phase (s 1×k ). • Run SVM (the classification model has already been trained). • In the presented approach, the result of the SVM is a probabilistic vector that indicates the percentage of the unknown model to fit in the classes trained in the SVM (the libsvm was the library select to implement the SVM). 3.4.1 SVM results Classification tests were performed with this procedure considering the eight models presented in Figs. 4–7. The data was divided into training F _ train n × m (n is equal to 25 samples of each class and m equal to 72 (number of features)) and testing data F _ test i × m (i is equal to 25 samples of each class and m equal to 72 (number of features)). As for the SVM internal parameters, the gamma value was defined after some tests as gamma = 1 / number of features , the kernel was defined as a polynomial with degree 4 and the C parameter defined to equal to 5. After the procedure described in Fig. 15 is completed, the following results were obtained: • Initial Generalization error: 91% with 72 features. • Final Generalization error: 97.5% with 31 features. 4 Final architecture – cascade approach Finally, this chapter presents the architecture for the classification and pose estimation purposes. This system is no more than a cascade of two well-known approaches: one in the pattern recognition field (Support Vector Machine), and another in the robot localization area (Perfect Match adapted for object recognition and pose estimation). Note that, the architecture presented in this section can be extrapolated to other scenarios, either at 3D sensing hardware or for other object recognition applications. Possibly the only modifications necessary are the features to be extracted from the object used by the SVM. At this stage, it is considered that the Perfect Match and the SVM have already been trained. The Perfect Match requires a database with an example of the 3D models, distance matrix and gradient matrices for each of the types of objects considered. The SVM only needs the trained model and the vector of active features computed by simulated annealing. Imagine now that, an object (belonging to one of the classes trained) is inserted in the production line. The object will go through the camera laser triangulation system and a 3D Model of the object is created. The following step is to extract features ( F 0 , F 1 , … , F j ) and the run of SVM classifier with the features extracted. The result of the SVM is a probabilistic vector ( P 1 , P 2 , … , P n ) that indicates the percentage of the unknown model to fit in the classes trained in the SVM. Then, the first approach followed was to select the two best candidate classes (K=2) returned by the SVM and performing the Perfect Match in those two classes. The result with the lower fit error (K FeatErrors ) would be the classification of the object with pose estimation equal to K PoseEstimations . Although this is a valid approach, much of the information returned by the SVM would be ignored. Therefore, a condition was created. If this condition is respected, SVM classification result is accepted. The condition defined is that the best object classification class returned by the SVM must have a probability higher than or equal to 70% and the second best fit class must have a probability that is lower than or equal to 6%. Then the Perfect Match is only applied for pose estimation. If the condition is not respected, the two best probabilistic values (k=2) are selected from the SVM classification test that correspond to the two best fit classes labels of the unknown object. Then, the Perfect Match is applied between the unknown model and the two best fit templates estimated by SVM. The result label class is the matching with the lowest error, see Fig. 16 . 4.1 Final results With this final architecture and using the same data used to compute the generalization error in the SVM training phase (8 classes and 200 samples), a 99.5% accuracy was achieved. As it is possible to conclude, the classification rate increased when comparing with the results using only the SVM classifier. Table 1 presents interesting results obtained by this approach, where a bad/imprecise classification of SVM is presented and the corresponding correction executed by the Perfect Match. These two cases illustrate perfectly the increase in the classification rate that the Perfect Match algorithm brings to the architecture. 4.1.1 Confusion matrix Considering the SVM model tuned for the cascade system, with a generalization error of 97.5%, the correspondent confusion matrix was evaluated (see Table 2 ). For this first confusion matrix, any concern about the probability value returned by SVM was taken. In this sense, the part with higher probability was selected as the SVM classification. Looking to the confusion matrix it is possible to see that the part type E is the one for which the SVM model has more difficult to correctly classify. However, in our cascade system the probability returned by SVM classification (SVM classification “confidence”) is considered and have high importance in the final result. Therefore, a new confusion matrix was built, but now considering the difference between the better and the second better probability. If this difference is lower than 20% it is considered that a “confusion” (misclassification) with another class has occurred. Analyzing Table 3 it is possible to verify the difficulty of SVM to correctly classify parts type E and F. Now considering the cascade system, the confusion matrix presented in Table 4 was obtained. One more time, the increase of classification is shown with the introduction of the Perfect Match for classification purposes. 4.1.2 Perfect match – precision of the 3 DoF pose estimation As explained before, Perfect Match will be used also as the objects pose estimator. This information will then be transferred to the industrial manipulator so it can correct its trajectory. Therefore, it is necessary to validate the Perfect Match pose estimation precision. To perform this study, one type of object was attached to the industrial robot (industrial robots have a movement repetitively value of 0.01mm and a volumetric accuracy of 0.5mm – high precision). Then, 5 new 3D model samples were taken: • with no pose changes with the objective to measure pose estimation repeatability. • with 5 and −5 degree angle rotation about the objects geometrical centre. • and with 0.05m translation along x and y. The results are presented in Table 5 . These results include pose precision maximum absolute error, mean absolute error and standard deviation values. From Table 5 it is possible to conclude that Perfect Match pose estimation presents consistence and robust values, considering different models of the same object type with the same position. Presenting a maximum error of 0.003m and variance of 0.0015m. Tests with 5 and -5 degree object rotation (along the z axis – depth direction) were also performed, and new 3D models were captured. Perfect Match showed one more time good performance in the pose estimation. Since a rotation around the center of the object was performed, before applying the Perfect Match both the mean value of the 3D model and template were removed from the correspondent models. Analyzing the results, it is possible to verify a light increase in x/y maximum absolute error. This may be explained for the presence of some noise on the extracted model and in the respective computation of the mean values. Finally, for the object displacement of 0.05 along x and y axis, it is possible to see that the Perfect Match algorithm one more time estimated these variation with precision. After the presentation of all results, it stays that the geometric matching algorithm detects well and with precision all the displacements verified along all considered axis. 4.2 Recognition results – cascade classifier vs viewpoint feature histogram (VFH) Considering texture-less objects and to evaluate the performance of the proposed method in comparison with state of the art solutions the algorithm proposed by Rusu et al. in [33] and available in the Point CLoud Library 3 3 site: was considered. The main principle behind PFH (Point Feature Histogram), and as a consequence in Viewpoint Feature Histogram (VFH), is to attempt to capture the sampled surface variations by taking considering all the interactions between the directions of the estimated normals. The result is a highly dimensional hyperspace that provides an informative signature for these features representation (invariant to the 6D pose). As a prerequisite for the application of Point Feature Histogram, it is necessary to estimate the surface normals in a Point Cloud considering its neighbors. Furthermore, the computed hyperspace is dependent on the quality of these surface normals. With VFH, the authors main idea is to estimate the PFH of the entire object cluster, and to compute additional statistics between the viewpoint direction and the normals estimated at each point. The authors in [33] tested this approach for 60 different objects regularly used at home, along 54,000 scenes achieving a classification rate of 98.52%. They also refer that the 6 DOF pose estimation is precise enough for mobile manipulation and grasping. Note that, this precision is directly related with the number of different viewpoint samples in database (resolution of viewpoint samples in the training phase). This approach is not always possible at industrial environments and also force the use of a large database as well as a position ground truth mechanism in the object teaching phase. Therefore, using their approach and the implementation available and developed by PCL authors, the recognition of the 200 samples from the 8 different object classes was performed. From these 200 samples, it was achieved a 97% classification ratio (considering 4 nearest neighbors). For the computation of the normal vectors (features) for each model, it was considered a radius of 0.05m with a processing time around 4–5s. For the construction of the viewpoint histogram, it was considered as the default values. Its default implementation uses 45 bins for each of the three extended PFH values, plus 45 binning for distances between each point and the centroid and 128 binning for viewpoint component. Finally, for the step of object recognition and having constructed the VFH (View Point Feature Histogram), their algorithm resorts to a fast implementation of K-Nearest Neighbor (search tree). Therefore, the achieved processing time of this step was about 1millisecond considering 4 closest neighbors. Note that, all values were computed not using any special software or hardware set-up. As it is possible to see, and first looking only for processing time, the bottleneck of their approach is the computation of the models normal vectors. The time need for the execution of this step is greater than the time needed for the approach proposed in this work (3s maximum). Furthermore, the result of classifying the same 200 samples using the cascade approach was about 99.5%. These results are better than the ones achieved with VFH (97%). 5 Conclusions and future work This article presents a novel architecture that is computationally efficient and reliable for object classification (SVM and Perfect Match). There is a clear advantage in using the Perfect Match algorithm since it does not depend on any a priori information about the object, such as CAD models or any known features. The proposed architecture was tested in an industrial environment and the classification of 200 samples from 8 different object classes was performed, resulting in an accuracy classification rate of 99.5%. It is important to highlight that the entire architecture is generic and can be applied to other industrial scenarios or applications. Since the Perfect Match is only performed in 3 dimensions, it is expected that rotation and translation of objects in the other directions will make the correct classification and pose estimation more difficult to achieve. However, during the extraction of the industrial results, no special measurements were taken when placing the objects on the support (carrier). Therefore, it is possible to conclude that the Perfect Match algorithm also performs well in cases where small displacements have occurred in the dimensions not estimated by the algorithm. For good computational speed, the 3D Models embedded in the 3D world Grid Map were down-sampled by an amount of 2 in the x and y axis (half of the points used in those directions). The number of iterations of RPROP was defined to be equal to 300. For these parameters, the maximum computational time, and considering the architecture presented in Fig. 16, was about 3s (tests performed by using an Intel Core 2.93GHz). As future work it is proposed to implement the perfect match using multi-threads or GPU to speed up the algorithm, allowing at the same time the test of perfect match in more models than the two used in this work. This increase in processing speed will also allow the test of the Perfect Match algorithm for 6 DOF. Acknowledgments The work presented in this paper, being part of the Project PRODUTECH PTI (number 13851) – New Processes and Innovative Technologies for the Production Technologies Industry, has been partly funded by the Incentive System for Technology Research and Development in Companies (SI I&DT), under the Competitive Factors Thematic Operational Programme, of the Portuguese National Strategic Reference Framework, and EU׳s European Regional Development Fund”. The authors also thanks the FCT (Fundação para a Ciência e Tecnologia) for supporting this work trough the project PTDC/EME-CRO/114595/2009 – High-level programming for industrial robotic cells: capturing human body motion. References [1] Ben-Tzvi P, Charifa S, Shick M. Extraction of 3d images using pitch-actuated 2d laser range finder for robotic vision. In: IEEE international workshop on robotic and sensors environments (ROSE). 2010. p. 1–6. [2] S. Carpin A. Birk V. Jucikas On map merging Robot Auton Syst 53 2005 1 14 [3] Cui Y, Schuon S, Chan D, Thrun S, Theobalt C. 3d shape scanning with a time-of-flight camera. In: IEEE conference on computer vision and pattern recognition (CVPR). 2010. p. 1173–80. [4] Dalal N, Triggs B. Histograms of oriented gradients for human detection. In: Schmid C, Soatto S, Tomasi C, editors. International conference on computer vision & pattern recognition, vol. 2. INRIA Rhône-Alpes, ZIRST-655, av. de l׳Europe, Montbonnot-38334. 2005. p. 886–93. [5] Dale M, Joshi M. Fingerprint matching using transform features. In: TENCON 2008 – 2008 IEEE region 10 conference; 2008. p. 1–5. [6] M. Ferreira A. Moreira P. Neto A low-cost laser scanning solution for flexible robotic cells spray coating Int J Adv Manuf Technol 58 2012 1031 1041 [7] Ferreira MA, Moreira AP, Malheiros PS, Pires N. Highly Flexible Robotized Cells: aplication to small series painting. In: Proceedings of FAIM 2010 – flexible automation and intelligent manufacturing. East Bay – Oakland, USA; July 2010. p. 244–251. [8] Jan Flusser, Tomáš Suk, Barbara Zitová, Moments and moment invariants in pattern recognition. Wiley & Sons Ltd.; 2009. 312 pp. ISBN 978-0-470-69987-4 [9] J.J. Hernandez-Lopez A.L. Quintanilla-Olvera J.L. Lopez-Ramirez F.J. Rangel-Butanda M.A. Ibarra-Manzano D.L. Almanza-Ojeda Detecting objects using color and depth segmentation with kinect sensor Proced Technol 3 2012 196 204 The 2012 Iberoamerican Conference on Electronics Engineering and Computer Science [10] S. Hinterstoisser V. Lepetit S. Ilic S. Holzer G. Bradski K. Konolige Model based training K. Lee Y. Matsushita J. Rehg Z. Hu Computer vision ACCV 2012, Lecture notes in computer science vol. 7724 2012 Springer Berlin, Heidelberg 548 562 [11] Jayawardena S, Hutter M, Brewer N. A novel illumination-invariant loss for monocular 3d pose estimation. In: DICTA׳11; 2011. p. 37–44. [12] M.A. Khabou L. Hermi M.B.H. Rhouma Shape recognition using eigenvalues of the Dirichlet Laplacian Pattern Recognit 40 2007 141 153 [13] Kingsbury N. Rotation-invariant local feature matching with complex wavelets. In: Proceedings of the European conference signal processing (EUSIPCO). 2006, p. 4–8. [14] S. Kirkpatrick C.D. Gelatt M.P. Vecchi Optimization by simulated annealing Science 220 1983 671 680 [15] Klimentjew D, Arli M, Zhang J. 3d scene reconstruction based on a moving 2d laser range finder for service-robots. In: IEEE international conference on robotics and biomimetics (ROBIO). 2009. p. 1129–34. [16] Klimentjew D, Hendrich N, Zhang J. Multi sensor fusion of camera and 3d laser range finder for object recognition. In: IEEE conference on multisensor fusion and integration for intelligent systems (MFI). 2010. p. 236–41. [17] Koker R, Oz C, Ferikoglu A. Development of a vision based object classification system for an industrial robotic manipulator. In: The 8th IEEE international conference on electronics, circuits and systems. ICECS, vol. 3, 2001. p. 1281–4. [18] Lauer M, Lange S, Riedmiller M. Calculating the perfect match: an efficient and accurate approach for robot self-localization. In: RoboCup 2005: Robot Soccer World Cup IX, Lecture notes in computer science, vol. 4020. Springer; 2005. [19] Li J, Miao Z, Liu X, Wan Y. 3D reconstruction based on stereovision and texture mapping. In: Paparoditis N, Pierrot-Deseilligny M, Mallet C, Tournaire O, editors. IAPRS, vol. XXXVIII, Part 3B – Saint-Mande, France, September 1–3, 2010. [20] A. Lzaro I. Serrano J.P. Oria C. de Miguel Identification and defect inspection with ultrasonic techniques in foundry pieces J. Mira A.P.D. Pobil Methodology and tools in knowledge-based systems, 11th international conference on industrial and engineering applications of artificial intelligence and expert systems, IEA/AIE-98, Castelln, Spain, June 1–4, 1998, Proceedings, Volume I, Lecture notes in computer science, vol. 1415 1998 Springer 822 830 [21] Y. Mingqiang K. Kidiyo R. Joseph A survey of shape feature extraction techniques Pattern Recognit 2008 43 90 [22] K. Murphy A. Torralba D. Eaton W. Freeman Object detection and localization using local and global features J. Ponce M. Hebert C. Schmid A. Zisserman Toward category-level object recognition. Lecture notes in computer science vol. 4170 2006 Springer Berlin, Heidelberg 382 400 [23] Nedevschi S, Danescu R, Frentiu D, Marita T, Oniga F, Pocol C, et al. High accuracy stereovision approach for obstacle detection on non-planar roads. In: IEEE intelligent engineering systems, INES. 2004. p. 211–6. [24] Oliveira M, Santos V. Combining view-based object recognition with template matching for the identification and tracking of fully dynamic targets. In: 7th conference on mobile robots and competitions; 2007. [25] Oliveira M, Santos V. Automatic detection of cars in real roads using haar-like features. In: 8th conference on automatic control; July 2008. [26] Paschalakis S, Lee P. Pattern recognition in grey level images using moment based invariant features. In: Seventh international conference on image processing and its applications. (Conf. Publ. No. 465), vol. 1. 1999. p. 245–9. [27] A.M. Pinto L.F. Rocha A.P. Moreira Object recognition using laser range finder and machine learning techniques Robot Comput-Integr Manuf 29 2013 12 22 [28] Pinto M, Moreira AP, Costa P, Ferreira M, Malheiros P. Robotic manipulator and artificial vision system for picking cork pieces in a conveyor belt. In: 20th international conference flexible automation and intelligent manufacturing. California State University; 2010. [29] Paulo Moreira A, Aníbal Matos. Robots localisation in indoor and service scenarios: two approaches: landmark-based and three-dimensional map-based. LAP LAMBERT Academic Publishing; February 5, 2013. ISBN: 978-3659336478. [30] D. Ramachandram M. Rajeswari Neural network-based robot visual positioning for intelligent assembly J Intell Manuf 15 2 2004 219 231 [31] Rocha LF, Ferreira M, Veiga G, Moreira AP, Santos V. Recognizing industrial manipulated parts using the perfect match algorithm. FAIM, 2013. [32] D. Rockmore The FFT: an algorithm the whole family can use Comput Sci Eng 2 2000 60 64 [33] Rusu R, Bradski G, Thibaux R, Hsu J. Fast 3d recognition and pose using the viewpoint feature histogram. In: IEEE/RSJ international conference on intelligent robots and systems (IROS).2010. p. 2155–62. [34] G. Sansoni M. Trebeschi F. Docchio State-of-the-art and applications of 3D imaging sensors in industry, cultural heritage, medicine, and criminal investigation Sensors 9 2009 568 601 [35] Wopfner M, Brich J, Hochdorfer S, Schlegel C. Mobile manipulation in service robotics: scene and object recognition with manipulator-mounted laser ranger. In: Robotics (ISR), 2010 41st international symposium on and 2010 6th German conference on robotics (ROBOTIK); 2010. p. 1–7. "
    },
    {
        "doc_title": "Navigating in populated environments by following a leader",
        "doc_scopus_id": "84889605753",
        "doc_doi": "10.1109/ROMAN.2013.6628558",
        "doc_eid": "2-s2.0-84889605753",
        "doc_date": "2013-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Quality of life",
            "Service robots"
        ],
        "doc_abstract": "Service robots have a great potential of improving human quality of life by aiding in everyday tasks. However, robots that share an environment and interact with humans still face some challenges that limits their acceptance. One of these challenges is how to move and behave among groups of people, which is a task performed seamlessly by humans and some animals. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Reproduction of human arm movements using Kinect-based motion capture data",
        "doc_scopus_id": "84898833907",
        "doc_doi": "10.1109/ROBIO.2013.6739574",
        "doc_eid": "2-s2.0-84898833907",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            }
        ],
        "doc_keywords": [
            "Artificial systems",
            "Human arm movements",
            "Human demonstrations",
            "Human motion capture",
            "Imitation learning",
            "Kinematics modeling",
            "Motion capture data",
            "Parallel mechanisms"
        ],
        "doc_abstract": "Transferring skills from humans to robots is an appealing way for teaching artificial systems to perform a variety of different tasks. In this context, imitation learning appears as an important approach for teaching robots due to the generation of human-like movements and the ease of teaching new tasks. This paper addresses the use of a Kinect-based human motion capture system and the reproduction of arm movements in an upper-body humanoid robot. The objectives are threefold: (1) to deal with the lack of a kinematics model that assure coherence in the recorded 3D human poses; (2) to explore the inclusion of a shoulder complex based on a parallel mechanism; and (3) to demonstrate and evaluate how two robot models can be used to reproduce the human demonstrations. Several experimental results are included showing the upper-limb reproduction of human arm movements. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual detection of vehicles using a bag-of-features approach",
        "doc_scopus_id": "84890891280",
        "doc_doi": "10.1109/Robotica.2013.6623539",
        "doc_eid": "2-s2.0-84890891280",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Attribute selection",
            "Bag-of-Features",
            "Speeded up robust features",
            "Vehicle classification",
            "Vehicle detection",
            "Visual detection"
        ],
        "doc_abstract": "This paper presents and evaluates the performance of a method for vehicle detection using a bag-of-features methodology. The algorithm combines Speeded Up Robust Features with a Support Vector Machine. An optimization to the bag-of-features dictionary based on a genetic algorithm for attribute selection is also described. The results obtained show that this method can successfully address the problem of vehicle classification. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Recognizing industrial manipulated parts using the perfect match algorithm",
        "doc_scopus_id": "84888417545",
        "doc_doi": "10.1007/978-3-642-39223-8_14",
        "doc_eid": "2-s2.0-84888417545",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Classification rates",
            "Industrial manufacturing",
            "Perfect matches",
            "Production line",
            "Recognition algorithm",
            "Robotic automation",
            "Technological process"
        ],
        "doc_abstract": "The objective of this work is to develop a highly robust 3D part localization and recognition algorithm. This research work is driven by the needs specified by enterprises with small production series that seek for full robotic automation in their production line, which processes a wide range of products and cannot use dedicated identification devices due to technological processes. With the correct classification of the part, the robot will be able to autonomously select the correct program to execute. For this purpose, the Perfect Match algorithm, which is known by its computational efficiency, high precision and robustness, was adapted for object recognition achieving a 99.7% of classification rate. The expected practical implication of this work is contributing to the integration of industrial robots in highly dynamic and specialized lines, reducing the companies' dependency on skilled operators. © Springer-Verlag Berlin Heidelberg 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real time egomotion of a nonholonomic vehicle using LIDAR measurements",
        "doc_scopus_id": "84870877317",
        "doc_doi": "10.1002/rob.21441",
        "doc_eid": "2-s2.0-84870877317",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational speed",
            "Ego-motion",
            "Ego-motion compensation",
            "Experimental testing",
            "Improving performance",
            "Iterative Closest Points",
            "Laser range data",
            "Laser scans",
            "Lidar measurements",
            "Motion estimates",
            "Motion estimation algorithm",
            "Non-linear motions",
            "Nonholonomic vehicles",
            "Nonholonomics",
            "Point-to-line",
            "Polar scans",
            "Real motion",
            "Real time",
            "Robot motion",
            "Scan alignment",
            "Scan matching",
            "Scan-matching technique",
            "Situation awareness",
            "Target tracking algorithm",
            "Vehicle motion"
        ],
        "doc_abstract": "This paper presents a technique to estimate in real time the egomotion of a vehicle based solely on laser range data. This technique calculates the discrepancy between closely spaced two-dimensional laser scans due to the vehicle motion using scan matching techniques. The result of the scan alignment is converted into a nonlinear motion measurement and fed into a nonholonomic extended Kalman filter model. This model better approximates the real motion of the vehicle when compared to more simplistic models, thus improving performance and immunity to outliers. The motion estimate is intended to be used for egomotion compensation in a target-tracking algorithm for situation awareness applications. In this paper, several recent scan matching algorithms were evaluated for their accuracy and computational speed: metric-based iterative closest point (MbICP), point-to-line ICP (PIICP), and polar scan matching. The proposed approach is performed in real time and provides an accurate estimate of the current robot motion. The MbICP algorithm proved to be the most advantageous scan matching algorithm, but it is still comparable to PlICP. The motion estimation algorithm is validated through experimental testing in real world conditions. © 2012 Wiley Periodicals, Inc.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Leader selection and following in dynamic environments",
        "doc_scopus_id": "84876064303",
        "doc_doi": "10.1109/ICARCV.2012.6485145",
        "doc_eid": "2-s2.0-84876064303",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Dynamic environments",
            "Leader following",
            "Planning and execution",
            "Probabilistic approaches",
            "Real environments",
            "Robotic wheelchairs",
            "Static features",
            "Static obstacles"
        ],
        "doc_abstract": "A crucial requirement for service robots is to be able to move in dynamic environments shared with humans as well as interact with them. Navigation in such environments is a challenging task, as the environment is constantly changing, future states have to be predicted and planning and execution must be carried on-line. However, even in very complex situations, humans can easily find a path that avoid both dynamic agents and static obstacles. This paper proposes a technique to take advantage of the human movement in such populated environments, using a probabilistic approach for the leader selection, according to the robot's desired destination. By choosing a leader to be followed in dynamic environments, the robot can take advantage of the paths traveled by humans or other robots, effortlessly avoiding dynamic and static features as its leader does, relieving the robot from the burden of having to generate its own path. Both the leader selection and the leader following algorithms have been tested in a real environment, with a robotic wheelchair. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mechatronic design of a new humanoid robot with hybrid parallel actuation",
        "doc_scopus_id": "84868005545",
        "doc_doi": "10.5772/51535",
        "doc_eid": "2-s2.0-84868005545",
        "doc_date": "2012-10-10",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Bio-inspired",
            "Distributed architecture",
            "Humanoid robotics",
            "Hybrid actuation",
            "Modular hardware",
            "Multimodal perception"
        ],
        "doc_abstract": "Humanoid robotics is unquestionably a challenging and long-term field of research. Of the numerous and most urgent challenges to tackle, autonomous and efficient locomotion may possibly be the most underdeveloped at present in the research community. Therefore, to pursue studies in relation to autonomy with efficient locomotion, the authors have been developing a new teen-sized humanoid platform with hybrid characteristics. The hybrid nature is clear in the mixed actuation based on common electrical motors and passive actuators attached in parallel to the motors. This paper presents the mechatronic design of the humanoid platform, focusing mainly on the mechanical structure, the design and simulation of the hybrid joints, and the different subsystems implemented. Trying to keep the appropriate human proportions and main degrees of freedom, the developed platform utilizes a distributed control architecture and a rich set of sensing capabilities, both ripe for future development and research. © 2012 Santos et al.;.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Color correction for onboard multi-camera systems using 3D Gaussian mixture models",
        "doc_scopus_id": "84865045058",
        "doc_doi": "10.1109/IVS.2012.6232141",
        "doc_eid": "2-s2.0-84865045058",
        "doc_date": "2012-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Color correction",
            "Color space",
            "Corrected image",
            "Gaussian Mixture Model",
            "Image data",
            "Local color",
            "Multicamera systems",
            "Performance comparison",
            "Processing time",
            "Single-step"
        ],
        "doc_abstract": "The current paper proposes a novel color correction approach for onboard multi-camera systems. It works by segmenting the given images into several regions. A probabilistic segmentation framework, using 3D Gaussian Mixture Models, is proposed. Regions are used to compute local color correction functions, which are then combined to obtain the final corrected image. An image data set of road scenarios is used to establish a performance comparison of the proposed method with other seven well known color correction algorithms. Results show that the proposed approach is the highest scoring color correction method. Also, the proposed single step 3D color space probabilistic segmentation reduces processing time over similar approaches. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Traffic signs detection using blob analysis and pattern recognition",
        "doc_scopus_id": "84864203331",
        "doc_doi": "10.1109/CarpathianCC.2012.6228752",
        "doc_eid": "2-s2.0-84864203331",
        "doc_date": "2012-07-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Blob analysis",
            "Traffic sign detection"
        ],
        "doc_abstract": "Computer vision has been increasingly used as a tool for orientation of machines in unknown areas. This paper deals with the challenge of perceiving traffic signs for autonomous driving. It is concerned with the competition of fully autonomous robots that takes place in a track with the shape of a traffic road. A specific set of signs has been addressed and solved with very good results. A combination of two techniques based on blob analysis and pattern recognition has been used and selected results of the experiments are presented along with the description of the main algorithms. Introduction also contains a brief research of work that has been already published in this area of engineering. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Color correction using 3D Gaussian mixture models",
        "doc_scopus_id": "84864127490",
        "doc_doi": "10.1007/978-3-642-31295-3_12",
        "doc_eid": "2-s2.0-84864127490",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color correction",
            "Color space",
            "Corrected image",
            "Gaussian Mixture Model",
            "Image mosaicing",
            "Large datasets",
            "Local color",
            "Processing time",
            "Single-step"
        ],
        "doc_abstract": "The current paper proposes a novel color correction approach based on a probabilistic segmentation framework by using 3D Gaussian Mixture Models. Regions are used to compute local color correction functions, which are then combined to obtain the final corrected image. The proposed approach is evaluated using both a recently published metric and two large data sets composed of seventy images. The evaluation is performed by comparing our algorithm with eight well known color correction algorithms. Results show that the proposed approach is the highest scoring color correction method. Also, the proposed single step 3D color space probabilistic segmentation reduces processing time over similar approaches. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D-2D laser range finder calibration using a conic based geometry shape",
        "doc_scopus_id": "84864114823",
        "doc_doi": "10.1007/978-3-642-31295-3_37",
        "doc_eid": "2-s2.0-84864114823",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "2D laser scanners",
            "2d lasers",
            "3D point cloud",
            "Calibration method",
            "Inertial sensor",
            "Laser calibration",
            "Laser range finders",
            "Object geometries",
            "Sensor data",
            "Sensor fusion",
            "Stereo cameras"
        ],
        "doc_abstract": "The AtlasCar is a prototype that is being developed at the University of Aveiro to research advanced driver assistance systems. The car is equipped with several sensors: 3D and 2D laser scanners, a stereo camera, inertial sensors and GPS. The combination of all these sensor data in useful representations is essential. Therefore, calibration is one of the first problems to tackle. This paper focuses on 3D/2D laser calibration. The proposed method uses a 3D Laser Range Finder (LRF) to produce a reference 3D point cloud containing a known calibration object. Manual input from the user and knowledge of the object geometry are used to register the 3D point cloud with the 2D Lasers. Experimental results with simulated and real data demonstrate the effectiveness of the proposed calibration method. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised local color correction for coarsely registered images",
        "doc_scopus_id": "80052901688",
        "doc_doi": "10.1109/CVPR.2011.5995658",
        "doc_eid": "2-s2.0-80052901688",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Color segmentation",
            "Local color",
            "Mean shift",
            "Recent performance",
            "Registered images"
        ],
        "doc_abstract": "The current paper proposes a new parametric local color correction technique. Initially, several color transfer functions are computed from the output of the mean shift color segmentation algorithm. Secondly, color influence maps are calculated. Finally, the contribution of every color transfer function is merged using the weights from the color influence maps. The proposed approach is compared with both global and local color correction approaches. Results show that our method outperforms the technique ranked first in a recent performance evaluation on this topic. Moreover, the proposed approach is computed in about one tenth of the time. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ATLASCAR - Technologies for a computer assisted driving system on board a common automobile",
        "doc_scopus_id": "78650471844",
        "doc_doi": "10.1109/ITSC.2010.5625031",
        "doc_eid": "2-s2.0-78650471844",
        "doc_date": "2010-12-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Active safety",
            "Computer assisted",
            "Data gathering",
            "Driver assistance",
            "Driving systems",
            "Intelligent vehicles",
            "Vehicle autonomy"
        ],
        "doc_abstract": "The future of intelligent vehicles will rely on robust information to allow the proper feedback to the vehicle itself, to issue several kinds of active safety, but before all, to generate information for the driver by calling his or her attention to potential instantaneous or mid-term risks associated with the driving. Before true vehicle autonomy, safety and driver assistance are a priority. Sophisticated sensorial and perceptive mechanisms must be made available for, in a first instance, assisting the driver and, on a latter phase, participate in better autonomy. These mechanisms rely on sensors and algorithms that are mostly available nowadays, but many of them are still unsuited for critical situations. This paper presents a project where engineering and scientific solutions have been devised to settle a full featured real scale platform for the next generation of ITS vehicles that are concerned with the immediate issues of navigation and challenges on the road. The car is now ready and running, and the data gathering has just begun. ©2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Development of a hybrid humanoid platform and incorporation of the passive actuators",
        "doc_scopus_id": "79952935767",
        "doc_doi": "10.1109/ROBIO.2010.5723408",
        "doc_eid": "2-s2.0-79952935767",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Advanced control",
            "Commercial availability",
            "Design and Development",
            "Elastic element",
            "Initial design",
            "Mechanical response",
            "Mechanical stiffness",
            "Motion tasks",
            "Passive actuators",
            "Power efficiency",
            "Saving energy",
            "Simulation environment",
            "Simulation result"
        ],
        "doc_abstract": "This paper describes the design and development of a new hybrid humanoid platform conceived to use both active and passive actuators. Power efficiency and mechanical response capability of the robot were the main concerns driving this development. Maintaining the use of off-the-shelf RC servomotors, due to their limited cost and commercial availability, the platform was nonetheless custom-designed for lightness, mechanical stiffness and prone to vast sensorial enrichment for future advanced control. Low-cost actuators may degrade and perform poorly and erroneously in demanding conditions; therefore, one major inspiration for this work relies on the potential energy storage mechanism, using elastic elements to overcome the motors limitation, avoiding their operation near the limits, while saving energy and wearing, and also obtain faster responses of the overall platform in various motion schemes and gaits. A standard simulation environment allows the initial design and future tuning of the passive actuators for several joints in motion tasks. The early simulation results show that the elastic elements approach indeed eases the actuators tasks and is a must in the future development of the new platform now presented. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Development of an integrated electrical stimulation system with feedback for physical rehabilitation",
        "doc_scopus_id": "77956318478",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956318478",
        "doc_date": "2010-09-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Electrical stimulation system",
            "Electrical stimulations",
            "Electrostimulation",
            "Feed-back sensors",
            "Feedback signal",
            "Integrating information",
            "Motor performance",
            "Patient safety",
            "Physical rehabilitation",
            "Portable device",
            "Real-time feedback",
            "Rehabilitation",
            "Signal generation",
            "Spinal cord injury",
            "Stroke",
            "Therapeutic method",
            "Wave forms",
            "Waveform generation"
        ],
        "doc_abstract": "In physical rehabilitation, electrical stimulation is widely used as a therapeutic method. However, as it is not common to find portable devices, capable of integrating information from different sensors, and also with flexibility in signal generation and triggering. This paper presents an integrated electrostimulation system that encompasses all those facilities. The system integrates feedback signals coming from an accelerometer and is capable of adapting electrostimulation depending on motor performance. The device uses a microcontroller for the waveform generation, and allows controlled waveforms to be produced in response to signals read from feedback sensors. Besides this high versatility, the principle of the power generation employed by the device and additional hardware circuitry also provides mechanisms to ensure patient safety in the unlikely cases of malfunction of the microcontroller. Here we also present an example of application of the device that uses real time feedback information to control electrical stimulation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual guidance of an autonomous robot using machine learning",
        "doc_scopus_id": "80052001723",
        "doc_doi": "10.3182/20100906-3-it-2019.00012",
        "doc_eid": "2-s2.0-80052001723",
        "doc_date": "2010-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Autonomous Mobile Robot",
            "Fast computation",
            "Human drivers",
            "Human like",
            "Real-time navigation",
            "Visual guidance"
        ],
        "doc_abstract": "The aim of this work is to create a method to compute the steer direction of an autonomous robot, moving in a road-like environment. It uses artificial neural networks to learn behaviours based on examples from human drivers, replicating and sometimes even improving human-like behaviours. Artificial neural networks perform very fast computations, which make them well suited for real-time navigation. They also have the possibility to perceive information that was undetected by humans and therefore could not be coded in classical programs, improving the overall performance for the steering of an autonomous mobile robot.",
        "available": true,
        "clean_text": "serial JL 314898 291210 291718 291882 291883 31 IFAC Proceedings Volumes IFACPROCEEDINGSVOLUMES 2016-04-16 2016-04-21 2016-04-16 2016-04-21 2011-03-02 2016-04-21T17:11:09 S1474-6670(16)35032-7 S1474667016350327 10.3182/20100906-3-IT-2019.00012 S350 S350.2 HEAD-AND-TAIL 2022-06-09T16:35:31.034286Z 0 0 20100101 20101231 2010 2016-04-16T11:56:25.660076Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate isbn isbns isbnnorm isbnsnorm issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1474-6670 14746670 978-3-902661-87-6 9783902661876 false 43 43 16 16 Volume 43, Issue 16 12 55 60 55 60 2010 2010 2010-01-01 2010-12-31 2010 7th IFAC Symposium on Intelligent Autonomous Vehicles article fla Copyright © 2010 IFAC. Published by Elsevier Ltd. All rights reserved. VISUALGUIDANCEAUTONOMOUSROBOTUSINGMACHINELEARNING STEIN P ALPAYDIN 2004 E INTRODUCTIONMACHINELEARNING DARTER 2005 374 379 M DEMCENKO 2008 1181 1186 A KUNZLE P MONTEMERLO 2008 569 597 M MONTEMERLO 2003 2436 2441 M OLIVEIRA 2007 18 25 M POMERLEAU 1991 88 97 D POMERLEAU 1995 161 181 D HANDBOOKBRAINTHEORYNEURALNETWORKS NEURALNETWORKVISIONFORROBOTDRIVING RIEDMILLER 1993 586 591 M RIEDMILLER 2007 645 650 M PROCEEDINGS2007FRONTIERSINCONVERGENCEBIOSCIENCEINFORMATIONTECHNOLOGIES LEARNINGDRIVEAREALCARIN20MINUTES SARLE W SIMMONS 1998 1931 1937 R STEINX2010X55 STEINX2010X55X60 STEINX2010X55XP STEINX2010X55X60XP item S1474-6670(16)35032-7 S1474667016350327 10.3182/20100906-3-IT-2019.00012 314898 2016-04-21T12:34:18.54986-04:00 2010-01-01 2010-12-31 true 1297147 MAIN 6 66800 849 656 IMAGE-WEB-PDF 1 Visual Guidance of an Autonomous Robot Using Machine Learning Proc´pio Silveira Stein V´ o itor Santos Universidade de Aveiro, Portugal, (e-mail: procopio,vitor@ua.pt). Abstract: The aim of this work is to create a method to compute the steer direction of an autonomous robot, moving in a road-like environment. It uses artificial neural networks to learn behaviours based on examples from human drivers, replicating and sometimes even improving human-like behaviours. Artificial neural networks perform very fast computations, which make them well suited for real-time navigation. They also have the possibility to perceive information that was undetected by humans and therefore could not be coded in classical programs, improving the overall performance for the steering of an autonomous mobile robot. Keywords: neural networks, navigation systems, machine learning, autonomous mobile robots, robot vision 1. INTRODUCTION The objective of this research is to replicate human driving behaviours using machine learning techniques. This proposal is based in the belief that algorithms can automatically improve their performances after extracting relevant information from examples that are not evident to humans programmers, see Alpaydin (2004). So, in this sense, it is expected that a behavioral, non-parametric, approach may overcome the limitations of a deterministic steer direction computation algorithm. In the navigation area, it is hard for the programmer to code the expertise of driving a car into a robot program. One solution would be to simplify the steer direction computation in geometric terms, based on the road geometry and in a series of logic rules, but this inevitably leads to loss of information, and this logic conditions do not reproduce human behavior at all. With the proposed algorithms it is not the programmer, but the algorithm itself that will infer what is relevant, learning what should be the steer direction to stay inside the road, based on human examples. However, the human factor and therefore bias, are always present, since it is the programmer who ultimately will decide how information will be represented, what will be the learning technique and what type of computation framework will be used. The tool chosen to learn from human examples, replicating behaviours, was the Artificial Neural Networks (ANN). The ANN are very fast and computationally efficient, two important characteristics for a vehicle navigation system. They also have the capacity to adapt and generalize to new situations, making them an interesting choice for realworld navigation. The choice of an ANN as the tool for a program capable of computing a steering direction to keep the robot Partially sponsored by Funda¸~o para a Ci^ncia e Tecnologia ca e FCT, Portugal inside the road, was mostly inspired by Pomerleau work with ALVINN (Autonomous Land Vehicle in a Neural Network), see Pomerleau (1991) and Pomerleau (1995). In that work, the gray-scale levels of a 30 × 32 image was used as the input of an ANN that calculated a steer angle to keep an autonomous vehicle inside the road limits. To improve training, simulated images and steer angles were generated based on the original road image and steering, allowing ALVINN to quickly learn how to navigate in new roads. In Kunzle (2003), the author simulates a vehicle that uses neural networks to compute an acceleration in order to avoid obstacles. All the tests are simulated, with the distance from the vehicle to the obstacles always known and used as the input of the network. A different approach, from Darter and Gordon (2005), proposes the use of groups of ANNs to calculate the future lateral position of a snow removal vehicle, based on readings of several different sensors. However, no real implementation is shown in this work, and the data used for training and validating the ANN is generated and not acquired in the real world. In a more recent work, Demcenko et al. (2008), an ANN is trained to replicate the form a human steers in a country-side road. Real road curvature information is used, collected indirectly with a gyroscope. The ANN results are compared with collected data, but the predicted steer is not used to actually command the vehicle, so the real behavior of such net in autonomous vehicle guidance is unknown. A remarkable work is conducted by Riedmiller et al. (2007), where the authors use Neural Fitted Q Iteration to teach a real car to steer from scratch, without previous models of the vehicle. Only real data is used, and the goal is to keep the cross-track-error under an acceptable value. Fig. 1. Road limits extraction to feed the ANN Fig. 3. Typical network input vector Fig. 2. The structure of a the ANNs The system rely on a group of sensors capable of giving both absolute and relative position of the vehicle. The next chapter will present the learning framework created. It will give a description of the structure of the created ANN, detailing how the information is represented, the created methods for data acquisition and processing, followed by the description of the training process for the ANNs. Chapter 3 will describe the tests carried out with different configuration parameters to assess the impact in the overall performance of the robot. An analysis of the results is done in the end of this chapter, followed by the conclusions. 2. ARTIFICIAL NEURAL NETWORKS STRUCTURE In the environment used for this research, the road was marked as white stripes over a dark surface, without texture distinction between inside and outside road regions, rendering Pomerleau technique inadequate. Differently from ALVINN, the proposed ANN approach will focus only in the computation of the steering direction. The road detection will be performed by an specific algorithm, detailed in Oliveira and Santos (2007). Figure 1 illustrates the points that represent the road limits, obtained using the aforementioned technique. In the current work, no simulations were performed and no training data was artificially created. The system used real sensors, the cameras, and provided output to real actuators, the motors. The analysis of the performance of this system will be based on results obtained in tests conducted in a real (but small scale) test road. The basic network structure used is a feed-forward multilayer perceptron, and the activation function of all the neurons is the sigmoid function (as in ALVINN). Several different network architectures were created, and then trained with different algorithms. Only the networks that had the best performance will be discussed here, which have 6 inputs, one hidden layer with 2 neurons, and one neuron on the output layer, as shown in Figure 2. 2.1 Input Data Representation The input data for the network is a vector with values that represent the position of road boundaries at constant Fig. 4. Robot steer direction reference frame heights, measured from the image center, in pixels. With this reference frame, boundary points equally spaced from the center of the image will have similar displacement values, as can be seen in Figure 3, where a typical input structure is presented. 2.2 Output Data Representation The output data is the steer angle of the robot, measured in degrees with two decimal places. This value is measured over a reference frame that has the positive x axis pointing in the direction the robot is facing, and the positive y axis pointing toward the left of the robot. The angles are measured counter-clockwise, with angles growing from positive x to positive y, as shown in Figure 4. 2.3 Equipment and System Setup The test platform used is a small scale, non-holonomic, autonomous robot, built in the Automation and Robotics Laboratory of University of Aveiro, called Atlas 2009. The robot structure tries to replicate, in a simple form, the mechanisms used in real cars for steering and moving, allowing the development of techniques that can be applied in real cars. The software used is based on CARM EN , the Carnegie Mellon Robot Navigation Toolkit Montemerlo et al. (2003). This toolkit relies on IPC (see Simmons and Apfelbaum (1998)) for intercommunication, which has been used in Carnegie Mellon University robots and also in the winner system of the DARPA Grand Challenge, Stanley, from Stanford University, see Montemerlo et al. (2008). Using this architecture, each module is a stand-alone program responsible for specific tasks, such as: image acquisition, visual feature extraction, navigation computation, hardware interface, and so on. This results in a very flexible research framework, where new modules can be easily integrated. The road images are obtained by overlapping images acquired with two cameras. The overlapped road image size is 630 × 240 pixels. The image processing, the network Fig. 6. Normal drive (a) and correction maneuvers (b) Fig. 5. Small scale road used for data acquisition training and network implementation are performed using the OpenCV library with minor modifications in the source files, so the programs could output relevant information for further analysis. This system will have two distinct working modes, one for network training and another for tests. In the training mode, data will be acquired for off-line processing and network training. While in the test mode, a trained network is actually used for the computation of the steering direction to keep the robot inside the small scale test road. 2.4 Data Acquisition The data acquisition involves the creation of a data bank, that will provide input/target pairs to be used for training an ANN. As the objective of this work is to create a program that learns how to drive by real human example, all the data used for training the network is acquired while a person is actually driving the robot, with a gamepad, along the road, shown in Figure 5. The direction commanded by the driver is the target for a given group of boundary point, which are the inputs of the network. In this research no training data is artificially created. The images acquired by the robot cameras can be considered the \"raw\" data of this process. Therefore, the creation of an image database gives more flexibility to the research, as different forms of data representation or number of boundary points can be experimented using the same images. Besides that, the image data bank can also be used for different tests in the future, using different methods to extract points that represent the road. The chosen option to store sensors and command data was by means of images with metadata, namely using the Extensible Metadata Platform (XMP) standard to embed information in an image. Images from the cameras were logged with embedded information of the time it was taken, the steer direction and the robot speed. Acquisition Process The acquisition of data is performed while someone is driving the robot, using a gamepad. There are two main types of situations where data was acquired. The normal drive are the situations where the robot is moving approximately in the middle of the road, this will be the behavior that the ANNs are expected to learn and replicate. Another situations are the correction maneuvers: in this case the robot will be put in demanding positions and maneuvered toward desired positions, so the algorithm can learn how to escape from undesired situations. Both situations are illustrated in Figure 6. Human Machine Interface In this work the traditional interfaces with the logger program (keyboard and screen) were enhanced with a gamepad and a sound interface. That enables remote log start, pause and stop. The sound interface gives an audible feedback, for the driver, of the chosen commands. 2.5 Training Training was done off-line and will use both the standard Backpropagation (BACKPROP) and the Resilient Backpropagation (RPROP) training algorithms, see Riedmiller and Braun (1993). BACKPROP is the most common training method for ANNs, but it requires empirical tunning of the training parameters. The best results were attained using a momentum of 0.5 and a learning rate of 0.01. The BACKPROP training algorithm used in this research performs incremental learning. The second algorithm, RPROP, uses an adaptive learning rule to change training parameters while the training is occurring. One of the greatest advantages of this method, is that, due to its adaptive nature, the tedious task of trying different combinations of momentum and gradient is unnecessary, Sarle (2002). Different from the previous training method, the resilient propagation uses batch learning. For both algorithms, the performance of the training is measured as the mean squared error (MSE) of the network outputs in relation to the training targets. Two criteria may finish the training process: the maximum number of epochs, set at 5000; and the MSE goal, defined as an error of 2% in relation to the targets, or 20.00 × 10-3 . To generate the training data, a sequence of images are read from the image database; each image loaded has its metadata read to obtain the steer direction at the moment it was acquired. The road limits are then extracted and the training information containing a target (the steer direction read from metadata) and several inputs (extracted road boundary points from the images) are stored into a text file. The training data set will consist of two complete laps, and several correction maneuvers in different parts of the road, in a total of 1598 input/target pairs. One of the issues in this work is the impossibility to validate a network training without actually testing it with the robot. A measure of how close the computed steer direction is from human steer direction at one moment is not a very precise form of assessing the trained ANN performance because, differently from real tests, the computed steer direction will not affect the acquisition of the next image. Figure 7 illustrates how is the steer computation process during real navigation with ANNs. It can be represented Fig. 7. Steer direction control loop, using visual feedback Fig. 9. Road division labels for quantitative analysis constant reference value. In control systems, it is often desired that the system output (steer angle in this case) follows a reference without oscillation and small errors. In these tests, due to mechanical and electronic imperfections, there is not a clear reference for each section of the road. So, in order to facilitate the tests analysis, the graphs will be divided into regions, and each region will have a \"suggested\" steer angle. Three regions can be defined, and are represented in the graphs ahead by two vertical lines. The middle region, where the steer angle is about 11 degrees is the curve region. It is the curve region that defines the region divisions, with the straight road region before it, and the road inflection region after it. These division are illustrated in Figure 9. In the steer graphs, three horizontal lines will represent the \"suggested\" steer angle for the three road divisions. It is important to emphasize that this marks are only an estimation, and are used only as visual aids for comparison and analysis of the tests allowing an insight in the behavior of the trained ANNs. For this evaluation, each of the trained ANNs is tested using the default test configuration: speed of 1m/s, program frequency of 7Hz and right lane start position. After the initial tests and analysis of the trained networks, new tests are conducted but with different parameters, as speed, program frequency and robot start positions. The objective is to study how these parameters affect the overall performance of the robot over a lap. 3.1 Standard Tests Results The steer graph of the network trained with the BACKPROP algorithm (Figure 10) shows a very smooth driving, with minor oscillations in the first region, and no oscillations at all during the curve and inflection road regions. The performance of the ANN trained with the RPROP algorithm, illustrated in Figure 11, shows a smooth overall performance, with little oscillation. The main difference when compared with its BACKPROP counterpart is a small oscillation during the curve region and a not so precise turning in the road inflection region. 3.2 Speed Influence Results In these tests, the robot speed was increased to 1.5m/s, to study the influence of speed in the steering. For the BACKPROP trained network, shown in Figure 12, there Fig. 8. Detail of steer angle commanded by a human driver during a constant curve as a closed control loop with visual feedback, where the classical control reference does not appear, as it can be considered inherent to the ANN behavior in this model. To evaluate the ANN performance without real tests, a model that substitutes the cameras position and orientation block would be necessary. However, the creation of such model is not the focus of this research. Another possible method for the training evaluation could be a geometrical ideal steer angle, that would be constant along curves and straight lines. This is also not feasible as the calibration of the robot steering was not very precise and worn mechanical parts deteriorated even more a correct steer angle measurement for the robot. For example, while driving in a straight line the ideal steer angle would be 0 degree, however in real tests it is necessary to command a steer angle of about 3 degrees for the robot to keep a straight direction. Besides that, the human driver seems to have an unstable and coarse steering behavior while using the gamepad, making occasional corrections instead of maintaining a constant correction angle. This behavior can be seen in Figure 8, where the human driver has almost discrete-like levels for steering during a constant curve. 3. SYSTEM TESTS In the system tests, a file containing information about a neural network is loaded by the ANN program. This module receives the road boundaries points that are extracted from the road image, then compute a steer direction and send it as a message to the module responsible for the low-level interface with the robot. The performance of the ANNs will be evaluated based on the steer smoothness. This criteria was chosen due to the fact that when the road has a smooth or continuous shape, be it a straight line or a semicircle, a good driver should maintain an almost constant steer angle (ideally 0, in straight roads, but mechanical and electronic imperfections can offset this value). This can also be understood as a reference for a feedback control system, where a constant road shape would be a Fig. 10. Standard test results for BACKPROP network Fig. 14. Initial position influence results for BACKPROP network Fig. 11. Standard test results for RPROP network Fig. 15. Initial position influence results for RPROP network expected that the initial position would work as an offset of this usual position and, therefore, the first maneuver of the robot would be an attempt to reach such position in the road. For this test, both experimented networks have similar results, shown in Figures 14 and 15. When the robot is positioned in the right lane, the first maneuver is a left turn, to approach the robot to the the right-center of the road. In the middle road and left lane positions, the initial maneuver is a right turn, for the same reason as before. Also, in both cases the steering oscillates during the straight road region and when reaching the curve region, there is no distinguishable difference in the steering direction of the three different initial positions. 3.4 Image Frame Rate Influence Results The influence of the image frame rate in the steering behavior of the robot is of great interest. This may serve as an indication of how well the trained networks can correct from undesired positions, when there is a longer time between commands to the steering motor. To test the influence of this parameter, the frequency of the motion planner algorithm was forced to only 2Hz. The network trained with the BACKPROP algorithm has a very good result (Figure 16), with a behavior very similar to the tests with the default frequency. There is a minor oscillation in the first region, followed by a very smooth curve and a smooth change of direction during the road inflection region. However, the network trained with the RPROP algorithm suffered more with this limited frequency. Results (Figure 17) show a much higher oscillation during the first region and also during the curve. This network seems to have a more reactive behavior, which requires it to perform strong corrections from older decision, which may lead to the presented behavior when using a lower frequency, as it is in this case. Fig. 12. Speed influence results for BACKPROP network Fig. 13. Speed influence results for RPROP network are oscillations with a low frequency and medium amplitude during the first region. In the curve, the oscillations almost disappear, and are not present at all in the final part. The performance graph of the resilient propagation case is shown in Figure 13. In this case, there is a high amplitude, but low frequency oscillation is present in the first region. During this part of the test, the oscillation even seems to be getting higher, which is interrupted by the beginning of the curve region. Oscillations are also present in the curve region, but with a smaller amplitude. This network was clearly more affected by the speed increase than its BACKPROP counterpart. 3.3 Initial Position Influence Results Experimental tests showed that the initial positions had a strong effect in the robot behavior. This characteristic served as inspiration for the current test, where the robot was positioned at three different parts of the road at the begin of each test. The initial positions were at the right lane, at the middle of the road and at the left lane. Tests already showed that the created ANNs tend to maintain a (mostly) constant position during straight regions of the road. So it was without compromising the overall robot performance, and providing better inputs for the network. ACKNOWLEDGEMENTS This work would not be possible without the help of Miguel de Oliveira, R´mi Sabino, Jorge Almeida, David Gameiro e and former members of the Atlas Project. Fig. 16. Frequency influence results for BACKPROP network REFERENCES Alpaydin, E. (2004). Introduction to Machine Learning. The MIT Press, Cambridge, MA. Darter, M. and Gordon, V. (2005). Vehicle steering control using modular neural networks. In Proceedings of International Conference on Information Reuse and Integration, IRI-2005, 374­379. Demcenko, A., Tamosiunaite, M., Vidugiriene, A., and Saudargiene, A. (2008). Vehicle's steering signal predictions using neural networks. In Proceedings of IEEE Intelligent Vehicles Symposium, 1181­1186. Kunzle, P. (2003). Gamedev.net - vehicle control with neural networks. URL reference/articles/article1988.asp. Retrieved May, 2009. Montemerlo, M., Becker, J., Bhat, S., Dahlkamp, H., Dolgov, D., Ettinger, S., Haehnel, D., Hilden, T., Hoffmann, G., Huhnke, B., Johnston, D., Klumpp, S., Langer, D., Levandowski, A., Levinson, J., Marcil, J., Orenstein, D., Paefgen, J., Penny, I., Petrovskaya, A., Pflueger, M., Stanek, G., Stavens, D., Vogt, A., and Thrun, S. (2008). Junior: The stanford entry in the urban challenge. Journal of Field Robotics, 25(9), 569­597. Montemerlo, M., Roy, N., and Thrun, S. (2003). Perspectives on standardization in mobile robot programming: The carnegie mellon navigation (CARMEN) toolkit. In Proceedings of the Conference on Intelligent Robots and Systems (IROS), volume 3, 2436­2441. Oliveira, M. and Santos, V. (2007). A vision-based solution for the navigation of a mobile robot in a road-like environment. Robotica, (69), 18­25. Pomerleau, D. (1991). Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1), 88­97. Pomerleau, D. (1995). Neural network vision for robot driving. In M.A. Arbib (ed.), The Handbook of Brain Theory and Neural Networks, 161­181. The MIT Press, Cambridge, MA, 2nd edition. Riedmiller, M. and Braun, H. (1993). A direct adaptive method for faster backpropagation learning: the RPROP algorithm. In Proceedings of IEEE International Conference on Neural Networks, 586­591. Riedmiller, M., Montemerlo, M., and Dahlkamp, H. (2007). Learning to drive a real car in 20 minutes. In Proceedings of the 2007 Frontiers in the Convergence of Bioscience and Information Technologies, 645­650. IEEE Computer Society. Sarle, W.S. (2002). Neural network faq, part 2 of 7: Learning. URL FAQ2.html. Retrieved May, 2009. Simmons, R. and Apfelbaum, D. (1998). A task description language for robot control. In Proceedings of the Conference on Intelligent Robots and Systems (IROS), volume 3, 1931­1937. Fig. 17. Frequency influence results for RPROP network 4. CONCLUSION This research showed the implementation of a system where humans could teach an autonomous robot by examples. A system that uses this concept was implemented using artificial neural networks. To validate the created networks, real tests were performed and the robot successfully completed several laps in the test circuit, in an inverse circuit and in different roads, showing good capacities for recovery and for generalization with relatively small training data sets. Regarding the training algorithms, the BACKPROP technique had better overall results, but the RPROP training was much faster and it also did not required empirical adjustments of the training parameters. Besides that, not enough tests and training were performed to determine which method is the most efficient for the presented problem. The successful implementation of a learning framework together with the modular architecture in the robot, allows new modules that use machine learning techniques to be integrated with previous classical modules, creating very complex and robust robot behaviours. Another interesting development would be to use artificial neural network to measure humans performance while driving, giving suggestions and possible scores that could be used for granting a drivers license, for example. The speed can also be used as an output for the neural network in an attempt of further incorporating humanlike behaviours as slowing down on curves and speeding up in straight road. Among the several tests performed, the program frequency test results are the most interesting. They show a high robustness of the used ANNs, that were capable of completing a lap, even with a frequency three times slower than the default one. This represent a good recover capacity and also indicates that the frequency is important, but not crucial for a good performance of the robot. The possibility to navigate with slower frequencies also indicates that the feature extractor programs may be further enhanced to perform better but slower computations et/ reference/articles/article1988.asp. Retrieved May, 2009. Montemerlo, M., Becker, J., Bhat, S., Dahlkamp, H., Dolgov, D., Ettinger, S., Haehnel, D., Hilden, T., Hoffmann, G., Huhnke, B., Johnston, D., Klumpp, S., Langer, D., Levandowski, A., Levinson, J., Marcil, J., Orenstein, D., Paefgen, J., Penny, I., Petrovskaya, A., Pflueger, M., Stanek, G., Stavens, D., Vogt, A., and Thrun, S. (2008). Junior: The stanford entry in the urban challenge. Journal of Field Robotics, 25(9), 569­597. Montemerlo, M., Roy, N., and Thrun, S. (2003). Perspectives on standardization in mobile robot programming: The carnegie mellon navigation (CARMEN) toolkit. In Proceedings of the Conference on Intelligent Robots and Systems (IROS), volume 3, 2436­2441. Oliveira, M. and Santos, V. (2007). A vision-based solution for the navigation of a mobile robot in a road-like environment. Robotica, (69), 18­25. Pomerleau, D. (1991). Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1), 88­97. Pomerleau, D. (1995). Neural network vision for robot driving. In M.A. Arbib (ed.), The Handbook of Brain Theory and Neural Networks, 161­181. The MIT Press, Cambridge, MA, 2nd edition. Riedmiller, M. and Braun, H. (1993). A direct adaptive method for faster backpropagation learning: the RPROP algorithm. In Proceedings of IEEE International Conference on Neu IPV 35032 S1474-6670(16)35032-7 10.3182/20100906-3-IT-2019.00012 IFAC Partially sponsored by FundaçáTo para a Ciância e Tecnologia - FCT, Portugal Visual Guidance of an Autonomous Robot Using Machine Learning Procópio Silveira Stein * Vítor Santos * * Universidade de Aveiro, Portugal Universidade de Aveiro Portugal The aim of this work is to create a method to compute the steer direction of an autonomous robot, moving in a road-like environment. It uses artificial neural networks to learn behaviours based on examples from human drivers, replicating and sometimes even improving human-like behaviours. Artificial neural networks perform very fast computations, which make them well suited for real-time navigation. They also have the possibility to perceive information that was undetected by humans and therefore could not be coded in classical programs, improving the overall performance for the steering of an autonomous mobile robot. Keywords neural networks navigation systems machine learning autonomous mobile robots robot vision References Alpaydin, 2004 E. Alpaydin Introduction to Machine Learning 2004 The MIT Press Cambridge, MA Darter and Gordon, 2005 M. Darter V. Gordon Vehicle steering control using modular neural networks Proceedings of International Conference on Information Reuse and Integration, IRI-2005 2005 374 379 Demcenko et al., 2008 A. Demcenko M. Tamosiunaite A. Vidugiriene A. Saudargiene Vehicle's steering signal predictions using neural networks Proceedings of IEEE Intelligent Vehicles Symposium 2008 1181 1186 Kunzle, 2003 P. Kunzle Gamedev. net - vehicle control with neural networks URL 2003 Retrieved May, 2009. Montemerlo et al., 2008 M. Montemerlo J. Becker S. Bhat H. Dahlkamp D. Dolgov S. Ettinger D. Haehnel T. Hilden G. Hoffmann B. Huhnke D. Johnston S. Klumpp D. Langer A. Levandowski J. Levinson J. Marcil D. Orenstein J. Paefgen I. Penny A. Petrovskaya M. Pflueger G. Stanek D. Stavens A. Vogt S. Thrun Junior: The stanford entry in the urban challenge Journal of Field Robotics 25 9 2008 569 597 Montemerlo et al., 2003 M. Montemerlo N. Roy S. Thrun Perspectives on standardization in mobile robot programming: The carnegie mellon navigation (CARMEN) toolkit Proceedings of the Conference on Intelligent Robots and Systems (IROS) 3 2003 2436 2441 Oliveira and Santos, 2007 M. Oliveira V. Santos A vision-based solution for the navigation of a mobile robot in a road-like environment Robotica 69 2007 18 25 Pomerleau, 1991 D. Pomerleau Efficient training of artificial neural networks for autonomous navigation Neural Computation 3 1 1991 88 97 Pomerleau, 1995 D. Pomerleau Neural network vision for robot driving M.A. Arbib The Handbook of Brain Theory and Neural Networks 2nd edition 1995 The MIT Press Cambridge, MA 161 181 Riedmiller and Braun, 1993 M. Riedmiller H. Braun A direct adaptive method for faster backpropagation learning: the RPROP algorithm Proceedings of IEEE International Conference on Neural Networks 1993 586 591 Riedmiller et al., 2007 M. Riedmiller M. Montemerlo H. Dahlkamp Learning to drive a real car in 20 minutes Proceedings of the 2007 Frontiers in the Convergence of Bioscience and Information Technologies 2007 IEEE Computer Society 645 650 Sarle, 2002 W.S. Sarle Neural network faq, part 2 of 7: Learning URL 2002 Retrieved May, 2009. Simmons and Apfelbaum, 1998 R. Simmons D. Apfelbaum A task description language for robot control Proceedings of the Conference on Intelligent Robots and Systems (IROS) 3 1998 1931 1937 "
    },
    {
        "doc_title": "3D reconstruction and Auralisation of the \"painted dolmen\" of antelas",
        "doc_scopus_id": "47949110073",
        "doc_doi": "10.1117/12.766607",
        "doc_eid": "2-s2.0-47949110073",
        "doc_date": "2008-07-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D acquisition",
            "3D geometries",
            "3D reconstructions",
            "3d visual models",
            "Acoustic absorption coefficients",
            "Archaeological sites",
            "Audio sources",
            "Audio visuals",
            "Augmented reality",
            "Auralisation",
            "Dark rooms",
            "Geometric acoustics",
            "Geometric models",
            "Head Related Transfer Functions",
            "In-situ",
            "Irregular surfaces",
            "Iterative algorithms",
            "Iterative Closest points",
            "Laser range finder",
            "Orientation sensors",
            "Reconstruction softwares",
            "Reverberation times",
            "Software computes",
            "Sound waves",
            "Source localisation",
            "Stereo headphones",
            "Uniform grids",
            "Visual scenes",
            "Visualization toolkits"
        ],
        "doc_abstract": "This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with \"free-space nodes\", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of \"free-space nodes\" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling and simulation of a thermostatic mixer with an anti-scalding or anti-cold system",
        "doc_scopus_id": "42949156224",
        "doc_doi": "10.1016/j.ijthermalsci.2007.07.008",
        "doc_eid": "2-s2.0-42949156224",
        "doc_date": "2008-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Safe operation",
            "Thermostatic mixers"
        ],
        "doc_abstract": "This paper presents the detailed modeling of a thermostatic mixer for domestic use with an anti-scalding or anti-cold device, which is operated by the temperatures and pressures of the hot and cold streams entering the mixer. The response time of the anti-scalding/cold device is shorter than the response time of the temperature adjustment system, which is based on a bulb that expands or contracts when temperature changes, thus leading to the immediate closing of the mixer inputs if any of the hot or cold streams is absent. This can be critical when the mixer is used by people with long reaction times. The modeling describes the location of all the mixer mobile parts, as well as the temperature evolution along the various identifiable chambers in the mixer. The model is numerically implemented on a well-known modeling tool and is simulated with an implicit Runge-Kutta based method, suitable to the numerical integration of stiff systems. Results for the transient and steady-sate operations are presented, which are relevant both in what concerns the output mixture temperature and the main operating characteristics of the mixer. © 2007 Elsevier Masson SAS. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272088 291210 291787 291811 31 International Journal of Thermal Sciences INTERNATIONALJOURNALTHERMALSCIENCES 2007-08-27 2007-08-27 2010-10-05T20:53:21 S1290-0729(07)00171-8 S1290072907001718 10.1016/j.ijthermalsci.2007.07.008 S300 S300.1 HEAD-AND-TAIL 2015-05-15T05:28:57.042508-04:00 0 0 20080701 20080731 2008 2007-08-27T00:00:00Z rawtext articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype volfirst volissue affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 1290-0729 12900729 47 47 7 7 Volume 47, Issue 7 11 903 917 903 917 200807 July 2008 2008-07-01 2008-07-31 2008 article fla Copyright © 2007 Elsevier Masson SAS. All rights reserved. MODELINGSIMULATIONATHERMOSTATICMIXERANTISCALDINGANTICOLDSYSTEM COSTA V SELIKTAR 1994 474 479 M SAMAN 1996 395 400 N MATSUI 1995 1253 1258 H BESSEGHINI 2004 42 48 S MATTSSON 1997 16 19 S THEMATHWORKSINC 2000 MATLABLANGUAGETECHNICALCOMPUTINGVOL12 THEMATHWORKSINC 2000 SIMULINKDYNAMICSYSTEMSIMULATIONFORMATLAB MORAN 1993 M FUNDAMENTALSENGINEERINGTHERMODYNAMICS MERRIT 1967 H HYDRAULICCONTROLSYSTEMS WHITE 1988 F FLUIDMECHANICS INCROPERA 2002 F FUNDAMENTALSHEATMASSTRANSFER SHAMPINE 1996 L COSTAX2008X903 COSTAX2008X903X917 COSTAX2008X903XV COSTAX2008X903X917XV item S1290-0729(07)00171-8 S1290072907001718 10.1016/j.ijthermalsci.2007.07.008 272088 2010-11-08T11:56:05.100611-05:00 2008-07-01 2008-07-31 true 1782725 MAIN 15 68081 849 656 IMAGE-WEB-PDF 1 R.A.A.T o, C ised 27 domestic the than the response time of the temperature adjustment system, which is based on a bulb that expands or contracts when temperature changes, thus leading to the immediate closing of the mixer inputs if any of the hot or cold streams is absent. This can be critical when the mixer is used by people with long reaction times. The modeling describes the location of all the mixer mobile parts, as well as the temperature evolution along the various identifiable chambers in the mixer. The model is numerically implemented on a well-known modeling tool and is simulated with an implicit Runge–Kutta based method, suitable to the numerical integration of stiff systems. Results for the transient and steady-sate operations are presented, which are relevant both in what concerns the output mixture temperature and the main operating characteristics of the mixer. © 2007 Elsevier Masson SAS. All rights reserved. Keywords: Thermostatic mixer; Anti-scalding/cold system; Safe operation; Modeling and simulation; Response time analysis 1. Introduction Thermal equipment is becoming increasingly sophisticated due to consumers’ demands when comfort and safety are con- cerned. Usual thermostatic mixers include a bulb that senses the outlet temperature and then adjusts accordingly to a desired temperature value, specified by the user. This system works well, but it shows up some limitations if sudden changes occur in the hot or cold streams, due to the fact that it has a response time of some seconds. Under these conditions, children, elderly or handicapped people can be subjected, during some seconds, to hot or to cold water streams. The same can be true if ac- cidents occur with people in the bathroom. In order to prevent these possibilities, an anti-scalding/cold system can be incorpo- rated in the mixer, with a very short response time, which closes the water entries to the mixer if any of the hot or cold streams are absent. Regarding the thermostatic part of the mixer, there is a great similarity to that of the usual thermostatic mixers. The * Corresponding author. E-mail address: v_costa@mec.ua.pt (V.A.F. Costa). model presented can be used for both cases of thermostatic mix- ers with or without the anti-scalding/cold device, as the action of the anti-scalding/cold device can be easily inhibited in the model. To the knowledge of the authors, only simple thermosta- tic mixers have been studied [1], as well as hot water supply systems [2,3], and there are no studies concerned with the modeling and simulation of such devices including the anti- scalding/cold system. Some studies can be found in the liter- ature but they deal with thermostatic valves associated to re- frigeration or air conditioning systems, as well as with the use of shape memory materials for temperature control in thermo- static mixers [4,5]. To model a system like a thermostatic mixer it is necessary to take into account the position of all its mobile parts, as well as the temperature evolution of the water along the entire valve, from the inlets to the output. This includes the evaluation of the pressure of the fluid streams along the valve, and how these pressures act on the anti-scalding/cold system, which is linked to the involved flow rates and the particular geometry of the fluid pathways. The proposed anti-scalding/cold system is self- operated by the pressures of the hot and cold streams. The user International Journal of Thermal Sciences Modeling and simulation of a thermostatic or anti-cold V.A.F. Costa ∗ , J.A.F. Ferreira, Departamento de Engenharia Mecânica, Universidade de Aveir Received 8 September 2006; received in rev Available online Abstract This paper presents the detailed modeling of a thermostatic mixer for by the temperatures and pressures of the hot and cold streams entering 1290-0729/$ – see front matter © 2007 Elsevier Masson SAS. All rights reserved. doi:10.1016/j.ijthermalsci.2007.07.008 47 (2008) 903–917 www.elsevier.com/locate/ijts mixer with an anti-scalding system . Igreja, V.M.F. Santos ampus Universitário de Santiago, 3810-193 Aveiro, Portugal form 19 July 2007; accepted 20 July 2007 August 2007 use with an anti-scalding or anti-cold device, which is operated mixer. The response time of the anti-scalding/cold device is shorter of R dimensionless parameter m averaged value, for material Re Reynolds number t time ........................................ s T temperature ............................... ◦ C v velocity ................................. m/s V volume ................................... m 3 x space co-ordinate . . . . . . . . . . . . . . . . . . . . . . . . . . . m max maximum value min minimum value out outlet rrod ss steady state 0 reference value can specify both the output flow rate and the desired mixed tem- perature. Energy conservation analysis gives the time evolution of the temperature in the different chambers of the valve, and the time evolution of the output mixture temperature can be pre- dicted. Such a model includes also the heat exchanges between the valve and the environment. The steady-state model infers from the transient model whose analysis leads to important conclusions, both regarding the output mixture temperature and the main operating char- acteristics of this kind of device. The paper presents results for the steady-state situation, the complete implementation of the model and results of the unsteady mixer simulation. Re- sults provided by the complete unsteady model can be useful in many ways. They can help the designer of a thermostatic mixer, in order to anticipate its behavior and performances. Also very important is the analysis of the output temperature associated with intense transient regimes, due to intense heating, intense cooling, rapid operation by the user of the temperature and flow rate control levers, or sudden absence of the inlet hot or cold streams. Response time of the mixer can be analyzed, as a combination of the mixer (bulb) system, with a response time of some seconds, and of the anti-scalding/cold system with a used to implement the mixer’s model. As an equation based non-causal modeling language, Modelica allows that the equa- tions may be introduced directly. The Matlab [7] and Simulink [8] packages were used to generate the thermostatic mixer in- puts, run the simulations and monitoring the results. 2. Modeling The thermostatic mixer under consideration is schematically represented in Fig. 1. It consists mainly of a mixing chamber, where the streams of hot and cold water are mixed in order to obtain the desired outlet temperature, and a pressure-driven in- ner rod with three assembled cylinders, which close the mixer inlets if the hot or the cold inlet pressures decrease. The later one is the anti-scalding/cold system, as the outlet mass flow rate is reduced to zero if any of the hot or cold streams is reduced to or closely to zero. Referring to Fig. 1, if, for instance, the inlet pressure of the cold stream decreases relatively to the inlet pres- sure of the hot stream, then the pressure in chamber 4 becomes lower than the pressure in chamber 1 and the anti-scalding/cold device moves to the right and decreases the hot water inlet into the mixer. The inverse situation occurs if a decrease is observed 904 V.A.F. Costa et al. / International Journal Nomenclature A area of fluid passage . . . . . . . . . . . . . . . . . . . . . . . m 2 b friction coefficient . . . . . . . . . . . . . . . . . . . . . . . kg/s c specific heat . . . . . . . . . . . . . . . . . . . . . . . . . J/(kg K) C d discharge coefficient d diameter ................................... m e thickness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . m E total energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J f fraction of perimeter used for passage h convection heat transfer coefficient . . . . . . . . . . . . . . . . . . . . . . . . . W/(m 2 K) h specific enthalpy . . . . . . . . . . . . . . . . . . . . . . . . . J/kg ˙ H convection heat transfer rate . . . . . . . . . . . . . . . . . W k thermal conductivity . . . . . . . . . . . . . . . . . W/(m K) L length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . m m mass ...................................... kg ˙m massflowrate ........................... kg/s P pressure ................................... Pa Pr Prandtl number ˙ Q volumetricflowrate ..................... m 3 /s shorter response time. In what concerns model implementation and numerical sim- ulation issues, a modeling language called Modelica [6] was Thermal Sciences 47 (2008) 903–917 Greek symbols α expansion factor for the bulb . . . . . . . . . . . . . . m/K β compressibility coefficient . . . . . . . . . . . . . . . . . . Pa γ factor, for the desired temperature value . . . . m/K Delta1 difference value µ dynamic viscosity of the fluid . . . . . . . . . . kg/(m s) ρ density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . kg/m 3 Subscripts a anti-scalding/cold device amb environment value atm atmospheric value bbulb c cold cv control volume d desired temperature value ext exterior f fluid h hot in inlet on the inlet pressure of the hot stream and the anti-scalding/cold device acts to close the inlet cold water into the mixer. This self-actuated anti-scalding/cold device, driven by pressure, has of from a specific relation describing the bulb behavior. 2.1. Mass conservation equation For each of the volumetric chambers defined inside the mixer, the mass conservation equation reads [9] dm dt =˙m in −˙m out (1) If the fluid is assumed to be slightly compressible, with a high compressibility coefficient evaluated as β = ρ(dP/dρ) T ,the mass conservation equation for each chamber becomes [10] dV dt + V β dP dt = ˙ Q in − ˙ Q out (2) where ˙ Q is the volumetric flow rate and V is the chamber’s volume. Volumes of chambers 2 and 3 are equal and constant, and making use of Fig. 2 they are given by V 2 = V 3 = L 2 π parenleftbig d 2 a − d 2 r parenrightbig /4(3) chambers 1 and 4 depend on that position, given by distance x a , and the volume of chambers 1 and 4 is obtained from V 1 = L 1 πd 2 a /4 (5a) V 4 = L 4 πd 2 a /4 (5b) where lengths L 1 and L 4 are obtained as L 1 = L 1,0 + x a − x d (6a) L 4 = L 4,0 + x d − x a (6b) Lengths L 1,0 and L 4,0 are the reference values for L 1 and L 4 , respectively. From Eqs. (6a) and (6b) it can be obtained that L 1 + L 4 = L 1,0 + L 4,0 = constant. The mass conservation equations for chambers 1 and 4 can be obtained, in the form of Eq. (2), respectively, as π d 2 a 4 bracketleftbigg (v a − v d ) + (L 1,0 + x a − x d ) 1 β dP 1 dt bracketrightbigg =− ˙ Q 12 (7a) π d 2 a 4 bracketleftbigg (v d − v a ) + (L 4,0 + x d − x a ) 1 β dP 4 dt bracketrightbigg = ˙ Q 34 (7b) V.A.F. Costa et al. / International Journal Fig. 1. Schematic representation a response time considerably shorter that the bulb driving the thermostatic action of the mixer. The mixing of the hot and cold streams is made inside the mixer and the relative contribution of each stream is conditioned by the position of the mixing cham- ber element. This position is controlled both by a bulb, which affects the position of its outlet element proportionally to the temperature of the mixed water, and by a lever used by the op- erator to settle on the desired outlet water temperature, T d .The outlet flow rate is set by the operator through the position of another lever, x r . The mass conservation principle, the energy conservation principle, and the Newton’s Second Law of Mo- tion are used to relate the variables that describe the behavior of the thermostatic mixer. Additional information to derive the full solution is borrowed from the pressure drop relations and while volume V 5 is also constant, and given by the particular construction details of the internal body of the mixer. Thermal Sciences 47 (2008) 903–917 905 of the thermostatic mixer. The mass conservation equations for chambers 2, 3 and 5 give, respectively, V 2 β dP 2 dt = ˙ Q h + ˙ Q 12 − ˙ Q h,out (4a) V 3 β dP 3 dt = ˙ Q c − ˙ Q 34 − ˙ Q c,out (4b) V 5 β dP 5 dt = ˙ Q h,out + ˙ Q c,out ˙ Q out (4c) where it is to be noted that ˙ Q 12 > 0ifP 1 >P 2 and that ˙ Q 12 < 0 otherwise, and that ˙ Q 34 > 0ifP 3 >P 4 ad that ˙ Q 34 < 0oth- erwise. It is always ˙ Q out greaterorequalslant 0 as it is always P 5 greaterorequalslant P 6 .The volumetric flow rates ˙ Q 12 and ˙ Q 34 are related to the change of position of the anti-scalding/cold device, and the volumes of where v a and v d are the velocities dx a /dt and dx d /dt, respec- tively. x d is the position of the lever left to the user to set the of mix chamber in the mixer to the next chamber. are obtained as Regarding the fluid flow from chamber 1 to 2, and from chamber 3 to 4, assuming that the flow takes place in the lam- inar regime, which is a reasonable approach given the small velocities involved, the volumetric flow rates ˙ Q 12 and ˙ Q 34 can be obtained from the Hagen–Poiseille solution as [10] ˙ Q 12 = πd 4 h 128µ (P 1 − P 2 ) e 1 (8a) ˙ Q 34 = πd 4 h 128µ (P 3 − P 4 ) e 3 (8b) where d h is the diameter of the hole and e 1 and e 3 are the thick- nesses of the separating discs (the hole depth). It is assumed that the remaining fluid exchanges occurring among the mixer chambers take place in the turbulent regime and, in this case, the pressure drop depends on the volumetric flow rate, the discharge coefficient C d and the cross-section area of the passage, A[10]. For the hot water stream, from the inlet to chamber 2, and after from chamber 2 to chamber 5, it can be written that A 2 = Delta1x 2 πd a f 1 (11a) A 3 = Delta1x 3 πd a f 1 (11b) A 5 = Delta1x 5 πd d f 2 (11c) A 6 = Delta1x 6 πd d f 2 (11d) where f 1 and f 2 are the fractions of the perimeter correspond- ing to the ports from one chamber to the next one. For a port of width Delta1x, which varies with the position of the mobile element under consideration, if all the perimeter of the tubular element of diameter d is used, the area of the cross-section (fluid pas- sage) is A = Delta1xπd and it is f = 1. If only one-half of the perimeter of the tubular element is used for the fluid passage it is A = Delta1xπd/2, and then f = 1/2. Distances Delta1x 2 and Delta1x 3 change depending on the position of the anti-scalding/cold system and, following Fig. 2, they can be obtained as Delta1x 2 = ⎧ ⎨ 0ifDelta1x 2,0 + x d − x a < 0 Delta1x + x − x if 0lessorequalslantDelta1x + x − x lessorequalslantDelta1x 906 V.A.F. Costa et al. / International Journal Fig. 2. Schematic representation of the desired outlet temperature value of the mixed water, and x a is the position of the anti-scalding/cold system, as shown in Fig. 2. The sum of Eqs. (7a) and (7b) results in an expression that is null only for an incompressible fluid: − ˙ Q 12 + ˙ Q 34 = bracketleftbig πd 2 a /(4β) bracketrightbigbracketleftbig (L 1,0 + x a − x d )(dP 1 /dt) + (L 4,0 + x d + x a )(dP 4 /dt) bracketrightbig 2.2. Pressure drop equations A fluid flows towards lower pressures, and therefore expe- riences a pressure drop when flowing in a duct, or from one ˙ Q h = C d,2 A 2 radicalBigg 2(P h,0 − P 2 ) ρ (9a) Thermal Sciences 47 (2008) 903–917 er, including its geometrical variables. ˙ Q h,out = C d,5 A 5 radicalBigg 2(P 2 − P 5 ) ρ (9b) For the cold water stream, from the inlet to chamber 3, and after from chamber 3 to chamber 5, the following holds: ˙ Q c = C d,3 A 3 radicalBigg 2(P c,0 − P 3 ) ρ (10a) ˙ Q c,out = C d,6 A 6 radicalBigg 2(P 3 − P 5 ) ρ (10b) Areas of cross-sections A 2 , A 3 , A 5 and A 6 can change and they ⎩ 2,0 d a 2,0 d a 2,max Delta1x 2,max if Delta1x 2,0 + x d − x a >Delta1x 2,max (12a) ofV.A.F. Costa et al. / International Journal Delta1x 3 = ⎧ ⎨ ⎩ 0ifDelta1x 3,0 + x a − x d < 0 Delta1x 3,0 + x a − x d if 0lessorequalslantDelta1x 3,0 + x a − x d lessorequalslantDelta1x 3,max Delta1x 3,max if Delta1x 3,0 + x a − x d >Delta1x 3,max (12b) Similarly, distances Delta1x 5 and Delta1x 6 change depending on the po- sition of the mixer and, once again from Fig. 2, they can be obtained as Delta1x 5 = ⎧ ⎨ ⎩ 0ifDelta1x 5,0 + x d − x b < 0 Delta1x 5,0 + x d − x b if 0lessorequalslantDelta1x 5,0 + x d − x b lessorequalslantDelta1x 5,max Delta1x 5,max if Delta1x 5,0 + x d − x b >Delta1x 5,max (13a) Delta1x 6 = ⎧ ⎨ ⎩ 0ifDelta1x 6,0 + x b − x d < 0 Delta1x 6,0 + x b − x d if 0lessorequalslantDelta1x 6,0 + x b − x d lessorequalslantDelta1x 6,max Delta1x 6,max if Delta1x 6,0 + x b − x d >Delta1x 6,max (13b) In the foregoing equations Delta1x 2,0 , Delta1x 3,0 , Delta1x 5,0 and Delta1x 6,0 are the reference values for Delta1x 2 , Delta1x 3 , Delta1x 5 and Delta1x 6 , and Delta1x 2,max , Delta1x 3,max , Delta1x 5,max , and Delta1x 6,max are their maximum respective values, as illustrated in Fig. 2. At the mixer outlet there exists also a pressure drop, evalu- ated as ˙ Q out = C d,7 A 7 radicalBigg 2(P 5 − P 6 ) ρ (14) where A 7 is the cross-section of the output opening, controlled by the operator, and is expressed as a fraction of the overall available cross-section usually given by a duct of 1 2 primeprime internal diameter. According to Merrit [10] and White [11], for an orifice in a plate, for high values of the local Reynolds number, the dis- charge coefficient C d is independent of the volumetric flow rate, and it varies between 0.60 and 0.61. The local Reynolds number in a particular passage can be obtained as Re = ρvDelta1x/µ (15) where Delta1x is the smallest dimension of the fluid passage un- der analysis. The local volumetric flow rate can be obtained as ˙ Q = vA = v(fπd)Delta1x, where f is the fraction of the perime- ter associated with the passage, and the local Reynolds number can be expressed alternatively as Re = ρ ˙ Q µ(fπd) (16) For the usual dimensions in a real mixer, for water, and for f = 1/2, it can be obtained that Re ≈ 10 3 ˙ Q. For a typical value of ˙ Q = 10 l/min the following holds: Re ≈ 10 4 . Different internal constructions of the valve lead to different resistances to the flow of water, and thus to different overall volumetric flow rates through the mixer. In this work, even if the pathways are not holes in a plate, and the local flow is not turbulent in all the situations, the value C d = 0.60 will be used for all the involved discharge coefficients [10]. Attached to the mixer output other appliances can be in- stalled such as, for example, a shower, which represents an additional pressure drop. This can be expressed as P 6 − P atm = (1/2)K 8 ρ( ˙ Q out /A 8 ) 2 (17) Thermal Sciences 47 (2008) 903–917 907 where A 8 is the cross-section of the output duct (typically with the internal diameter 1 2 primeprime ), and K 8 is the pressure drop coeffi- cient associated with the tube length and possibly other fittings, such as bends and the showerhead. Atmospheric pressure P atm is taken as P atm = 0, and thus all the remaining involved pressures are relative pressures, as usually made when dealing with domestic hydraulic systems. 2.3. Momentum equations The position of the mixer mobile parts needs to be evaluated at all instants in order to obtain the corresponding volumetric flow rates and the water outlet temperature. For the anti-scalding/cold system, which moves with some fluid, the momentum equation is (m a + m f ) dv a dt = π(d 2 a − d 2 h ) 4 (P 1 − P 4 ) + πd 2 h 4 (P 2 − P 3 ) − bv a (18) where v a = dx a /dt and b is the friction coefficient associated with the existing contact between the anti-scalding/cold system and its external cylinder. High values of the friction coefficient b can be used to consider the situation of a thermostatic mixer from which the anti-scalding/cold system is absent (with low mobility relative to the mixer element). In this way, the pre- sented model can be used for both cases of thermostatic mixers with or without the anti-scalding/cold device, as the action of the anti-scalding/cold device can be easily inhibited giving to the friction coefficient b a high value. In this equation, m a and m f are the masses of the anti-scalding/cold system and of the involved fluid, respectively, which can be evaluated as m a = (ρ a π/4) bracketleftbig d 2 a (e 1 + e 2 + e 3 ) + d 2 r (L 2 + L 3 ) − d 2 h (e 1 + e 3 ) bracketrightbig (19) m f = (ρ f π/4) bracketleftbigparenleftbig d 2 a − d 2 r parenrightbig (L 2 + L 3 ) + d 2 h (e 1 + e 3 ) bracketrightbig (20) Once the mechanical part of the problem has been defined, its thermal part remains to be modeled. As it will be seen, they are strongly linked, and a simultaneous solution is needed to predict the whole behavior of the thermostatic mixer. 2.4. Energy conservation equation The energy conservation equation, when applied to an open thermodynamic system, with negligible variations in the kinetic and gravitational potential energies gives [9] dE cv dt = ˙ H + summationdisplay in ˙mh − summationdisplay out ˙mh (21) where ˙ H is the rate of heat received by the mixer from its sur- roundings, and h is the specific enthalpy. Total energy E is expressed as E = m(u + 0.5v 2 + gz) [9], and in this work only its internal energy term, mu is relevant. In the term dE cv /dt it will be considered that the changes in the internal energy can be taken as similar to the changes in the enthalpy, and this term is of908 V.A.F. Costa et al. / International Journal taken as dE cv /dt ≈ d(mh)/dt. The energy conservation equa- tion to be applied in the present work is d dt (ρ m V m h m + ρ f V 5 h f ) = ˙ H + summationdisplay in ˙mh − summationdisplay out ˙mh (22) where ρ m is the averaged density of the material of the mixer, and h m is its corresponding specific enthalpy. As the tempera- ture range is small, the specific enthalpy of the water can be eva- luated as h f = h f,0 +c f (T −T 0 ), and the specific enthalpy of the material of the mixer is evaluated as h m = h m,0 + c m (T − T 0 ), where c m is its (constant) corresponding averaged specific heat, T 0 is a reference temperature for which h = h 0 [9], and c f is the (constant) specific heat of the water. Application of the energy conservation equation for the mixer, assuming that volume of chamber 5 is constant and that it is the most relevant volume of the involved chambers, leads to (ρ m V m c m + ρ f V 5 c f ) dT 5 dt = ˙ H + summationdisplay in ρ f c f ˙ QT − summationdisplay out ρ f c f ˙ QT (23) where it is assumed that the temperature of the material of the mixer equals that of the fluid in chamber 5, T 5 . This is not strictly the case, but a better modeling needs a very detailed knowledge of the particular inner construction of the mixer. It is also assumed that the small effects due to fluid compressibil- ity are not important in the energy conservation equation. The heat exchanged between the surroundings and the mixer, ˙ H, can be evaluated as [12] ˙ H = h ext A ext (T amb − T 5 ) (24) where h ext is the exterior convection heat transfer coefficient for the mixer, usually subjected to natural convection with still air, A ext is the exterior surface area of the mixer exposed to the environment, and it is assumed that the exterior surface of the mixer is kept at temperature T 5 . Considering that the mixer has two inlet ports and one outlet port, the energy conservation equation gives (ρ m V m c m + ρ f V 5 c f ) dT 5 dt = h ext A ext (T amb − T 5 ) + ρ f c f ( ˙ Q h,0 T h + ˙ Q c,0 T c − ˙ Q out T 5 ) (25) where ˙ Q h,0 and ˙ Q c,0 are the inlet volumetric flow rates of the hot and cold water streams, respectively, both conditioned by the inlet pressures of the hot and cold streams and the internal geometry of the mixer. The relationship to time of the outlet temperature, T 5 , will be obtained from this equation. 2.5. Equation for the bulb Regarding the bulb, the position of its mobile part, x b ,isa function of its temperature T b , and it can be expressed as x b = x b,0 + α(T b − T 0 ) (26) Thermal Sciences 47 (2008) 903–917 In this equation, T 0 is a reference temperature for which x b = x b,0 , and α is the linear expansion coefficient of the bulb. Com- mon values for α are close to 0.25 mm/K. The coefficient α can simply stand for the volumetric expansion coefficient of the liquid or the wax that fills the bulb, or it can be amplified by using a rubber cone in a conical tube [1]. Due to this fact, the assembly of this kind of mixer is carried out in a temperature controlled environment, at temperature T 0 . The bulb temperature is related to the outlet (mixture) fluid temperature, and the energy balance for the bulb gives [12] m b c b dT b dt = h b A b (T 5 − T b ) (27) where h b is the convection heat transfer coefficient between the fluid and the bulb, and A b is the area of its wetted surface. The convection heat transfer coefficient h b can be obtained by using some available correlations for heat transfer from or to cylinders in cross flow, through an expression of the type [12] h b = C Re m Pr 1/3 k f /d b (28) In the foregoing equations, Pr is the Prandtl number, Re is the Reynolds number based on the bulb diameter, k f is the fluid thermal conductivity, and C and m are parameters dependent on the Reynolds number [12]. The value obtained for the con- vection heat transfer coefficient will be strongly dependent on the particular mixer geometry and on the flow around the bulb. An expression for its evaluation, for water at room temperature, can be h b = 1.87 × 10 5 ˙ Q 0.54 out W/(m 2 K) (29) 2.6. Equation for the temperature control lever The user sets the mixer on the desired temperature by mov- ing the temperature control lever, thus setting the position x d . The desired temperature and the lever position are linearly re- lated through the expression x d = x d,0 + γ(T d − T 0 ) (30) Usually, this lever consists of a screw with a given fixed pitch. If, for example, the screw has a 5 mm pitch, and it is to ob- tain the temperature range 20–60 ◦ C in one turn, then γ = 5/40 mm/K. 3. Steady-state solution When the mixer operates in steady-sate conditions, with con- stant inlet pressures and temperatures, and when there are no changes in the flow and temperature control levers, the forego- ing model can be simplified and its steady-state solution can be obtained. The energy conservation equation, Eq. (25), can be written as T 5,ss = ˙ Q h,0 T h + ˙ Q c,0 T c +[h ext A ext /(ρ f c f )]T amb ˙ Q h,0 + ˙ Q c,0 +[h ext A ext /(ρ f c f )] (31) given that, under these conditions, ˙ Q h,out = ˙ Q h,0 , ˙ Q c,out = ˙ Q c,0 and ˙ Q out = ˙ Q h,0 + ˙ Q c,0 . It is to be noted that, in of Fig. 3. Response of the mixer when an increasing step in the hot temperature is applied, from 20 to 60 ◦ C. [T d = 38 ◦ C, T c = 20 ◦ C, x r = 50%, P h = 3bar, P c = 3bar,T 5ss = 38.6 ◦ C (steady state).] steady-state conditions, the anti-scalding/cold device reaches an equilibrium position, thus given that P 2 = P 3 , and Eqs. (9b) and (10b) give ˙ Q h,out = C d,5 A 5 √ 2Delta1P/ρ f and ˙ Q c,out = C d,6 A 6 √ 2Delta1P/ρ f , with Delta1P = (P 2 − P 5 ) = (P 3 − P 5 ).Ifall the discharge coefficients, C d , are made equal, the energy con- servation equation gives T 5,ss = A 5 T h + A 6 T c + bracketleftbig h ext A ext ρ f c f 1 C d radicalBig ρ f 2Delta1P bracketrightbig T amb A 5 + A 6 + bracketleftbig h ext A ext ρ f c f 1 C d radicalBig ρ f 2Delta1P bracketrightbig (32) temperatures, and the weighting factors are the areas of the pas- sages of the hot and cold streams. For a theoretical mixer with h ext A ext →+∞, the output temperature will tend to the ambi- ent temperature, as the heat exchange with the environment is the dominant mechanism in this case. Eq. (32) can be worked out, with areas A 5 and A 6 given by Eqs. (11c) and (11d), and Delta1x 5 = Delta1x 5,0 + x d − x b and Delta1x 6 = Delta1x 6,0 + x b − x d as given by Eqs. (13a) and (13b), respectively, to give T 5,ss = T d + bracketleftbig (T c − T d ) + (R 1 − R 2 )(T 0 − T d ) V.A.F. Costa et al. / International Journal In the limiting situation of h ext A ext → 0 (when the mixer is not exchanging heat with the surroundings), the output tempera- ture is the weight averaged temperature of the inlet hot and cold Thermal Sciences 47 (2008) 903–917 909 + R 3 (T h − T c ) + R 4 (T amb − T d ) bracketrightbig /(1 + R 1 + R 4 ) (33) of Fig. 4. Response of the mixer when a decreasing step in the hot temperature is applied, from 60 to 50 ◦ C. (T d = 38 ◦ C, T c = 20 ◦ C, x r = 50%, P h,0 = 3bar, P c,0 = 3bar.) where the dimensionless parameters R 1 , R 2 , R 3 and R 4 are de- fined as R 1 ≡ α(T h − T c ) Delta1x 5,0 + Delta1x 6,0 (34) R 2 ≡ γ(T h − T c ) Delta1x 5,0 + Delta1x 6,0 (35) R 3 ≡ Delta1x 5,0 + x d,0 − x b,0 Delta1x 5,0 + Delta1x 6,0 (36) R 4 ≡ h ext A ext ρ f c f 1 C d radicalbigg ρ f 2Delta1P 1 πd d f 2 1 Delta1x 5,0 + Delta1x 6,0 (37) For the thermostatic mixer that is not exchanging heat with the ambient, that is, with h ext A ext → 0, Eq. (33) applies with R 4 = 0. The main point of Eq. (33) is that, in steady-state conditions, the outlet mixture temperature T 5 is different from the desired temperature T d , and the term in the fraction of the right-hand side of Eq. (33) can be seen as the deviation from the desired temperature value. Only for some combinations of the involved parameters (typically non-realistic situations) will be T 5 = T d . operator, even if this difference is small. In this work, some studies will be made in order to assess how the different para- meters affect the mixture temperature deviation from its desired value, both under steady-state conditions and under transient conditions. 4. Numerical modeling Regarding the numerical model, a Simulink block diagram integrating the Modelica model of the thermostatic mixer was developed to simulate the thermostatic mixer with different in- puts. In order to reduce the system stiffness, introduced with possible fast changes in the input test signals, all the inputs are introduced with a smooth continuous trajectory modeled by a third order polynomial. The hot water pressure, P h,0 ,forex- ample, is generated with P h,0 = a · t 3 + b · t 2 + c · t + d.The parameters a, b, c and d depend on the start and end points and the initial and final values of the derivative of the variable being controlled, as well as the total time to complete the trajectory. The trajectory is expressed as a function of time (t) in seconds. This third order polynomial generates gentle, smooth curves 910 V.A.F. Costa et al. / International Journal If this is true for steady-state conditions, it will be true also for transient conditions and, in general, the output mixture temper- ature will be different from the desired temperature set by the Thermal Sciences 47 (2008) 903–917 that reduce the overall model stiffness. The numerical simu- lations use the variable step “ode23tb” Matlab solver that is suitable to the numerical integration of stiff systems [13]. This of Fig. 5. Response of the valve when a sudden decrease on the inlet pressure of the cold stream is applied, from 3 to 0 Pa (relative). (T d = 38 ◦ C, T c = 20 ◦ C, x r = 50%, P h,0 = 3bar.) solver is an implementation of TR-BDF2, an implicit Runge– Kutta formula with a first stage that is a trapezoidal rule step and a second stage that is a backward differentiation formula of order two. 5. Illustrative results Some results are presented showing the capabilities of both the physical model and of the numerical model. The parameters considered are set out in Table 1; some of them can assume different values only if locally and explicitly to 60 ◦ C. This sudden change on the inlet temperature of the hot water stream induces changes in the valve that need nearly 5 s to occur. It is observed that the outlet temperature T 5 in- creases with time during nearly 2 s, reaching then the maximum value of the inlet hot water temperature, 60 ◦ C, hence creating a risk of scalding. After that, the output water temperature de- creases during nearly 3 s, and reaches the expected steady-state value of 38.6 ◦ C, which differs only by 0.6 ◦ C from the de- sired temperature value, T d = 38 ◦ C. Potential risk for scalding exists for nearly 2 s, the period during which the outlet temper- ature exceeds 50 ◦ C. In regards to the volumetric flow rate, it is observed that its initial value is given mainly by the hot wa- V.A.F. Costa et al. / International Journal specified in the text. Fig. 3 presents the valve response to changes in the hot wa- ter inlet temperature, whose value suddenly changes from 20 Thermal Sciences 47 (2008) 903–917 911 ter stream (when the hot water stream is at the temperature of 20 ◦ C), and that the cold inlet water begins contributing to the of Fig. 6. Response of the valve to a sudden decrease on the inlet pressure of the cold stream, from 3 to 0 bar, and a high value of the friction coefficient, b = 1000 N s/m (T d = 38 ◦ C, T c = 20 ◦ C, x r = 50%, P c,0 = 3bar.) overall water flow rate 2 s after the sudden change in the hot water inlet temperature. It is observed that, for the imposed in- ternal geometry of the valve, with x r = 0.5, the inlet pressure of 3 bar of both the hot and cold streams leads to an overall flow rate of nearly 7.5 l/min. When the steady-state conditions are reached, the cold water inlet has a slightly higher contri- bution than the hot water inlet to the overall volumetric flow rate. There are no noticeable changes on the pressure of the involved streams, including the output pressure, due to this sud- den change on the inlet hot water temperature. Fig. 4 plots the response of the valve to a sudden decrease of ◦ pected steady-state temperature, during nearly 4 s, after which the steady state conditions are reached. It is to be noted that the steady state temperature value changes with the temperatures of the inlet streams, as given by Eq. (33). It is also observed that, initially, the contribution of the hot water volumetric flow rate is higher than the cold water flow rate, and that the inverse sit- uation corresponds to the new equilibrium operating conditions of the valve. Pressures in the chambers of the valve keep the values of P 2 = P 3 = 2.986 bar, and P 5 = 2.976 bar, which are not affected by a sudden temperature change induced on the hot water inlet. 912 V.A.F. Costa et al. / International Journal the hot water inlet temperature, from 60 to 50 C. Once again it is seen that this modification induces changes that need nearly 5 s to take place. Output temperature decreases below the ex- Thermal Sciences 47 (2008) 903–917 Fig. 5 shows the valve response to a sudden decrease on the cold water inlet pressure from 3 to 0 bar. This variation induces a fast change on the anti-scalding/cold device position, and the of Fig. 7. Response of the mixer to a change on the opening, x r .(T h = 60 ◦ C, T c = 20 ◦ C, P h,0 = 3bar,P c,0 = 3bar.) output volumetric flow rate drops to a value close to zero in the very short time period of 0.25 s. The output temperature re- mains unchanged; as the valve loses heat to the environment, there is a small hot water volumetric flow rate entering the valve, releasing heat, and therefore keeps the valve at the ex- pected steady-state temperature. Regarding the hot and cold volumetric flow rate contributions, Fig. 5 allows the observa- tion of a behavior similar to a damped oscillation, during a short time period, with the hot and cold water streams alternating in importance to the overall volumetric flow rate. A relevant con- clusion taken from Fig. 5 is that the anti-scalding/cold device is working very well considering its main function, which is to reduce the valve response time when sudden decreases on the inlet pressures of one stream occur. Fig. 6 presents a similar situation to Fig. 5 but using a high value for the friction coefficient, which is equivalent to have the creases, the hot stream is dominant and the outlet temperature reaches a value close to the inlet hot water stream and creates a risk of scalding. Nearly 5 s after the pressure decrease, the outlet temperature decreases during nearly 5.5 s, and reaches a minimum some degrees below the desired temperature, and again starts rising to the expected steady-state value. When the inlet pressure of the cold stream decreases, a noticeable reduc- tion on the outlet volumetric flow rate from nearly 8 l/min to nearly 2 l/min is also observed. It is thus concluded that the anti-scalding/cold system needs to have a low friction (a nearly free possibility of displacement) in order to give a short re- sponse time to the mixer when the inlet pressure of the hot or cold streams suddenly drops. Fig. 7 shows the valve response to a variation on the valve opening from x r = 20% to x r = 80%. The major changes are observed on the output volumetric flow rate, which changes V.A.F. Costa et al. / International Journal anti-scalding/cold system acting very slowly. In this case, the mixer response time is delayed, and relevant changes occur on the outlet temperature. When the inlet cold stream pressure de- Thermal Sciences 47 (2008) 903–917 913 from nearly 4.5 l/min to nearly 8.5 l/min, as an overall contri- bution of both the hot and cold inlet water streams. The increase on the valve opening results in higher volumetric flow rates, be- of Fig. 8. Response of the mixer to a change on the desired temperature values from 32 to 40 ◦ C. (T h = 60 ◦ C, T c = 20 ◦ C, x r = 50%, P h,0 = 3bar,P c,0 = 3bar.) ing the overall pressure drop kept at its reference value of 3 bar. Regarding the temperature, no noticeable changes are observed after the imposed change on the valve opening. The positions of the anti-scalding/cold system and of the bulb remain essentially constant (x a = 4.64 mm, x b = 4.64 mm). Fig. 8 shows the valve response to a change on the desired temperature value from 32 to 40 ◦ C. The effects of this change are observed during nearly 5 s. The output temperature con- siderably increases to nearly 48 ◦ C in 1.5 s and then slowly decreases to the new expected steady state value in nearly 3.5 s. The volumetric flow rate contribution of the cold water stream is higher before the temperature change, and on the steady- state conditions the contributions of the hot and cold streams are nearly the same. The observed changes on the contributions able changes on the pressures are observed, and are kept about P 2 = P 3 = 2.986 bar and P 5 = 2.976 bar. Fig. 9 shows how the characteristics of the bulb affect the valve response time when subjected to a sudden increase in the hot water temperature inlet from 20 to 60 ◦ C. From Eq. (27), the bulb response time is given by m b c b /(h b A b ), which is thus proportional to the dimensional factor m b c b /A b . Fig. 9 presents the valve response for different values of this dimensional para- meter. Increasing values of this parameter lead to longer valve response time, and to an increase of potential risks of scald- ing/cold for the user. A compromise exists: larger bulbs, with higher values of the dimensional factor m b c b /(h b A b ), will pro- vide greater displacements under temperature influence, and are thus better for temperature control. However, due to their higher 914 V.A.F. Costa et al. / International Journal of the inlet water streams to the overall volumetric flow rate occur without any noticeable change on the outlet volumetric flow rate, which is maintained at nearly 8 l/min. No notice- Thermal Sciences 47 (2008) 903–917 masses, they have higher response times, and they will give a poor performance to the mixer when operating under unsteady conditions. of Fig. 9. Influence of the thermal properties of the bulb, through the dimensional factor m b c b /A b , when a change of the inlet hot stream exists, from 20 to 60 ◦ C. (T d = 38 ◦ C, T c = 20 ◦ C, x r = 50%, P h,0 = 3bar,P c,0 = 3bar.) Table 1 Numerical values used for governing parameters A b = 17 × 10 −4 m 2 A ext = 345 × 10 −4 m 2 b = 0.1Ns/m C b = 600 J/(kg K)C p,f = 4190 J/(kg K)C p,steel = 450 J/(kg K) d a = 0.02 m d h = 0.005 m d r = 0.005 m d t = 0.00635 m D d = 0.025 m e 1 = 0.004 m e 2 = 0.003 m e 3 = 0.004 m h ext = 5W/(m 2 K) L 10 = 0.005 m L 2 = 0.014 m L 3 = 0.014 m L 40 = 0.005 m m b = 0.0185 kg T amb = 20 ◦ C T 0 = 20 ◦ C V 5 = 60 × 10 −6 m 3 V steel = 20 × 10 −6 m 3 x b,0 = 0m x d,0 =−0.002 m α = 0.25 × 10 −3 m/K β e = 2.4 × 10 9 Pa γ = 2.5 × 10 −4 m/K Delta1 x2,0 = 0.002 m Delta1 x2,max = 0.004 m Delta1 x3,0 = 0.002 m Delta1 x3,max = 0.004 m Delta1 x5,0 = 0.002 m Delta1 x5,max = 0.004 m Delta1 x6,0 = 0.002 m Delta1 x6,max = 0.004 m µ = 6.82 × 10 −4 kg/(ms)ρ= 1000 kg/m 3 ρ steel = 7800 kg/m 3 hot water temperature exists, from 20 to 60 ◦ C. This parame- ter has no influence on the valve response time, but rather on the position reached by the mobile element linked to the bulb. It can be seen that small values of this parameter lead to too small displacements of the valve temperature regulator, which can never reach the required position corresponding to the de- sired outlet temperature. This is information of great value at the design stage, as no prototypes need to be built with such a bulb, as it never meets the desired performance. A bulb with a small value for α can have a good performance on a mixing valve only if the opening of the mixer, Delta1x 5,max + Delta1x 6,max ,is set in accordance with such a value of α. However, small values of α can lead to operation problems of the mixer due to solid particles contained in water, therefore requiring the use of fil- V.A.F. Costa et al. / International Journal Fig. 10 describes the influence of the bulb’s linear expansion parameter α on the valve response when a change on the inlet Thermal Sciences 47 (2008) 903–917 915 ters, which on the other hand introduce greater pressure drops and maintenance needs, and allows only lower volumetric flow rates. of Fig. 10. Behavior of the mixer for different values of parameter α, when a change on the inlet hot water temperature exists, from 20 to 60 ◦ C. (T d = 38 ◦ C, T c = 20 ◦ C, x r = 50%, P h,0 = 3bar,P c,0 = 3bar.) 6. Conclusions The modeling of thermal equipment and systems has been shown to be effective. It describes the main parameters and variables, characterizing their individual behavior and relation- ships to predict the equipment performance under real operating conditions. In the present case, modeling allows a complete un- derstanding about thermostatic mixers, their components, the individual behavior of each component alone, and the behavior of the assembled set. Simulation predicts the behavior of the mixer, when operating both in transient or steady-state condi- Especially important is the dynamic response of the mixer when subjected to sudden changes on the input streams condi- tions or/and on the control levers, which enables assessment of the conditions under which scalding or cold shock exist. A steady-state solution can be analytically obtained, lead- ing to important conclusions, both regarding the output mixture temperature and the main operating characteristics of the mixer. The analytical steady state solution is also important to assess the results of the complete model when simulating the steady- state operation. It was observed that the anti-scalding/cold system gives the 916 V.A.F. Costa et al. / International Journal tions and analysis of results gives valuable information to be considered at the development and design phases of this kind of device. Thermal Sciences 47 (2008) 903–917 mixer a very short response time when inlet pressures of the hot or cold streams decrease considerably and suddenly. The anti-scaling/cold system response time is considerably shorter V.A.F. Costa et al. / International Journal of Thermal Sciences 47 (2008) 903–917 917 than the bulb response time, provided that system can move freely. If friction is considerable, as typically found in a mixer without the anti-scalding/cold device, its response is delayed and is not adequate. For a mixer without the anti-scalding/cold system some potential for scalding/cold exists, what can be cru- cial when the thermostatic mixer is to be used by children, old people, handicapped people, or others with reduced reaction ca- pacity. References [1] K. Stork, Thermal system analysis: Heat transfer in glass forming and fluid-temperature control systems, PhD Thesis, Department of Mechan- ical Engineering, Linköping University, Sweden, 1998. [2] M. Seliktar, C. Rorres, The flow of hot water from a distant hot water tank, SIAM Rev. 36 (3) (1994) 474–479. [3] N. Saman, H. Mahdi, Analysis of the delay hot/cold water problem, En- ergy 21 (5) (1996) 395–400. [4] H. Matsui, W. Enoki, T. Kato, Application of shape memory alloy to a thermostatic mixing valve, Journal de Physique 5 (1995) 1253–1258. [5] S. Besseghini, A. Tuissi, B. Binda, E. Capello, B. Previtali, S. Lecco, Caratterizzazione e comportamento delle leghe a memoria di forma, L’In- dustria Meccanica 11/2004 (2004) 42–48. [6] S.E. Mattsson, H. Elmqvist, J.F. Broenink, Modelica: An International ef- fort to design the next generation modelling language, Journal A, Benelux Quarterly Journal on Automatic Control 38 (3) (1997) 16–19. Special is- sue on Computer Aided Control System Design, CACSD, 1998. [7] The Math Works Inc., Matlab: The Language of Technical Computing, vol. 12, sixth ed., The MathWorks Inc., Natick MA, USA, 2000. [8] The Math Works Inc., Simulink: Dynamic System Simulation for Matlab, The MathWorks Inc., Natick MA, USA, 2000. [9] M.J. Moran, H.N. Shapiro, Fundamentals of Engineering Thermodynam- ics, second ed., Wiley, New York, 1993. [10] H.E. Merrit, Hydraulic Control Systems, Wiley, New York, 1967. [11] F.M. White, Fluid Mechanics, second ed., McGraw–Hill, New York, 1988. [12] F.P. Incropera, D.P. DeWitt, Fundamentals of Heat and Mass Transfer, fifth ed., Wiley, New York, 2002. [13] L. Shampine, M.E. Hosea, Analysis and implementation of TR-BDF2, Applied Numerical Mathematics 20 (1996). THESCI 2693 S1290-0729(07)00171-8 10.1016/j.ijthermalsci.2007.07.008 Elsevier Masson SAS Modeling and simulation of a thermostatic mixer with an anti-scalding or anti-cold system V.A.F. Costa ⁎ J.A.F. Ferreira R.A.A.T. Igreja V.M.F. Santos Departamento de Engenharia Mecânica, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal ⁎ Corresponding author. This paper presents the detailed modeling of a thermostatic mixer for domestic use with an anti-scalding or anti-cold device, which is operated by the temperatures and pressures of the hot and cold streams entering the mixer. The response time of the anti-scalding/cold device is shorter than the response time of the temperature adjustment system, which is based on a bulb that expands or contracts when temperature changes, thus leading to the immediate closing of the mixer inputs if any of the hot or cold streams is absent. This can be critical when the mixer is used by people with long reaction times. The modeling describes the location of all the mixer mobile parts, as well as the temperature evolution along the various identifiable chambers in the mixer. The model is numerically implemented on a well-known modeling tool and is simulated with an implicit Runge–Kutta based method, suitable to the numerical integration of stiff systems. Results for the transient and steady-sate operations are presented, which are relevant both in what concerns the output mixture temperature and the main operating characteristics of the mixer. Keywords Thermostatic mixer Anti-scalding/cold system Safe operation Modeling and simulation Response time analysis References [1] K. Stork, Thermal system analysis: Heat transfer in glass forming and fluid-temperature control systems, PhD Thesis, Department of Mechanical Engineering, Linköping University, Sweden, 1998 [2] M. Seliktar C. Rorres The flow of hot water from a distant hot water tank SIAM Rev. 36 3 1994 474 479 [3] N. Saman H. Mahdi Analysis of the delay hot/cold water problem Energy 21 5 1996 395 400 [4] H. Matsui W. Enoki T. Kato Application of shape memory alloy to a thermostatic mixing valve Journal de Physique 5 1995 1253 1258 [5] S. Besseghini A. Tuissi B. Binda E. Capello B. Previtali S. Lecco Caratterizzazione e comportamento delle leghe a memoria di forma L'Industria Meccanica 11/2004 2004 42 48 [6] S.E. Mattsson H. Elmqvist J.F. Broenink Modelica: An International effort to design the next generation modelling language Journal A, Benelux Quarterly Journal on Automatic Control 38 3 1997 16 19 Special issue on Computer Aided Control System Design, CACSD, 1998 [7] The Math Works Inc. Matlab: The Language of Technical Computing, vol. 12 sixth ed. 2000 The MathWorks Inc. Natick MA, USA [8] The Math Works Inc. Simulink: Dynamic System Simulation for Matlab 2000 The MathWorks Inc. Natick MA, USA [9] M.J. Moran H.N. Shapiro Fundamentals of Engineering Thermodynamics second ed. 1993 Wiley New York [10] H.E. Merrit Hydraulic Control Systems 1967 Wiley New York [11] F.M. White Fluid Mechanics second ed. 1988 McGraw–Hill New York [12] F.P. Incropera D.P. DeWitt Fundamentals of Heat and Mass Transfer fifth ed. 2002 Wiley New York [13] L. Shampine M.E. Hosea Analysis and implementation of TR-BDF2 Applied Numerical Mathematics 20 1996 "
    },
    {
        "doc_title": "Merging topological maps for localisation in large environments",
        "doc_scopus_id": "84892474542",
        "doc_doi": "10.1142/9789812835772_0015",
        "doc_eid": "2-s2.0-84892474542",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Geometric maps",
            "Individual sequences",
            "Localisation",
            "Map merging",
            "Multiple sequences",
            "Topological map",
            "View sequence"
        ],
        "doc_abstract": "This article presents a method for the creation of a topological map without having to previously create a geometric map. Independently obtained topological paths are compared pairwise to search for possible overlap in the view sequence. Each individual topological path is created from a sequence of raw data views that are sampled by leading the robot along a path in the environment. The multiple topological paths are 'stitched together' into a generic Topological map by identifying overlapping segments in the individual sequences. A general topological map can be created by considering all the multiple sequences or separate runs through the environment. Results on the merger of upto 8 separate paths indicate that this method of mapping large environments, by selective touring through the environment, can generate robust topological maps.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual tracking on an autonomous self-contained humanoid robot",
        "doc_scopus_id": "84886942471",
        "doc_doi": "10.1142/9789812835772_0152",
        "doc_eid": "2-s2.0-84886942471",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Distributed control architectures",
            "Embedded controllers",
            "Hardware and software",
            "Local controllers",
            "Onboard components",
            "Sensor monitoring",
            "Vision capability",
            "Visual Tracking"
        ],
        "doc_abstract": "This paper describes the hardware and software setups that allow a humanoid robot developed from scratch to perform visual tracking based exclusively on onboard components. The robot which was started on earlier projects was finally given its full autonomy in what concerns perception and vision capabilities. An embedded PCI04-based controller running Linux is now able to interface a IEEE1394 color camera and, using the OpenCV library, can now perform visual tracking of some objects moving on its neighborhood. This embedded controller, besides being responsible for image acquisition and processing, serves as an interface between external monitoring and the distributed control architecture based on a master-multi-slave CAN bus of local controllers for joint actuation and sensor monitoring. Copyright © 2008 by World Scientific Publishing Co. Pte. Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Robust place recognition within multi-sensor view sequences using Bernoulli mixture models",
        "doc_scopus_id": "79960946490",
        "doc_doi": "10.3182/20070903-3-fr-2921.00090",
        "doc_eid": "2-s2.0-79960946490",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Bayesian network models",
            "Bernoulli mixtures",
            "Binary data",
            "Expectation-maximisation",
            "Incidence matrices",
            "Local image features",
            "Place recognition",
            "Robot localization"
        ],
        "doc_abstract": "This article reports on the use of Hidden Markov Models to improve the results of Localization within a sequence of Sensor Views. Local image features (SIFT) and multiple types of features from a 2D laser range scan are all converted into binary form and integrated into a single, binary, Feature Incidence Matrix (FIM). To reduce the large dimensionality of the binary data, it is modeled in terms of a Bernoulli Mixture providing good results that were reported in an earlier presentation. We have improved the good performance of the approach by incorporating the Bernoulli mixture model inside a Bayesian Network Model, an HMM, that accumulates evidence as the robot travels along the environment.",
        "available": true,
        "clean_text": "serial JL 314898 291210 291718 291882 291883 31 IFAC Proceedings Volumes IFACPROCEEDINGSVOLUMES 2016-04-20 2016-04-21 2016-04-20 2016-04-21 2016-04-20T17:20:47 S1474-6670(16)34715-2 S1474667016347152 10.3182/20070903-3-FR-2921.00090 S350 S350.1 HEAD-AND-TAIL 2022-05-24T08:00:36.180846Z 0 0 20070101 20071231 2007 2016-04-20T17:59:56.692034Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate isbn isbns isbnnorm isbnsnorm issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids content subj tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref 1474-6670 14746670 978-3-902823-65-6 9783902823656 false 40 40 15 15 Volume 40, Issue 15 90 529 534 529 534 2007 2007 2007-01-01 2007-12-31 2007 6th IFAC Symposium on Intelligent Autonomous Vehicles article fla Copyright © 2007 IFAC. Published by Elsevier Ltd. All rights reserved. ROBUSTPLACERECOGNITIONWITHINMULTISENSORVIEWSEQUENCESUSINGBERNOULLIMIXTUREMODELS FERREIRA F WITKIN 1983 1019 1023 A PROC8THJOINTCONFERENCEARTIFICIALINTELLIGENCE8THJOINTCONFERENCEARTIFICIALINTELLIGENCE SCALESPACEFILTERING BAKER 1998 S PHDTHESIS DESIGNEVALUATIONFEATUREDETECTORS FERREIRA 2006 F IEEECONFERENCEMULTISENSORFUSIONINTEGRATION INTEGRATIONMULTIPLESENSORSUSINGBINARYFEATURESABERNOULLIMIXTUREMODEL FORNEY 1973 268 278 G GARCIAHERNANDEZ 2004 J PROCLEARNING04ISBN8468884537 BERNOULLIMIXTUREBASEDCLASSIFICATION GONZALEZ 2002 R DIGITALIMAGEPROCESSING HU 1962 179 187 M JUAN 2004 367 370 A KABAN 2000 A KE 2004 869 876 Y MULTIMEDIA04PROCEEDINGS12THANNUALACMINTERNATIONALCONFERENCEMULTIMEDIA EFFICIENTPARTSBASEDNEARDUPLICATESUBIMAGERETRIEVALSYSTEM KOENDERINK 1984 363 370 J KOHAVI 1997 273 324 R LINDEBERG 1994 T SCALESPACETHEORYINCOMPUTERVISION LOWE 2001 682 688 D PROCEEDINGIEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION LOCALFEATUREVIEWCLUSTERINGFOR3DOBJECTRECOGNITION MARSLAND 2001 241 260 S MCLACHLAN 2000 G FINITEMIXTUREMODELS MURASE 1997 375 384 H POPE 2000 149 167 A RABINER 1989 257 286 L SAJAMA 2005 PROCEEDINGS22NDINTERNATIONALCONFERENCEMACHINELEARNING SUPERVISEDDIMENSIONALITYREDUCTIONUSINGMIXTUREMODELS LEE 2000 3505 3511 S PROCCEEDINGSIEEEINTCONFERENCEROBOT LOCALIZATIONBASEDVISIBILITYSECTORSUSINGRANGESENSORS FERREIRAX2007X529 FERREIRAX2007X529X534 FERREIRAX2007X529XF FERREIRAX2007X529X534XF item S1474-6670(16)34715-2 S1474667016347152 10.3182/20070903-3-FR-2921.00090 314898 2016-04-21T10:24:33.026044-04:00 2007-01-01 2007-12-31 true 1342512 MAIN 6 52653 849 656 IMAGE-WEB-PDF 1 ROBUST PLACE RECOGNITION WITHIN MULTI-SENSOR VIEW SEQUENCES USING BERNOULLI MIXTURE MODELS Filipe Ferreira ,1 Vitor Santos Jorge Dias Department Of Mechanical Engineering, University of Aveiro, Portugal Department Of Electronics Engineering and Computer Science, University of Coimbra, Portugal Abstract: This article reports on the use of Hidden Markov Models to improve the results of Localization within a sequence of Sensor Views. Local image features (SIFT) and multiple types of features from a 2D laser range scan are all converted into binary form and integrated into a single, binary, Feature Incidence Matrix (FIM). To reduce the large dimensionality of the binary data, it is modeled in terms of a Bernoulli Mixture providing good results that were reported in an earlier presentation. We have improved the good performance of the approach by incorporating the Bernoulli mixture model inside a Bayesian Network Model, an HMM, that accumulates evidence as the robot travels along the environment. Keywords: Bernoulli Mixture model, Binary data, Expectation Maximisation, Dimensionality reduction, Robot Localization. 1. INTRODUCTION Improving the robustness of localisation is a critical problem in the context of appearance and viewbased localization since the appearance of an environment changes over time. Previous work by the authors(Ferreira et al., 2006) lies at the heart of the place recognition approach presented here. The method can handle a large number of features originating from multiple sensors. After leading the robot, once, through a path in the environment, and allowing it to collect a sequence of [sensor] Views, our method allows the robot to localize itself when it travels through the same stretch of environment(within the original sequence of Views), a second time. The problem is reduced to the alignment of two sequence of views as shown in Fig. 1. We first present examples of some approaches that have used range finders and cameras to perform viewbased localization. Methods that depend on range sensors have used landmark and free-space boundary depictions to represent places. To increasing sensory reliability, many range-sensor based methods extract lines and other primitive features from the range scans. The extraction of lines from the laser scan continues to be a popular approach in the robust segmentation of laser scan data, see (Nguyen et al., 2005) and (Sack and Burgard, 2004) for recent reviews of popular lineextraction algorithms. Other approaches eschew segmentation into simple primitive features and favour the description of the 2D Laser scan in some reduced variable space, such as in (Sooyong Lee, 2000) where each feature extracted from the laser range scan is given a symbol and each scan is described in the form of a string for example mMmMmMmMmDCm, where the string alphabet in this case is (M)axima, (D)iscontinuity, (m)inima, (c)onnection). 1 Partially supported by the EU-BACS FP6-IST-127041 project Current View Sequence (images + scans) ? Sequence of Sampled Views/ Reference View Sequence these intermediate KDTrees would normally require the sorting of SIFT descriptors, and the pairwise comparison (without recourse to a KDTree) of descriptors required to check for duplicate descriptors. Adding a small amount of noise to the SIFT descriptors prior to the creation of the KDTree makes the creation of the KDTrees much faster, enabling the use of SIFT features for continuous image sequences, as described in Algorithm 1 in (Ferreira et al., 2006). Various approaches have been proposed to combine sensors and, given the variety of features-based methods using vision or range scans, the combinations are many (see the bibliography maintained by Keith Price at Place recognition, image retrieval and robot localisation methods (even single sensor platforms) typically make use of large numbers of features whose correlations among each other is unknown. Two principal approaches to feature integration are possible; filter-based and wrapper based. In filterbased methods, physical sensor models are imposed on new data as it comes in. wrapper-based approaches, on the other hand, attempt to facilitate the NP-hard mathematical procedures that are used to approximate the integration of all features simultaneously (Kohavi and John, 1997). Among methods that use the latter approach, some, such as (Newman et al., 2006), employ a distance metric based on the number of repeated features in the entire set of images and a 'Rankreduction' method to identify the important similarities between images. Others such as (Marsland et al., 2001) attempt to 'learn' the features or landmarks that are good and use these for localisation. For place recognition, speech recognition and other procedures that use a large number of features and which seek to explicitly reduce the dimensionality of the 'feature space', Principal Component Analysis (PCA) and more application-specific methods derived from PCA constitute an important class of data-reduction methods. Mixture models are another common solution to modeling data that is believed to follow non-parametric distributions and reducing its dimensionality, (Sajama and Orlitsky, 2005) (McLachlan and Peel, 2000). There is previous work that goes some way to demonstrate the usefulness of binary features (Kaban and Girolami, 2000) (Wang and Kaban, 2005) by modelleing binary data as mixtures of appropriate distribitions. Mixtures of Bernoulli distributions have been used to model data containing binary features, (Juan and Vidal, 2004), (García-Hernández et al., 2004) and (Gonzalez et al., 2001). Converting features into binary form offers significant advantages, the main ones being that binary data can represent both qualitative and categorical data and that this scheme allows us to integrate very disparate variables. Given that we wish to integrate thousands of features, in real-time we look at approximate techniques to reduce the dimensionality of the features. Fig. 1. A schematic description of the Localization problem In our work we have used multiple types of range scan features, namely 1)wall-like (line) features, 2)scan contour (HU) properties and 3)scan region properties. Line segments were extracted using the incremental method (Nguyen et al., 2005). Scan countour features were identified at scan discontinuities and the values of the Hu Moments (Gonzalez and Woods, 2002), (Hu, 1962), for regions around these discontinuities were used as descriptors. A further set of features included some properties of the laser scan including the area within the scan and its distribution around the range scanner. The use of cameras on mobile robots has become widespread over the last few years. Feature extraction for vision-based robots varies from local-image descriptors to global image properties derived over the entire image. Both approaches seek to avoid having to store the entire image itself. In seminal work, Murase and Nayar (Murase and Nayar, 1997), attempted to represent objects in terms of a 'parametric Eigenspace representation'. Among other feature extraction methods, Baker (Baker, 1998), attempts to create a generalised descriptor for local image features and the introduction to his thesis provides a perspective on the development of gradient based methods. The stability and repeatability of points extracted at local Maxima (or Minima) in gradient images that have been repeatedly smoothed using operators, has been known for some time (Koenderink, 1984) (A.P.Witkin, 1983), and research in the field finally culminated in the Scale-Space theory proposed by Lindeberg(Lindeberg, 1994). In work that combined the lessons of Scale-Space with the reliable characterisation of features, Lowe (Lowe, 1999) describes the use of gradient histograms taken at various points close to some point of interest. Since their introduction, SIFT features have been widely applied, among various applications, to object recognition (Pope and Lowe, 2000) (Lowe, 2001), in the panoramic assembly of images (Brown and Lowe, 2003) and in image retrieval (Ke et al., 2004). We typically extract between fifty and two hundred SIFT features per image and have adopted a simple procedure involving the creation of a number of intermediate KDTrees that are created from SIFT features extracted from images obtained as the robot progresses through the environment. The creation of k Feature Vector construction Match and detect presence /absence (Index of sampled views) Camera 2 Place k Feature Categorisation Feature Discretisation Continuous Feature Value feature Frequency Counter calculation (a) Creation of a view of Binary Features.(b) The Feature Incidence Matrix. F(Features) Feature Segmentation Laser Range Scanner Fig. 2. The creation of binary features is performed in different ways for different sensors. We have extracted features, using the methods described above, and converted them into binary form by one of the following 1)matching extracted features against a feature database to detect their presence (or absence), 2)categorising features and 3)discretizing continuous-value features as seen in 2a. We end up with a matrix of binary values Fig. 2b, where each row denotes a particular feature that was extracted from at least one image or laser range scan. Each column represents a place at which an image or scan was obtained. The presence of a 'one' in any column signifies that the feature was observed in an image or laser scan taken at that place. In the next section, we shall briefly review the application of our method to integrate laser range finder and vision features for place-recognition, details of which appear in an earlier publication, (Ferreira et al., 2006). In section 3 we shall present a framework by which the same procedure can be applied to multiple views in the Reference Sequence to improve the robustness of the localization process. Section 4 concludes the article by reviewing the results and providing suggestions for future work. Camera 1 Fig. 3. The Robuter mobile robot platform with two cameras and a Laser Range Finder. tions between features will themselves be different at different parts of the environment, being significant in some regions and, in other regions, being not so significant. In order to capture some of these correlations and make better place-recognition estimates for the view that is currently available, we use a Bernoulli Mixture Model to classify the original, large number of features so that place recognition can be performed, in a smaller dimensional space, where the correlations between sets of features is taken into account. To perform place recognition, the robot is first led through the environment during which the sensors sample the environment, generating a sequence of views, called the Reference Sequence. The record of binary features extracted from each of these views is represented within a Feature Incidence Matrix (FIM), V. Each row i, of the FIM corresponds to a feature Yi and each column j, to an index view, V j , from the Reference Sequence (each entry in the FIM might be represented as Yi,j where the first subscript indicates the feature and the second subscript, the view). Y i,j takes value 1 if feature Y i appears (is visible) in view Vj , 0 otherwise, see Fig. 2. Y1,1 Y1,2 . . . Y1,K Y2,1 Y2,2 . . . Y2,K (1) V= . . .. . . . . .. . . YN,1 YN,2 . . . YN,K Given that features arise in groups and persist/disappear as a result of the structure of the environment, an assumption of independence between the features does not hold. Inferences made by using this assumption would be biased toward certain views in the Reference Sequence as, in practice, some of the features are highly correlated while others are less. To address this problem, the Feature Incidence Matrix (FIM), V is modeled as a Bernoulli Mixture Model where any single View V obs appears as a vector of binary features 2. INTEGRATING LASER AND VISION FEATURES FOR PLACE RECOGNITION Our robot platform is equipped with cameras capable of taking VGA- images and a SICK laser range finder which provides a set of 361 range measurements through a 180 degree interval, Fig 3. Binary features from the Laser range scan are created by classifying the number of extracted lines and their distance from the range scanner, by matching the contour features and by classifying the free, open space within the range scan. In a similar way, in the case of the camera features, each SIFT feature in the KDTree is taken to be a separate binary feature. The use of all the SIFT and LRF features results in a very large number of features, the information from all of which we want to integrate, in order to estimate the position of the robot. Each feature will be correlated, to varying extents, with other features. The correla- {0, 1}D which is obtained from a particular mixture of Bernoulli distributions, as in (2), where denotes the parameters of the distribution of the views that compose our Mixture Model. These parameters include the M component vectors, the i s, and the proportions in which these are mixed, the i s. Each i represents the prior probabilities of the component i in the mixture model, subject to the constraint i i = 1. The likelihood of matching the V obs with each View k in the Reference Sequence can be determined using (3). The Maximum Likelihood Estimation approach is used to obtain the [best] matching view. P (V obs |) = M i=1 M obs |j ) j=1 P (Vk )zki j P (V K M obs | ) j k=1 j=1 P (Vk )zkj j P (V (a) Image 19. (b) Image 77. (c) Image 107. (d) Image 19. (e) Image 77. (f) Image 107. i Pi (V obs |i ) (2) P (V obs = Vk ) = (3) The parameters i s, i s and the Z(the hidden variables) of the Bernoulli Mixture Model and obtained by running the well known Expectation Maximisation (EM) Algorithm. More details can be found in (Ferreira et al., 2006). (g) Scan 19 (h) Scan 77 (i) Scan 107 Fig. 4. Representative images and laser range scans from a sequence taken by Camera 1 (top row), Camera 2 (middle row) and the Laser range finder (bottom row). the parameters of the HMM are expressed as in (4), where N = 2×K corresponds to the number of states, M = K + 1 the total number of possible observations, represents the initial probability on the states, the ij s correspond to the transition probabilities between a pair of states i and j and b i (n) represents the probability of viewing symbol m at state n. An additional, hypothetical, observation is added to the existing observations, i.e. the views of the Reference Sequence. This observation is the most likely observation that can be obtained at any one of the lost places. action t-3 action t-3 action t-3 action t-3 3. ROBUST PLACE RECOGNITION USING HIDDEN MARKOV MODELS The views in a Reference Sequence are taken in real life conditions and could include people moving in the environment and very similar or unchanging stretches of environment. To gain robustness for our place recognition, we have attempted to integrate the information that is available from the matching of multiple Views within the Reference Sequence. This Reference Sequence is modeled as a simple, left-to-right Markov Chain as shown in Fig. 6a. Since we know of only one route that connects each pair of consecutive views, the action/behaviour that is recorded along with each view in the Reference Sequence will take us to the next view and, any other action will take us somewhere else (where, we do not know!). Also, depending on the frequency with which the scans and images are taken during localization, relative to the frequency of sampling in the Reference Sequence, the robot might some times end up inbetween the Views of the Reference Sequence. As a result, the Markov Chain depicted in Fig. 6a will be modified to Fig. 6b, where a Lost_P lace is inserted between every pair of places in the original Reference Sequence. The incomplete, dotted lines represent the state transitions that have not been drawn in order to avoid cluttering the figure. This modified Markov Chain is used as a model for the transition between the 'hidden states' of the Hidden Markov Model, shown in Fig. 5. As a result of this, state t - 3 state t - 2 state t - 1 state t V t-3 V t-2 V t-1 V t Fig. 5. A classic representation of a HMM for localization in a Reference Sequence showing an action that will propel the robot from one view or place to another. = N, M, {i }, {aij }, {bn (m)} bn (LostV iew_m) = (4) 1 (5) K +1 The Viterbi algorithm, a type of Dynamic Programming algorithm, is commonly used in the context P0 P i-1 P i Pi + 1 Pi + 2 PK - 1 (a) Markov Chain for the original Reference Sequence P Lost_0 PLost_i-1 PLost_i+1 P 0 P i-1 P i P i+1 P P i+2 k-1 (a) Cam 1 + Cam 2 + LRF, Single View localisation PLost_i (b) Markov Chain for the Complete Reference Sequence with Lost Places Fig. 6. The original Reference Sequence, at top, is modified to create the complete Reference Sequence, at bottom, by introducing 'lost places' in between original views. of HMMs to determine the most probable sequence of hidden states (Places) that gave rise to a particular sequence of observations (Views)(Forney, 1973), (Rabiner, 1989). Using one hidden state at a time, the Viterbi algorithm calculates all the outcomes that could be possible for that state - and then keeps only the most likely sequence of states. After traversing the length of the HMM, the 'surviving' sequence of places is the sequence that is most likely to have generated the complete sequence of observations. The robot is moved along a stretch of corridor and 2 camera images and a laser range scan obtained at regular intervals. The images and laser scans obtained at three places in the environment are shown in Fig. 4. The robot is then guided along the same stretch of corridor and, once more, acquires images and scans using to perform place recognition against the Reference Sequence. The application of the Bernoulli mixture model to the 2 cameras and a laser range finder was evaluated over the entire path and the posterior probability distribution over all the views in the Reference Sequence is shown in Fig. 7, with and without recourse to HMMs. The results of the application of a plain Bernoulli Mixture model (the posterior probability distribution over all the views in the Reference Sequence) to integrate features from all three sensors, but without using an HMM, is shown in Fig. 7a. No motion model was utilised and the prior probability in (3) was assumed to be uniform(there would be no consistent way of maintaining such a probability without explicit use of an estimation filter). The use of the HMM allows us to modify the prior distribution for Place recognition of subsequent Views. By constraining the positions that the robot can take at any time the place recognition results become much more reliable, as can be seen in Fig. 7b. The experiment was repeated a number of times for the same Reference Sequence. The failed attempts at (b) Cam 1 + Cam 2 + LRF, Localization using an HMM with 5 consecutive Views Fig. 7. Posterior Probability distribution when comparing two sequences. Table 1. Mission Number 1 2 3 4 5 6 Reference Sequence 6 6 6 6 6 6 No-HMM failure. 5 2 5 6 3 3 HMM failure. 2 2 1 1 3 0 Place Recognition are compared, in Table 1, for the cases in which the HMM was used and that in which no HMM was used. As can be seen there are situations in which the HMM was not able to improve on the number of Place-recognition failures mostly because the environments had changed too much since the creation fo the Reference Sequence, because of lighting conditions or because of the presence of people. 4. CONCLUSIONS Robustness in the place recognition has been increased by accumulating evidence from sequential views using a Hidden Markov Model. Place recognition is performed independently for each view using the Bernoulli Mixture model developed earlier. The use of the Hidden Markov Model allows the introduction of a prior probability in the Bernoulli Mixture Model in a consistent way which greatly improves the Place Recognition results and seems to be a promising approach for appearance-based localization methods to deal with dynamic environments. Improvements that must be made include the development of schemes to handles features from sensors with different error models. We need to make modifications to our application of the Bernoulli Mixture Model so that variation of more accurate features appearing in smaller numbers is taken into account. We are also looking at ways to modify the parameters of the HMM in order to improve the probability of correctly detecting the places represented in the Reference Sequence. References A.P.Witkin (1983). Scale-space filtering. In: Proc. 8th Joint conference on Artificial Intelligence. 8th Joint conference on Artificial Intelligence. Karlsruhe, W. Germany. pp. 1019­1023. Baker, Simon (1998). Design and Evaluation of Feature Detectors. PhD thesis. Columbia University. Brown, Matthew and David G. Lowe (2003). Recognising Panoramas. In: Tenth International Conference on Computer Vision (ICCV 2003). Ferreira, Filipe, Vitor Santos and Jorge Dias (2006). Integration of multiple sensors using binary features and a bernoulli mixture model. In: IEEE Conference on Multisensor Fusion and Integration. Heidelberg, Germany. Forney, G. D. (1973). The viterbi algorithm. In: Proceedings of the IEEE. Vol. 61. pp. 268­ 278. García-Hernández, José, Vicent Alabau, Alfons Juan and Enrique Vidal (2004). Bernoulli mixturebased classification. In: Proc. of the LEARNING04, ISBN 84-688-8453-7 (A. R. FigueirasVidal et al., Ed.). Elche (Spain). Gonzalez, J., A. Juan, P. Dupont, E. Vidal and F. Casacuberta (2001). A Bernoulli Mixture Model for Word Categorisation. In: Symposium Nacional de Reconocimiento de Formas y Analises de Imagenes. Gonzalez, R. C. and R. E. Woods (2002). Digital Image Processing. Addison-Wesley Pub. Co. Hu, Ming-Kuei (1962). Visual pattern recognition by moment invariants. IEEE Transactions on Information Theory 8(2), 179­ 187. Juan, Alfons and Enrique Vidal (2004). Bernoulli Mixture Models for Binary Images. In: International Conference on Pattern Recognition (ICPR'04). Vol. 3. pp. 367­370. Kaban, Ata and Mark Girolami (2000). Initialized and Guided EM-Clustering of Sparse Binary Data with Application to Text Based Documents. In: Proceedings of the 15th International Conference on Pattern Recognition (ICPR'00). Vol. 2. Ke, Yan, Rahul Sukthankar and Larry Huston (2004). An efficient parts-based near-duplicate and subimage retrieval system. In: MULTIMEDIA '04: Proceedings of the 12th annual ACM international conference on Multimedia. ACM Press. New York, NY, USA. pp. 869­876. Koenderink, Jan J (1984). The Structure Of Images. Biological Cybernetics 50(5), 363­370. Kohavi, Ron and George H. John (1997). Wrappers for feature subset selection. Artificial Intelligence 97(1-2), 273­324. Lindeberg, Tony (1994). Scale-Space Theory in Computer Vision. Kluwer Academic Press. Lowe, David G. (1999). Object Recognition from Local Scale-Invariant Features. In: Proc. of the International Conference on Computer Vision, Corfu. pp. 1150­1157. Lowe, David G. (2001). Local Feature View Clustering for 3D Object Recognition. In: Proceeding of the IEEE Conference on Computer Vision and Pattern Recognition. Kauai, Hawaii. pp. 682­ 688. Marsland, Stephen, Ulrich Nehmzow and Tom Duckett (2001). Learning to select distinctive landmarks for mobile robot navigation. Robotics and Autonomous Systems 37, 241­260. McLachlan, Geoffrey and David Peel (2000). Finite Mixture Models. John Wiley and Sons. Murase, Hiroshi and Shree K. Nayar (1997). Detection of 3d objects in cluttered scenes using hierarchical eigenspace. Pattern Recognition Letters 18, 375­384. Newman, P., D. Cole and K. Ho (2006). Outdoor slam using visual appearance and laser ranging. In: ICRA 06. Nguyen, Viet, Agostino Martinelli, Nicola Tomatis and Roland Siegwart (2005). A comparison of line extraction algorithms using 2d laser rangefinder for indoor mobile robotics. In: International Conference on Intelligent Robots and Systems. Pope, Arthur and David G. Lowe (2000). Probabilistic Models of Appearance for 3-D Object Recognition. IJCV 40(2), 149 ­ 167. Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE 77, 257­ 286. Sack, Daniel and Wolfram Burgard (2004). A comparison of methods for line extraction from range data. In: IAV 2004. Sajama and Alon Orlitsky (2005). Supervised dimensionality reduction using mixture models. In: Proceedings of the 22 nd International Conference on Machine Learning. Bonn, Germany. Sooyong Lee, Nancy M. Amato, James Fellers (2000). Localization based on Visibility Sectors using Range Sensors. In: Procceedings of the IEEE Int. Conference Robot. Autom. (ICRA),. pp. 3505­ 3511. Wang, Xin and Ata Kaban (2005). Finding Uninformative Features in Binary Data. In: Sixth International Conference on Intelligent Data Engineering and Automated Learning (IDEAL '05). ognising Panoramas. In: Tenth International Conference on Computer Vision (ICCV 2003). Ferreira, Filipe, Vitor Santos and Jorge Dias (2006). Integration of multiple sensors using binary features and a bernoulli mixture model. In: IEEE Conference on Multisensor Fusion and Integration. Heidelberg, Germany. Forney, G. D. (1973). The viterbi algorithm. In: Proceedings of the IEEE. Vol. 61. pp. 268­ 278. García-Hernández, José, Vicent Alabau, Alfons Juan and Enrique Vidal (2004). Bernoulli mixturebased classification. In: Proc. of the LEARNING04, ISBN 84-688-8453-7 (A. R. FigueirasVidal et al., Ed.). Elche (Spain). Gonzalez, J., A. Juan, P. Dupont, E. Vidal and F. Casacuberta (2001). A Bernoulli Mixture Model for Word Categorisation. In: Symposium Nacional de Reconocimiento de Formas y Analises de Imagenes. Gonzalez, R. C. and R. E. Woods (2002). Digital Image Processing. Addison-Wesley Pub. Co. Hu, Ming-Kuei (1962). Visual pattern recognition by moment invariants. IEEE Transactions on Information Theory 8(2), 179­ 187. Juan, Alfons and Enrique Vidal (2004). Bernoulli Mixture Models for Binary Images. In: International Conference on Pattern Recognition (ICPR'04). Vol. 3. pp. 367­370. Kaban, Ata and Mark Girolami (2000). Initialized and Guided EM-Clustering of Sparse Binary Data with Application to Text Based Documents. In: Proceedings of the 15th International Conference on Pattern Recognition (ICPR'00). Vol. 2. Ke, Yan, Rahul Sukthankar and Larry Huston (2004). An efficient parts-based near-duplicate and subimage retrieval system. In: MULTIMEDIA '04: Proceedings of the 12th annual ACM international conference on Multimedia. ACM Press. New York, NY, USA. pp. 869­876. Koenderink, Jan J (1984). The Structure Of Images. Biological Cybernetics 50(5), 363­370. Kohavi, Ron and George H. John (1997). Wrappers for feature subset selection. Artificial Intelligence 97(1-2), 273­324. Lindeberg, Tony (1994). Scale-Space Theory in Computer Vision. Kluwer Academic Press. Lowe, David G. (1999). Object Recognition from Local Scale-Invariant Features. In: Proc. of the International Conference on Computer Vision, Corfu. pp. 1150­1157. Lowe, David G. (2001). Local Feature View Clustering for 3D Object Recognition. In: Proceeding of the IEEE Conference on Computer Vision and Pattern Recognition. Kauai, Hawaii. pp. 682­ 688. Marsland, Stephen, Ulrich Nehmzow and Tom Duckett (2001). Learning to select distinctive landmarks for mobile robot navigation. Robotics and Autonomous Systems 37, 241­260. McLachlan, Geoffrey and David Peel (2000). Finite Mixture Models. John Wiley and Sons. Murase, Hiroshi and Shree K. Nayar (1997). Detection of 3d objects in cluttered scenes using hierarchical eigenspace. Pattern Recognition Letters 18, 375­384. Newman, P., D. Cole and K. Ho (2006). Outdoor slam using visual appearance and laser ranging. In: ICRA 06. Nguyen, Viet, Agostino Martinelli, Nicola Tomatis and Roland Siegwart (2005). A comparison of line extraction algorithms using 2d IPV 34715 S1474-6670(16)34715-2 10.3182/20070903-3-FR-2921.00090 IFAC ROBUST PLACE RECOGNITION WITHIN MULTI-SENSOR VIEW SEQUENCES USING BERNOULLI MIXTURE MODELS Filipe Ferreira * 1 Vitor Santos * Jorge Dias ** * Department Of Mechanical Engineering, University of Aveiro, Portugal Department Of Mechanical Engineering University of Aveiro Portugal ** Department Of Electronics Engineering and Computer Science, University of Coimbra, Portugal Department Of Electronics Engineering and Computer Science University of Coimbra Portugal 1 Partially supported by the EU-BACS FP6-IST-127041 project This article reports on the use of Hidden Markov Models to improve the results of Localization within a sequence of Sensor Views. Local image features (SIFT) and multiple types of features from a 2D laser range scan are all converted into binary form and integrated into a single, binary, Feature Incidence Matrix (FIM). To reduce the large dimensionality of the binary data, it is modeled in terms of a Bernoulli Mixture providing good results that were reported in an earlier presentation. We have improved the good performance of the approach by incorporating the Bernoulli mixture model inside a Bayesian Network Model, an HMM, that accumulates evidence as the robot travels along the environment. Keywords Bernoulli Mixture model Binary data Expectation Maximisation Dimensionality reduction Robot Localization References Witkin, 1983 A.P. Witkin Scale-space filtering Proc. 8th Joint conference on Artificial Intelligence. 8th Joint conference on Artificial Intelligence 1983 Karlsruhe, W. Germany Karlsruhe, W. Germany 1019 1023 Baker, 1998 Baker Simon Design and Evaluation of Feature Detectors PhD thesis 1998 Columbia University Ferreira, 2006 Brown, Matthew and David G. Lowe (2003). Recognising Panoramas. In: Tenth International Conference on Computer Vision (ICCV 2003). 4 Ferreira Filipe Vitor Santos Jorge Dias Integration of multiple sensors using binary features and a bernoulli mixture model IEEE Conference on Multisensor Fusion and Integration 2006 Heidelberg, Germany Heidelberg, Germany Forney, 1973 Forney G.D. The viterbi algorithm Proceedings of the IEEE 61 1973 268 278 García-Hernández, 2004 García-Hernández José Vicent Alabau Alfons Juan Enrique Vidal Bernoulli mixture-based classification A.R. Figueiras-Vidal Proc. of the LEARN- ING04, ISBN 84-688-8453-7 2004 Elche (Spain) Gonzalez, 2002 Gonzalez, J., A. Juan, P. Dupont, E. Vidal and F. Casacuberta (2001). A Bernoulli Mixture Model for Word Categorisation. In: Symposium Nacional de Reconocimiento de Formas y Analises de Imagenes. 8 Gonzalez R.C. R.E. Woods Digital Image Processing 2002 Addison-Wesley Pub. Co Hu, 1962 Hu Ming-Kuei Visual pattern recognition by moment invariants IEEE Transactions on Information Theory 8 2 1962 179 187 Juan, 2004 Juan Alfons Enrique Vidal Bernoulli Mixture Models for Binary Images International Conference on Pattern Recognition (ICPR'04) 3 2004 367 370 Kaban, 2000 Kaban Ata Mark Girolami Initialized and Guided EM-Clustering of Sparse Binary Data with Application to Text Based Documents Proceedings of the 15th International Conference on Pattern Recognition (ICPR'00) 2 2000 Ke, 2004 Ke Yan Rahul Sukthankar Larry Huston An efficient parts-based near-duplicate and sub-image retrieval system MULTIMEDIA '04: Proceedings of the 12th annual ACM international conference on Multimedia 2004 ACM Press New York, NY, USA 869 876 Koenderink, 1984 Koenderink Jan J The Structure Of Images Biological Cybernetics 50 5 1984 363 370 Kohavi, 1997 Kohavi Ron George H. John Wrappers for feature subset selection Artificial Intelligence 97 1-2 1997 273 324 Lindeberg, 1994 Lindeberg Tony Scale-Space Theory in Computer Vision 1994 Kluwer Academic Press Lowe, 2001 Lowe, David G. (1999). Object Recognition from Local Scale-Invariant Features. In: Proc. of the International Conference on Computer Vision, Corfu. pp. 1150- 1157. 17 Lowe David G. Local Feature View Clustering for 3D Object Recognition Proceeding of the IEEE Conference on Computer Vision and Pattern Recognition 2001 Kauai, Hawaii 682 688 Marsland, 2001 Marsland Stephen Ulrich Nehmzow Tom Duckett Learning to select distinctive landmarks for mobile robot navigation Robotics and Autonomous Systems 37 2001 241 260 McLachlan, 2000 McLachlan Geoffrey David Peel Finite Mixture Models 2000 John Wiley and Sons Murase, 1997 Murase Hiroshi Shree K. Nayar Detection of 3d objects in cluttered scenes using hierarchical eigenspace Pattern Recognition Letters 18 1997 375 384 Pope, 2000 Newman, P., D. Cole and K. Ho (2006). Outdoor slam using visual appearance and laser ranging. In: ICRA 06. 22 Nguyen, Viet, Agostino Martinelli, Nicola Tomatis and Roland Siegwart (2005). A comparison of line extraction algorithms using 2d laser rangefinder for indoor mobile robotics. In: International Conference on Intelligent Robots and Systems. 23 Pope Arthur David G. Lowe Probabilistic Models of Appearance for 3-D Object Recognition IJCV 40 2 2000 149 167 Rabiner, 1989 Rabiner L.R. A tutorial on hidden markov models and selected applications in speech recognition Proceedings of the IEEE 77 1989 257 286 Sajama, 2005 Sack, Daniel and Wolfram Burgard (2004). A comparison of methods for line extraction from range data. In: IAV 2004. 26 Sajama Alon Orlitsky Supervised dimensionality reduction using mixture models Proceedings of the 22 nd International Conference on Machine Learning 2005 Bonn, Germany Bonn, Germany Lee, 2000 Sooyong Lee Nancy M. Amato James Fellers Localization based on Visibility Sectors using Range Sensors Procceedings of the IEEE Int. Conference Robot 2000 Autom. (ICRA) 3505 3511 Wang, 2005 Wang, Xin and Ata Kaban (2005). Finding Uninformative Features in Binary Data. In: Sixth International Conference on Intelligent Data Engineering and Automated Learning (IDEAL '05). "
    },
    {
        "doc_title": "Local-level control of a humanoid robot prototype with force-driven balance",
        "doc_scopus_id": "67649674678",
        "doc_doi": "10.1109/ICHR.2007.4813925",
        "doc_eid": "2-s2.0-67649674678",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Adaptive behaviour",
            "Advanced control algorithms",
            "Coordinated motion",
            "Distributed architecture",
            "Force interaction",
            "Hardware experiment",
            "Humanoid robotics",
            "Level controllers"
        ],
        "doc_abstract": "The recent trend of humanoid robotics research has been deeply influenced by concepts such as distributed architectures, local control, force interaction and emergence of coordinated motions. A hypothesis is that feedback control from several sensors, such as force sensors and inertial devices, and more advanced control algorithms will be a key issue for the next developments. In this paper, we discuss how these concepts have been applied to a custom low-cost humanoid platform developed for research purposes. The humanoid robot is equipped with a rich set of sensors enabling the evaluation of simple feedback rules used online to control the robot. A great focus has been given to a force-driven controller based on the Jacobian transpose. A kind of intermediate local-level controller is implemented based on force sensing, providing robust and adaptive behaviour. The proposed ideas and concepts are introduced and validated in several hardware experiments. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integration of multiple sensors using binary features in a bernoulli mixture model",
        "doc_scopus_id": "40949089299",
        "doc_doi": "10.1109/MFI.2006.265672",
        "doc_eid": "2-s2.0-40949089299",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Bernoulli mixture models",
            "Binary features",
            "Multiple sensors"
        ],
        "doc_abstract": "This article reports on the use of a Bernoulli Mixture model to integrate features extracted independently from two or more distinct sensors. Local image features(SIFT) and multiple types of features from a 2D laser range scan are all converted into Binary form and integrated into a single binary Feature Incidence Matrix(FIM). The correlation between the different features is captured by modeling the resultant FIM in terms of a Bernoulli Mixture Model. The integration of binary features from different sensors allows for good place recognition. The use of binary features also promises a much simpler integration of features from dissimilar sensors. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and low-level control of a humanoid robot using a distributed architecture approach",
        "doc_scopus_id": "33751556042",
        "doc_doi": "10.1177/1077546306070592",
        "doc_eid": "2-s2.0-33751556042",
        "doc_date": "2006-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Aerospace Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2202"
            },
            {
                "area_name": "Mechanics of Materials",
                "area_abbreviation": "ENGI",
                "area_code": "2211"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            }
        ],
        "doc_keywords": [
            "Distributed control",
            "Humanoid robots",
            "Modular architectures",
            "Servomotor control"
        ],
        "doc_abstract": "This article describes methods and strategies used to develop a humanoid robot with a distributed architecture approach where centralized and local control co-exist and concur to provide robust full monitoring and efficient control of a complex system with 22 DOF. A description of the hardware is given before introducing the architecture, since that greatly influences the methods implemented for the control systems and helps in understanding the general decisions. The platform is still undergoing improvement, but the results are very promising, mainly because many potential approaches and research issues have presented themselves and will provide opportunities to test distributed control systems with possibilities that go far beyond the classical control of robots. Some practical issues of servomotor control are also considered since that turned out to be necessary before implementing higher levels of control-these are, in turn, addressed in the last part the article, which gives an example to demonstrate the possibility of keeping a humanoid robot in an upright balanced position using only local control after reaction forces on the ground. © 2006 SAGE Publications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D reconstruction of real world scenes using a low-cost 3D range scanner",
        "doc_scopus_id": "33746696563",
        "doc_doi": "10.1111/j.1467-8667.2006.00453.x",
        "doc_eid": "2-s2.0-33746696563",
        "doc_date": "2006-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            }
        ],
        "doc_keywords": [
            "Laser scanner",
            "Textured polygonal models",
            "Three dimensional modeling",
            "Three dimensional reconstruction"
        ],
        "doc_abstract": "This article presents a 3D reconstruction technique for real world environments based on a traditional 2D laser range finder modified to implement a 3D laser scanner. The article describes the mechanical and control issues addressed to physically achieve the 3D sensor used to acquire the data. It also presents the techniques used to process and merge range and intensity data to create textured polygonal models and illustrates the potential of such a unit. The result is a promising system for 3D modeling of real world scenes at a commercial price 10 or 20 times lower than current commercial 3D laser scanners. The use of such a system can simplify measurements of existing buildings and produce easily 3D models and ortophotos of existing structures with minimum effort and at an affordable price. © 2006 Computer-Aided Civil and Infrastructure Engineering.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Development of a low-cost humanoid robot: Components and technological solutions",
        "doc_scopus_id": "84881164437",
        "doc_doi": "10.1007/3-540-26415-9_50",
        "doc_eid": "2-s2.0-84881164437",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Autonomous platforms",
            "Distributed control",
            "Distributed control architectures",
            "Engineering methods",
            "Force sensor",
            "Humanoid robot",
            "Off-the-shelf technologies",
            "Technological solution"
        ],
        "doc_abstract": "The paper presents a set of solutions to build a humanoid robot at reduced costs using off-the-shelf technology, but still aiming at a fully autonomous platform for research. The main scope of this project is to have a working prototype capable of participating in the RoboCup humanoid league, and to offer opportunities for under and pos-graduate students to apply engineering methods and techniques in such an ambitious and overwhelming endeavor. The most relevant achievements on this implementation include the distributed control architecture, based on a CAN network, and the modularity at the system level. These features allow for localized control capabilities, based both on global and local feedback from several sensors, ranging from joint position monitoring to force sensors. Force sensors on the feet were designed and integrated using strain gauges properly calibrated and electrically conditioned. Although some issues are yet to be completed, the stage of development is already enough for practical experiments and to obtain positive conclusions about the solutions proposed. © 2006 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Motion planning and control strategies for a distributed architecture humanoid robot",
        "doc_scopus_id": "80051583080",
        "doc_doi": "10.3182/20060906-3-it-2910.00129",
        "doc_eid": "2-s2.0-80051583080",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Biped walking",
            "Control parameters",
            "Distributed architecture",
            "Distributed control architectures",
            "Force interaction",
            "Fractional-order controllers",
            "Legged robots",
            "Trajectory Planning"
        ],
        "doc_abstract": "This paper describes motion-control algorithms for a humanoid robot based on distributed control architecture. Towards its implementation, two different approaches based on positional and force control algorithms are revisited. At the same time, a new design procedure is proposed that consists of a fractional-order controller combined with a genetic algorithm for optimal tuning of the control parameters. The control algorithms are tested through several computer simulations and its robustness is discussed. Copyright © 2006 IFAC.",
        "available": true,
        "clean_text": "serial JL 314898 291210 291718 291882 291883 31 IFAC Proceedings Volumes IFACPROCEEDINGSVOLUMES 2016-04-23 2016-04-23 2016-04-23 2016-04-23 2016-04-23T09:02:33 S1474-6670(16)38608-6 S1474667016386086 10.3182/20060906-3-IT-2910.00129 S350 S350.1 HEAD-AND-TAIL 2022-05-30T11:56:57.326567Z 0 0 20060101 20061231 2006 2016-04-23T09:18:07.111766Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate isbn isbns isbnnorm isbnsnorm issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1474-6670 14746670 978-3-902661-16-6 9783902661166 false 39 39 15 15 Volume 39, Issue 15 129 773 778 773 778 2006 2006 2006-01-01 2006-12-31 2006 8th IFAC Symposium on Robot Control article fla Copyright © 2006 IFAC. Published by Elsevier Ltd. All rights reserved. MOTIONPLANNINGCONTROLSTRATEGIESFORADISTRIBUTEDARCHITECTUREHUMANOIDROBOT PUGA J HIRAI 1998 1321 1326 K YAMAGUCHI 1999 368 374 J PARK 2000 3353 3358 J SUGIHARA 2002 1404 1409 T KAJITA 2003 1644 1650 S VUKOBRATOVIC 1990 M BIPEDLOCOMOTIONDYNAMICSSTABILITYCONTROLAPPLICATION KAJITA 1996 13 19 S FUJIMOTO 1998 2030 2035 Y PARK 2001 870 882 J OLDHAM 1974 K FRACTIONALCALCULUS MILLER 1993 K INTRODUCTIONFRACTIONALCALCULUSFRACTIONALDIFFERENTIALEQUATIONS TENREIRO 1997 107 122 J PUGAX2006X773 PUGAX2006X773X778 PUGAX2006X773XJ PUGAX2006X773X778XJ item S1474-6670(16)38608-6 S1474667016386086 10.3182/20060906-3-IT-2910.00129 314898 2016-04-23T04:18:07.111766-04:00 2006-01-01 2006-12-31 true 2019772 MAIN 6 53810 849 656 IMAGE-WEB-PDF 1 MOTION PLANNING AND CONTROL STRATEGIES FOR A DISTRIBUTED ARCHITECTURE HUMANOID ROBOT José R. T. Puga1, Filipe M. T. Silva2, Vítor M. F. Santos3, (1) Polytechnic Institute of Porto, 4200-072, Porto, Portugal (2,3) University of Aveiro, 3810-193 Aveiro, Portugal, Abstract: This paper describes motion-control algorithms for a humanoid robot based on distributed control architecture. Towards its implementation, two different approaches based on positional and force control algorithms are revisited. At the same time, a new design procedure is proposed that consists of a fractional-order controller combined with a genetic algorithm for optimal tuning of the control parameters. The control algorithms are tested through several computer simulations and its robustness is discussed. Copyright © 2006 IFAC Keywords: 5­10 Legged robots, biped walking, trajectory planning, COG control, force interaction control. 1. INTRODUCTION Nowadays, there is an enormous interest around the world in humanoid robots, both from the industry and the academic communities. Much work has been conducted in the past to formulate planning and control algorithms for biped walking. Current works in motion generation fall largely into two groups: trajectory replaying and realtime generation. The former is characterised by pre-planned trajectories that are then played back during walking and modified online through feedback (K. Hirai et al, 1999; J-I. Yamaguchi et al, 1999; J.H. Park, 2000). By contrast, the later generates a trajectory in realtime, feeding back the present state of the system in accordance with the pre-provided goal of the motion (T. Sugihara, 2002; S. Kajita, et al, 2003). With this method, planning and control are executed in a unified way, although requiring a larger amount of computation power. One of the most prominent schemes used to enhance trajectory tracking controllers and to analyse their stability is the socalled Zero Moment Point (ZMP) criterion (M. Vukobratovic et al, 1990). Instead of these timedependent algorithms, other research groups use timeinvariant control schemes based on heuristics and simple feedback rules (J. Pratt and G. Pratt, 1998; S. Kajita, K. Tani, 1996). This paper focuses on the realtime motion generation and the application of a localised fractional order controller for biped locomotion. The most relevant feature of this implementation includes the distributed architecture in which independent and self-contained control units may allow for localized control based on feedback from several sensors. The question is how to implement the local controllers in order to achieve both high mobility and stability. The approach followed in this paper consists of studying a simple model, but keeping enough complexity to allow a comparative evaluation of two distinctive control principles. One algorithm based on the COG Jacobian and other based on the ground reaction forces with a fractional-order controller tuned using a genetic algorithm are studied. Section 2 describes the design and the technological solutions in building the humanoid platform. Section 3 presents the planning and control algorithms. Section 4 proposes the application of fractional-order controllers to enhance the system's performance. Section 5 compares how the principles of previous sections afford the tradeoffs between mobility and stability. Section 6 concludes the paper and outlines the perspectives towards future research. 2. FRAMEWORK: THE HUMANOID PLATFORM The main goal of the project beneath this paper has been the development of a small-size humanoid platform to carry out research on control, navigation and perception in such an ambitious and overwhelming endeavour. Moreover, the authors sought a platform with enhanced flexibility, namely at the level of the system control. 2.1 Mechanical Design The platform has 22 DOF with 12 of them dedicated to the legs, which represent the most challenging part both for designing and control. The structure is made essentially of aluminium and steel for axles and other small components, weights about 6 kg and is about 60 cm tall. Fig. 1 shows the platform. units of sensing, processing and acting capabilities play a key role to allow for localized control based on feedback from several sensors, ranging from joint position monitoring to force sensors. Fig. 2 shows a generic diagram of the architecture control units. Main control Sensorial data Control signals dt MASTER SLAVES Arm Hip SCU6 SCU2 SCU7 SCU3 Ankle and knee SCU4 SCU5 Foot sensors Fig. 2. Simplified diagram of the distributed control architecture. Without loss of generality, some motion joints have been grouped (e.g., entire foot and knee) by vicinity criteria and they are controlled locally by a dedicated board based on a PIC microcontroller. A detailed description of the technical and engineering solutions can be found elsewhere (V. Santos, F. Silva, 2005). 3. PLANNING AND CONTROL: A LOCAL APPROACH Fig. 1. Front view of the humanoid robot. 2.2 Actuators and Sensors Currently, the system actuators are 22 servomotors of three different types according to torque requirements of the several joints: more power on legs and less power on neck and arms. The advantageous trade-off involving cost, power, dimensions and availability explain the option for servomotors. However, dedicated motor control boards proved essential to achieve an accurate control (position, velocity and torque). Perception assumes also a major role in all systems intended to be autonomous. For this platform several perception devices was planned: joint position; joint motor current; force sensors on the feet; inclination of some links; angular velocity of some links and a vision unit. Up to now, only vision has not yet been approached. The remainder sensors were addressed with different levels of accuracy, but all potentially usable with current hardware. 2.3 Computing System The most relevant feature of this implementation include the distributed architecture, based on a CAN bus, in which independent and self-contained control units may allow either a cooperative or a standalone operation. The integration in these simpler control Biped robots exhibit complex dynamic phenomena that make difficult their analysis and control. One solution is to closely relate the motion planning and control problems. In this section, two commonly used approaches are revisited: balance control by manipulating the CoM and control based on the interaction forces (direct force control). A dominant theme of work in realtime motion generation is their flexibility in the sense of both strong tunability and multi-functionality. We raise the question to what extent the control algorithms formalised below are able to absorb and compensate disturbances and to what extent the disturbances act on the movement dynamics. The open challenge is to allow local controllers to perform actuator control based on sensor feedback and a general directive. As it appears, it is up to the degrees of freedom nearest to the ground ­ ankle and knee ­ to assure the mobility and stability of the system, and to the degrees of freedom more distant from the ground ­ hip and trunk ­ the main role of compensation. For instance, supposing that the global order is to keep balance in an upright position, although all actuators can intervene, the ankle and knee joints have a relevant role to keep balance. It is important to point out that sensory feedback to the central unit may play a role in this process, and it is not surprising that the system shows most strongly a level of movement planning that is little affected by disturbances. 3.1 Simple Model For the present purposes, we restrict our attention to a simple model that keeps enough complexity by considering the unilateral constraints forces at the foot. The robot model attempted is a kinematics chain consisting of a planar two-DOF leg with a maximum height of L = 33cm and a total mass of M = 5kg (see Table I). Table I ­ Robot and environment parameters. Robot Link Thigh Shank Foot Mass (kg) 4.0 0.6 0.4 Length li (m) 0.165 0.165 0.12 ri (m) 0.09 0.12 0.04 Spring-damper model Kx (N/m) 50.0×103 Ky (N/m) 200.0×10 3 T position of the COG, respectively, the matrix J G is the transpose of the COG Jacobian, and K G is a constant positive gain. Second, it is possible to define a desired joint trajectory that gives rise to the following term: Jref = K J ( ref - ) ref (2) Bx (Ns/m) 1000.0 By (Ns/m) 250.0 and are the reference and the real joint where position, respectively. K J is a constant positive gain. The last equations integrate in a hierarchical way the dynamics associated with the COG and with the desired joint trajectories, such as: ref ref = COG + Jref e ref - K h COG (3) Robotic System It is, also, assumed the existence of two actuators (ankle and knee) and two contact points where the force sensors are inserted. The contact of the foot with the constraint surface is modelled through a linear spring-damper system in the horizontal and vertical directions (Fig. 3). Further, the friction is assumed to be large enough to avoid any foot's slippage. mt force sensor y x Task Description CoM Balance Control Joint Motion References + Motion Goals: - Joint trajectory - CoM trajectory Position Controller Biped Robot CoG Motion References Fig. 4. Blocks diagram of the COG Balance Control. lt force sensor support foot Bx Kx 3.3 Force Interaction Control The difficult relation between planning and stability has justified a different line of thought where the skill of locomotion emerges from the physical interaction between the machine and the environment itself (Y. Fujimoto et al, 1998; J. Park, 2001). In the following, we propose a control algorithm based on simple motion goals taking into account the interaction forces between the foot and the ground (Fig. 5). The relevant aspect of the proposed algorithm is the consideration of the ground reaction forces as control inputs for regulating the desired motion goals. Here, the two variables to be controlled are the normal reaction forces across the heel and at the toe. The reference signals are generated automatically in result of the imposed motion goals. For the present purposes, the desired normal force is calculated from the errors in the vertical hip pose using a linear controller: d d f nd = BW + K pf yhip - yhip + K vf yhip - yhip Bx By ground Ky Ky By Kx Detailed view of the foot-ground contact model ms ls Y X lf Fig. 3. Planar 2-dof foot-leg model and constraint surface. 3.2 COG Balance Control A question that may bring up is to know how to conciliate two imperatives ­ mobility and stability ­ that in many circumstances are contradictories. On one hand, whenever the mobility goals can be achieved, it is advantageous to maximize the stability. On the other hand, it may be necessary to sacrifice the mobility goals (general directives) to assure the dynamic stability. In this line of thought, the other algorithm can be divided in two steps ( Fig. 4). First, the COG Jacobian is used to obtain a relation between the velocity of the centre of gravity, COG , and the joint velocities, . In order to reduce the computation time, this relation is written using the transpose of the Jacobian, as follows: (1) x G are the reference and the real and ref COG ( ) ( ) (4) and K vf where BW is the total system's weight, f Kp are appropriate constant feedback gains. On the other hand, the desired location of the centre of pressure (COP) is calculated as function of the horizontal centre of gravity (COG), xG , as follows: ref ref COP d = K COP xG - xG + KvCOP xG - xG p = J KG x T G ( ref G - xG ) ( ) ( ) (5) where ref xG This means that the reference COP is actively used to calculate the distribution of the total reaction force along the extremities of the support foot. These are the variables that some force control algorithm must follow. Finally, the output signals obtained for each extremity of the foot are then transformed into joint torques by using the transpose of the Jacobian matrices. In other words, the stance leg \"feels\" the forces while the controller distributes them as driving torques in order to regulate the desired high level directive. This view has important consequences because the system is able to adapt its behaviour in face of changing conditions (e.g., environment parameters or additional load) and under perturbation (e.g., external forces). Task Description Force Interaction Control ref ref f heel , f toe The central contribution of this paper emerges when answering two questions. The first is related with the basic control actions: why not to extend them in a continuous way? Most of the time-dependent algorithms involve the tracking of some computed trajectories using a PID control law. It is important to realize that all these types of controllers are particular cases of the PI D µ controller. In general form, its transfer function is given by: H ( s ) = K + K s + K µ s µ (9) Robotic System where the pair ( K i , i ) represents the control gain and the non-integer order. The central idea behind the use of a fractional controller is illustrated in Fig. 6. For the present purposes, the control laws are designed independently: (i) a fractional order controller is selected for the inner control loop; and (ii) position control for the outer control loops consist of PD laws whose parameters are selected by trialand-error. In the present study the design procedure involves the parameters of a fractional controller with three terms: C P = K Motion Goals: - Hip trajectory - CoM trajectory Force Virtual References Force Controller Biped Robot f heel , f toe Force Sensor/ Environment Fig. 5. Blocks diagram of the Force Interaction Control. 4. FRACTIONAL-ORDER CONTROLLER Fractional calculus is a collection of relatively little-known mathematical results concerning the generalization of the ordinary differentiation and integration to non-integer order. While the subject dates back to the origins of calculus, they have until recently found little application as a scientific and engineering tool. This situation is beginning to change and there are a growing number of research areas that employ fractional calculus, such as: biology; electronics; signal processing; modelling and control theory (K.B. Oldham et al, 1974; K.S. Miller; J.A. Tenreiro Machado 1997). First, we give an overview of the fundamentals of fractional calculus and its application in control. Based on these results, we present a non-integer order controller and its tuning by a genetic algorithm. One of the several definitions of non-integer order derivatives is due to the work of Grünwald and Letnikoff: D f (t ) = lim k =0 ( K , ) ( K µ , µ ) (10) Given the additional fit parameter, the second question remains: how to select the scheme and the parameters of the controller? It can be expected that fractional order controllers may achieve better performance and robustness results when compared with conventional ones. However, the selection and tuning of the parameters (i.e., control gains and noninteger orders) is not always straightforward. In this paper, a genetic algorithm was preferred over other randomized search methods (F. Silva, V. Santos, 2005). µ PID PD 1 PI -1 P O 1 h ( -1 ) k ( + 1 ) f ( t - k h ) , ( k + 1 ) ( - k + 1 ) (6) k =0 + Fig. 6. Generalisation of a PID controller. where ( ) is the gamma function. In order to compute a non-integer order derivative of a sampled signal, definition (6) can be approximated such that the total sum is truncated after a finite number of terms and the time-step h is approximated by the sampling time T ( t = n T ): D f (t ) 1 T 5. SIMULATION RESULTS In order to verify the effectiveness of the proposed controllers several simulations are carried out. The results displayed below are all based on the following path: the double inverted pendulum is standing, moves down and up again to the initial posture. The initial state is set to ( xhip , yhip ) = ( -0.039, 0.31) and the desired xG should be zero along the motion. In the following, we discuss the controller's design, the properties and relationship between both control strategies, as well as the behaviour of the system under perturbation. f (t - k T ) k k =0 n (7) n where the coefficients k can be calculated iteratively using the expression: 0 =1 k = 1 - +1 k -1 , k = 1, 2,3, k (8) 5.2 Behaviour under Perturbation The controller's performance is evaluated by applying two unpredicted perturbations. The first perturbation corresponds to a horizontal force of +10 N applied at the hip during 20 ms. Another common disturbance experienced by a walking robot is a change on body mass. An external virtual load of 10% of the body mass is add instantaneously on the hip at a predefined instant of time (4.25 s). For comparative purposes, the simulation results for level ground include the control approaches proposed in Section 3. The present paper restricts the attention to the single support phase, but ignoring the impact and the transfer of support. From the results presented in Fig. 8 and Fig. 9, a few remarks ought to be made. First, the results obtained provide an intuitive understanding of the postural recovery. The results illustrate the capability of the system to adapt to external forces and to changes in the body mass. The second observation is that the system's behaviour is similar. The force and the COG controllers are effectives to generate the desired motion, while the COP remains inside the support covered by the stance foot. Further, the controllers achieve the desired height profile, while maintaining an adaptive control solution in face of external disturbances. At this stage, the trade-off for each implementation resumes to the feedback information needed. However, it must be pointed out the apparent differences between position control and force control remains for unknown terrain profiles and whenever impacts occur. In conclusion, the proposed algorithms seem to be well-adapted to the regulation of the basic requirements proposed above. Fig. 7. Snapshots of the computer simulation. 5.1 Controller's Optimization We first consider the controller's design and discuss how the fractional order controller enhances the overall performance. In the simulations below, it was decided to adopt the control parameters obtained by taking the tracking errors (position or force). No effort has been made to optimise the fit of the parameters of other sub-controllers (outer loops). The optimization algorithm produced optimal solutions for the following cases: an integer-order PI controller, a fractional PI controller and a fractional PI D µ controller. Table II shows the performance of the best controllers found by the genetic algorithm considering the force interaction approach. From the view point of tracking errors, the improvements are clear when evolving from the integer-order PI to the fractional controller. More generally, however, an attempt to reduce the overshoot or the settling time implies an increase of the tracking errors, and vice-versa. This issue clearly deserves further experimentation. Table II ­ Control parameters optimised by GA. K 0.0 0.0 0.0 K -1.0 -0,82 Kµ µ - Force Errors (N) PI PI µ 17.5 120.0 50.0 3000.0 -740 2500.0 - 0.53 0.41 0.32 PI D -1.04 580.0 -0.55 (a) (b) (c) Fig. 8. COG Balance Control ­ temporal evolution of the: (a) hip motion; (b) normal ground reaction forces; (c) joint torques (up); centre of pressure COP and vertical projection of the COG (down). (a) (b) (c) Fig. 9. Force Interaction Control ­ temporal evolution of the: (a) hip motion; (b) normal ground reaction forces; (c) joint torques (up); centre of pressure COP and vertical projection of the COG (down). 6. CONCLUSIONS In this paper, two control algorithms are proposed to allow for localized control of humanoids based on feedback, ranging from joint position to force sensors. The simulation results give some intuitive understanding of the capability to explore the disturbance adaptation. The paper also demonstrates by simulation the application of a fractional-order controller in the tracking of the desired trajectories. The use of a genetic algorithm helps to find an adequate selection of the control parameters and, therefore, to enhance the overall performance. Besides the obvious necessary extension to the physical robot, there are a number of issues to be investigated. Ongoing research focuses the main directions to: i) extend to the lateral motion; ii) the complete robot model, incorporating the trunk and swing leg; and iii) address the challenging issues of impact and smooth transfer of support. In this sense, it seems essential to combine the proposed control methods with more advanced algorithms such as adaptive and learning strategies. ACKNOWLEDMENTS The first author would like to acknowledge FCT, FEDER, POCTI, POSI, POCI, POSC and ISEP for their support to R&D Projects and GECAD Unit. REFERENCES K. Hirai et al., The Development of Honda Humanoid Robot, Proc. IEEE Int. Conf. on R&A, pp. 1321-1326, 1998. J-I. Yamaguchi et al., Development of a Bipedal Humanoid Robot ­ Control Method of Whole Body Cooperative Dynamic Biped Walking. Proc. IEEE Int. Conf. Robotics & Automation, pp. 368-374, 1999. J.H. Park, H.C. Cho, An On-line Trajectory Modifier for the Base Link of Biped Robots to Enhance Locomotion Stability, Proc. IEEE Int. Conf. Robotics & Automation, pp. 3353-3358, 2000. T. Sugihara, Y. Nakamura, H. Inoue, Realtime Humanoid Motion Generation Through ZMP Manipulation Based on Inverted Pendulum Control, Proc. IEEE Int. Conf. Robotics & Automation, pp. 1404-1409, 2002. S. Kajita, et al., Resolved Momentum Control: Humanoid Motion Planning Based on the Linear Angular Momentum, Proc. IEEE Int. Conf. Intelligent Robots and Systems, pp. 1644-1650, 2003. M. Vukobratovic et al., Biped Locomotion ­ Dynamics, Stability, Control and Application, Springer-Verlag, 1990. J. Pratt and G. Pratt, Intuitive Control of a Planar Bipedal Walking Robot, Proc. IEEE Int. Conf. on R&A, pp. 20142021, 1998. S. Kajita, K. Tani, Experimental Study of Biped Dynamic Walking, IEEE Control Systems, vol. 16, n. 1, pp. 13-19, 1996. V. Santos, F. Silva, Development of a Low-Cost Humanoid Robot: Components and Technological Solutions, Proc. Int. Conf. on Climbing and Walking Robots, CLAWAR05, 12-15 Sept., London, 2005. Y. Fujimoto, A. Kawamura, Robust Biped Walking with Active Interaction Control between Foot and Ground, Proc. IEEE Int. Conf. on R&A, pp. 2030-2035, 1998. J. Park, Impedance Control for Biped Robot Locomotion, IEEE Trans. on Robotics & Automation, vol. 17, n. 6, pp. 870-882, 2001. K.B. Oldham and J. Sparier, The Fractional Calculus, Academic Press, New York, 1974. K.S. Miller and B. Ross, An Introduction to the Fractional Calculus and Fractional Differential Equations, Wiley, New York, 1993. J.A. Tenreiro Machado, Analysis and Design of FractionalOrder Digital Control Systems, Journal of System Analysis-Modeling-Simulation, 27, 107-122, 1997. F.M. Silva, V.M Santos, \"Towards an Autonomous SmallSize Humanoid Robot: Design Issues and Control Strategies\", Proc. on Computational Intelligence in Robotics and Automation, CIRA2005, Espoo, Finland, 2005. (N) PI PI µ 17.5 120.0 50.0 3000.0 -740 2500.0 - 0.53 0.41 0.32 PI D -1.04 580.0 -0.55 (a) (b) (c) Fig. 8. COG Balance Control ­ temporal evolution of the: (a) hip motion; (b) normal ground reaction forces; (c) joint torques (up); centre of pressure COP and vertical projection of the COG (down). (a) (b) (c) Fig. 9. Force Interaction Control ­ temporal evolution of the: (a) hip motion; (b) normal ground reaction forces; (c) joint torques (up); centre of pressure COP and vertical projection of the COG (down). 6. CONCLUSIONS In this paper, two control algorithms are proposed to allow for localized control of humanoids based on feedback, r IPV 38608 S1474-6670(16)38608-6 10.3182/20060906-3-IT-2910.00129 IFAC MOTION PLANNING AND CONTROL STRATEGIES FOR A DISTRIBUTED ARCHITECTURE HUMANOID ROBOT José R.T. Puga 1 Filipe M.T. Silva 2 Vítor M.F. Santos 3 1 Polytechnic Institute of Porto, 4200-072, Porto, Portugal Polytechnic Institute of Porto Porto 4200-072 Portugal 2 University of Aveiro, 3810-193 Aveiro, Portugal University of Aveiro Aveiro 3810-193 Portugal 3 University of Aveiro, 3810-193 Aveiro, Portugal University of Aveiro Aveiro 3810-193 Portugal This paper describes motion-control algorithms for a humanoid robot based on distributed control architecture. Towards its implementation, two different approaches based on positional and force control algorithms are revisited. At the same time, a new design procedure is proposed that consists of a fractional-order controller combined with a genetic algorithm for optimal tuning of the control parameters. The control algorithms are tested through several computer simulations and its robustness is discussed. Keywords 5-10 Legged robots biped walking trajectory planning COG control force interaction control References Hirai, 1998 K. Hirai The Development of Honda Humanoid Robot Proc. IEEE Int. Conf. on R&A 1998 1321 1326 Yamaguchi, 1999 J.-I. Yamaguchi Development of a Bipedal Humanoid Robot – Control Method of Whole Body Cooperative Dynamic Biped Walking. Proc IEEE Int. Conf. Robotics & Automation 1999 368 374 Park and Cho, 2000 J.H. Park H.C. Cho An On-line Trajectory Modifier for the Base Link of Biped Robots to Enhance Locomotion Stability Proc. IEEE Int. Conf. Robotics & Automation 2000 3353 3358 Sugihara et al., 2002 T. Sugihara Y. Nakamura H. Inoue Realtime Humanoid Motion Generation Through ZMP Manipulation Based on Inverted Pendulum Control Proc. IEEE Int. Conf. Robotics & Automation 2002 1404 1409 Kajita, 2003 S. Kajita Resolved Momentum Control: Humanoid Motion Planning Based on the Linear Angular Momentum Proc. IEEE Int. Conf. Intelligent Robots and Systems 2003 1644 1650 Vukobratovic, 1990 M. Vukobratovic Biped Locomotion – Dynamics, Stability, Control and Application 1990 Springer-Verlag Pratt and Pratt, 1998 J. Pratt and G. Pratt, Intuitive Control of a Planar Bipedal Walking Robot, Proc. IEEE Int. Conf. on R&A, pp. 2014- 2021, 1998. Kajita and Tani, 1996 S. Kajita K. Tani Experimental Study of Biped Dynamic Walking IEEE Control Systems 16 1 1996 13 19 Santos and Silva, 2005 V. Santos, F. Silva, Development of a Low-Cost Humanoid Robot: Components and Technological Solutions, Proc. Int. Conf. on Climbing and Walking Robots, CLAWAR05, 12-15 Sept., London, 2005. Fujimoto and Kawamura, 1998 Y. Fujimoto A. Kawamura Robust Biped Walking with Active Interaction Control between Foot and Ground, Proc. IEEE Int. Conf. on R&A 1998 2030 2035 Park, 2001 J. Park Impedance Control for Biped Robot Locomotion, IEEE Trans Robotics & Automation 17 6 2001 870 882 Oldham and Sparier, 1974 K.B. Oldham J. Sparier The Fractional Calculus 1974 Academic Press New York Miller and Ross, 1993 K.S. Miller B. Ross An Introduction to the Fractional Calculus and Fractional Differential Equations 1993 Wiley New York Tenreiro and Machado, 1997 J.A. Tenreiro Machado Analysis and Design of Fractional-Order Digital Control Systems Journal of System Analysis-Modeling-Simulation 27 1997 107 122 Silva and Santos, 2005 F.M. Silva, V.M Santos, “Towards an Autonomous Small-Size Humanoid Robot: Design Issues and Control Strategies”, Proc. on Computational Intelligence in Robotics and Automation, CIRA2005, Espoo, Finland, 2005. "
    },
    {
        "doc_title": "Towards an autonomous small-size humanoid robot: Design issues and control strategies",
        "doc_scopus_id": "28444472385",
        "doc_doi": null,
        "doc_eid": "2-s2.0-28444472385",
        "doc_date": "2005-12-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Distributed control",
            "Fractional calculus",
            "Humanoid robot"
        ],
        "doc_abstract": "This paper presents the design considerations of a small-size humanoid robot. The design process has revealed much about the several problems, challenges and tradeoffs imposed by biped locomotion. Among them, we here focus on the control of a single leg and its behaviour when assuming a forward motion. The controller is based on simple motion goals taking into account the reaction forces between the feet and the ground. A new method is proposed which appears to be well adapted to the class of problem considered: the use of a fractional-order controller combined with a genetic algorithm for optimal tuning of the control parameters. The control algorithm is tested through several simulations and its robustness is discussed. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Engineering solutions to build an inexpensive humanoid robot based on a distributed control architecture",
        "doc_scopus_id": "33846634997",
        "doc_doi": "10.1109/ICHR.2005.1573550",
        "doc_eid": "2-s2.0-33846634997",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Humanoid robots",
            "Mechanical design",
            "Modular architectures"
        ],
        "doc_abstract": "Building a humanoid robot is a formidable engineering task requiring the combination of mechanical, electrical and software technologies. This paper presents the main steps to design a low cost fully autonomous humanoid platform and the set of solutions proposed. The main scope of the project beneath this paper is to carry out research on control, navigation and perception, whilst offering opportunities for under and pos-graduate students to apply engineering methods and techniques. The main features of the 22 degrees-of-freedom robot include the distributed control architecture, based on a CAN bus, and the modularity at the system level. Although some issues are yet to be addressed, the stage of development is already mature for practical experiments and to obtain the first conclusions on the potential of the proposed solutions. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A low-level framework for a probabilistic treatment of the topological description of a robot mission",
        "doc_scopus_id": "79957987107",
        "doc_doi": "10.1109/IROS.2005.1545362",
        "doc_eid": "2-s2.0-79957987107",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Autonomous navigation",
            "Mathematical basis",
            "Probabilistic treatments",
            "Semantic descriptions",
            "Topological description",
            "Topological features",
            "Topological information",
            "Uncertainty estimates"
        ],
        "doc_abstract": "This article describes a mathematical basis required to integrate features obtained for perception for topological navigation. It is intended for application to navigation in an environment that is not mapped, but in which a mission is described in the form of a semantic description of the perception stimulus that the robot is expected to encounter. The need to integrate features from different sensors led to the use of an uncertainty estimate employed in information theory; binary entropy. By using entropy, the features are ranked in order of decreasing uncertainty. This article describes the state of the work in an as yet preliminary stage, but appears promising for application to navigation using topological information. It also offers interesting perspectives on commonly used sensory data such as local intensity image features. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Nested-loop architecture for mobile robot navigation",
        "doc_scopus_id": "0034476142",
        "doc_doi": "10.1177/02783640022068048",
        "doc_eid": "2-s2.0-0034476142",
        "doc_date": "2000-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Local navigation",
            "Navigation architecture",
            "Nested loop architecture",
            "Three dimensional environment reconstruction"
        ],
        "doc_abstract": "This paper describes a navigation architecture for mobile robots, structured as a set of nested control loops whose depth is related to their knowledge of the environment and the ability to drive the actuator, and involving as well competing behaviors that will ultimately generate the robot motion. The architecture has been successfully used on a mobile platform to support three-dimensional environment reconstruction tasks. The architecture may be classified as belonging to the hybrid type but made up of hybrid elements as well, allowing virtually any level of input awareness and ranging from high-level task planning to direct motion commands issued by external user or applications. A monitorized data path ensures the construction of the most adequate and safe motion, as well as an unlimited set of behaviors depending on the already known and perceived environment. Added concepts of path recovering and assisted navigation fulfill the demands for the three-dimensional acquisition scheme involved. Some comparison with existing architectures is carried out throughout the text. The versatility and robustness of the architecture are supported by extensive results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An autonomous sensor for 3d reconstruction",
        "doc_scopus_id": "84957668332",
        "doc_doi": "10.1007/3-540-64594-2_82",
        "doc_eid": "2-s2.0-84957668332",
        "doc_date": "1998-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D reconstruction",
            "Augmented reality applications",
            "Automated approach",
            "Autonomous mobile platforms",
            "Autonomous sensors",
            "Degree of accuracy",
            "Partial modeling",
            "Structural information"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 1998.We describe an automated approach to the reconstruction of 3D interiors from laser range data and digital images. This is achieved using a scanning laser rangefmder and digital camera that are mounted on an autonomous mobile platform known as the AEST. The objective is to reproduce complete interiors that are accurate enough for surveying, virtual studio and Augmented Reality applications. The AEST selects and navigates to a series of capture points to progressively reconstruct a 3D textured model to the required degree of accuracy. Navigation and structural information is used to register the data from each new capture point relative to the partial model. The user interface is a web browser with a radio link to the AEST. Results can be viewed in a VRML window as they are obtained. The AEST has been developed in EU-ACTS project RESOLV.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-loop robust navigation architecture for mobile robots",
        "doc_scopus_id": "0031625292",
        "doc_doi": "10.1109/ROBOT.1998.677213",
        "doc_eid": "2-s2.0-0031625292",
        "doc_date": "1998-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3-D environments",
            "Global navigation",
            "Modular navigation",
            "Multi-loops",
            "Navigation tasks",
            "Robust navigation",
            "Safe navigations",
            "Multi loop robust navigation architecture"
        ],
        "doc_abstract": "© 1998 IEEE.Describes a multi-loop, modular navigation architecture for mobile robots whose structure allows the execution of most types of navigation tasks in a highly robust manner. The modularity allows the clear separation of functions according to their complexity and priority. We propose a clear separation between local and global navigation in such a way that both can run independently, but an adequate alternation/competition between them allows accomplishment of trajectory execution including avoidance of unknown obstacles. The multi-loop nature of the architecture ensures adequate stability at different levels yielding safe navigation and accomplishment of higher level tasks. Examples may range from goal reaching in some point of the environment to a 3D-environment mapping application, as this is the case in this work.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Local perception maps for autonomous robot navigation",
        "doc_scopus_id": "0030394520",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030394520",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Perception maps",
            "Ultrasonic data"
        ],
        "doc_abstract": "This paper describes a mobile robotics system that was implemented and is capable of building instantaneous representations of the free space available, and use those representations to perform navigation. The free space representations are done by means of perception maps specially designed to hold and combine ultrasonic data. The maps are built by neural networks appropriately trained to minimise undesired ranging errors due to specular reflections; that is achieved by taking advantage of the redundancy of sensorial data. The concept of local navigation is developed as a new navigation approach, and is based on full independence on the environment, relying purely on sensorial perception. Local motion is generated according to simple generic behaviour descriptions: the local navigation strategies. The system, which was implemented, guarantees a safe local motion throughout the environment, requiring no a priori information nor any pre-defined path to follow. An adequate navigation architecture integrates the local navigation module within a framework of other several modules for more complete navigation tasks. The results obtained are quite promising in pointing the way to a truly autonomous system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perception maps for the local navigation of a mobile robot: a neural network approach",
        "doc_scopus_id": "0028594462",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0028594462",
        "doc_date": "1994-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Perception maps",
            "Ultrasonic ranging"
        ],
        "doc_abstract": "Commonly used sensors such as ultrasonic ranging devices, are known for their associated problems: specular reflections and crosstalk, essentially. This paper deals with these problems by means of special perception maps using ultrasound data. A generalised grid serves as the base of maps, and its cells have simply binary values: free or occupied. The relation between the topology of the perception map and the environment is a determinant factor for accurate reasoning. A 3-layer feed-forward neural network is used to perform the mapping between sensorial scans and grid occupancy. It was verified that the neural network handles most of the situations of specular reflections, and gives good perception maps for mid-range distances. Changes in environment, such as obstacles in vehicle's trajectory, have also been detected, which stresses the network's ability to generalise.",
        "available": false,
        "clean_text": ""
    }
]