[
    {
        "doc_title": "DICOMization of Proprietary Files Obtained from Confocal, Whole-Slide, and FIB-SEM Microscope Scanners",
        "doc_scopus_id": "85126853613",
        "doc_doi": "10.3390/s22062322",
        "doc_eid": "2-s2.0-85126853613",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Biomedical imaging",
            "DICOM",
            "Digitisation",
            "FIB SEM",
            "Glass slides",
            "Image modality",
            "Imaging technology",
            "Low-cost solution",
            "Microscopy imaging",
            "Pathogen niche",
            "Microscopy",
            "Records"
        ],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.The evolution of biomedical imaging technology is allowing the digitization of hundreds of glass slides at once. There are multiple microscope scanners available in the market including low-cost solutions that can serve small centers. Moreover, new technology is being researched to acquire images and new modalities are appearing in the market such as electron microscopy. This reality offers new diagnostics tools to clinical practice but emphasizes also the lack of multivendor system’s interoperability. Without the adoption of standard data formats and communications methods, it will be impossible to build this industry through the installation of vendor-neutral archives and the establishment of telepathology services in the cloud. The DICOM protocol is a feasible solution to the aforementioned problem because it already provides an interface for visible light and whole slide microscope imaging modalities. While some scanners currently have DICOM interfaces, the vast majority of manufacturers continue to use proprietary solutions. This article proposes an automated DICOMization pipeline that can efficiently transform distinct proprietary microscope images from CLSM, FIB-SEM, and WSI scanners into standard DICOM with their biological information main-tained within their metadata. The system feasibility and performance were evaluated with fifteen distinct proprietary modalities, including stacked WSI samples. The results demonstrated that the proposed methodology is accurate and can be used in production. The normalized objects were stored through the standard communications in the Dicoogle open-source archive.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the Use of Virtual Reality for Medical Imaging Visualization",
        "doc_scopus_id": "85111544006",
        "doc_doi": "10.1007/s10278-021-00480-z",
        "doc_eid": "2-s2.0-85111544006",
        "doc_date": "2021-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Advanced visualizations",
            "Development frameworks",
            "Development tools",
            "Medical analysis",
            "Medical imaging visualization",
            "Real time performance",
            "Standard data format",
            "Surgical planning",
            "Augmented Reality",
            "Diagnostic Imaging",
            "Humans",
            "Radiography",
            "Software",
            "Virtual Reality"
        ],
        "doc_abstract": "© 2021, Society for Imaging Informatics in Medicine.Advanced visualization of medical imaging has been a motive for research due to its value for disease analysis, surgical planning, and academical training. More recently, attention has been turning toward mixed reality as a means to deliver more interactive and realistic medical experiences. However, there are still many limitations to the use of virtual reality for specific scenarios. Our intent is to study the current usage of this technology and assess the potential of related development tools for clinical contexts. This paper focuses on virtual reality as an alternative to today’s majority of slice-based medical analysis workstations, bringing more immersive three-dimensional experiences that could help in cross-slice analysis. We determine the key features a virtual reality software should support and present today’s software tools and frameworks for researchers that intend to work on immersive medical imaging visualization. Such solutions are assessed to understand their ability to address existing challenges of the field. It was understood that most development frameworks rely on well-established toolkits specialized for healthcare and standard data formats such as DICOM. Also, game engines prove to be adequate means of combining software modules for improved results. Virtual reality seems to remain a promising technology for medical analysis but has not yet achieved its true potential. Our results suggest that prerequisites such as real-time performance and minimum latency pose the greatest limitations for clinical adoption and need to be addressed. There is also a need for further research comparing mixed realities and currently used technologies.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicomization of LSM fluorescence composite microscopic image with its bioimaging information",
        "doc_scopus_id": "85110884199",
        "doc_doi": "10.1109/CBMS52027.2021.00086",
        "doc_eid": "2-s2.0-85110884199",
        "doc_date": "2021-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Clinical environments",
            "Health care professionals",
            "Laser scanning microscope",
            "Microscopic imaging",
            "Non-biological particles",
            "Optical imaging technique",
            "Proprietary systems",
            "Visualization process"
        ],
        "doc_abstract": "© 2021 IEEE.In recent years, the quality of fluorescence microscopic imaging produced by a laser scanning microscope (LSM) system has been increased through the use of optical imaging techniques being used for small biological or nonbiological particles like instance, bacteria or cells in tissue samples. Currently, multiple manufactures provide LSM equipment able to capture single-layer or stacked images. However, the manufactures still using distinct data formats including proprietary ones, limiting the interoperability with third systems. In the clinical environment, the scanners need to be integrated with a vendor-neutral archive, storing images in a standard format and interface, making them accessible to healthcare professionals regardless of what proprietary system created it. Having distinct software solutions to manage the data is tiresome and time-consuming. This article proposes a normalization pipeline that can convert distinct vendor formats in a standard DICOM structure including pixel data and metadata. After conversion, the images are sent to a DICOM-compliant repository being able to be consumed in the network using normalized communication and visualization processes. The proposed solution is reliable and uses efficiently the less memory, a critical issue since the resolution of pathology whole-slide images can reach several gigapixels.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Pseudonymisation Protocol with Implicit and Explicit Consent Routes for Health Records in Federated Ledgers",
        "doc_scopus_id": "85107475113",
        "doc_doi": "10.1109/JBHI.2020.3028454",
        "doc_eid": "2-s2.0-85107475113",
        "doc_date": "2021-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Cryptographic assumptions",
            "Data subjects",
            "Feeding machines",
            "Health records",
            "Practical importance",
            "Public information",
            "Secondary use",
            "Threshold secret sharing",
            "Algorithms",
            "Computer Security",
            "Confidentiality",
            "Humans",
            "Informed Consent",
            "Research Design"
        ],
        "doc_abstract": "© 2013 IEEE.Healthcare data for primary use (diagnosis) may be encrypted for confidentiality purposes; however, secondary uses such as feeding machine learning algorithms requires open access. Full anonymity has no traceable identifiers to report diagnosis results. Moreover, implicit and explicit consent routes are of practical importance under recent data protection regulations (GDPR), translating directly into break-the-glass requirements. Pseudonymisation is an acceptable compromise when dealing with such orthogonal requirements and is an advisable measure to protect data. Our work presents a pseudonymisation protocol that is compliant with implicit and explicit consent routes. The protocol is constructed on a (t,n)-threshold secret sharing scheme and public key cryptography. The pseudonym is safely derived from a fragment of public information without requiring any data-subject's secret. The method is proven secure under reasonable cryptographic assumptions and scalable from the experimental results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the Visualization and Dicomization process for the Stacked Whole Slide Imaging",
        "doc_scopus_id": "85125187276",
        "doc_doi": "10.1109/BIBM52615.2021.9669349",
        "doc_eid": "2-s2.0-85125187276",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Bone tissue",
            "Clinical practices",
            "Dicomization",
            "High resolution",
            "Preprocess",
            "Proprietary file",
            "Research purpose",
            "Technical challenges",
            "Telematics platforms",
            "Whole-slide imaging"
        ],
        "doc_abstract": "© 2021 IEEE.Because of its high-resolution visibility across bone tissue, digital whole slide imaging (WSI) in pathology has become increasingly popular in biomedicine for diagnostic, educational, and research purposes. The technology, however, is not yet sufficiently mature to support remote clinical practice in pathology. Several technical challenges have hampered the gradual rollout of telematics platforms. Most notably, there is a lack of system interoperability because scanner manufacturers have yet to adopt a standard for data format and communications processes, specifically the DICOM standard, which already supports WSI. At the moment, it is uncommon to find an institutional repository capable of acquiring, storing, and visualizing samples scanned by different vendors' equipment. Although some vendors offer DICOM interfaces, the vast majority still use proprietary solutions. This article proposes a framework for multi-vendor data integration through the dicomization of proprietary images, including metadata, and optimization of the visualization process, with a focus on the stacked gigapixel WSI produced by new generation scanners. These images are difficult to preprocess and visualize in most environments due to their size, which is typically in the hundreds of Gigabytes range, and there are no reliable packages that can load, preprocess, and visualize these stacked WSI on a single platform. In this work, we created a simple automated pipeline that can read the majority of proprietary WSI images and focal planes, convert them to DICOM for preprocessing and visualization, and make them compatible with modern Web PACS platforms. The solution was tested with eight distinct proprietary files, two of which were stacked images, from acquisition to Web visualization, demonstrating good performance and efficient use of computational resources.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Decentralizing the storage of a DICOM compliant PACS",
        "doc_scopus_id": "85125175773",
        "doc_doi": "10.1109/BIBM52615.2021.9669902",
        "doc_eid": "2-s2.0-85125175773",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Cloud services",
            "Current technology",
            "Decentralisation",
            "Dicoogle",
            "Digital imaging and communication in medicines",
            "Distributed",
            "Exponentials",
            "Healthcare institutions",
            "Imaging modality",
            "Picture archiving and communications systems"
        ],
        "doc_abstract": "© 2021 IEEE.Nowadays, the medical imaging laboratories are an important piece in the diagnosis within the healthcare institutions. Every day, medical imaging modalities generate new data and the growth over the last recent years has been exponential. The storage and distribution of such data is orchestrated by the Picture Archiving and Communication System (PACS) but the current technologies are facing new challenges. On one hand, the massive amount of data requires a huge local infrastructure to handle the load, which may be expensive. On the other hand, the outsourcing of the data to cloud services may be slow and not compliant with regulations. It becomes urgent to investigate new ways to handle the storage and distribution of the medical images in an healthcare institution. In this work, we discuss the current solutions for the problem of storage and management of medical images. Furthermore, we introduce a solution based on a private network of nodes that can overcome problems as privacy, redundancy and high availability. The developed system offers features that totally match with the Digital Imaging and Communications in Medicine (DICOM) standard in terms of data integrity, immutability, and tolerance to failure. The results show that the proposed architecture performance are very similar with the baseline solutions. So, it reveals itself as a good solution considering the advantages pointed out.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web Platform for Medical Deep Learning Services",
        "doc_scopus_id": "85125171722",
        "doc_doi": "10.1109/BIBM52615.2021.9669704",
        "doc_eid": "2-s2.0-85125171722",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Automl",
            "Computer assisted diagnosis",
            "Deep learning",
            "Diagnosis tools",
            "Images classification",
            "Learning services",
            "Ranking system",
            "Training process",
            "Web",
            "Web solutions"
        ],
        "doc_abstract": "© 2021 IEEE.In the last decade, deep learning has been transforming the healthcare scenario with the provision of new computer-assisted diagnosis tools in an unprecedented way. The spread of specialized hardware and software has been supporting this reality. The development of such predictive models, however, requires expertise and time. In this context, this article proposes and describes the implementation of a no-coding software framework for the automation of deep learning training processes, aiming to support the development of medical image diagnostic classifiers by healthcare professionals with limited expertise in deep learning. It is a web solution that allows the easy management of data, models, and training processes associated with user requests, where the benefits extend to non-experts in the machine learning field. It is an intuitive web solution that contains a set of additional modern tools like Automatic Machine Learning (AutoML) to aid design models and a ranking system to help improve and share them.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Highly scalable medical imaging repository based on Kubernetes",
        "doc_scopus_id": "85125171107",
        "doc_doi": "10.1109/BIBM52615.2021.9669559",
        "doc_eid": "2-s2.0-85125171107",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Collaborative Work",
            "Data storage",
            "Imaging services",
            "Kubernetes",
            "Large volumes",
            "Medical imaging systems",
            "PACS-as-a-service",
            "Telework",
            "Work environments"
        ],
        "doc_abstract": "© 2021 IEEE.The use of medical imaging in clinical practice has increased dramatically in recent decades. The adoption and migration to the Cloud of medical imaging systems and services is an excellent opportunity for telemedicine, telework and collaborative work environments. However, the adoption of this paradigm has been slow in this scenario. While the migration has many advantages, it also introduces new challenges mainly related with data storage and management. One of most important open problems is with the efficient handling and transmission of large volumes of data. This issue is particularly critical if the service requests data regional redundancy and high scalability. In this context, this article proposes and describes a new architecture compliant with medical imaging requirements and following standard Cloud and medical imaging interfaces. The solution is based on Kubernetes, an open-source system to deploy, scale and manage containerized applications anywhere. The proposal includes an intelligent component for distributed management of the medical studies based on service policies. The result was a scalable Cloud-based medical imaging repository that can be deployed in a standard way in multiple providers that can achieve a better performance than single node.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A non-relational approach for distributed medical imaging databases",
        "doc_scopus_id": "85124558560",
        "doc_doi": "10.1109/BHI50953.2021.9508612",
        "doc_eid": "2-s2.0-85124558560",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health (social science)",
                "area_abbreviation": "SOCI",
                "area_code": "3306"
            }
        ],
        "doc_keywords": [
            "Couchdb",
            "DICOM",
            "Distributed",
            "Document-based",
            "Healthcare institutions",
            "Load-Balancing",
            "MongoDB",
            "New modality",
            "Processing systems",
            "Querying of data"
        ],
        "doc_abstract": "© 2021 IEEEHealth institutions, particularly medical imaging laboratories, are producing an ever-increasing amount of data with each passing day. The easy access to new modalities, the propagation of diagnosis clinics, and the need of having the acquired data readily accessible imply the usage of very large storage archives and powerful processing systems to handle the querying of data. Additionally, healthcare institutions usually need to have the data stored across multiple repositories for redundancy in case of failure, or load balancing to improve performance. The non-relational technology tackles the scalability problem, allowing horizontal scaling. Particularly, the document-based databases as MongoDB or CouchDB, for instance, match the DICOM Information Model. In fact, DICOM objects may be directly converted to JSON objects, allowing the storing and indexing of the metadata in databases as the document-based ones. In this paper, we present an overview of the current state of the non-relational databases, discussing the strengths and weaknesses of this type of database in our use cases. Moreover, we focus on the implementation of such databases in the medical imaging use case. We present an implementation that was integrated into the Dicoogle open-source PACS archive. The results of such implementation are then revealed and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Holder-of-key threshold access token for anonymous data resources",
        "doc_scopus_id": "85123209787",
        "doc_doi": "10.1109/ISCC53001.2021.9631259",
        "doc_eid": "2-s2.0-85123209787",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "'current",
            "Break-the-glass",
            "Centralised",
            "Data resources",
            "GDPR",
            "Generation process",
            "Identity and access managements",
            "Third party services",
            "Third-party service providers",
            "Threshold cryptography"
        ],
        "doc_abstract": "© 2021 IEEE.Centralized identity and access management providers (IdAM) are a source of mistrust when deploying federated services. The access token is the main piece that carries a trusted signature between the IdAM and the third-party service. Our holder-of-key access token proposal aims to reduce the risk of token forgery from the IdAM by decentralizing the generation process via (t, n)-threshold cryptography. Nonetheless, implicit consent routes are still a requirement under current legislation; and, non-encrypted personal records are useful and many times required to the third-party service provider. Our token scheme and architecture can grant access to pseudonymised resources via explicit or implicit consents. The access token is publicly verifiable and is bound to a specific pseudonym and secret key. The token has no information that can disclose the true identity behind the pseudonym. The scheme is proven secure under reasonable assumptions and scalable from the experimental results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DICOM Standard Pipeline for Microscope Imaging Modalities",
        "doc_scopus_id": "85123172921",
        "doc_doi": "10.1109/ISCC53001.2021.9631529",
        "doc_eid": "2-s2.0-85123172921",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Communication process",
            "Communications data",
            "DICOM",
            "Dicomization",
            "Imaging modality",
            "Microscope imaging",
            "Microscopy imaging",
            "Radiology departments",
            "Standard format"
        ],
        "doc_abstract": "© 2021 IEEE.In the nineties, the adoption of the DICOM standard format in radiology departments brought numerous advantages to clinical practice. The setup of PACS with standard communication processes and data formats allowed the creation of central repositories, fast retrieval of images, visualization of images acquired with several modalities, and simultaneous access at distributed places. Nowadays, microscopy imagining faces the same normalization challenge with the proliferation of equipment that stores data in a proprietary format and provides dedicated visualization software. This reality severely limits the implementation of vendor-neutral archives with common visualization processes, conditioning the research work and its integration in clinical environments. This paper proposed a pipeline for the integration of multiple microscopy imaging modalities into the PACS-DICOM universe, including the numerous metadata elements. A proof-of-concept system was developed, for validation purposes, and integrated with the Dicoogle open-source PACS, providing image storage, metadata indexing and visualization.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Performant Protocol for Distributed Health Records Databases",
        "doc_scopus_id": "85114739333",
        "doc_doi": "10.1109/ACCESS.2021.3111008",
        "doc_eid": "2-s2.0-85114739333",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Computational system",
            "Consensus protocols",
            "Distributed database",
            "Electronic health record",
            "General data protection regulations",
            "Insertion operations",
            "Quality of health care",
            "Retrieval performance"
        ],
        "doc_abstract": "© 2013 IEEE.Electronic Health Records (EHR) have a distributed nature and can be managed by distinct affinity domains. Sharing patient health information across distinct organisations helps to deliver a well-informed diagnosis, improving the quality of healthcare service. The federation of those information systems can take the form of a distributed database where data are partitioned and possibly replicated across distinct computational systems. However, the benefits of having a distributed system, such as consistency, availability, and data protection, are mostly absent. This article proposes a distributed database consensus protocol designed to improve the performance of EHR insertion operations, a particularly critical issue in medical imaging cases due to the data volume. It explores the personal and non-transferable nature of EHR and the proposed methodology reduces the data contention through data isolation, improving the overall retrieval performance and detection of misbehaving parties. Furthermore, the proposal follows the recent European General Data Protection Regulation (GDPR), which states that appropriate mechanisms should be used in order to protect data against accidental loss, destruction, or damage, using appropriate technical or organisational measures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pathobox: The Collaborative Tele-pathology Platform with Access Management",
        "doc_scopus_id": "85107261969",
        "doc_doi": "10.1007/978-3-030-72379-8_20",
        "doc_eid": "2-s2.0-85107261969",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Access management",
            "Cloud-based architectures",
            "Collaborative tools",
            "Cross-platform",
            "Digital pathologies",
            "Medical practice",
            "Role-based Access Control",
            "Technological advances"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Recent technological advances in medical informatics led to the adoption of new technologies in pathology clinical area. Digital pathology is allowing the acquisition, storage, and distribution of pathological digital samples, which are gathered by scanners and displayed in network workstations. This paper discusses the opportunities and challenges of digital pathology, and how it is changing education, training, and medical practice nowadays. A new paradigm of collaborative telepathology is proposed through a cloud-based architecture, fully compliant with the DICOM standard, that integrates a cross-platform pathology viewer, collaborative tools, virtual work-spaces, and personal storage areas. Data management and privacy are ensured through the implementation of a role-based access control mechanism. The solution was designed to serve distinct use cases, including telepathology and e-academy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "NoSQL distributed database for DICOM objects",
        "doc_scopus_id": "85100352386",
        "doc_doi": "10.1109/BIBM49941.2020.9313430",
        "doc_eid": "2-s2.0-85100352386",
        "doc_date": "2020-12-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Architecture-based",
            "Data distributed",
            "Distributed database",
            "Document database",
            "Load-balancing mechanisms",
            "Multiple operations",
            "Nosql database",
            "Research environment"
        ],
        "doc_abstract": "© 2020 IEEE.Every year, the amount of data produced in health-care institutions in the field of medical imaging is increasing. The easy accessibility to digital modalities and the proliferation of diagnosis centers working in network of services, creates the need for very large repositories of data. Moreover, the institutions have redundant storage across the network and load balancing mechanism to improve access performance. NoSQL databases provide horizontal scalability which allows the system to grow easily and effortlessly. Document databases are particularly suitable to store the DICOM metadata and the JSON format is already supported by the standard. This paper, we discuss the use of document database model in the medical imaging field, trying to focus on both clinical and research environments. Moreover, we focused on the proposal of an architecture based on MongoDB to distribute the load between multiple nodes. The proposal was integrated with an open-source PACS and validated by simulating multiple operations over data distributed across multiple virtual locations, and the results are promising.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Cloud-Ready Architecture for Shared Medical Imaging Repository",
        "doc_scopus_id": "85087916517",
        "doc_doi": "10.1007/s10278-020-00373-7",
        "doc_eid": "2-s2.0-85087916517",
        "doc_date": "2020-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Access control mechanism",
            "Accounting architectures",
            "Client applications",
            "Open-source solutions",
            "Production environments",
            "Proposed architectures",
            "Quantitative assessments",
            "Repository environments",
            "Computers",
            "Diagnostic Imaging",
            "Humans",
            "Radiography",
            "Radiology Information Systems"
        ],
        "doc_abstract": "© 2020, Society for Imaging Informatics in Medicine.Nowadays usage paradigms of medical imaging resources are requesting vendor-neutral archives, accessible through standard interfaces, with multi-repository support. Regional repositories shared by distinct institutions, tele-radiology as a service at cloud, teaching, and research archives are illustrative examples of this new reality. However, traditional production environments have a server archive instance per functional domain where every registered client application has access to all studies. This paper proposes an innovator ownership concept and access control mechanisms that provide a multi-repository environment and integrates well with standard protocols. A secure accounting mechanism for medical imaging repositories was designed and instantiated as an extension of a well-known open-source archive. A new web service layer was implemented to provide a vendor-neutral solution compliant with modern DICOM Web protocols for storage, search, and retrieval of medical imaging data. The concept validation was done through the integration of proposed architecture in an open-source solution. A quantitative assessment was performed for evaluating the impact of the mechanism in the usual DICOM Web operations. This article proposes a secure accounting architecture able to easily convert a standard medical imaging archive server in a multi-repository solution. The proposal validation was done through a set of tests that demonstrated its robustness and usage feasibility in a production environment. The proposed system offers new services, fundamental in a new era of cloud-based operations, with acceptable temporal costs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A safe architecture for authorisation grant in healthcare ecosystems",
        "doc_scopus_id": "85103202445",
        "doc_doi": "10.1109/ICHI48887.2020.9374380",
        "doc_eid": "2-s2.0-85103202445",
        "doc_date": "2020-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Decision Sciences (miscellaneous)",
                "area_abbreviation": "DECI",
                "area_code": "1801"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health (social science)",
                "area_abbreviation": "SOCI",
                "area_code": "3306"
            }
        ],
        "doc_keywords": [
            "Access rights",
            "Cloud environments",
            "Discretionary access control models",
            "Health records",
            "New approaches",
            "Retrieval process",
            "Security and privacy",
            "Security methods"
        ],
        "doc_abstract": "© 2020 IEEE.Recent regulations for security and privacy of personal data request new approaches in data access and retrieval processes. In the context of healthcare, the regulations point two types of consent to access sensitive personal medical data: explicit and implicit consent. This paper focus on explicit consent cases and presents an architecture where the patient grants on-demand access to private health records. The architecture is supported by a Discretionary Access Control model suited for cross-domain and cloud environments. Each resource belongs to a patient that has the power to grant or deny any access rights to users or groups of users. All the process is designed to be secure, from the authentication of the physician in the terminal until the communications between entities, passing by the physician's terminal check by the patient. Furthermore, the security methods are discussed and evaluated. Finally, it is presented a summary of the developed work and the plans to apply in the future work.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "RAIAP: renewable authentication on isolated anonymous profiles: A GDPR compliant self-sovereign architecture for distributed systems",
        "doc_scopus_id": "85085097324",
        "doc_doi": "10.1007/s12083-020-00914-5",
        "doc_eid": "2-s2.0-85085097324",
        "doc_date": "2020-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Asymmetric key",
            "Consolidated architecture",
            "Distributed systems",
            "General data protection regulations",
            "Key management",
            "Minimisation",
            "Non repudiation",
            "Secret sharing"
        ],
        "doc_abstract": "© 2020, Springer Science+Business Media, LLC, part of Springer Nature.Implementing pseudonymity, key-management, non-repudiation and data minimisation features in isolated procedures is trivial. However, integrating all of them in one consistent architecture has several challenges to tackle. This work proposes data structures to represent Self-Sovereign Identities and to handle those features in a consolidated architecture. Key-management is constructed using secret sharing principles, capable of recovering from a lost or compromised key to a new one without losing track of the original account. Pseudonymity and data minimisation is established using anonymous profiles, showing different views of the same identity. Non-repudiation is contemplated in the profile disclosure process. Profiles are protected against tampering with the use of digital signatures and blockchain cryptographic constructions. All profiles and registries are controlled with a single asymmetric key pair that can be provided by a smart card. Flexible structures are defined that can be used to register claims, attestations, authorisation grants, user consents, or any other activities. All definitions take into consideration the rules of the General Data Protection Regulation (GDPR).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle Framework for Medical Imaging Teaching and Research",
        "doc_scopus_id": "85094119901",
        "doc_doi": "10.1109/ISCC50000.2020.9219545",
        "doc_eid": "2-s2.0-85094119901",
        "doc_date": "2020-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Applied research",
            "Architectural concepts",
            "Collaborative Work",
            "Core functionality",
            "Medical laboratories",
            "Recent researches",
            "Teaching and researches",
            "Usage statistics"
        ],
        "doc_abstract": "© 2020 IEEE.One of the most noticeable trends in healthcare over the last years is the continuous growth of data volume produced and its heterogeneity. In the medical imaging field, the evolution of digital systems is supported by the PACS concept and the DICOM standard. These technologies are deeply grounded in medical laboratories, supporting the production and providing healthcare practitioners with the ability to set up collaborative work environments with researchers and academia to study and improve healthcare practice. However, the complexity of those systems and protocols makes difficult and time-consuming to prototype new ideas or develop applied research, even for skilled users with training in those environments. Dicoogle emerges as a reference tool to achieve those objectives through a set of resources aggregated in the form of a learning pack. It is an open-source PACS archive that, on the one hand, provides a comprehensive view of the PACS and DICOM technologies and, on the other hand, provides the user with tools to easily expand its core functionalities. This paper describes the Dicoogle framework, with particular emphasis in its Learning Pack package, the resources available and the impact of the platform in research and academia. It starts by presenting an overview of its architectural concept, the most recent research backed up by Dicoogle, some remarks obtained from its use in teaching, and worldwide usage statistics of the software. Moreover, a comparison between the Dicoogle platform and the most popular open-source PACS in the market is presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Role-based architecture for secure management of telepathology sessions",
        "doc_scopus_id": "85086886304",
        "doc_doi": "10.3233/SHTI200243",
        "doc_eid": "2-s2.0-85086886304",
        "doc_date": "2020-06-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Collaborative platform",
            "Collaborative Work",
            "Computer technology",
            "Digital pathologies",
            "Proposed architectures",
            "Role-based Access Control",
            "Session management",
            "Virtual microscopy",
            "Diagnostic Tests, Routine",
            "Microscopy",
            "Telepathology"
        ],
        "doc_abstract": "© 2020 European Federation for Medical Informatics (EFMI) and IOS Press.Digital pathology is the computer technology that allows the management of the information generated by the whole-slide scanners from a microscopic slide, encompassing the virtual microscopy. This paper proposes and describes the architecture of a secure collaborative platform that integrates a web pathology viewer with role-based access control. The proposed architecture is ensured by a shared medical repository that serves web pathology viewer with the medical images, using the DICOM standard. The system offers collaborative work session management tools as the managing of users, sessions, access control to sessions, and many others. Furthermore, the use cases related to telepathology and e-learning are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clinico-environmental system for personal monitoring",
        "doc_scopus_id": "85086886303",
        "doc_doi": "10.3233/SHTI200131",
        "doc_eid": "2-s2.0-85086886303",
        "doc_date": "2020-06-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Environmental contexts",
            "Environmental health",
            "Environmental systems",
            "External systems",
            "Information and Communication Technologies",
            "Personal health care",
            "Personal monitoring",
            "Physical activity",
            "Air Pollution",
            "Delivery of Health Care",
            "Environmental Monitoring"
        ],
        "doc_abstract": "© 2020 European Federation for Medical Informatics (EFMI) and IOS Press.The ever-growing use of information and communication technologies in the past decades and the proliferation of mobile devices for monitoring vital signs and physical activity is enhancing the emergence of a new healthcare paradigm. More recently, citizens are becoming more sensible to the necessity of monitoring environmental health indicators and to its direct impact on personal health. This article proposes and describes the development of a clinico- environmental system for personal monitoring. The result is ContinuousCare, a personal healthcare information system that integrates personal smart devices with air quality monitors. The solution helps citizens to better understand their health and body activity with environmental context, aiding professional doctors with analysis tools and making available valuable data for external systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Health and wellness tourism as source of happiness and quality of life",
        "doc_scopus_id": "85119015762",
        "doc_doi": "10.34190/IRT.20.046",
        "doc_eid": "2-s2.0-85119015762",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Cultural Studies",
                "area_abbreviation": "SOCI",
                "area_code": "3316"
            },
            {
                "area_name": "Management, Monitoring, Policy and Law",
                "area_abbreviation": "ENVI",
                "area_code": "2308"
            },
            {
                "area_name": "Tourism, Leisure and Hospitality Management",
                "area_abbreviation": "BUSI",
                "area_code": "1409"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© the authors, 2020. All Rights Reserved.Health and wellness tourism as always been in the tourism history of Portugal and Hungary. Nowadays, it still figures as one of the strategic products for development of tourism in both countries. The constant and solid growth of this type of tourism is due to the large range of modern life circumstances and constraints. The stress levels and the working hours and the improvement of the life hope, are factors that converge and obey to a bigger concern with the quality of life levels and also the sense of happiness. Consequently, Health and Wellness activities, specifically the ones that are rewarded with the therapeutic qualities and characteristics of thermal waters, are particularly valorised by different segments of demand. The literature review allowed the comprehension of the concepts of quality of life and happiness in the tourism context and reflected the reality and the potential nature of health and wellness tourism as a global touristic product for the contemporary world, mainly in the developed societies. The purpose of the present paper is to analyse the relation of health and wellness tourism and the improvement of quality of life and the levels of happiness, in a comparative study between Portuguese and Hungarian realities. The analysis was made by the supply side. The empirical study was developed in a four months period and the empirical data were collected through the administration of a questionnaire as a data collection tool in some of the most important Thermal Baths, Hot Springs and SPAs of those countries. The study sample is made up of 753 Portuguese and Hungary users of those SPAs and thermal baths. The results of the empirical research are analysed and both cases are compared. The obtained conclusions are presented and the implications to the development of health and wellness tourism are demonstrated in a marketing and consumer behaviour perspective.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data and sessions management in a telepathology platform",
        "doc_scopus_id": "85083695241",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85083695241",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Collaborative platform",
            "Digital environment",
            "Digital pathologies",
            "Distributed network systems",
            "Education and training",
            "Pathological data",
            "Telepathology",
            "Usage context"
        ],
        "doc_abstract": "© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Digital pathology refers to the acquisition, storage, and interpretation of pathological data gathered by scanners and displayed in a digital environment, using a distributed network system. This paper discusses the challenges and opportunities of a collaborative platform applied to digital pathology considering the advantages that it may carry on regarding education and training but also new paradigms of telepathology and telemedicine. Furthermore, it proposes the implementation of a secure collaborative platform that integrates a web pathology viewer with personal areas and virtual archives. The described approach introduces a modern collaborative concept into the digital pathology workflow supported by a customized medical imaging infrastructure where data management is ensured by an innovator DICOM standard multi-repository server. The solution was designed to serve distinct usage contexts, including telepathology and e-academy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ETL Framework for Real-Time Business Intelligence over Medical Imaging Repositories",
        "doc_scopus_id": "85067819132",
        "doc_doi": "10.1007/s10278-019-00184-5",
        "doc_eid": "2-s2.0-85067819132",
        "doc_date": "2019-10-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Business practices",
            "Continuous production",
            "DICOM",
            "Institutional repositories",
            "Medical institutions",
            "Real time analysis",
            "Real time business intelligence",
            "Web-based interface",
            "Data Mining",
            "Data Warehousing",
            "Humans",
            "Metadata",
            "Radiology Information Systems"
        ],
        "doc_abstract": "© 2019, Society for Imaging Informatics in Medicine.In the last decades, the amount of medical imaging studies and associated metadata has been rapidly increasing. Despite being mostly used for supporting medical diagnosis and treatment, many recent initiatives claim the use of medical imaging studies in clinical research scenarios but also to improve the business practices of medical institutions. However, the continuous production of medical imaging studies coupled with the tremendous amount of associated data, makes the real-time analysis of medical imaging repositories difficult using conventional tools and methodologies. Those archives contain not only the image data itself but also a wide range of valuable metadata describing all the stakeholders involved in the examination. The exploration of such technologies will increase the efficiency and quality of medical practice. In major centers, it represents a big data scenario where Business Intelligence (BI) and Data Analytics (DA) are rare and implemented through data warehousing approaches. This article proposes an Extract, Transform, Load (ETL) framework for medical imaging repositories able to feed, in real-time, a developed BI (Business Intelligence) application. The solution was designed to provide the necessary environment for leading research on top of live institutional repositories without requesting the creation of a data warehouse. It features an extensible dashboard with customizable charts and reports, with an intuitive web-based interface that empowers the usage of novel data mining techniques, namely, a variety of data cleansing tools, filters, and clustering functions. Therefore, the user is not required to master the programming skills commonly needed for data analysts and scientists, such as Python and R.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pseudonymisation with break-the-glass compatibility for health records in federated services",
        "doc_scopus_id": "85078577294",
        "doc_doi": "10.1109/BIBE.2019.00056",
        "doc_eid": "2-s2.0-85078577294",
        "doc_date": "2019-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Health records",
            "Health-care system",
            "Pseudonymisation",
            "Public information",
            "Threshold secret sharing"
        ],
        "doc_abstract": "© 2019 IEEE.Pseudonymisation is a major requirement in recent data protection regulations, and of especial importance when sharing healthcare data outside of the boundaries of the affinity domain. However, healthcare systems require important break-the-glass procedures, such as accessing records of patients in unconscious states. Our work presents a pseudonymisation protocol that is compliant with break-the-glass procedures, established on a (t, n)-threshold secret sharing scheme and public key cryptography. The pseudonym is safely derived from a fragment of public information without any private secret requirement. The protocol is proven secure and scalable under reasonable assumptions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Collaborative framework for a whole-slide image viewer",
        "doc_scopus_id": "85071033894",
        "doc_doi": "10.1109/CBMS.2019.00053",
        "doc_eid": "2-s2.0-85071033894",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Collaborative",
            "DICOM",
            "Digital pathologies",
            "Telepathology",
            "Whole slide imaging (WSI)"
        ],
        "doc_abstract": "© 2019 IEEE.Digital pathology is a new branch of medical imaging referring to the aggregation of equipment and software to acquire, store and display microscopic images in a distributed network environment. This article proposes an architecture and describes the implementation of a collaborative pathology web platform. The solution brings the modern collaborative concept, common in social and business networks, into Digital Pathology workflows supported by a customised PACS-DICOM infrastructure. The system assures services like the creation of working sessions, users groups, access control to sessions, synchronisation of operations in a rich web interface, replaying of the actions performed in a session, among others. The solution data management is ensured by a PACS compliant with the DICOM standard, more concretely the recent Whole Slide Imaging format and the DICOM Web communication services.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GDPR impacts and opportunities for computer-aided diagnosis guidelines and legal perspectives",
        "doc_scopus_id": "85071000360",
        "doc_doi": "10.1109/CBMS.2019.00128",
        "doc_eid": "2-s2.0-85071000360",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Architectural Guidelines",
            "GDPR",
            "General data protection regulations",
            "Healthcare record",
            "Screening programs",
            "Signaling networks",
            "System developers",
            "Telecommunication sector"
        ],
        "doc_abstract": "© 2019 IEEE.The General Data Protection Regulation (GDPR) is lengthy and it is essential to resume the impacts of it in specific use-cases to diminish the gap between system developers and regulators. Computer-aided diagnosis is one of such use-cases with increased importance on clinical screening programs. The regulation has distinct mentions that affect automated-decision solutions and healthcare records. This work identifies the fundamental legal issues, challenges and opportunities for this scenario and propose architectural guidelines to tackle them. The result is purely theoretical, however it is based on known architectures such as signaling networks, already applied in the telecommunication sector.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Volumetric feature learning for query-by-example in medical imaging archives",
        "doc_scopus_id": "85070952799",
        "doc_doi": "10.1109/CBMS.2019.00038",
        "doc_eid": "2-s2.0-85070952799",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D volume retrieval",
            "Adversarial networks",
            "Auto encoders",
            "Compact representation",
            "Regions of interest",
            "Representation learning",
            "Representation method",
            "Scientific community"
        ],
        "doc_abstract": "© 2019 IEEE.The increasing challenges and requirements of medical image retrieval systems are leading the scientific community towards exploring modern representation methods as a means to improve clinical information retrieval as we know it. While current research tackles medical image retrieval through text-based, visual-based, or mixed approaches, representation learning can play an important role in improving retrieval capabilities by encoding medical image content into compact representations, addressing the problem of dimensionality. This paper introduces the potential of representation learning for the retrieval of high dimensionality imaging studies through automatically learned representations for regions of interest. Preliminary results are presented for feature learning through adversarial auto-encoding, based on the VISCERAL medical image retrieval benchmark.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An accounting mechanism for standard medical imaging services",
        "doc_scopus_id": "85065506012",
        "doc_doi": "10.1109/ENBENG.2019.8692545",
        "doc_eid": "2-s2.0-85065506012",
        "doc_date": "2019-04-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            }
        ],
        "doc_keywords": [
            "Centralisation",
            "Imaging objects",
            "Imaging resources",
            "Imaging services",
            "Open sources",
            "PACS archive",
            "Picture archiving and communication systems (PACS)",
            "Production environments"
        ],
        "doc_abstract": "© ENBENG 2019. All Rights Reserved.Nowadays, Digital Imaging and Communications in Medicine (DICOM) is universally accepted as the standard that defines the formats and transmission processes in the medical imaging field. It is widely used in the production environment and allowed the development of interoperable Picture Archiving and Communication System (PACS). However, current open-source PACS do not provide accounting mechanisms to deal with permission granting while accessing to archived imaging objects. This article proposes a new architecture that can be integrated with a common PACS archive for providing ownership concept and authorisation control over medical imaging resources. Moreover, the system provides means for the centralisation of different instances of repositories. Finally, the proposed solution allows permission managing in the DICOMWeb extensions, protecting the services made available through HTTP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Competitiveness and business performance in the portuguese hotel industry",
        "doc_scopus_id": "85090706884",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85090706884",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Cultural Studies",
                "area_abbreviation": "SOCI",
                "area_code": "3316"
            },
            {
                "area_name": "Management, Monitoring, Policy and Law",
                "area_abbreviation": "ENVI",
                "area_code": "2308"
            },
            {
                "area_name": "Tourism, Leisure and Hospitality Management",
                "area_abbreviation": "BUSI",
                "area_code": "1409"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© The Authors, 2019. All Rights Reserved.The growing importance of the tourism sector in the Portuguese economy makes it critical the assessment of its financial achievements and the competitive advantages of this sector (Bank of Portugal, 2016). Over the past few decades the competitiveness has been growing interest in international research into tourism in general and particularly in hospitality. The main aim of this research is to analyze the economic and financial achievement of the hotel sector as well as find out its financial potential. So, and bearing in mind the goal of this paper, the methodology used was based on the collection of data relating to a selection of the companies with the Portuguese classification of economic activities (CEA) 55 – Accommodation, in 2017, a whole of 6.792 companies. The economic and financial information was collected from balance sheets and financial reports of 2007 to 2016, through SABI (Iberian Balance Sheet Analysis System) database. Furthermore, we analyze the differences in financial ratios of hotels and other tourism companies and statistical significance test were applied to test the existence of statistically significant differences between this two groups of companies. The indicators of corporate performance under consideration are Return on Assets (ROA), Return on Equity (ROE), Earnings before Interest, Taxes, Depreciation and Amortization (EBITDA), Business Volume (BV); Gross Value Added (GVA), Apparent Labor Productivity (ALP), General Liquidity (LG), Solvency (SLV) and Financial Autonomy (FAUT). The examination of the economic and financial performance of the hotel sector allows, through the evolution of its indicators, to highlight the main strengths and weaknesses of this sector. This examination is shown to be crucial in defining the strategy of the sector, both from the point of view of investors and hotel managers and government tourism policymakers. We conclude that most of the variables present statistically significant differences comparing the results between the accommodation and the other tourist activities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Informative and intriguing visual features: UA.Pt bioinformatics in ImageCLeF caption 2019",
        "doc_scopus_id": "85070498865",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85070498865",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Auto encoders",
            "Automatic Detection",
            "Clinical decision support",
            "Convolutional neural network",
            "Learning techniques",
            "Multi label classification",
            "Representation learning",
            "Supervised image classifications"
        ],
        "doc_abstract": "© 2019 CEUR-WS. All rights reserved.Digital medical imaging has opened new advances in clinical decision support and treatment procedures since its inception. This leads to the creation of huge amounts of data that are often not fully exploited. The development and evaluation of representation learning techniques for automatic detection of concepts in medical images can make way for improved indexing, processing and retrieval capabilities in medical imaging archives. This paper discloses several independent approaches for multi-label classification of biomedical concepts, in the context of the ImageCLEFmed Caption challenge of 2019. We emphasize the use of threshold tuning to optimize the quality of sample retrieval, as well as the differences between training a convolutional neural network end-to-end for supervised image classification, and training unsupervised learning models before linear classifiers. In the test results, the best mean F1-score of 0.206 was obtained with the supervised approach, albeit with images of a larger resolution than for the dual-stage approaches.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A community-driven validation service for standard medical imaging objects",
        "doc_scopus_id": "85048972717",
        "doc_doi": "10.1016/j.csi.2018.06.003",
        "doc_eid": "2-s2.0-85048972717",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Law",
                "area_abbreviation": "SOCI",
                "area_code": "3308"
            }
        ],
        "doc_keywords": [
            "Data inconsistencies",
            "De facto standard",
            "DICOM",
            "Quality of health care",
            "System operation",
            "Validation results",
            "Validator",
            "Working environment"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.Digital medical imaging laboratories contain many distinct types of equipment provided by different manufacturers. Interoperability is a critical issue and the DICOM protocol is a de facto standard in those environments. However, manufacturers’ implementation of the standard may have non-conformities at several levels, which will hinder systems’ integration. Moreover, medical staff may be responsible for data inconsistencies when entering data. Those situations severely affect the quality of healthcare services since they can disrupt system operations. The existence of software able to confirm data quality and compliance with the DICOM standard is important for programmers, IT staff and healthcare technicians. Although there are a few solutions that try to accomplish this goal, they are unable to deal with certain situations that require user input. Furthermore, these cases usually require the setup of a working environment, which makes the sharing of validation information more difficult. This article proposes and describes the development of a Web DICOM validation service for the community. This solution requires no configuration by the user, promotes validation results’ share-ability in the community and preserves patient data privacy since files are de-identified on the client side.",
        "available": true,
        "clean_text": "serial JL 271914 291210 291791 291813 291869 291870 31 Computer Standards & Interfaces COMPUTERSTANDARDSINTERFACES 2018-06-18 2018-06-18 2018-09-12 2018-09-12 2018-09-12T15:24:28 S0920-5489(17)30414-2 S0920548917304142 10.1016/j.csi.2018.06.003 S300 S300.1 FULL-TEXT 2018-09-12T17:04:36.151238Z 0 0 20190101 20190131 2019 2018-06-19T00:00:25.452253Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor grantsponsorid highlightsabst orcid primabst ref 0920-5489 09205489 true 61 61 C Volume 61 11 121 128 121 128 201901 January 2019 2019-01-01 2019-01-31 2019 article fla © 2018 Elsevier B.V. All rights reserved. ACOMMUNITYDRIVENVALIDATIONSERVICEFORSTANDARDMEDICALIMAGINGOBJECTS SILVA J 1 Introduction 2 DICOM constitution 3 Related work 4 Proposed architecture 4.1 Overview 4.2 Functional modules 4.3 Software layers 4.4 Workflows 4.4.1 Pixel data removal and de-identification 5 Validation process 6 Results 6.1 Management dashboard 6.2 Validation service 7 Discussion 7.1 Relevance of the work 7.2 Comparison with State-of-the-art and solution limitations 7.3 Future work 8 Conclusion Acknowledgments Appendix A Supplementary materials References HUANG 2010 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS CREIGHTON 1999 138 143 C STCYR 2013 1 5 T 2013PROCEEDINGSIEEESOUTHEASTCON OVERVIEWHEALTHCARESTANDARDS MILDENBERGER 2002 920 927 P WEBB 2003 A INTRODUCTIONBIOMEDICALIMAGING 2003 529 537 DREYER 2006 K PACSAGUIDEDIGITALREVOLUTION NEMA 2018 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART2CONFORMANCE HORII 2011 48 74 S INFORMATICSINMEDICALIMAGING DICOM NEMA 2018 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART3INFORMATIONOBJECTDEFINITIONS NEMA 2018 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART5DATASTRUCTURESENCODING OOSTERWIJK 2005 H DICOMBASICS NEMA 2018 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART6DATADICTIONARY HEWETT 1997 480 487 A PROCEEDINGSSPIEVOLUME3035P4804871997 CONFORMANCETESTINGDICOMIMAGEOBJECTS NEMA 2018 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART10MEDIASTORAGEFILEFORMATFORMEDIAINTERCHANGE POTTER 2007 47 62 G LEEKWOKPUNFLUSHING 2001 N SPECIFYINGDICOMSEMANTICCONSTRAINTSINXML GUODONGBAINASHUA 2008 N TECHNIQUESFORCHECKINGACOMPLEXDIGITALOBJECTCONFORMSASTANDARD SINGH 2011 23 R 2017 190 197 SILVA 2018 81 90 J NEMA 2018 DICOMSTANDARDPS315ANNEXE DOURAKI 1999 421 427 T MANAGEPERISH ETHICALLEGALDIMENSIONSMEDICALCONFIDENTIALITYINEUROPEANLAWHUMANRIGHTS BALJON 1999 M QUALITYCONTROLUSINGAUTOMATEDVALIDATIONTOOLSCANIMPROVEINTEROPERABILITYDICOMIMPLEMENTATIONS ONKENA 2007 290 292 M CLUNIE 2015 D DICOMVALIDATORDCIODVFY SARIYAR 2015 263 270 M SILVAX2019X121 SILVAX2019X121X128 SILVAX2019X121XJ SILVAX2019X121X128XJ 2020-09-12T00:00:00.000Z 2020-09-12T00:00:00.000Z © 2018 Elsevier B.V. All rights reserved. item S0920-5489(17)30414-2 S0920548917304142 10.1016/j.csi.2018.06.003 271914 2018-09-12T16:31:28.839718Z 2019-01-01 2019-01-31 true 1755297 MAIN 8 56902 849 656 IMAGE-WEB-PDF 1 gr1 6856 75 219 gr10 6460 164 124 gr2 9655 124 219 gr3 10785 160 219 gr4 8129 164 214 gr5 11156 140 219 gr6 10413 164 209 gr7 11512 164 189 gr8 6547 164 52 gr9 10199 138 219 gr1 17594 133 388 gr10 39357 472 357 gr2 26866 218 386 gr3 29010 282 386 gr4 23777 296 387 gr5 22352 246 386 gr6 26220 303 386 gr7 32719 334 386 gr8 70746 939 298 gr9 31646 224 357 gr1 124119 704 2061 gr10 371278 2513 1900 gr2 209381 1159 2050 gr3 229315 1496 2050 gr4 203159 1575 2059 gr5 177370 1088 1708 gr6 190073 1340 1708 gr7 266971 1479 1708 gr8 623651 4161 1321 gr9 288381 1193 1900 mmc1 false 48736 APPLICATION mmc2 false 48697 APPLICATION si1 142 14 15 CSI 3295 S0920-5489(17)30414-2 10.1016/j.csi.2018.06.003 Elsevier B.V. Fig. 1 Constitution of an IOD. Fig. 1 Fig. 2 Platform validation process. Fig. 2 Fig. 3 Process of creation of description files by the community. Fig. 3 Fig. 4 Platform creation or editing of modality IOD or module configurations use-case. Fig. 4 Listing 1 Platform IOD configuration example for CR modality. Listing 1 Listing 2 Platform module example of a definitions XML file for the patient module. Listing 2 Fig. 5 DICOM validator dashboard. Fig. 5 Fig. 6 View of existing definition files for the Modality IODs. Fig. 6 Fig. 7 Edition of the definition file for the CR Modality IOD. Fig. 7 Fig. 8 Sample of the validation results obtained for the CR, CT and WSI files, showing for each file the errors occurred in each module ordered by the Data Element Type. Fig. 8 Table 1 Validation ID of for each modality file validated. Table 1 Modality Validation code CR 8r5vl6hcsm0ljoendga4jfiqud CT 593cik8vd9p4n3t0g7f172j1lq WSI 8cvjgffoat1ivin10stk6u7i8r Table 2 Qualitative comparison analysis between related works. Table 2 Validation software Open source Web based De-identification Preconditions Consistency between DICOM objects Update capability dicom3tools/dciodvfy ✓  ×   ×   ×   ×   ×  dicom3tools/ dcentvfy ✓  ×   ×   ×  ✓  ×  DVTK ✓  ×   ×   ×   ×   ×  dcm4che3 ✓  ×   ×   ×   ×   ×  DCMCHECK  ×   ×   ×   ×   ×   ×  DICOM Validator ✓ ✓ ✓ ✓  ×  ✓ A community-driven validation service for standard medical imaging objects Jorge Miguel Silva ⁎ Tiago Marques Godinho David Silva Carlos Costa DETI/IEETA, University of Aveiro, Portugal DETI/IEETA, University of Aveiro Portugal ⁎ Corresponding author. Digital medical imaging laboratories contain many distinct types of equipment provided by different manufacturers. Interoperability is a critical issue and the DICOM protocol is a de facto standard in those environments. However, manufacturers’ implementation of the standard may have non-conformities at several levels, which will hinder systems’ integration. Moreover, medical staff may be responsible for data inconsistencies when entering data. Those situations severely affect the quality of healthcare services since they can disrupt system operations. The existence of software able to confirm data quality and compliance with the DICOM standard is important for programmers, IT staff and healthcare technicians. Although there are a few solutions that try to accomplish this goal, they are unable to deal with certain situations that require user input. Furthermore, these cases usually require the setup of a working environment, which makes the sharing of validation information more difficult. This article proposes and describes the development of a Web DICOM validation service for the community. This solution requires no configuration by the user, promotes validation results’ share-ability in the community and preserves patient data privacy since files are de-identified on the client side. Keywords Web DICOM PACS Medical imaging Validator 1 Introduction In recent decades, healthcare institutions have been continuously increasing the production of digital medical imaging data. In part, this was due the increase of digital medical imaging equipment and information systems, which are now fundamental in medical diagnosis, decision support and treatment procedures. Picture Archiving and Communication System (PACS) is predominant in this field, providing tools for data acquisition, storage, distribution and visualization. It is a mature concept supported by a set of hardware and software technologies, being grounded in the Digital Imaging and Communications in Medicine (DICOM) standard to ensure normalized data formats and processes. It is a universally accepted standard in medical imaging laboratories, designed to encompass all functional aspects [1–3]. Nowadays, the communication between equipment and information systems is usually done using the DICOM standard [4]. This defines the reference information model, how data is encoded and communicated. Data is merged in structured objects that follow normalized templates per image modality, which contain metadata related with the procedure, patient, acquisition technique and institution, besides pixel data. Regular workflows are so supported by PACS [5,6], that the existence of non-conforming applications or equipment may disrupt the regular operation with potential losses in the medical undertaking [7]. Despite the existence of DICOM standard, the reality is that challenges to interoperability still arise. Furthermore, technology is constantly evolving and DICOM needs to be updated, thus, hindering compliance between equipment. The baseline to ensure interoperability between different systems is the DICOM Conformance Statement, since it provides a foundation to determine connectivity and assess the potential interoperability of two products. In some cases, it is possible to identify potential problems without ever having the products physically connected. It is a public document that must be provided by the vendor which describes the DICOM capabilities and functions implemented in a product, allowing connectivity comparisons and defining all the necessary information to perform a certain functionality [8]. DICOM validation software is important to assist in the testing of products’ DICOM conformance, providing an independent measurement of the accuracy of products’ DICOM interface. Notwithstanding, verifying the compliance of data produced by PACS applications is not trivial, since the DICOM standard supports a significant variety of modalities and information entities, each one with its specifications and dependencies. The intrinsically complex nature of this scenario motivates the development of tools and methodologies capable of testing the compliance of produced DICOM objects with the standard. This article proposes and describes the development of a Web DICOM validation service for the medical imaging community that agglutinates, in a unique way, a set of functionalities. Its use can be as simple as uploading DICOM objects to be checked, without requiring platform registration or authentication, but ensuring data privacy by removing the patient’s personal health information on the client side. Then, more complex validation tasks can be performed by the community in a collaborative way. 2 DICOM constitution DICOM Information Model (DIM) rules the organization of information structures in the standard. It specifies the relationship between DICOM objects’ information entities (IE) and real-world entities such as the patient, study, series and image. IE are computer data model abstractions for the real-word objects. Each IE may contain one or more Modules that are basic aggregations of related Data Elements (or Attributes). For instance, the patient module contains the name, ID, birth-date and sex attributes [9]. An object template is denominated as DICOM Information Object Definitions (IODs) and may contain one or more IE. IODs are normalized collections of DICOM Data Elements organized in Modules and IE (Fig. 1 ). The IOD Modules may be mandatory, conditional or user optional, as described in the DICOM Standard Part 3 [10]. Data Elements follow a TLV (Tag-Length-Value) encoding schema according to Part 5 of the standard [11]. The DICOM tag identifies an attribute using two hexadecimal numbers, called the group and element respectively. These numbers are specified as the ordered pair ( < group > ,  < element > ). For instance, Tag (0010,0010) identifies the Patient Name element of the Patient group. Length defines the size of the attribute’s Value. The Value field contains the attribute’s data. According to DICOM transfer syntax, an optional Value Representation (VR) element may also be present and specifies the attribute data type. There is also a Value Multiplicity that specifies the number of values that can be encoded in the value field of that Data Element [12]. The list of normalized Data Elements is defined in the DICOM Dictionary available in part 6 of the standard [13] and, according to the presence in Modules, are classified as: • Type 1: Attribute presence is mandatory and must have a valid value; • Type 2: Attribute presence is mandatory, but its value may be left blank; • Type 3: Attribute presence is optional. Furthermore, all types of attributes can be conditional (C), since IODs and Service-Object Pair (SOP) Classes, a combination of a DICOM service command (DIMSE) and an IOD, can define Data Elements that shall be included under certain specified conditions. Conditional types have the same requirements as their type (1, 2 or 3) under these conditions. As such, it is a protocol violation if the specified conditions are met, and the Data Element is not included. On the other hand, when the specified conditions are not met, Type C elements shall not be included in the dataset [11]. 3 Related work DICOM IODs are flexible structures and verification of objects’ compliance with standard definitions may be a complex task. Due to the need to ensure the robustness and accuracy of software applications, programmers were the first to feel the need for verification tools. Then, healthcare IT staff requested end-user software applications to confirm the conformity of enterprise DICOM network nodes and debug abnormal events. DCMCHECK 1 1 DCMCHECK: is commercial software that tries to solve this issue. It uses a specialized IOD description language which allows extensions (e.g. private elements, DICOM correction proposals) to be added to the IOD definition without changing the application itself [14]. The DICOM files are verified as conforming to the standard IOD definition (DICOM Part 3 [10])]), data structures and encoding (DICOM Part 5 [11]) and the data dictionary (DICOM Part 6 [13]). Furthermore, the DICOM File Meta Information (Preamble + DICOM Prefix + File Meta Information (0002, xxxx)) is evaluated according to the DICOM Part 10 specifications [15], as well as the consistency between it and the rest of the DICOM meta data information on the file. DICOM Validation Toolkit (DVTk) 2 2 DVTk: is an open source project for testing, validating and diagnosing problems with communication protocols in medical imaging environments [16]. It supports DICOM, HL7 and IHE integration profiles, and provides a DICOM Attribute Validator for validating DICOM files against definition files. The validator application includes GUI and command line versions, and a collection of .NET libraries for creating new validation and test tools. Moreover, it provides a DICOM Attribute Validator for validating DICOM files against definition files. There are also other examples of open-source validation software like, for instance, the dicom3tools/dciodvfy 3 3 dicom3tools/dciodvfy: and the dcm4che3 validator 4 4 dcm4che3: . In general, they can check for inconsistencies in the DICOM files against Part 3 and 5 of the standard, Multiplicity against the Data Dictionary and Data Element Value content against encoding rules defined by the standard. Moreover, dcm4che3 validator uses an undocumented XML file structure to determine the IOD structure and the mandatory Data Elements validation. The XML structure contemplates the IOD as the root element and the nested Data Elements. Furthermore, to enforce Value content validation against the VR attribute, Data Element can have an associated list of adaptable values that are useful for some attributes, for instance, the Patient’s Sex (0010,0040). The XML file also supports conditional elements by using the clauses And, If and Or, which allow the definition of dependencies. In terms of patents, an invention from 1997 [17] proposes an object-oriented structure that includes a plurality of semantic definition and validation objects, and a method that semantically validates the DICOM message by passing them through the structure and comparing the DICOM message to the provided definitions. In 2001, another patent [18] proposed a method for providing DICOM SR constraints within an XML document. To do so, the XML document was created containing DICOM SR constraints using declarative language. Moreover, a work from 2008 [19] proposes a technique that employs a XML validation document with a set of constraints specified for DICOM objects and makes use of them in validation processes. Previously described validators are representative of the state-of-the-art in this field. They provide very useful functionalities but also have major limitations. First, these validators cannot resolve static preconditions that are dependent on the exam’s protocol, rather than on the IOD itself. In other words, conditions that require input from the user to know how to validate the DICOM file. An example is the condition “C Required if contrast media was used in this image”, which is present in many IODs. Secondly, the complexity of defining an entire configuration file for each IOD. This problem is aggravated by the first limitation since it creates the need to specify many configurations for the same IOD. 4 Proposed architecture 4.1 Overview The proposed platform provides a set of functionalities supported by state-of-the-art solutions but solves the precondition problem by creating an engine that uses the input from the user to deal with the preconditions and load the required modules. Since defining an entire configuration file for an IOD is extremely complex, a modular approach was followed where each module of the IOD file is defined in a nested separated file. This is done by using an enhanced configuration interface that defines both IODs and Modules. This architecture allows re-use of the modules when creating new IOD definitions. Aiming to support programmers and research teams from academy and industry, in a collaborative way, it was decided to incorporate the validation software into an online platform where users can create and share their modules and definitions. Firstly, this creates a dynamic and synergistic environment easily able to adapt to the creation of new modalities, and secondly, reduces the need to define new configurations and simplifies use of the application. 4.2 Functional modules The community-driven DICOM Validator has two main functionalities: Validation of DICOM files and support for the creation of description files, for Modality IODs and Modules, by the community. The description file is XML-based and specifies the Modules required for a given Modality IOD or what Attributes and respective Values are necessary for each Module. The documentation regarding the structure of the definition file for both files is included in the platform, and can be accessed at Fig. 2 illustrates the validation use-case, where a user’s chosen file is uploaded to a remote server for validation. Before being sent, the file is locally modified (without overriding the user’s original one) to strip out pixel data and remove the patient’s identifiable data for security and confidentiality reasons, as shown in Section 4.4.1 After receiving the file, the server loads the suitable description file for its Modality IOD and prompts the user to select which modules they want to validate. The user is then presented with several questions regarding the preconditions necessary for better validation results. A default answer to the questions can be provided. Finally, the validation results are shown. DICOM Validator provides a dashboard where users can follow up the validation processes. It also provides resources for the creation and editing of description files (Fig. 3 ) using an embedded editor. These editing capabilities are required to make sure every description file is up-to-date and contains no errors. File editions are treated as contributions that can be commented on, and reviewed by the community before being accepted. The use of dashboard features is only accessible to authenticated members to ensure the community concept and tracing of actions. Anyone can register and sign-in to the DICOM Validator platform. All authenticated users have access to the dashboard and can manually create or edit description files in the platform using a user-friendly editor, which are validated by the platform and can then be submitted as merge requests (pull requests). These merge requests are treated as contributions that can be commented on by the community before being accepted. Platform’s administrator has the responsibility of accepting and merging these changes with the master repository to ensure the quality standards of the definition files. The authentication system is handled by third-parties such as Google, GitHub, or Dropbox by using the OAuth protocol 5 5 OAuth 2.0: . 4.3 Software layers DICOM Validator consists of three software layers: a front-end, a back-end and a dashboard which encompasses a collaborative platform for editing description files. The front-end is built using Angular 6 6 and Bootstrap 7 7 , communicating with the back-end module through a REST API. It also includes a de-identification tool developed with dcmjs 8 8 dcmjs: . The back-end module was built using Jetty 9 9 to ensure communication, and additional Java Classes to perform validation, module loading, and user session logic. This module also stores the latest versions of the description files required by the validator. Lastly, a modified version of Gitea 10 10 was used to construct the dashboard. The changes include support for communication with the back-end and XML Editing using Xonomy 11 11 . As Gitea is backed by a Git Server, that allows us to take advantage of this feature to keep the back-end description files up-to-date by syncing their respective folders with their repository’s master branch as shown in Fig. 4 . 4.4 Workflows 4.4.1 Pixel data removal and de-identification Typically, the DICOM file element requiring most storage space is the pixel data. For instance, the extreme case of DICOM objects for pathology imaging may require dozens of gigabytes [20,21]. Since the validation process does not request analysis of visual information, removal of pixel data on the client-side has the advantage of reducing the file’s upload time as well as the storage space required at the server. Furthermore, by removing pixel data, we are also eliminating any PHI that can be burned into images of some modalities such as in Ultrasound (US) and External-camera Photography [22]. DICOM standard PS3.15 Annex E provides the standard de-identification guidelines [23]. Table 3 in the Additional Material provides comprehensive information regarding actions that the DICOM standard recommends for the Basic Profile attributes that we de-identified, and examples of what we provided as value for that attribute. However, our de-identification mechanism is not completely compliant with the standard, since we intended to create a fast mechanism that could easily replace the values of the attributes with dummy values consistent with the original attribute’s value in terms of length and data type. This was done to ensure that we would not affect the DICOM object’s validation regarding its structure. The unique objective is to use the data in the validation process, not the archiving or sharing of DICOM anonymized data. The proposed platform uses dcmjs, a Javascript cross-compilation of dcmtk 12 12 dcmtk: , to transform the input DICOM file into a representative XML file. The pixel data is removed in this transcoding process and the metadata is de-identified to ensure patient privacy and confidentiality [24]. The de-identification process is done by reading the XML elements and replacing the fields containing the patient’s personal health information (PHI). The replacement (anonymous) sequence has the same length and domain type of original data, and is in accordance with the lists of adaptable values for that Data Element (see Section 5). For example, a PatientName attribute with the content “DoeJohn” would become “REMOVEDR” and a PatientAge with 26 would become 00. Once this process is concluded, the XML file is transformed again into DICOM and passed to the server for validation. 5 Validation process Our web interface lies in a Model-View-Controller (MVC) paradigm and the framework works following the Service-Oriented Architecture (SOA), where the client-server communication is performed via Restless services. Our services are stateful and can be consumed by third party. The services exposed by our API are the following: • /configure: endpoint where files are uploaded to the platform and, as response, it is returned a validation ID and a set of modules and options. • /validate: to provide the answer to “/configure” service. For instance, which modules will be validated, as well as the usage (or not) of the default answers for the preconditions. It returns the precondition questions or the file’s validation result if the user opted by the default answers to the preconditions. • /result: receives a validation ID and a response to the precondition questions (optional); and returns the file’s validation results. The validation process is released on the server side. When a file reaches the application’s server, the DICOM metadata is read to obtain the SOPClassUID, which is used to identify the medical imaging modality and select the respective description file to be used in the validation process. The selection of an appropriate description file is essential to obtain reliable results, since distinct DICOM modalities have different requirements. An IOD description file is made by including the modules and preconditions that the user must answer. The preconditions can be placed either at the level of the IOD or the module depending on where they will be used (i.e., for inclusion of Modules or Data Elements, respectively). Listing 1 shows part of the IOD description file for the CR Modality IOD. Included Modules are defined in the file by their Information Entity (IE), module name and use (Mandatory (M), Conditional (C) or User Defined (U)). A precondition is defined with an id name, a default value and a question that will be asked to the user during the validation process. Besides preconditions, Module description files may include Macro Attribute tables and contain the Data Elements in accordance with the standard (Listing 2 ). Each Data Element is categorized by a keyword, a tag, a VR, a VM, a type and the number of items (only if VR is a Sequence (SQ)). Furthermore, each Data Element can have an associated list of acceptable values. Platform users may be questioned for two reasons: to select the modules to be validated and to answer preconditions. Depending on the user’s actions, modules may be excluded or interpreted in diverse ways. Description file parsing is made using a custom XML handler based on dcm4che3’s SAXHandler 13 13 . and data validation is made using dcm4che3’s validator. Since dcm4che3 validator natively ignores the validation of the Data Elements of type 3, it was necessary to adapt its behaviour to add support for validation of elements of this type. Moreover, the proposed validator extends the dcm4che3 validator by adding additional conditions, preconditions and a more modular approach to inclusion of the module in the IOD description files. The results of the validation process are shown in 5 types of categories regarding the validation of each module: • Valid, when all of the module’s content is in accordance with the standard; • Warning, when Data Elements of type 3 are not compliant with the standard; • Skipped, when the user chooses not to select a specific IOD module for validation; • Unsatisfied condition, when a requested condition for validating a module was not met; • Has Errors, when Data Elements of type 1 and 2 are not in accordance with the standard. All the messages exchanged between the client and server modules (e.g. user’s answer to questions and results of validation process) are based on JSON object. 6 Results 6.1 Management dashboard The Validator’s Dashboard page allows users to browse and edit the contents of Modules, Modality IODs and Macro Attribute Table repositories (Fig. 5 ). The user can view the existing definition files for the Modality IODs (Fig. 6 ) and Modules. The dashboard can show the user the history of their previous validations. Since the solution is back-end by a Git server, the platform can track the updating of description files by the different community members, accepting (merging) or rejecting the contribution requests, controlling in this way the quality of definition files used in production. Before submitting a new (file) contribution, the user must specify what changes were made by providing a title for the contribution and a description. The community can comment on the changes and suggest improvements, and the platform administrator can even approve the changes, merging the new code with the existing one in the master workspace and triggering the back-end validation service to update its description files to support the new ones. A description file can be edited in the platform using a user-friendly editor (Fig. 7 ). This facilitates the creation of description files by automating several important tasks. Besides editing, the user can “open issues” for situations that may be happening, and view changes made by other users. After the editing is finished, the changed file is submitted to the back-end service, where the validator evaluates this new contribution and enumerates eventual errors found while parsing the description file. To do so, it first tries to identify the content as belonging to a file definition of a module or IOD. Once identified, it attempts to convert the XML to an IOD Module object or an IOD object, respectively. If the build fails, it is because the file structure is wrong, and an error is reported to the user. The user can then identify in which lines an error is present and correct them. If the validator ensures there are no errors, the description file can be submitted for community review. 6.2 Validation service Our proposed platform can be easily accessed via the URL To evaluate the validation service, 3 DICOM files from distinct modalities were used and originated from a publicly available dataset: • A Computed Radiography (CR) modality, from Belarus Tuberculosis Portal (Belarus TB) 14 14 Belarus TB: • A Computed Tomography (CT) from the DICOM Library 15 15 DICOM Library: • A Whole Slide Imaging (WSI) fromDICOM WG26 -Pathology 16 16 Nema: . For each of the previous samples, the corresponding IOD definition and module files were created and made available in the DICOM Validator platform. Next, the files went through the pipeline described in Fig. 2, where preconditions were answered as default. The validation results are presented in the Fig. 8 and can be consulted by clicking on the validation codes shown in Table 1 . Each validated module is classified using one of the categories presented in Section 5 (Valid, Has Errors, Warning, Skipped and Unsatisfied Condition). The results can be displayed as a summary or in a detailed view. The latter shows the attributes that are missing or have invalid values. While analyzing other validators we noticed a lack of warnings regarding Type 3 attributes. Although they are legal, our line of reasoning was that their presence or absence as well as if they are in accordance with the standard should be displayed to the user. However, the display of such results may make it hard to distinguish important from unimportant results. As such, the platform provides a checkbox list that allows the users to select what types of error they wish to view after the validation. An important aspect is that, after the validation, the user can go directly to DICOMLookup 17 17 DICOMLookup: . This provides the ability to link to context-sensitive information about each error. However, the DICOMLookup service is not normative, nor necessarily maintained nor current. Therefore, we added a link directly to the CHTML online DICOM standard itself. 7 Discussion 7.1 Relevance of the work The acceptance of DICOM standard by medical industry and researchers has brought to the market many devices that are sold as being DICOM conformant. However, in practice, the interoperability between different DICOM equipment is not always accomplished easily and completely. For instance, a study evaluated the DICOM conformance of cardiac X-ray Angiography objects from ten vendors using two validation tools and five visualization applications [25]. The results showed that only two datasets were completely conformant with the DICOM standard and three contained serious errors that inhibited their correct visualization. Another important example was performed at the German Congress of Radiology 2006, where radiologists were invited to bring their CD for a short test against the CD specification published by German Society of Radiology, and 80% of those “real world” CDs failed the test [26]. 7.2 Comparison with State-of-the-art and solution limitations Table 2 compares the different State-of-the-art solutions with proposed DICOM Validator. Currently, neither the DICOM Standards Committee nor MITA (NEMA) have any official tool or certification mechanism. However, evidence points to the fact that the existence of validation tools is fundamental to ensure interoperability between systems, since the dciodvfy tool and DVTk are used in IHE Connectathons for helping in the evaluation of systems interoperability [27]. As it can be observed in Table 2, most state-of-the-art solutions possess limitations that are covered by our solution, namely the fact that these validators are not web based, cannot resolve static preconditions and have a limited capability of keeping updated with the standard. On the one hand, the fact that our system can run on a Web browser allows the user to perform simple tests without the need to deploy the system locally. On the other hand, the ability to resolve preconditions, as well as to host a community, provides users with more reliable validation results whilst maintaining the platform up to date with the standard. The dciodvfy tool was Internet-deployed, but it is no longer available for the community. It used a CGI script on a web server that returned the output of an executable as if it runs in a command line. The tool did not support the de-identification of the images prior to uploading nor provided any mechanism for specifying predicates or to address real world conditional requirements. Another important aspect is that the majority of these validators do not perform de-identification of PHI tags. Relatively to the platform trust model, there are no benefits from sending the identifiable object to the server. Independently of the trust in the organization that operates the server, it is always preferred to not share sensitive data with third entities, inclusively by legal restrictions [28]. The user feels more comfortable to use a service with this modus operandi. Moreover, the de-identification at client-side is an automatic process, not being a burden to the user. Finally, by removing the pixel data, the volume of data and consequent upload time is significantly reduced, which is fundamental to improve user experience. Furthermore, there are some validators such as DVTK and dicom3tools/dcentvfy that support the evaluation of consistency between DICOM objects. Although this feature is important, it is worth noting that our validator was designed to validate individual DICOM objects and not to evaluate or compare the consistency between DICOM objects. As such, to perform such tasks one requires the use of the priorly mentioned validators. 7.3 Future work Although we covered a large amount of requirements in order to create a solid, updated system, there are some aspects that we hope to tackle in future developments of our work. Firstly, although the online service provided by the validator platform can be useful for some usage scenarios, developers may also request bulk verification that runs locally as well as its incorporation in automated regression testing frameworks. As such, one good improvement to this work will be the development of a command line extension that will consume the definition files created by the Validator’s online community. Secondly, a side-effect associated to the de-identification of identifiers’ attributes is that we are not able to validate them at server side. A possible solution is the user select to bypass the de-identification mechanism. Removing pixel data is useful in terms of efficiency, since we are reducing the file’s upload time as well as the storage space required at the server. However, by doing so, proposed solution is failing to validate the actual pixel data, regarding its length and encoding. Currently, the platform maintains the number of frames of the file but does not consider the encoding and consequentially the length. As such, one possible improvement would be to upload pixel data at the expense of performance or to pass this information to the validator by extracting the information from the pixel data (7FE0,0010) prior to remove it at client side of the application, and then compare these values to the ones present in the metadata of the file. Another important remark regarding the validator’s design was the use of preconditions. Although useful, answering all the precondition questions can be a quite tiresome and time-consuming task. On the other hand, leaving the selection to default can lead to the creation of many errors. As such, in future work, one could improve this work by creating a Management Dashboard feature that could let the user specify the granularity of the preconditions the user intends to respond. Finally, another future improvement would be to provide users with a tool that could parse the DocBook XML of DICOM standard automatically and build our XML files. This tool would decrease the amount of user effort when creating new description files. 8 Conclusion Digital medical imaging laboratories rely greatly on DICOM standard to ensure interoperability between different equipment. However, manufacturers’ implementation of this standard may have non-conformities at several levels and medical staff may be responsible for data inconsistencies when entering data. The capacity to validate the quality of data and its compliance with the DICOM standard is a fundamental issue to avoid disruption in services. This article presents an innovative community-driven web validation service for DICOM files. It runs in a common Web browser and can be safely used to validate real-world files since they are de-identified on the client side. The community can contribute to improving the platform by creating or editing description files used in the validation process. This means the platform can be updated whenever a new change is made to the DICOM standard or to resolve any issue that may produce unsatisfactory results and lead to unintended mistakes by the user. These contributions are always guided by a graphic interface tool and final verification is ensured before being submitted for the community’s approval, meaning description files are less prone to errors. The results of file validation are saved on the database and can be accessed and shared by the user. In conclusion, we propose a solution that solves problems faced by other state-of-the art validators and is also prepared to evolve along with the DICOM standard. Acknowledgments Jorge Miguel Silva is funded by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation COMPETE 2020 Programme, and by National Funds through the FCT Portugal (Fundação para a Ciência e a Tecnologia) within project CMUP-ERI/ICT/0028/2014 SCREEN-DR. Tiago Marques Godinho is funded by FCT under grant agreement SFRH/BD/104647/2014. Supplementary material Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.csi.2018.06.003. Appendix A Supplementary materials Supplementary Data S1 Supplementary Raw Research Data. This is open data under the CC BY license Supplementary Data S1 Supplementary Data S2 Supplementary Raw Research Data. This is open data under the CC BY license Supplementary Data S2 References [1] H.K. Huang PACS and Imaging Informatics: Basic Principles and Applications second ed. 2010 Wiley [2] C. Creighton A literature review on communication between picture archiving and communication systems and radiology information systems and/or hospital information systems J. Digit. Imag. 12 3 1999 138 143 10.1007/BF03168632 [3] T.J. St. Cyr An overview of healthcare standards 2013 Proceedings of IEEE Southeastcon 2013 IEEE 1 5 10.1109/SECON.2013.6567436 [4] P. Mildenberger M. Eichelberg E. Martin Introduction to the DICOM standard Eur. Radiol. 12 4 2002 920 927 10.1007/s003300101100 [5] A.A.G. Webb Introduction to Biomedical Imaging 2003 Wiley-Interscience [6] Realization of integration and working procedure on digital hospital information system Comput. Stand. Interf. 25 5 2003 529 537 10.1016/S0920-5489(03)00017-5 [7] K.J. Dreyer D.S. Hirschorn J.H. Thrall A. Mehta PACS: A Guide to the Digital Revolution 2006 Springer New York [8] NEMA Digital Imaging and Communications in Medicine (DICOM) Part 2 : Conformance 2018 [9] S.C. Horii Dicom S.G.L. George C. Kagadis Informatics in Medical Imaging 2011 CRC Press 48 74 10.1201/B11382-6 [10] NEMA Digital Imaging and Communications in Medicine (DICOM) Part 3 : Information Object Definitions 2018 [11] NEMA Digital Imaging and Communications in Medicine (DICOM) Part 5: Data Structures and Encoding 2018 [12] H. Oosterwijk P.T. Gihring DICOM Basics 2005 OTech Inc [13] NEMA Digital Imaging and Communications in Medicine (DICOM) Part 6 : Data Dictionary 2018 [14] A.J. Hewett H. Grevemeyer A. Barth M. Eichelberg P.F. Jensch Conformance testing of DICOM image objects S.C. Horii G.J. Blaine Proceedings of the SPIE, Volume 3035, p. 480–487 (1997). vol. 3035 1997 480 487 10.1117/12.274604 [15] NEMA Digital Imaging and Communications in Medicine (DICOM) Part 10: Media Storage and File Format for Media Interchange 2018 [16] G. Potter R. Busbridge M. Toland P. Nagy Mastering DICOM with DVTk J Digit Imaging 20 1 2007 47 62 10.1007/s10278-007-9057-0 [17] T.M.B.C.C.C.M.D.G.T. Tian Helen He (Plano, Method for validating a digital imaging communication standard message, 1997. [18] N.Y.U. Lee Kwok Pun (Flushing Specifying DICOM Semantic Constraints in XML 2001 [19] N.H.U. Guo Dongbai (Nashua Techniques for checking whether a complex digital object conforms to a standard 2008 [20] R. Singh L. Chubb L. Pantanowitz A. Parwani Standardization in digital pathology: supplement 145 of the DICOM standards. J. Pathol. Inform. 2 2011 23 10.4103/2153-3539.80719 [21] An efficient architecture to support digital pathology in standard medical imaging repositories J. Biomed. Inform. 71 2017 190 197 10.1016/J.JBI.2017.06.009 [22] J.M. Silva E. Pinho E. Monteiro J.F. Silva C. Costa Controlled searching in reversibly de-identified medical imaging archives J. Biomed. Inform. 77 2018 81 90 10.1016/J.JBI.2017.12.002 [23] NEMA DICOM standard PS3.15 Annex E 2018 [24] T. Douraki Ethical and legal dimensions of medical confidentiality in European law of human rights Manage or Perish? 1999 Springer US Boston, MA 421 427 10.1007/978-1-4615-4147-9_50 [25] M.H. Baljon M.G. Gerritsen M. Eichelberg P. Jensch Quality Control using Automated Validation Tools can Improve Interoperability of DICOM Implementations 1999 [26] M. Onkena M. Eichelberga J. Riesmeierb P. Mildenbergerc Image distribution and integration strategies - exchange of radiological images on DICOM CD: a survey of the state of technology in germany Int. J. Comput. Assist. Radiol. Surg. 2 1 2007 290 292 10.1007/s11548-007-0101-9 [27] D. Clunie DICOM Validator - dciodvfy 2015 [28] M. Sariyar I. Schluender C. Smee S. Suhr Sharing and reuse of sensitive data and samples: supporting researchers in identifying ethical and legal requirements. Biopreserv Biobank. 13 4 2015 263 270 10.1089/bio.2015.0014 "
    },
    {
        "doc_title": "SCREEN-DR: Collaborative platform for diabetic retinopathy",
        "doc_scopus_id": "85055449567",
        "doc_doi": "10.1016/j.ijmedinf.2018.10.005",
        "doc_eid": "2-s2.0-85055449567",
        "doc_date": "2018-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Artificial intelligence algorithms",
            "Classification models",
            "Collaborative platform",
            "Computer aided methods",
            "Diabetic retinopathy",
            "Diabetic retinopathy screening",
            "Production environments",
            "Role-based Access Control",
            "Algorithms",
            "Artificial Intelligence",
            "Diabetic Retinopathy",
            "Humans",
            "Image Interpretation, Computer-Assisted",
            "Machine Learning",
            "Mass Screening",
            "Software"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.Background and objective: Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss. Screening programs, based on retinal imaging techniques, are fundamental to detect the disease since the initial stages are asymptomatic. Most of these examinations reflect negative cases and many have poor image quality, representing an important inefficiency factor. The SCREEN-DR project aims to tackle this limitation, by researching and developing computer-aided methods for diabetic retinopathy detection. This article presents a multidisciplinary collaborative platform that was created to meet the needs of physicians and researchers, aiming at the creation of machine learning algorithms to facilitate the screening process. Methods: Our proposal is a collaborative platform for textual and visual annotation of image datasets. The architecture and layout were optimized for annotating DR images by gathering feedback from several physicians during the design and conceptualization of the platform. It allows the aggregation and indexing of imagiology studies from diverse sources, and supports the creation and annotation of phenotype-specific datasets to feed artificial intelligence algorithms. The platform makes use of an anonymization pipeline and role-based access control for securing personal data. Results: The SCREEN-DR platform has been deployed in the production environment of the SCREEN-DR project at http://demo.dicoogle.com/screen-dr, and the source code of the project is publicly available. We provide a description of the platform's interface and use cases it supports. At the time of publication, four physicians have created a total of 1826 annotations for 701 distinct images, and the annotated data has been used for training classification models.",
        "available": true,
        "clean_text": "serial JL 271161 291210 291773 291870 291901 291919 31 International Journal of Medical Informatics INTERNATIONALJOURNALMEDICALINFORMATICS 2018-10-18 2018-10-18 2018-10-26 2018-10-26 2018-11-05T07:54:53 S1386-5056(18)30483-0 S1386505618304830 10.1016/j.ijmedinf.2018.10.005 S300 S300.1 FULL-TEXT 2018-12-15T18:47:24.199866Z 0 0 20181201 20181231 2018 2018-10-18T19:21:25.150617Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor highlightsabst primabst ref specialabst 1386-5056 13865056 true 120 120 C Volume 120 15 137 146 137 146 201812 December 2018 2018-12-01 2018-12-31 2018 Regular Papers article fla © 2018 Elsevier B.V. All rights reserved. SCREENDRCOLLABORATIVEPLATFORMFORDIABETICRETINOPATHY PEDROSA M 1 Introduction 2 Background 2.1 Digital medical imaging laboratories 2.2 DR screening platforms 3 Methods 3.1 Platform requirements 3.2 Architecture 3.3 RTS 3.4 Annotation service 3.5 Search, transfer and dataset creation 3.6 Upload and anonymization 3.7 Access control 3.8 User interface features 3.8.1 Searching 3.8.2 Dataset management 3.8.3 The upload tab 3.8.4 Textual annotation 3.8.5 Lesion annotation 3.8.6 PACScenter integration 4 Results 5 Discussion 6 Conclusions Author's contribution Acknowledgements References 2008 DIABETICRETINOPATHY WORLDHEALTHORGANIZATION 2010 GLOBALDATAVISUALIMPAIRMENTS2010TECHREP YAU 2012 556 564 J PRENTASIC 2016 281 292 P HUANG 2009 H PACSIMAGINGINFORMATICS DREYER 2006 K PACSAGUIDEDIGITALREVOLUTION HU 2009 788 794 J JORRITSMA 2014 27 36 W PIANYKH 2012 O DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOM VALENTE 2013 e61888 F COSTA 2009 71 77 C GODINHO 2016 367 375 T RIBEIRO 2014 1404 1412 L VALENTE 2012 356 364 F MOORE 1999 245 254 M BASTIAOSILVA 2018 33 42 L HAMDI 2014 100 112 O CHIU 2006 369 383 Y SUZUKI 2013 772 783 K ACHARYA 2012 2011 2020 U ROYCHOWDHURY 2014 1717 1728 S EGE 2000 165 175 B KAUPPI 2013 368514 T TRUCCO 2013 3546 E GULSHAN 2016 2402 V RAHIM 2015 69 79 S ENGINEERINGAPPLICATIONSNEURALNETWORKS AUTOMATICDETECTIONMICROANEURYSMSFORDIABETICRETINOPATHYSCREENINGUSINGFUZZYIMAGEPROCESSING RUBIN 2008 626 630 D PAPADOPOULOS 2017 D EXTREMECLICKINGFOREFFICIENTOBJECTANNOTATIONVOL8 BINNS 2018 377 R VALENTE 2016 284 296 F KULKARNI 2011 402 417 P PEDROSA 2018 330 337 M HURSCH 1995 W SEPARATIONCONCERNS SILVA 2018 81 90 J MONTEIRO 2017 89 E NEMA 2017 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMSUPPLEMENT163STOREOVERWEBBYREPRESENTATIONSSTATETRANSFERRESTSERVICESSTOWRS YAN 2012 1350 1357 Y ACTIVELEARNINGMULTIPLEKNOWLEDGESOURCES GRISAN 2008 310 319 E TRIBUNA 2017 672 680 L MEYER 2017 507 515 M ADEEPNEURALNETWORKFORVESSELSEGMENTATIONSCANNINGLASEROPHTHALMOSCOPYIMAGESVOL7 COSTA 2017 516 523 P ADVERSARIALSYNTHESISRETINALIMAGESVESSELTREESVOL7 REMESEIRO 2017 4520 4527 B ARAUJO 2017 101341K T INTERNATIONALSOCIETYFOROPTICSPHOTONICSVOL3 ESTIMATIONRETINALVESSELCALIBERUSINGMODELFITTINGRANDOMFORESTS MENDONCA 2017 101341L A INTERNATIONALSOCIETYFOROPTICSPHOTONICSVOL3 AUTOMATICSEMIAUTOMATICAPPROACHESFORARTERIOLARTOVENULARCOMPUTATIONINRETINALPHOTOGRAPHS COSTA 2017 10 P PEDROSAX2018X137 PEDROSAX2018X137X146 PEDROSAX2018X137XM PEDROSAX2018X137X146XM 2019-10-26T00:00:00.000Z 2019-10-26T00:00:00.000Z © 2018 Elsevier B.V. All rights reserved. item S1386-5056(18)30483-0 S1386505618304830 10.1016/j.ijmedinf.2018.10.005 271161 2018-12-15T18:47:24.199866Z 2018-12-01 2018-12-31 true 1567951 MAIN 10 53299 849 656 IMAGE-WEB-PDF 1 ga1 true 7188 164 165 gr1 4100 164 153 gr2 3847 164 114 gr3 7502 133 219 gr4 7117 164 164 gr5 7096 164 213 gr6 4974 164 186 gr7 5158 84 219 gr8 12154 164 145 gr9 10958 110 219 ga1 true 6661 200 201 gr1 14500 363 339 gr2 24087 487 339 gr3 35059 342 565 gr4 23080 339 339 gr5 18819 261 339 gr6 12037 298 339 gr7 33474 217 565 gr8 67036 637 565 gr9 40820 339 678 ga1 true 57304 886 892 gr1 110496 1604 1500 gr2 197797 2153 1500 gr3 338052 1514 2500 gr4 188171 1500 1500 gr5 142752 1153 1500 gr6 90935 1319 1500 gr7 286527 958 2500 gr8 767468 2819 2500 gr9 379866 1500 3000 am 3343358 IJB 3763 S1386-5056(18)30483-0 10.1016/j.ijmedinf.2018.10.005 Elsevier B.V. Fig. 1 Use-cases that the SCREEN-DR platform must contemplate. Fig. 1 Fig. 2 SCREEN-DR architecture. Fig. 2 Fig. 3 R-PACS semi-structured data model, comprising the DICOM model and the annotation extensions. Fig. 3 Fig. 4 RTS overall architecture. Fig. 4 Fig. 5 Textual annotation pipeline for the SCREEN-DR platform. Fig. 5 Fig. 6 SCREEN-DR authentication process. Fig. 6 Fig. 7 View of the SCREEN-DR platform search tab. Fig. 7 Fig. 8 Views of the SCREEN-DR platform annotation and lesion tabs. Annotation and contrast tools are shown at the top left. Fig. 8 Fig. 9 View of the SCREEN-DR platform PACSCenter tab. Fig. 9 Table 1 Number of annotations performed in each stage. Table 1 Annotated Image quality 1826 Bad 116 Partial 685 Good 1025 Diagnosis 1710 Table 2 The average time of an image evaluation on the quality stage, discriminated per readability. Table 2 Quality Time Actions TpA Bad 13.8 2.6 5.4 Partial 18.4 6.4 2.9 Good 15.7 6.2 2.5 Table 3 The average time taken to diagnose an image, discriminated per characteristic. Table 3 Quality &diagnosis Time Actions TpA R0 auto M0,P0 16.4 4.3 3.8 Retinopathy R0 22.5 4.6 4.8 R1 32.3 5.4 6.0 R2-M 32.3 6.8 4.8 R2-S 42.6 7.2 5.9 R3 67.0 7.7 8.7 RX 49.2 8.6 5.7 Maculopathy M0 41.6 6.3 6.6 M1 34.5 6.7 5.1 MX 36.2 6.3 5.8 Photocoagulation P0 32.7 6.6 5.0 P1 62.2 8.6 7.2 P2 46.9 7.0 6.7 PX 33.4 6.7 5.0 SCREEN-DR: Collaborative platform for diabetic retinopathy Micael Pedrosa Jorge Miguel Silva ⁎ João Figueira Silva Sérgio Matos Carlos Costa DETI/IEETA, University of Aveiro, Portugal DETI/IEETA, University of Aveiro Portugal ⁎ Corresponding author. Graphical abstract Background and objective Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss. Screening programs, based on retinal imaging techniques, are fundamental to detect the disease since the initial stages are asymptomatic. Most of these examinations reflect negative cases and many have poor image quality, representing an important inefficiency factor. The SCREEN-DR project aims to tackle this limitation, by researching and developing computer-aided methods for diabetic retinopathy detection. This article presents a multidisciplinary collaborative platform that was created to meet the needs of physicians and researchers, aiming at the creation of machine learning algorithms to facilitate the screening process. Methods Our proposal is a collaborative platform for textual and visual annotation of image datasets. The architecture and layout were optimized for annotating DR images by gathering feedback from several physicians during the design and conceptualization of the platform. It allows the aggregation and indexing of imagiology studies from diverse sources, and supports the creation and annotation of phenotype-specific datasets to feed artificial intelligence algorithms. The platform makes use of an anonymization pipeline and role-based access control for securing personal data. Results The SCREEN-DR platform has been deployed in the production environment of the SCREEN-DR project at and the source code of the project is publicly available. We provide a description of the platform's interface and use cases it supports. At the time of publication, four physicians have created a total of 1826 annotations for 701 distinct images, and the annotated data has been used for training classification models. Keywords Diabetic retinopathy screening Collaborative PACS Telemedicine Computer-aided diagnosis Image annotation 1 Introduction Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss [1], being a leading cause of blindness worldwide. According to the World Health Organization (WHO), 1% of all blindness cases in 2010 were attributed to this disease [2]. Moreover, it has a significant economic impact, particularly in industrialized countries. The disease has distinct progression phases, and an accurate diagnosis of the disease stage is required for a prompt and appropriate treatment to prevent visual loss. However, the disease is asymptomatic during early stages which makes its detection difficult. Therefore, many communities promote regular screening programs in the diabetic population to ensure a timely treatment [1,3]. Following these guidelines, the Portuguese North Health Administration (ARSN 1 1 ) is implementing a mass DR screening to reach around 75% of a population of 250,000 diabetic patients in the north of Portugal. The screening process consists of taking pictures of the patients’ eye fundus using mobile retinographers, which are sent to a centralized repository and posteriorly analyzed by experts. Healthy cases and examinations with bad image quality are discarded from the next screening stages. Cases identified with relevant DR are forwarded for complementary evaluation and treatment in a center of reference. Other pathologies are not considered in this screening process and the images will follow the current clinical protocol. In such mass screenings, diagnosis is a daunting task due to the large number of cases that must be analyzed. Moreover, in about 75% of cases, the patient does not show any DR manifestations, which means that experts spend the majority of time viewing non-relevant cases. The SCREEN-DR project was created to address this limitation in the DR screening process. It aims to research and develop an image analysis and machine learning (ML) platform for innovation in DR screening, based on the hypothesis that computer-aided DR detection methods [4] could be used to identify pathologic cases and indicate them to ophthalmologists. The project faces three main challenges: automatically discarding low quality images, which are not usable for diagnosing DR; automatically detecting cases without This article presents the SCREEN-DR collaborative platform, created to address the project's specific requirements, which cannot be satisfied by conventional Information and Communications Technology (ICT) platforms available in radiology departments, and to promote collaboration between physicians and researchers in the scope of a regional DR screening program. Within the project, the role of the researchers is to create classification algorithms to evaluate image quality, discard cases without DR, locate possible lesions and grade DR severity, while physicians are responsible for annotating datasets, including visual delineation of lesions. The collaborative platform collects the studies, indexes image metadata, and supports the creation and annotation of datasets. Result from an advanced search mechanism, which supports multi-source queries over the annotated data, can be exported or used to create new training datasets. 2 Background 2.1 Digital medical imaging laboratories Digital medical imaging environments are grounded on the Picture Archiving and Communication System (PACS) concept, referring to the set of software and hardware units responsible for the acquisition, storage, distribution and visualization of medical imaging studies [5–7]. The core component is the storage server (i.e. the archive), which receives images from different imaging modalities, stores them in a structured way, and supports content discovery and retrieval. These digital medical imaging environments are fundamental to ensure a fully digital and distributed workflow where the stakeholders can be anywhere, including the departmental intranet or at home [8]. The interoperability between equipment and information in digital medical imaging laboratories is ensured by the Digital Imaging and Communications in Medicine (DICOM) standard [9]. It defines the reference information model, that is, how data is encoded and transmitted. Data is agglutinated in DICOM object structures containing metadata related to procedure, patient, acquisition modality and institution, besides pixel data. In a traditional PACS environment, the archive serves a single organizational domain and authorized users have access to the whole repository, making accounting mechanisms unnecessary. However, more recent usage scenarios brought new requirements such as the coexistence of distinct ownership domains in the same server instance. For instance, research and collaborative environments deployed at the Web need to support policies for controlling access to resources. In the context of the SCREEN-DR project, a researcher may have a private dataset and share part of the resources with one or more physicians. Moreover, production medical imaging repositories are often looked at as “inert bags“ of DICOM objects. However, there are already some research-oriented PACS, such as the Dicoogle open source platform [10], a new PACS archive concept with an agile indexing and retrieval mechanism [11] that can be used in DICOM data mining tasks. Following the recent trends of Internet market, healthcare institutions are outsourcing their repositories to the Cloud and providing solutions as a service [12,13]. In such context, developing and managing DICOM based applications with special communication, decoding and visualization requirements is often a problematic barrier. Zero-Footprint Viewers (ZFV) are the state-of-the-art clients that use standard browsers and plugins to provide access to medical imaging repositories [14]. The integration of those DICOM-Web tools is fundamental in the context of a modern collaborative platform that aims to support medical imaging workflows. 2.2 DR screening platforms Healthcare is ever-evolving from the initial telemedicine software for videoconferencing, telemonitoring and teleradiology [15] to the integration of computer-aided diagnosis in telemedicine solutions. In the last few years, a large number of medical research projects have emerged around the world that make use of communication systems to achieve data integration [16], facilitate the patient access to their diagnosis [17] and create community-based screening platforms [18]. Several screening programs are currently in operation, for example in the UK, 2 2 UK Screening Programs: Ireland, 3 3 Ireland Screening Programs: Netherlands, 4 4 Netherlands Screening Programs: or in several states in the U.S. as in NC. 5 5 U.S.-NC Screening Programs: There are also evidences that automatic screening should be introduced in DR screening programs [19]. Regarding the DR automatic analysis, although there are positive results in the literature, the majority of research is highly optimized for small datasets [20–22], compromising generalization and the required standards for mass screening applications [20]. Furthermore, there is a lack of comprehensive studies applying novel image processing and big data analysis techniques to fundus images. Validation of automated DR screening methods is a key issue and has been reviewed by several authors [23,24]. These works describe several publicly available datasets of different detail and complexity that can be good starting points for training image processing pipelines. An ICT platform is a major need for the management of large amounts of image data, such as the retinal images produced in a screening program. Moreover, projects with large datasets require the extensive participation of ophthalmologists in the annotation phase [25], and an online platform for managing the process. Methods such as inserting annotations in Excel files [26] do not enable the collection of ROIs nor lesion pinpointing. Plugins for image viewers such as OsiriX-iPad [27] have the ability to annotate Regions of Interest (ROI), yet those are patient-centric platforms and do not offer an optimized pipeline to annotate massive numbers of images following a blind annotation process. There are already several attempts to construct such platforms, such as CrowdFlower 6 6 or RectLabel. 7 7 Yet, in some cases, there is the need to build customized annotation protocols that are highly optimized for working with thousands of images [28]. Google has created an annotation tool [25] for DR automatic diagnosis, but it was designed for annotating a single dataset containing 128,175 macula-centered images. Furthermore, it does not allow annotating lesions, and therefore it is not possible to validate if their algorithms can identify the regions where lesions are present on the image. Moreover, the interpretability of the model is an important step in making ML techniques accepted on computer-aided diagnosis (CAD) [29]. This importance is enlarged on GDPR Article 13, where it states that the subject has the right to “meaningful information about the logic involved“. Annotations of lesions can provide the required input to improve interpretability, hence, the importance of our work. 3 Methods 3.1 Platform requirements Fig. 1 shows the main actors and use-cases that must be supported by the SCREEN-DR platform. The platform needs to provide means for the creation and management of large textually and visually annotated phenotype-specific repositories to support training distinct ML classifiers. This requires a quick and user-friendly way for physicians to perform textual annotations on a dataset, as well as the necessary tools to perform detailed and lesion-specific visual annotations on the images. The platform must also accept multiple annotations from different physicians on the same image, allowing the creation of a consensus by combining different annotators and algorithms. Machine learning researchers also require the means to create and manipulate the datasets that will feed their classification models. As such, the platform must offer a way for researchers to browse, query and filter the annotated repository, using multi-source information from both textual and visual annotations, together with the metadata provided in the DICOM files. In addition, the platform must also encompass a way for researchers to easily and intuitively export the filtered datasets. The platform must also include an authentication module with single-sign-on support to allow simplified controlled access for each specific-role actor (physician, researcher, data provider), an anonymization pipeline to ensure patient confidentiality and, finally, provide a Web based interface to ensure an online collaborative environment. 3.2 Architecture Fig. 2 presents the overall architecture of the SCREEN-DR platform. It was developed on top of the Dicoogle open source PACS 8 8 dicoogle: and follows a modular approach, exploring Dicoogle's SDK and plugin-based structure for creating new functionalities [10,30]. Specifically, Dicoogle allowed us to develop a Retinopathy-PACS (R-PACS) responsible for storing and indexing the SCREEN-DR DICOM files. All storage, indexing and retrieval capabilities are delegated to independently deployed components, enabling the support of a multitude of use cases. The R-PACS has a semi-structured model, stored in a relational database and indexed with Elasticsearch 9 9 Elasticsearch: , a distributed index and analytics engine exposing RESTful services. Its data model extends the standard DICOM Information Model (DIM), a hierarchical organization that contemplates four levels (Patient, Study, Series and Image/Instance), to include a generic annotation model capable of accommodating distinct research requirements, as shown in Fig. 3 . The semi-structured model was implemented with PostgreSQL 10 10 PostgreSQL: tables and binary JSON for the flexible annotation schema. All annotations performed by an annotator are linked to the image, the smallest unit of the DICOM model. Annotation sections are provided by node types (e.g. image quality assessment or diagnosis), with new sections being easily created in the database at any time. Annotation data is inserted in the node fields, without having to obey any rigid structure. Lastly, the dataset and pointers provide a tracking sequence for annotated images in a dataset. Despite being primarily designed for the SCREEN-DR project, R-PACS can be used through small customizations in any medical imaging context where a set of annotations is applied to DICOM images. It also has a Command Line Interface (CLI) to drop and rebuild the Elasticsearch indexes. The R-PACS plug-in is available at the github repository On top of the R-PACS plug-in, a layer of diabetic retinopathy services were implemented, responsible for the dataset manipulation and encompassing the following services: • Annotation ⇒ to perform textual and visual annotations on the dataset. It allows to fetch, create and modify annotations, based on the image reference ID parameter; • Transfer ⇒ responsible for downloading / uploading DICOM files; • Index ⇒ uses selections of previously uploaded files and indexes them in the R-PACS. It also allows indexing image annotations; • Search ⇒ uses Lucene like queries 11 11 over DICOM metadata or annotation fields to retrieve images from the index; • Dataset Management ⇒ allows the creation of datasets from selected image unique identifiers (UIDs), provides subscription methods to datasets, and allows to define the default dataset to use in the annotation protocol. DR services were developed in the JAVA language and exposed by an in-house service stack denominated Reactive Through Services (RTS). This framework, which implements the web service endpoints, fills the gap between the reactive front-end and back-end services, maintaining the straightforward way of development that REST and JSON-RPC provides. Furthermore, it offers support to new protocols and models that extend REST and JSON-RPC. The role-based access control (RBAC), a very common feature of many online applications for sensitive information [31], was addressed through an authentication and authorization mechanism using Keycloak. 12 12 Keycloak: A Single Sign-On (SSO) mechanism was also added to grant JSON Web Tokens (JWT) 13 13 for the RBAC, which were intercepted and validated in the RTS pipeline. 3.3 RTS Reactive through services is publicly available at From the client point of view, RTS can be considered an extension of REST services, since it adds the push model to the existing pull network style of REST [32]. However, its architecture is much more complex, as depicted in Fig. 4 . The architecture was designed to be compatible with asynchronous server applications and message-driven architectures such as Vertx. 14 14 Vertx:vertx.io. On the other hand, its network endpoints and protocols were detached from service endpoints, such that with slight configurations it allows the same compatible service to be accessed from a REST endpoint or from an RTS endpoint. Moreover, RTS improves separation of concerns [33], providing clean service definitions and attaching points for other protocols, as long as they are compatible with the same communication model. Requests and replies from endpoints are transformed into a message based processing scheme, which after passing through the network endpoint enter a processing pipeline. Here, it can be forwarded to the next interceptor, delivered to the service or rejected. At the end of the pipeline, message delivery depends on a decision process based on some of the message fields; for instance, the message type decides if the message is to be delivered to a request or reply service handler. These message fields reflect the used communication model, thus avoiding the need for service descriptors to build client proxy implementations. Finally, the client-side has also the Pipeline and MessageBus components, and it is possible to have the same services on the client Pipeline and invert the request/reply flux from the server to the client. However, to avoid network issues, the network connection is always made in the client-server direction. 3.4 Annotation service The annotation service is responsible for fetching, creating and modifying annotations and it does so by making use of the image reference identifier. It contemplates two types of annotations: textual, in which physicians grade images, and visual, in which physicians delineate the lesions on the image. Both annotation types can be performed by several physicians. The textual annotation is itself divided in two gradings that can be performed either simultaneously or separately: quality and diagnosis. The overall pipeline of the textual annotation is depicted in Fig. 5 . In the quality stage, the physician decides if the image quality is acceptable (good/partial) or insufficient to allow an accurate diagnosis (bad). When a image is marked as of bad quality, the diagnosis stage is blocked. Otherwise, diagnosis can be performed, with the physician grading the image in terms of degree of retinopathy, maculopathy, photocoagulation and other suspected comorbidities. Images of healthy cases have a retinopathy preset at the R0 state and maculopathy at M0. Furthermore, if the physician is unable to classify the degree of a disease, the X option can be selected. It is worth mentioning that textual annotations take place before visual annotations, therefore lesion annotation can only be performed on images containing quality and diagnosis grading. Visual annotation uses images selected from the pool of textual annotated images that contain signs of DR (retinopathy states from R1 to R3). In this annotation process, physicians delineate lesions associated with retinopathy: microaneurysms, hemorrhages, hard exudates, soft exudates and neovascularizations. The platform provides several types of shapes and colors to delineate lesions, each specific for a type of lesion. 3.5 Search, transfer and dataset creation The DICOM standard does not possess sufficient flexibility to provide advanced search mechanisms to fulfill the SCREEN-DR requirements. As such, we incorporated search capabilities that allow researchers to access multiple sources in the archive, including DICOM metadata, image features and retinopathy annotations (e.g. image quality and pathology). The proposed search mechanism allows us to search over any attribute contained in R-PACS DICOM images metadata, as well as to combine attributes from distinct data sources, including labeled data from annotated datasets. This search service mechanism uses Lucene like queries 15 15 that make use of DICOM or annotation fields to retrieve the images from the index. Query results can be downloaded, including images and the associated annotations, or used to create new datasets. When downloading, a zip file is generated, containing the raw DICOM files and their respective annotations. The dataset service uses the unique identifiers (UIDs) of the selected images to create the datasets. This service also provides subscription methods to datasets, as well as the selection of the default dataset to be used in the annotation protocol. 3.6 Upload and anonymization The platform can receive and export retinopathy images and associated data using the DICOM standard interface or REST services. To address the upload of medical imaging studies, an upload module was created and integrated in the platform. This module allows users to upload examinations by dragging study folders or image files to the upload area. Compressed zip files containing several studies can also be uploaded. After validating the DICOM structure, they are indexed and stored in the R-PACS. The uploaded data may contain images with Protected Health Information (PHI) burned in the pixel data. Moreover, sensitive information is also included in DICOM headers [7]. To anonymize this information, uploaded studies are processed by a dedicated anonymization engine described in [34]. This pipeline is composed of two main parts: a visual anonymizer for removal of burned PHI, consisting of an improved version of the visual anonymizer described in [35], and a reversible de-identifier of medical imaging data that retains search capabilities over the original DICOM data index, used to anonymize sensitive data in DICOM headers. The generated de-identified patient reference is stored in the name field of the patient model (Fig. 3). The original name of the patient is never user. 3.7 Access control As expressed, SCREEN-DR is a collaborative platform that requires a role-based access control mechanism to support three main actors (Fig. 6 ): physicians, researchers and data providers. To satisfy this fundamental requirement, an authentication module was created with SSO support. It was integrated with Keycloak, an open source identity and access management solution aimed at modern applications and services. Instead of using a customized solution, users perform authentication with Keycloak, meaning the SCREEN-DR platform does not have to deal with login forms, nor with authenticating and storing users. Depending on the user, different endpoints of the platform are accessible. Physicians have access to the textual and visual annotation endpoints of the application, as well as to the PACS viewer endpoint. Researchers have access to the search and dataset creation endpoints. Finally, data providers have access to the platform's file upload endpoint. OpenID Connect protocol was used for the authentication process, as it will allow seamless integration with other external applications. Once logged-in to Keycloak, users do not have to login again to access a different application. Keycloak will also let users use credentials from other identity providers (e.g. Google) to access the SCREEN-DR platform, without further development. 3.8 User interface features The interface code and web services of the platform are publicly available at The platform graphical interface has six functional areas, accessible in the top menu of the main page, in accordance with the use cases presented in figure 1 : Search ⇒ typically used by researchers to query the repository, download files and create and manage datasets; Dataset Manager ⇒ exclusively used by researchers to associate physicians to datasets; Upload ⇒ mainly used by data providers to upload studies; Annotation ⇒ exclusively used by physicians to perform textual annotations on a given dataset; Lesion ⇒ exclusively used by physicians to perform visual annotations on a given dataset; Viewer ⇒ allows an integrated visualization of all images belonging to a study of a specific patient. 3.8.1 Searching The search area allows users to perform queries over the R-PACS repository. Fig. 7 presents an example of a query result. The query language is case insensitive, uses either free text or simple boolean logic, and supports queries over DICOM metadata, annotations or both. The designed query language has four field types: • Text ⇒ free text field • Numeric ⇒ natural numbers • Date ⇒ dates, with format YYYY-MM-DD • Option ⇒ a set of pre-defined text options Predicates can be enclosed in parentheses and four boolean operators are supported: • OR ⇒ joins 2 clauses indicating that the presence of one condition is a valid result; • AND ⇒ joins 2 clauses indicating that the presence of both conditions is required; • + ⇒ applied to a single clause indicating that this term must be present; • - ⇒ applied to a single clause indicating that this term must not be present. The following range operators can be specified for date, numeric or string fields: • TO ⇒ defines a range interval, ex: BirthDate:[1936-07-22 TO 1936-07-24] • <, >, <=, >= ⇒ defines range limits, ex: Columns:>3000 Finally, it is also possible to perform wildcard searches on individual terms, using the symbol ? to replace a single character, and * to replace zero or more characters. The returned results are image centric, the smallest entity in DIM. Each result includes a thumbnail for preview reasons, as well as information regarding the patient, station and annotations performed on the image. These results can be selected and used to generate a dataset or simply downloaded in a zip file format. 3.8.2 Dataset management The dataset management area allows the creation and assignment of multiple datasets to distinct users (e.g. physician). All images available in the R-PACS can be used to create a dataset in accordance with distinct selection criteria (patient gender, equipment, study date, etc). The search module can be used to select images from the R-PACS, allowing inclusively the use of another dataset already annotated as a query data source. For instance, the dataset for visual annotation of lesions is usually created as a sub-dataset of the first stage datasets, according to labeled data criteria. 3.8.3 The upload tab The upload tab is a simple frame that allows manual drag and drop of studies to the R-PACS. Uploaded files are stored and indexed using the Transfer service. Nevertheless, the platform also provides DICOM standard services, including the recent DICOM STOW-RS [36] that can be used by third-party systems to feed the platform with medical imaging studies. In both import options, cases are automatically anonymized when received. 3.8.4 Textual annotation Fig. 8a shows the annotation form. Here, physicians can grade image quality and diagnosis in accordance with the established protocol (Fig. 5). The two levels of annotations can be performed by the physician together or separately. All images are allocated to the physician's dataset through a blind process, so that physicians have no knowledge of which patients the images belong to. To help physicians with the grading process, the toolset provides a magnifier, a zoom and a contrast tool that allows them to see the image with more detail. Furthermore, there is a navigation bar allowing physicians to browse through the annotation dataset and correct possible grading mistakes as well as knowing how many annotations are required from them. 3.8.5 Lesion annotation The legion annotation page, depicted in Fig. 8b, is where physicians can delineate the lesions defined in the annotation protocol. It is worth mentioning that all images used to create the lesion annotation dataset are a subset of images that belong to a textually annotated dataset, from which only images with DR were selected. Besides having tools for lesion specific delineation, it also has tools for deleting and for moving markings around the image. To help with lesion delineation, physicians can zoom in and out on the image or use the magnifier tool. 3.8.6 PACScenter integration The SCREEN-DR platform was integrated with PACScenter 16 16 PACScenter: to satisfy the need to analyze all cases from a specific patient study (Fig. 9 ). BMD Software, 17 17 BMD: a project partner, provided and supported the integration of this commercial solution within the SCREEN-DR platform. PACScenter is a web based DICOM viewer entirely developed with web technologies, namely HTML5 and JavaScript, being accessible through a common web browser. A demo version is available at This multi-platform solution was designed to integrate with any external DICOM compliant archive, R-PACS in the context of SCREEN-DR. It allows accessing, downloading, visualizing, reviewing, reporting, and printing medical multi-modality image data in DICOM format. PACScenter shares the repository with the SCREEN-DR platform through Dicoogle, thus it is possible to search, view and manipulate retinopathy studies using a validated set of tools. While the visualization for annotation purposes is patient blind and image centric, the navigation through PACScenter is patient centric, being possible to see all studies from a patient and all images from a study, making it an important solution to handle cases where all images of a study are required in order to perform a correct diagnosis. 4 Results The first stable version of SCREEN-DR platform is accessible through the following web address The system is already in production and currently stores 1655 studies imported from the ARSN repository. The annotation process started in November 2017, and the platform currently has 4 physician annotators that have performed textual annotations on the data (Table 1 ) and will continue to do so as well as perform lesion annotations until June 2019. Currently, images have been annotated in terms of image quality and diagnosis. Furthermore, there are 4 researchers using the platform to navigate through annotated data and retrieve data together with the corresponding annotations to feed their machine learning algorithms. Annotations were performed on 701 distinct images with around 450 images per annotator, reflecting some degree of overlapping. Having multiple annotations on the same image may be relevant for machine learning techniques using multiple knowledge sources [37], and also for establishing a discrepancy baseline between different results. The software gathers the actions and timestamps for each annotated image. From these we compiled a quantitative analysis of the time taken on each stage. The summaries are presented in Tables 2 and 3 for image quality and diagnosis. Time refers to the average time (in seconds) taken to annotate an image when a certain characteristic is present, and Actions is the average number of actions such as clicking buttons, zooming, panning and saving. TpA is the average time-per-action (in seconds per action) calculated by the formula Time/Actions. At first glance, the T values seemed too large for a medical decision about the image. However, detailed analysis on the data showed that some physicians were about 2 times faster than others, influencing the average and reducing the T and TpA significance. This also indicates that A is probably the best measurement of effort. On the quality stage, the results show a minor effort when identifying a bad quality image, since this requires less actions, usually just a select and save. It is also expected that an image with partial readability would take more time to analyze than a good one. Since physicians prefer to perform quality and diagnosis at the same time, the diagnosis stage includes the quality grading process. Results show that the number of actions required to grade retinopathy level increases with the severity level. This is expected since a more careful analysis is required on more difficult cases. Also, the R0 and M0,P0 row concerning the auto-selection of M0 and P0 when R0 is selected, has lower values, suggesting that the feature is indeed useful in accelerating the diagnosis process. The minimum number of actions to complete the diagnose without the auto-selection feature is 6. Since the highest average is 8.6 with the possibility of inserting “suspected comorbidities“, this is in general in accordance with the expected results. 5 Discussion Regarding possible limitations, our system is optimized for annotating images, with the UI layout having been designed to present one image at a time with a set of tools for the annotations. However, adding spatial or time relations between images is not part of this research scope and can be treated as a limitation. Methods for automated annotations, such as for retinal vessel tortuosity [38], were not considered because we intended to obtain the physician's exact delineations rather than the results of automated tools. Moreover, it was our decision to export annotations in JSON format instead of DICOM-SR [39]. Despite not being DICOM compliant, JSON is an ubiquitous format from a developer perspective and ML experts do not always know the DICOM-SR format. Qualitative analyses of the software usability were not performed since the annotation module will not be used in a production environment. The specification and development was supervised by end-users (physicians and researchers) and should reflect the requirements imposed by them. The available annotation user interfaces and tools meet these requirements. In this work, bad quality images are discarded since these are not useful for training the machine learning algorithms considered in the project. The identification of reasons for bad quality, such as bad alignment, over exposure or other, could be of interest for training photographers, for example, but this aim is outside the scope of the current work and was left as possible future development. 6 Conclusions This article presents the collaborative platform of the SCREEN-DR project that promotes collaboration between physicians and researchers in the scope of a regional DR screening program. The role of researchers is to create classification algorithms to evaluate image quality, discard non-pathological cases, locate possible lesions and grade DR severity. Regarding that, significant output has already been made in the ML domain under the scope of the SCREEN-DR project [40–45]. Physicians are responsible for the annotation of datasets, including visual delineation of lesions. The collaborative platform collects the studies, indexes the images metadata, and manages the creation of datasets and the respective annotation process. An advanced searching mechanism supports multi-source queries over annotated datasets and exporting of results for feeding artificial intelligence algorithms. The described scenario could not be satisfied by traditional ICT platforms currently used in radiology departments and, as such, a novel modular architecture was designed that sits on top of an open source framework. The platform core module is the R-PACS repository that stores uploaded files, following a DICOM standard data model. Moreover, it was extended with a complementary indexing engine for supporting information retrieval over image metadata and physician annotations. A layer of web services was developed for dataset manipulation, which is exposed through endpoints using a dedicated architecture denominated as RTS. Concerning platform security, an intelligent anonymization framework was developed, and a role-based access control with SSO support was integrated. The system is already in production and currently stores 1655 studies imported from a repository of a regional public health entity. Author's contribution All authors participated in solution development, results analysis and drafting of all sections of the manuscript. Micael Pedrosa and Jorge Silva were the main developers of the created solution. Sergio Matos and Carlos Costa were the main responsibles for the project supervision. All authors have read and agreed to the paper being submitted as it is. Acknowledgements This work is financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalization – COMPETE 2020 Programme, and by National Funds through the FCT Fundação para a Ciência e a Tecnologia (Portuguese Foundation for Science and Technology) within project CMUP-ERI/TIC/0028/2014. Sérgio Matos is supported by a FCT Investigator grant. References [1] E.J. Duh Diabetic Retinopathy 2008 Humana Press Totowa, NJ 10.1007/978-1-59745-563-3 [Online] [2] World Health Organization Global Data on Visual Impairments 2010, Tech. Rep. 2010 [Online]. Available: [3] J.W.Y. Yau S.L. Rogers R. Kawasaki E.L. Lamoureux J.W. Kowalski T. Bek S.-J. Chen J.M. Dekker A. Fletcher J. Grauslund S. Haffner R.F. Hamman M.K. Ikram T. Kayama B.E.K. Klein R. Klein S. Krishnaiah K. Mayurasakorn J.P. O’Hare T.J. Orchard M. Porta M. Rema M.S. Roy T. Sharma J. Shaw H. Taylor J.M. Tielsch R. Varma J.J. Wang N. Wang S. West L. Xu M. Yasuda X. Zhang P. Mitchell T.Y. Wong Meta-Analysis for Eye Disease (META-EYE) Study Group Global prevalence and major risk factors of diabetic retinopathy Diabetes Care 35 3 2012 556 564 [Online]. Available: [4] P. Prentašić S. Lončarić Detection of exudates in fundus photographs using deep neural networks and anatomical landmark detection fusion Comput. Methods Prog. Biomed. 137 2016 281 292 [Online]. Available: [5] H.K. Huang PACS and Imaging Informatics 2009 John Wiley & Sons, Inc. Hoboken, NJ, USA 10.1002/9780470560525 [Online] [6] K.J. Dreyer PACS: A Guide to the Digital Revolution 2006 Springer [7] J. Hu F. Han A pixel-based scrambling scheme for digital medical images protection J. Netw. Comput. Appl. 32 4 2009 788 794 [Online]. Available: [8] W. Jorritsma F. Cnossen P.M. van Ooijen Merits of usability testing for PACS selection Int. J. Med. Inf. 83 1 2014 27 36 [Online]. Available: [9] O.S. Pianykh Digital Imaging and Communications in Medicine (DICOM) 2012 Springer Berlin Heidelberg Berlin, Heidelberg 10.1007/978-3-642-10850-1 [Online] [10] F. Valente C. Costa A. Silva Dicoogle, a PACS featuring profiled content based image retrieval PLoS ONE 8 5 2013 e61888 [Online]. Available: [11] C. Costa F. Freitas M. Pereira A. Silva J.L. Oliveira Indexing and retrieving DICOM data in disperse and unstructured archives Int. J. Comput. Assist. Radiol. Surg. 4 1 2009 71 77 10.1007/s11548-008-0269-7 [Online] [12] T.M. Godinho C. Viana-Ferreira L.A. Bastiao Silva C. Costa A routing mechanism for cloud outsourcing of medical imaging repositories IEEE J. Biomed. Health Inf. 20 1 2016 367 375 [Online]. Available: [13] L.S. Ribeiro C. Viana-Ferreira J.L. Oliveira C. Costa XDS-I outsourcing proxy: ensuring confidentiality while preserving interoperability IEEE J. Biomed. Health Inf. 18 4 2014 1404 1412 [Online]. Available: [14] F. Valente C. Viana-Ferreira C. Costa J.L. Oliveira A RESTful image gateway for multiple medical image repositories IEEE Trans. Inf. Technol. Biomed. 16 3 2012 356 364 [Online]. Available: [15] M. Moore The evolution of telemedicine Future Gener. Comput. Syst. 15 2 1999 245 254 [Online]. Available: [16] L. Bastião Silva A. Trifan J. Luís Oliveira MONTRA: An agile architecture for data publishing and discovery Comput. Methods Prog. Biomed. 160 2018 33 42 [Online]. Available: [17] O. Hamdi M.A. Chalouf D. Ouattara F. Krief eHealth: Survey on research projects, comparative study of telemonitoring architectures and main issues J. Netw. Comput. Appl. 46 2014 100 112 [Online]. Available: [18] Y.-H. Chiu L.-S. Chen C.-C. Chan D.-M. Liou S.-C. Wu H.-S. Kuo H.-J. Chang T.H.-H. Chen Health information system for community-based multiple screening in Keelung, Taiwan (Keelung Community-based Integrated Screening No. 3) Int. J. Med. Inf. 75 5 2006 369 383 [Online]. Available: [19] K. Suzuki Machine learning in computer-aided diagnosis of the thorax and colon in CT: a survey IEICE Tran. Inf. Syst., vol. E96-D 4 2013 772 783 [Online]. Available: [20] U.R. Acharya E.Y.K. Ng J.-H. Tan S.V. Sree K.-H. Ng An integrated index for the identification of diabetic retinopathy stages using texture parameters J. Med. Syst. 36 3 2012 2011 2020 10.1007/s10916-011-9663-8 [Online]. Available: [21] S. Roychowdhury D.D. Koozekanani K.K. Parhi DREAM: diabetic retinopathy analysis using machine learning IEEE J. Biomed. Health Inf. 18 5 2014 1717 1728 [Online]. Available: [22] B.M. Ege O.K. Hejlesen O.V. Larsen K. Møller B. Jennings D. Kerr D.A. Cavan Screening for diabetic retinopathy using computer based image analysis and statistical classification Comput. Methods Prog. Biomed. 62 3 2000 165 175 [Online]. Available: [23] T. Kauppi J.-K. Kämärä inen L. Lensu V. Kalesnykiene I. Sorri H. Uusitalo H. Kälviä inen Constructing benchmark databases and protocols for medical image analysis: diabetic retinopathy Comput. Math. Methods Med. 2013 2013 368514 [Online]. Available: [24] E. Trucco A. Ruggeri T. Karnowski L. Giancardo E. Chaum J.P. Hubschman B. al Diri C.Y. Cheung D. Wong M. Abràmoff G. Lim D. Kumar)Burlina N.M. Bressler H.F. Jelinek F. Meriaudeau G. Quellec T. MacGillivray B. Dhillon Validating retinal fundus image analysis algorithms: issues and a proposal Investig. Opthalmol. Vis. Sci. 54 5 2013 3546 [Online]. Available: [25] V. Gulshan L. Peng M. Coram M.C. Stumpe D. Wu A. Narayanaswamy S. Venugopalan K. Widner T. Madams J. Cuadros R. Kim R. Raman P.C. Nelson J.L. Mega D.R. Webster Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs JAMA 316 22 2016 2402 10.1001/jama.2016.17216 [Online] [26] S.S. Rahim V. Palade J. Shuttleworth C. Jayne R.N.R. Omar Automatic detection of microaneurysms for diabetic retinopathy screening using fuzzy image processing Engineering Applications of Neural Networks 2015 Springer 69 79 [27] D.L. Rubin C. Rodriguez)Shah C. Beaulieu iPad: Semantic annotation and markup of radiological images.” AMIA Annual Symposium Proceedings. AMIA Symposium, vol. 2008 2008 626 630 [Online]. Available: [28] D.P. Papadopoulos J.R.R. Uijlings F. Keller V. Ferrari Extreme Clicking for Efficient Object Annotation, vol. 8 2017 [Online]. Available: [29] R. Binns M. Van Kleek M. Veale U. Lyngs J. Zhao N. Shadbolt It's reducing a human being to a percentage’: perceptions of justice in algorithmic decisions Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM 2018 377 [30] F. Valente L.A.B. Silva T.M. Godinho C. Costa Anatomy of an extensible open source PACS J. Digit. Imaging 29 3 2016 284 296 10.1007/s10278-015-9834-0 [Online] [31] P. Kulkarni Y. Ozturk mPHASiS: Mobile patient healthcare and sensor information system J. Netw. Comput. Appl. 34 1 2011 402 417 [Online]. Available: [32] M. Pedrosa J. Miguel C. Costa Reactive through services - opinionated framework for developing reactive services Proceedings of the 8th International Conference on Cloud Computing and Services Science. SCITEPRESS – Science and Technology Publications, vol. 2018 2018 330 337 10.5220/0006661403300337 [Online] [33] W.L. Hürsch W.L. Hürsch C.V. Lopes Separation of Concerns 1995 [Online]. Available: [34] J.M. Silva E. Pinho E. Monteiro J.F. Silva C. Costa Controlled searching in reversibly de-identified medical imaging archives J. Biomed. Inf. 77 2018 81 90 [Online]. Available: [35] E. Monteiro C. Costa J.L. Oliveira A de-identification pipeline for ultrasound medical images in DICOM format J. Med. Syst. 41 5 2017 89 10.1007/s10916-017-0736-1 [Online] [36] NEMA Digital Imaging and Communications in Medicine (DICOM) Supplement 163: STore Over the Web by REpresentations State Transfer (REST) Services (STOW-RS) 2017 [37] Y. Yan R. Rosales G. Fung F. Farooq B. Rao J. Dy Active Learning from Multiple Knowledge Sources 2012 1350 1357 [Online]. Available: [38] E. Grisan M. Foracchia A. Ruggeri A novel method for the automatic grading of retinal vessel tortuosity IEEE Trans. Med. Imaging 27 3 2008 310 319 [Online]. Available: [39] L. Tribuna A. Silva P.S. Couto L. Bastião A study to understand the acceptance of DICOM Structured Reports on Breast Imaging Proc. Comput. Sci. 121 2017 672 680 [Online]. Available: [40] M.I. Meyer P. Costa A. Galdran A.M. Mendonça A. Campilho A Deep Neural Network for Vessel Segmentation of Scanning Laser Ophthalmoscopy Images, vol. 7 2017 Springer Cham 507 515 10.1007/978-3-319-59876-5_56 [Online] [41] P. Costa A. Galdran M.I. Meyer A.M. Mendonça A. Campilho Adversarial Synthesis of Retinal Images from Vessel Trees, vol. 7 2017 Springer Cham 516 523 10.1007/978-3-319-59876-5_57 [Online] [42] B. Remeseiro A.M. Mendonca A. Campilho Objective quality assessment of retinal images based on texture features 2017 International Joint Conference on Neural Networks (IJCNN). IEEE, vol. 5 2017 4520 4527 [Online]. Available: [43] T. Araújo A.M. Mendonça A. Campilho Estimation of retinal vessel caliber using model fitting and random forests S.G. Armato N.A. Petrick International Society for Optics and Photonics, vol. 3 2017 101341K 10.1117/12.2252025 [Online] [44] A.M. Mendonça B. Remeseiro B. Dashtbozorg A. Campilho Automatic and semi-automatic approaches for arteriolar-to-venular computation in retinal photographs S.G. Armato N.A. Petrick International Society for Optics and Photonics, vol. 3 2017 101341L 10.1117/12.2255096 [Online] [45] P. Costa A. Campilho Convolutional bag of words for diabetic retinopathy detection from eye fundus images IPSJ Trans. Comput. Vis. Appl. 9 1 2017 10 10.1186/s41074-017-0023-6 [Online] "
    },
    {
        "doc_title": "Automated Anatomic Labeling Architecture for Content Discovery in Medical Imaging Repositories",
        "doc_scopus_id": "85049322271",
        "doc_doi": "10.1007/s10916-018-1004-8",
        "doc_eid": "2-s2.0-85049322271",
        "doc_date": "2018-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Databases, Factual",
            "Diagnostic Imaging",
            "Information Storage and Retrieval",
            "Multimodal Imaging",
            "Radiology Information Systems",
            "Software"
        ],
        "doc_abstract": "© 2018, Springer Science+Business Media, LLC, part of Springer Nature.The combination of textual data with visual features is known to enhance medical image search capabilities. However, the most advanced imaging archives today only index the studies’ available meta-data, often containing limited amounts of clinically useful information. This work proposes an anatomic labeling architecture, integrated with an open source archive software, for improved multimodal content discovery in real-world medical imaging repositories. The proposed solution includes a technical specification for classifiers in an extensible medical imaging archive, a classification database for querying over the extracted information, and a set of proof-of-concept convolutional neural network classifiers for identifying the presence of organs in computed tomography scans. The system automatically extracts the anatomic region features, which are saved in the proposed database for later consumption by multimodal querying mechanisms. The classifiers were evaluated with cross-validation, yielding a best F1-score of 96% and an average accuracy of 97%. We expect these capabilities to become common-place in production environments in the future, as automated detection solutions improve in terms of accuracy, computational performance, and interoperability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised learning for concept detection in medical images: A comparative analysis",
        "doc_scopus_id": "85050373736",
        "doc_doi": "10.3390/app8081213",
        "doc_eid": "2-s2.0-85050373736",
        "doc_date": "2018-07-24",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 by the authors.As digitalmedical imaging becomesmore prevalent and archives increase in size, representation learning exposes an interesting opportunity for enhanced medical decision support systems. On the other hand, medical imaging data is often scarce and short on annotations. In this paper, we present an assessment of unsupervised feature learning approaches for images in biomedical literature which can be applied to automatic biomedical concept detection. Six unsupervised representation learning methods were built, including traditional bags of visual words, autoencoders, and generative adversarial networks. Each model was trained, and their respective feature spaces evaluated using images from the ImageCLEF 2017 concept detection task. The highest mean F1 score of 0.108 was obtained using representations from an adversarial autoencoder, which increased to 0.111 when combined with the representations from the sparse denoising autoencoder. We conclude that it is possible to obtain more powerful representations with modern deep learning approaches than with previously popular computer vision methods. The possibility of semi-supervised learning as well as its use in medical information retrieval problems are the next steps to be strongly considered.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Services Orchestration and Workflow Management in Distributed Medical Imaging Environments",
        "doc_scopus_id": "85050985319",
        "doc_doi": "10.1109/CBMS.2018.00037",
        "doc_eid": "2-s2.0-85050985319",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Distributed systems",
            "Healthcare institutions",
            "Information and communication systems",
            "Operational requirements",
            "Production environments",
            "System installation",
            "Workflow managements"
        ],
        "doc_abstract": "© 2018 IEEE.Medical imaging laboratories are supported by information and communication systems commonly denominated as PACS, that encompasses technology for acquisition, archive, distribution and visualization of digital images in network. Concerning the data and workflow management, traditional solutions used in production provide a limited set of services usually configured at system installation. As result, healthcare institutions are not able to fully explore their infrastructure or adapt it to new operational requirements, either for clinical or research procedures. This article proposes a framework for services orchestration and workflow management in distributed medical imaging environments. It was designed for end-user usage and is accessible through a Web portal that allows to document, repeat and allocate procedures and tasks to correct resources, either from information systems or human interventions. It provides an abstraction layer for integration with distinct data sources through standard services, allows the creation of new services through orchestration of existent ones and the scheduling of tasks. Moreover, it includes a logging and alert mechanism integrated with email service. The solution was validated through the specification of two use cases that were deployed in production environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Face De-Identification Service for Neuroimaging Volumes",
        "doc_scopus_id": "85050981625",
        "doc_doi": "10.1109/CBMS.2018.00032",
        "doc_eid": "2-s2.0-85050981625",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D reconstruction",
            "Clinical diagnosis",
            "De-identification",
            "DICOM",
            "Fundamental tools",
            "Healthcare institutions",
            "Information and communication systems",
            "Medical practice"
        ],
        "doc_abstract": "© 2018 IEEE.Digital medical imaging is a fundamental tool for improving medical practice workflows and supporting clinical diagnosis. Nowadays, healthcare institutions are usually very supported by information and communication systems that meet regular practice requirements. However, the usage of those platforms in collaborative, research and educational scenarios faces several problems. One of the key issues is related with patient data privacy, namely with concerns related with the visual anonymization of studies. In the neuroimaging field, this subject is more complex since, even after removing the patient's information from the images meta-data or burned in the pixel data, it is still possible to identify the patients through 3D reconstruction of the volume. This article proposes and describes the implementation of an end-user service that allows neuroimages facial de-identification of CT volumes, being fully interoperable with production repositories. The solution was validated using a public dataset and made available to the community through its integration with an open source archive server.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ejection Fraction Classification in Transthoracic Echocardiography Using a Deep Learning Approach",
        "doc_scopus_id": "85050975427",
        "doc_doi": "10.1109/CBMS.2018.00029",
        "doc_eid": "2-s2.0-85050975427",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D CNN",
            "Cardio-vascular disease",
            "Classification methods",
            "Computer Aided Diagnosis(CAD)",
            "Convolutional neural network",
            "Ejection fraction",
            "Transthoracic echocardiography",
            "Ventricular ejection"
        ],
        "doc_abstract": "© 2018 IEEE.Cardiovascular diseases are the leading cause of death worldwide. These diseases are related with a broad range of factors but usually show high correlation with diminished left ventricle function, which can be evaluated by measuring the ventricular ejection fraction through transthoracic echocardiography (TTE), a cost-effective and highly portable first-line diagnosing technique. Ejection fraction (EF) is currently determined through a semi-automatic process that requires manual delineation of the left ventricle area both in a diastolic and systolic frame of the patient's exam. To remove this manual annotation step, which is both time-consuming and user dependent, automatic Computer-Aided Diagnosis (CAD) systems can be used. Herein, we propose the first steps for such a system that classifies ejection fraction in four classes, based on TTE exams, with the objective of automatically providing valuable information to physicians. Our classification method is based on a 3D-Convolutional Neural Network (3D-CNN) trained on a dataset constructed with exams from a cardiology reference center. The dataset creation consisted of three main steps: firstly, for each exam, cine-loops showing the apical 4 chambers view were manually selected; then, 30 sequential frames were extracted from each cine-loop; finally, each frame was pre-processed to mask burned-in metadata. The neural network was designed to explore concepts such as convolutions using asymmetric filters and residual learning blocks. The model was trained on a dataset with 4000 TTE exams and tested on a separate dataset containing 1600 TTE cases. We obtained an accuracy of 78% and a F1 score of 71.3% for unhealthy EF (below 45%), 63.3% for intermediate EF (45-55%), 72.3% for healthy EF (55-75%) and 54.6% for abnormally high EF (above 75%). These results are promising and show that convolutional neural networks can be applied to this domain. Furthermore, this work will serve as a foundation for future research where other relevant cardiac metrics will be determined.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An analysis of destinations, events and national cultures and the possibly negative effects and costs for tourism",
        "doc_scopus_id": "85118460702",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85118460702",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Cultural Studies",
                "area_abbreviation": "SOCI",
                "area_code": "3316"
            },
            {
                "area_name": "Management, Monitoring, Policy and Law",
                "area_abbreviation": "ENVI",
                "area_code": "2308"
            },
            {
                "area_name": "Tourism, Leisure and Hospitality Management",
                "area_abbreviation": "BUSI",
                "area_code": "1409"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021, Academic Conferences and Publishing International Limited. All rights reserved.A number of events may negatively affect one’s holidays abroad – problems while travelling, or problems with national cultures. National culture may have a big impact on holiday experiences, and so needs to be understood by tourists in order for them to have a more memorable time when on holiday abroad. If treated badly, or with contempt, tourists may be more forgiving if they realize and understand that they are in the presence of more ethnocentric and masculine cultures. If treated well, tourists may appreciate this even more if it goes contrary to national values and with regards to how foreigners are normally treated in that location. Important is that tourists do not seek confrontation with less-forgiving and more power-distant cultures, such as the Chinese. This is an auto-ethnographic account of a family on an annual holiday abroad, for leisure purposes. Auto-ethnography is used to analyze certain situations which occurred during the Summer holidays of 2017, with the lead author and his family while on a trip from Portugal to Australia, passing through Spain on the way out and Hong Kong and the UK on the way back. Despite very memorable experiences, problems with the deadly Typhoon Hato and the Tropical Cyclone Pakhar resulted in a degree of apprehension by the lead-author and his family and a desire to stay at home for an indefinite period, before going on holiday (to far-away places) again. So, despite a need for an innovative experience while on holiday, certain negative occurrences (or life-changing experiences) may act against similar trips happening in the future. Being more culturally aware may result from a training scheme or a communication program – which may be undertaken at the national level. Finally, let it be stated that examples of welcoming people, as well as the opposite, were found in Spain, Australia, and Hong Kong. While this research seeks to show certain prevalent characteristics of national cultures, there will always be exceptions and so travellers should keep an open mind.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Feature learning with adversarial networks for concept detection in medical images: UA.PT bioinformatics at ImageCLEF 2018",
        "doc_scopus_id": "85051066758",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85051066758",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adversarial networks",
            "Auto encoders",
            "Classification algorithm",
            "Concept detection",
            "ImageCLEF",
            "Linear classifiers",
            "Representation Learning",
            "Scientific community"
        ],
        "doc_abstract": "As the subjects of representation learning and generative adversarial networks become increasingly attractive to the scientific community, they also bring an exciting perspective towards their application in the digital medical imaging domain. In particular, the ImageCLEF caption challenge is focused on automatic medical image understanding. This paper describes a set of feature learning approaches for the concept detection sub-task of ImageCLEFcaption 2018. The first approach consists on a traditional bag of words algorithm, using ORB keypoint descriptors. The remaining two methods are based on a variant of generative adversarial networks with an auto-encoding process. Subsequently, two kinds of classification algorithms were employed for concept detection over the feature spaces learned. Test results showed a best mean Fl score of 0.110176 for linear classifiers, by using the features of the adversarial autoencoder.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Reactive through services: Opinionated framework for developing reactive services",
        "doc_scopus_id": "85048862410",
        "doc_doi": "10.5220/0006661403300337",
        "doc_eid": "2-s2.0-85048862410",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (miscellaneous)",
                "area_abbreviation": "COMP",
                "area_code": "1701"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Asynchronous data stream",
            "Communication models",
            "Impedance mismatch",
            "Reactive",
            "Reactive programming",
            "Reference implementation",
            "Rest services",
            "Service specifications"
        ],
        "doc_abstract": "Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Front-end development is inherently asynchronous, and for some time there was no correct way on how to build it in a reactive manner. Reactive Programming is a paradigm of software development centered in asynchronous data streams. Lately, this paradigm has been getting a lot of visibility due to integrations with frameworks like Angular and React. However, there is (from the electronic jargon) an impedance mismatch between the reactive UI and the required server-side services. Since common REST services do not integrate well with this new paradigm and JSON-RPC lacks some of the communication models described in this publication, we present a possible alternative for an opinionated framework which fills the gap between the reactive front-end and back-end services, maintaining the straightforward way of development that REST and JSON-RPC provides. This framework offers possibilities for new protocols and models, extending the basic REST and JSON-RPC models. It also delivers a reference implementation, certifying the viability of the proposal. Available at https://github.com/shumy/reactive-through-services.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SCREEN-DR: Software architecture for the diabetic retinopathy screening",
        "doc_scopus_id": "85046539905",
        "doc_doi": "10.3233/978-1-61499-852-5-396",
        "doc_eid": "2-s2.0-85046539905",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Artificial intelligence algorithms",
            "Classification algorithm",
            "Collaborative platform",
            "Diabetic retinopathy",
            "Diabetic retinopathy screening",
            "Multi-modal queries",
            "Screening programs",
            "Searching mechanism",
            "Algorithms",
            "Artificial Intelligence",
            "Diabetic Retinopathy",
            "Humans",
            "Mass Screening",
            "Software"
        ],
        "doc_abstract": "© 2018 European Federation for Medical Informatics (EFMI) and IOS Press.Diabetic Retinopathy (DR) is a common complication of diabetes that may lead to blindness if not treated. However, since DR evolves without any symptoms in the initial stages, early detection and treatment can only be achieved through routine checks. This article presents the collaborative platform of the SCREEN-DR project that promotes partnership between physicians and researchers in the scope of a regional DR screening program. The role of researchers is to create classification algorithms to evaluate image quality, discard non-pathological cases, locate possible lesions and grade DR severity. Physicians are responsible for annotating datasets, including the visual delineation of lesions. The collaborative platform collects the studies, indexes the images metadata, and manages the creation of datasets and the respective annotation process. An advanced searching mechanism supports multimodal queries over annotated datasets and exporting of results for feeding artificial intelligence algorithms.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Shared medical imaging repositories",
        "doc_scopus_id": "85046530765",
        "doc_doi": "10.3233/978-1-61499-852-5-411",
        "doc_eid": "2-s2.0-85046530765",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Application scenario",
            "Collaborative platform",
            "DICOM",
            "Open source projects",
            "Proposed architectures",
            "RBAC",
            "Role-based Access Control",
            "Shared repositories",
            "Diagnostic Imaging",
            "Humans",
            "Radiology Information Systems"
        ],
        "doc_abstract": "© 2018 European Federation for Medical Informatics (EFMI) and IOS Press.This article describes the implementation of a solution for the integration of ownership concept and access control over medical imaging resources, making possible the centralization of multiple instances of repositories. The proposed architecture allows the association of permissions to repository resources and delegation of rights to third entities. It includes a programmatic interface for management of proposed services, made available through web services, with the ability to create, read, update and remove all components resulting from the architecture. The resulting work is a role-based access control mechanism that was integrated with Dicoogle Open-Source Project. The solution has several application scenarios like, for instance, collaborative platforms for research and tele-radiology services deployed at Cloud.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Controlled searching in reversibly de-identified medical imaging archives",
        "doc_scopus_id": "85038015452",
        "doc_doi": "10.1016/j.jbi.2017.12.002",
        "doc_eid": "2-s2.0-85038015452",
        "doc_date": "2018-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Collaborative platform",
            "Collaborative projects",
            "De-identification",
            "DICOM",
            "Fundamental tools",
            "Medical database",
            "Pseudonymisation",
            "Search capabilities",
            "Computer Communication Networks",
            "Computer Security",
            "Confidentiality",
            "Data Anonymization",
            "Diagnostic Imaging",
            "Humans",
            "Information Storage and Retrieval",
            "Machine Learning",
            "Medical Records Systems, Computerized",
            "Radiology Information Systems",
            "Search Engine"
        ],
        "doc_abstract": "© 2017 Elsevier Inc.Nowadays, digital medical imaging in healthcare has become a fundamental tool for medical diagnosis. This growth has been accompanied by the development of technologies and standards, such as the DICOM standard and PACS. This environment led to the creation of collaborative projects where there is a need to share medical data between different institutions for research and educational purposes. In this context, it is necessary to maintain patient data privacy and provide an easy and secure mechanism for authorized personnel access. This paper presents a solution that fully de-identifies standard medical imaging objects, including metadata and pixel data, providing at the same time a reversible de-identifier mechanism that retains search capabilities from the original data. The last feature is important in some scenarios, for instance, in collaborative platforms where data is anonymized when shared with the community but searchable for data custodians or authorized entities. The solution was integrated into an open source PACS archive and validated in a multidisciplinary collaborative scenario.",
        "available": true,
        "clean_text": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2017-12-07 2017-12-07 2017-12-14 2017-12-14 2020-11-25T16:01:31 S1532-0464(17)30272-1 S1532046417302721 10.1016/j.jbi.2017.12.002 S300 S300.2 FULL-TEXT 2020-11-25T16:11:07.810561Z 0 0 20180101 20180131 2018 2017-12-07T12:01:40.431103Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst primabst ref specialabst 1532-0464 15320464 true 77 77 C Volume 77 9 81 90 81 90 201801 January 2018 2018-01-01 2018-01-31 2018 Original research papers article fla © 2017 Elsevier Inc. CONTROLLEDSEARCHINGINREVERSIBLYDEIDENTIFIEDMEDICALIMAGINGARCHIVES SILVA J 1 Introduction 2 Related work 3 Proposed architecture 3.1 Design overview 3.2 Visual anonymization 3.3 Storage process 3.4 Content discovery 3.4.1 De-identification query provider 3.4.2 Query transformation 3.4.3 Result transformation 4 Validation and discussion 4.1 Visual anonymization 4.2 Reversible meta anonymization 5 Use cases results 5.1 PACS integration 5.2 SCREEN-DR 6 Conclusion Conflict of interest Acknowledgments References 2006 PACS HUANG 2010 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS PIANYKH 2012 O DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMAPRACTICALINTRODUCTIONSURVIVALGUIDE COSTA 2009 71 77 C FERNNDEZALEMN 2013 541 562 J GKOULALASDIVANIS 2014 1 3 A SCHNEIER 2004 B SECRETSLIESDIGITALSECURITYINANETWORKEDWORLD PELEG 2008 1028 1040 M CUSHMAN 2010 S51 S55 R BRENNER 2007 828 831 H ARYANTO 2015 3685 3695 K FREYMANN 2012 14 24 J CARPENTER 2010 1896 1904 T NEWHAUSER 2014 134 140 W HUANG 2009 743 750 L MONTEIRO 2017 89 E VALENTE 2016 284 296 F RIBEIRO 2014 1404 1412 L SILVAX2018X81 SILVAX2018X81X90 SILVAX2018X81XJ SILVAX2018X81X90XJ Full 2019-01-01T01:15:17Z ElsevierBranded OA-Window 2018-12-14T00:00:00.000Z This article is made available under the Elsevier license. © 2017 Elsevier Inc. 2020-11-18T15:40:57.450Z S1532046417302721 Fundação para a Ciência e Tecnologia PTDC/EEI-ESS/6815/2014 PD/BD/105806/2014 INCT-EN Instituto Nacional de Ciência e Tecnologia para Excitotoxicidade e Neuroproteção Jorge Miguel Silva is funded by the Research grant Project CMUP-ERI/ICT/0028/2014-SCREEN-DR, Research Unit 127/94-IEETA. Eduardo Pinho is funded by Fundação para a Ciência e Tecnologia (FCT) under the grant PD/BD/105806/2014 . João Figueira Silva is funded by the research grant of PTDC/EEI-ESS/6815/2014 project. item S1532-0464(17)30272-1 S1532046417302721 10.1016/j.jbi.2017.12.002 272371 2020-11-25T16:11:07.810561Z 2018-01-01 2018-01-31 true 784702 MAIN 10 58296 849 656 IMAGE-WEB-PDF 1 fx1 true 22179 200 496 fx2 12513 199 192 fx3 14107 247 215 fx4 10574 160 234 fx5 16688 257 243 gr1 35823 305 560 gr2 25419 373 561 gr3 21125 262 560 gr4 22759 288 560 gr5 21355 288 558 gr6 34823 363 366 gr7 30380 356 667 fx1 true 5102 88 219 fx2 7836 164 158 fx3 5935 164 143 fx4 8157 149 219 fx5 6272 164 155 gr1 7231 119 219 gr2 5331 146 219 gr3 5337 102 219 gr4 5486 113 219 gr5 5203 113 219 gr6 8180 163 165 gr7 9046 117 219 fx1 true 134422 886 2198 fx2 104613 883 850 fx3 119003 1095 955 fx4 92289 708 1038 fx5 141089 1141 1079 gr1 236038 1352 2480 gr2 174140 1652 2484 gr3 154402 1160 2480 gr4 172833 1275 2482 gr5 128335 1274 2471 gr6 254469 1606 1621 gr7 118312 1010 1890 si1 473 14 79 si2 771 15 185 si3 209 13 13 si4 682 15 141 si5 242 12 19 si7 219 12 18 si8 141 11 8 YJBIN 2897 S1532-0464(17)30272-1 10.1016/j.jbi.2017.12.002 Elsevier Inc. Fig. 1 Anonymization pipeline for pixel data in DICOM images. Fig. 2 Component diagram for the developed reversible anonymization system. The components named Anonymous Storage and De-Identification Query Provider are the only platform-facing components. As in, the PACS archive consumes the interfaces of these two components for DICOM storage and query requirements respectively. Fig. 3 Sequence diagram for Anonymous storage process. Fig. 4 Sequence diagram for de-identification Query process. Fig. 5 Performance comparison of systems with and without the Meta Anonymizer in milliseconds (ms) relatively to the query retrieve process. Fig. 6 The extended architecture of a PACS archive with reversible anonymization capabilities, where Dicoogle is used as the backbone. Fig. 7 SCREEN-DR project’s annotation menu, used by medical staff to make annotations on study cases. Table 1 Manufacturer, Models and Resolution from where the dataset was assembled. Modality Manufacturer Model Resolution (Pixels) XC Canon CR-2 2304 × 3456 Carl Zeiss AG VISUCAM 2056 × 2124 Meditec 200 US Philips iE33 600 × 450 Philips iE33 – 2 600 × 450 HP SONOS 7500 600 × 340 Philips EnVisor LabEco 600 × 425 HP SONOS 5500 600 × 640 Table 2 Performance results obtained in seconds for the Storage and Indexation of 400 Clinical cases for systems with and without the Meta Anonymization. Without anon With anon Storage 36.4 317.126 Indexing 6.329 6.830 Controlled searching in reversibly de-identified medical imaging archives Jorge Miguel Silva a ⁎ Eduardo Pinho a Eriksson Monteiro b João Figueira Silva a Carlos Costa a a DETI/IEETA, University of Aveiro, Portugal DETI/IEETA University of Aveiro Portugal b BMD Software, Portugal BMD Software Portugal ⁎ Corresponding author. Graphical abstract Nowadays, digital medical imaging in healthcare has become a fundamental tool for medical diagnosis. This growth has been accompanied by the development of technologies and standards, such as the DICOM standard and PACS. This environment led to the creation of collaborative projects where there is a need to share medical data between different institutions for research and educational purposes. In this context, it is necessary to maintain patient data privacy and provide an easy and secure mechanism for authorized personnel access. This paper presents a solution that fully de-identifies standard medical imaging objects, including metadata and pixel data, providing at the same time a reversible de-identifier mechanism that retains search capabilities from the original data. The last feature is important in some scenarios, for instance, in collaborative platforms where data is anonymized when shared with the community but searchable for data custodians or authorized entities. The solution was integrated into an open source PACS archive and validated in a multidisciplinary collaborative scenario. Keywords Medical Imaging PACS DICOM Medical databases indexing De-identification Pseudonymisation 1 Introduction Over the last years, the use of digital medical imaging in healthcare institutions has significantly increased and the expectation is for it to continue. For instance, Frost & Sullivan registered that the US in 2005 had a cumulative storage of 119,325 Terabytes and in 2015 reached 300,000 Terabytes [1]. It was also estimated that data production would be over 1 Exabyte in 2016 [2] and that storage volume will double every 24 months [3]. This enormous production of data was followed by the development of new digital medical imaging systems, which become fundamental in clinical practice to support diagnosis and treatment. Research and industry are constantly updating and proposing new systems, namely imaging acquisition modalities, grounded in the universally accepted concept of Picture Archiving and Communication System (PACS). This concept includes hardware and software for imaging acquisition, storage, distribution and visualization. It has revolutionized medical practice during the last two decades and its digital nature has brought many benefits, including facilitated image manipulation and interpretation for value-added diagnosis. Furthermore, it provides support for improved and advanced workflows, which triggered speedier healthcare delivery and a reduction in operational costs [4,5]. The normalization of data format and network services in PACS environments arrives with the development and adoption of the Digital Imaging and Communications in Medicine (DICOM) standard, fulfilling the essential requirements for systems’ interoperability and accurate representations of medical imaging data. This standard was designed to cover all functional aspects of digital medical imaging laboratories, including data structures and codification, but also data discovery and retrieval services [6]. A DICOM object contemplates instance representation (e.g. image pixel data) but also associated meta-data organized in groups of related attributes. However, traditional archive databases only store a small number of attributes imposed by the DICOM Information Model (DIM). So the majority of information elements contained in objects’ metadata are not searchable in those systems. In some recent platforms, this issue was solved with the inclusion of document-oriented retrieval mechanisms [7]. Independently of the repository database approach, maintaining privacy and confidentiality of the patient’s personal health information (PHI) is a key point in any implementation, even more when many consider health information to be among the most confidential types of personal information [8]. This issue is particularly critical in today’s environments where storage outsourcing, data sharing and collaborative platforms are a reality. Hence, protecting patient privacy is extremely important for any electronic healthcare records (EHR) system and legal protection rules must be enforced [9], with particular focus on ensuring the confidentiality, integrity and availability (CIA) of health information [8]. To address the challenging issue of preserving medical data privacy, various techniques can be implemented including cryptography, access control and data anonymization [10]. DICOM meta-data includes identifiable information about the patient, the study, acquisition equipment, clinical staff, etc. Moreover, part of this information can also be burned into the pixel data in some medical imaging modalities, such as ultrasound. As such, managing such sensitive data demands proper protection to ensure patient data privacy, especially in collaborative scenarios. Medical imaging anonymization is not a recent subject in the literature. However, there are no reports of DICOM medical imaging repositories that provide anonymization of private information present in the pixel data and meta-data, ensuring at the same time search functionalities that allow authorized users to identify the patient studied or use identifiable attributes in queries. Ordinary platform users only need to have access to anonymized data, preventing Cyphertext-only attacks where the distribution of the anonymized data is known as well as the language in which the information is written [11,12]. In order to control a given entity’s access to health information, mechanisms based on Role-Based Access Control (RBAC) or Situation-Based Access Control (SitBAC) can be used [13]. The deployment of such systems, with granular control over what health data is disclosed for each entity layer [14], is particularly useful in multi-disciplinary collaborative scenarios, for instance, in research platforms or screening programs where images are shared between distinct specialists, including computer science researchers and doctors. In these scenarios, it is usual to signal certain cases for subsequent analysis by authorized personnel, such as specialists performing blind diagnosis or annotation procedures. Sometimes, they signal the case so that authorized personnel can reverse the anonymization process and retrieve complementary information, thus making a more accurate diagnosis [15]. Another usage scenario occurs when statistical studies are being carried out, and there is a need to analyze factors that are related to the manifestation of the disease. These factors often consist of patients’ private data such as age and gender, and as such there is the need to de-anonymize this information [16]. This paper presents a solution to support the previously described scenarios. It comprises a pipeline that first applies an automatic visual anonymizer, which uses the image meta-data as well as a machine learning model, removing only the patient’s private information and preserving other textual information burned into the image. Then, a reversible anonymizer of medical imaging data is used that maintains search capabilities over the original DICOM data elements. This pipeline was integrated into an open source PACS archive to contemplate the multidisciplinary collaborative platform scenario. 2 Related work A DICOM persistent object is composed of data elements (or attributes) that are represented by a unique tag with specific values and data types [17]. These meta-data often possess identifiable information about the patient, study, institution, etc. There are two known methods for de-identification of patient-related information: anonymization and pseudonymization. The first method removes information carried by header elements or replaces the information with random data such that the remaining information cannot be used to reveal the patient’s identity at all. The second method replaces most identifying fields within a data record using one or more artificial identifiers that could be used by authorized personnel to track down the real identity of the patient. This is the most appreciated method since clinical practice benefits from this kind of tracking, namely when additional findings occur in this situation [18]. Besides meta-data, patients’ personal information may be present in the image pixel data. This happens mostly in modalities that existed before the era of digital imaging, and the process ensured a secure way of associating an examination with the respective patient. With the transition to DICOM, some of these modality devices remained with this modus operandi. Among others, the majority of these devices belong to the Ultrasound modality (US) and exams that make use of the External-camera Photography modality (XC) exhibit this behavior as well. Those identification elements burned in the image must be removed to ensure an anonymization procedure compliant with HIPAA [19]. There are several tools to perform de-identification of meta-data and pixel data in order to fulfill the requirements of patient data protection. However, to the best of our knowledge, there is currently no anonymized repository system able to provide a controlled search mechanism over the original DICOM data elements. In other words, none of the anonymization tools provide free text search over pseudoanonymized content and respective de-anonymized search results when the user is authorized to access that information in the PACS. With regard to meta-data anonymization, DICOM Standard for medical imaging communications and storage defines an attribute level confidentiality mechanism [20]. Nevertheless, this mechanism may not have been widely adopted yet [21]. Furthermore, the de-identification process will depend on the specific legal framework of each country and on particular circumstances of the use case. Related to meta-data anonymization, Aryanto et al. [18] evaluated the ability of open source tools to remove a patient’s PHI from DICOM headers. The selection criteria for the tools was the frameworks’ ability to perform de-identification and availability as freeware or an open source tool. The tools selected were Conquest DICOM software, 1 Conquest DICOM: 1 RSNA Clinical Trial Processor (CTP), 2 CTP: 2 DICOM library, 3 DICOM library: 3 DICOMworks, 4 DICOMworks: 4 DVTK DICOM anonymizer, 5 DVTK DICOM anonymizer: 5 GDCM, 6 GDCM: github.com/malaterre/GDCM. 6 K-Pacs, 7 K-Pacs: 7 PixelMed DICOMCleaner, 8 PixelMed DICOMCleaner: 8 Tudordicom, 9 Tudordicom: 9 and YAKAMI DICOM tools. 10 YAKAMI DICOM tools: 10 The de-identification was performed to fifty header elements. The results showed that only five tools could properly de-identify the defined DICOM elements, and in four cases, only after careful customization. Onken et al. [15] have shown that reversible de-identification of DICOM data can be achieved with good coverage by generating anonymization policies from the DICOM standard. The solution supposes that only DICOM searching services are used, which only supports a limited number of mandatory DIM attributes. Regarding annotations in pixel data, there are open source tools that can assist the manual removal of burned pixel information [22]. On the other hand, there are also automated processes that use commercial optical character recognition (OCR) solutions for detecting burned-in text on the image [23]. The problem with the latter solution is that it is not possible to remove only protected health information annotations and preserve other important textual information such as measurements. Huang et al. [24] also proposed a system for the de-identification of medical images, using pre-defined area filters and OCR to recognize and remove patient identifiers. While the authors reported a 65% success rate for pixel data de-identification, pixel data de-identification was not reliable due to errors when recognizing ambiguous characters and due to the medical image background. The anonymized medical imaging repository proposed in this article combines a solution for removing protected health information from pixel data, based on a machine-learning model that provides reliable accuracy, and a novel reversible de-identifier of meta-data that retains search capabilities from the original DICOM data index. 3 Proposed architecture 3.1 Design overview The proposed system contemplates two de-identification processes, where one is for the visual content and the other is for meta-data. Concerning the architecture workflows, it is divided in two major operations, which are storage and content discovery. In the proposed pipeline, visual anonymization takes places prior to the meta reversible de-identification. This is because visual anonymization requires the meta-data PHI fields in order to remove them from the pixel data (Fig. 1 ). If the automatic visual anonymization process fails, a manual alternative takes place in order to ensure that all of the pixel data containing PHI is removed. Only after this procedure does the meta de-identification process take place. The reversible anonymization system can be depicted as a modular combination of algorithms and databases encapsulated as components in Fig. 2 . The DIM Query Provider component is the PACS archive’s default means of querying for DICOM meta-data, comprising the typical archive’s database management system and index mechanisms. As the system is extended with reversible de-identification, this source will be stripped from protected data. The Identification Database component stores and indexes all identification data in secure premises. On the other hand, the DIM de-identifier component (further described in Section 3.3) extracts PHI from incoming DICOM files and replaces unique identifiers with randomly generated keys, while keeping the original content stored in a separate database. The Storage Anonymizer component handles DICOM file anonymization and persistent storage, whereas the De-Identification Query Provider requests identifying data in order to produce queries with anonymized data awareness and recover anonymized information. This component is further detailed in Section 3.4.1. The query transformer (Section 3.4.2) transforms query terms containing identifying data to terms in the anonymized database and the result transformer (Section 3.4.3) takes anonymized search results and restores its protected data. Finally, the developed solution was integrated in the PACS repository as a REST Web service. The next subsections will describe in detail all implemented processes. 3.2 Visual anonymization The visual anonymization module uses image processing functions and a machine-learning model, namely convolutional neural networks (CNN), to provide an automatic system for de-identification of pixel data in DICOM medical images. The visual anonymization pipeline is composed of 6 key parts (depicted in Fig. 1): retrieval of patients PHI from file header, image enhancement, object detection, character recognition, word reconstruction and, finally, the removal of image pixels containing PHI. The first step extracts, from DICOM meta-data, the PHI that is expected to be present in the pixel data. We used the following set of DICOM attributes to be removed from image pixel data: patient name, ID and accession number. In the image enhancement step, adaptive bilateral filtering and total-variation de-noising were used to enhance the image to better feed the next processes of character recognition and classification. Once the pre-processed image is obtained, the object detection step is performed using functions to identify contours and detect objects through bounding rectangles that are subsequently normalized. These filters were chosen with a trial and error methodology with the goal of detecting the character outlines on various types of machines, where the type of character also varies regarding font type and size. Next, each object is classified using a machine learning-based procedure that returns the identified character. In the OCR, there are cases where letters look very much like numbers, which makes it difficult for the machine learning-based model to classify the recognized character. To address these dubious cases, a complementary mechanism was introduced where, in case of doubt, the model assigns a lower similarity weight which is used in the computation of the word similarity distance during the next step. Finally, in the word reconstruction phase, the recognized characters are assembled to construct words, which are then compared to the attributes extracted from the metadata. If this comparison results in a match, the corresponding region in the image pixel-data is replaced with a white bar. Visual anonymization pipeline is available as a REST Web service and provides 3 methods: anonymize, status and download. The anonymize method allows the client to upload a set of DICOM images to be de-identified. Each upload submitted creates an anonymization task that will run on the server. Because the visual de-identification task is time-consuming, an asynchronous method available in the service’s API was used. After submitting the task, the service responds with an HTTP message that contains a Location (i.e. URL) where it is possible to follow the anonymization process. The returned URL points to the status method, including also the anonymization task ID. This service returns the current status of the task, which can be PENDING, FAILURE or PROGRESS. A response with PENDING status means that the task has been submitted and is waiting to be executed. If there is an error while executing the task, the status will be FAILURE. If the status is PROGRESS, the task is still being executed. Along with the PROGRESS status, the service returns information about the number of images already de-identified. This information is used to obtain the real progress of the task. A client can use pooling to find out the task’s progress and to know when the anonymization task has finished. When the anonymization task finishes, the API returns in the status response a URL that can be accessed to download the de-identified DICOM images. This URL points to the API download method. Besides pooling, the API also makes it possible to indicate a URL that will receive the de-identified images after fulfilling the task. This feature was used to enable the integration of this anonymization service in the final architecture. 3.3 Storage process The storage process follows the sequence diagram shown in Fig. 3 and can be summarized in the following steps: 1. A study containing several DICOM objects is received in the PACS archive through standard interfaces; 2. It is channeled to the visual anonymizer, which removes PHI in the image pixel data; 3. The meta anonymizer performs de-identification of PHI in the DICOM objects header; 4. The transformed DICOM objects are archived and system databases are updated. When DICOM files are received in the Dicoogle interface for storage, they are firstly submitted to the visual anonymization process. Next, they are passed to the Anonymous Storage for meta-data anonymization and update of system databases. During the anonymization process, the protected health information is extracted from the DICOM object and saved to the Identification Database. It is based on Apache Lucene, which provides simple field storage capabilities and will facilitate the query transformation step described in Section 3.4.1. The Anonymous Storage module returns a URI of the anonymized object. This URI will be provided to the Dicoogle storage plug-in for long-term physical archive. Here, all textual attributes presented in DICOM objects meta-data is extracted and also indexed, including anonymized attributes. Concerning the anonymization of DICOM objects meta-data, the standard published the Basic Application Level Confidentiality Profile for scenarios in which de-identification may be required, for instance, in clinical trials, creation of teaching files and other types of publication [20]. This profile removes all information related to: • the demographic characteristics of the patient. • the responsible parties or family members. • the personnel involved in the procedure. • the organizations involved in ordering or performing the procedure. • anything that could be used to match instances if given access to the originals, such as UIDs, dates and times; • Private attributes. The PHI attributes that are anonymized in the meta anonymizer were PatientName (0010,0010), PatientID (0010,0020) and the AccessionNumber (0008,0050) as their nature allows for a unique connection with the patient, the study, or the clinical case in the platform’s information system. For each new patient or study identifier obtained during the storage process, a universally unique identifier (UUID) is generated to serve as new patient key or study key, respectively. Distinct UUIDs are attributed to the PatientID and AccessionNumber fields. All images in a study will have the same UUID in the AccessionNumber field and distinct studies will possess different UUIDs, even if the study belong to the same patient. If the patient or study are already present in the database, the existent key is considered. Hence, DICOM object aggregation according to the DICOM Information Model (DIM) is not lost, while preventing further inference of protected data by observation of identifiers, which will not be present in anonymized query results. 3.4 Content discovery The proposed system provides content discovery services based on query operations with reversible identification that can be summarized in the following steps: 1. A structured or free-text search is performed in the PACS archive interface. 2. The meta anonymizer parses the query and channels it to the query transformer. 3. The query transformer uses the information recorded on an identification database to build the respective anonymized query. 4. The anonymized query is executed and a list of anonymized search results obtained. 5. The result transformer will de-anonymize the query results and return them to user. The next subsections will describe in detail the previous processes. 3.4.1 De-identification query provider The de-identification query process follows the sequence diagram shown in Fig. 4 . From a simplified mathematical perspective, the system’s information retrieval capabilities can be pictured as a universe R = ( Q , D ) augmented with a retrieval function f ( q ∈ Q ) → { d 0 , d 1 , … , d n } , where d i are ranked documents in D. Thus, reversible DICOM searching is achieved by establishing an isomorphism I : R ↔ R ′ = ( Q ′ , D ′ ) over the archive’s query functionality, where the right-hand universe R ′ does not contain any original identifying data from the files other than generated keys. The query transformer applies the necessary query term translations of a query q in space Q to query the database in the anonymized space R ′ , whereas the result transformer injects identifying data into the search results in D ′ , thus residing in D. In the system, the isomorphism I is implemented by this pair of components. 3.4.2 Query transformation The query transformer receives the query and identifies the kind of query performed based on the Lucene query syntax. A term query is a query that matches documents containing a term. A phrase query matches documents containing a particular sequence of terms. A boolean query is a query that matches documents matching a boolean combination, as in an intersection (AND) or a union (OR), of other queries. A transformation to the query will be applied if it is either a boolean, term or phrase query. Although more kinds of queries exist in the literature, the ones mentioned above are sufficient to ensure the searches performed over medical imaging repositories. Each of the three types of query has a different transformation algorithm applied to it, described in sequence. The basic principle is to replace the terms in the query with potential key value terms obtained from the identification database, thus matching anonymized entries in the DICOM database. It is worth mentioning that this transformation supports free text queries, the terms of which are encompassed in a default field. With reversible anonymization, these queries are still possible using all possible combinations of group identifier values. Algorithm 1 shows the pseudo-code for the term query transformation. The function simply verifies whether a given anonymized field is the term of the query, replacing it with the key value term from the identification database if it exists. Algorithm 1 Term Query Transformation pseudo-code If the query performed is a phrase query, transformation is performed according to Algorithm 2. This transformation is similar to the term query operation, with the difference that all terms in the phrase must occur in the same sequence. For every case where one of the terms was an anonymized field, a clause was created with a key value term from the identification database. After going through the list of terms, a new query would be built from the updated list of clauses. Algorithm 2 Phrase Query Transformation Pseudo-Code The boolean query transformation algorithm is depicted in Algorithm 3, in which each sub-query is transformed recursively, while leaving the boolean operation unaltered. Algorithm 3 Boolean Query Transformation Pseudo-Code 3.4.3 Result transformation The transformation of search results is described in Algorithm 4. The pseudo-code shows that the identification database is queried for each identification key, and the additional information retrieved from the second search is inserted into the previous search result. Algorithm 4 Search Result Transformation Pseudo-Code 4 Validation and discussion 4.1 Visual anonymization The visual de-identification pipeline used in this research consisted of a fine-tuned version of the model described in a previous contribution [25,26]. The fine-tuning procedure applied to the previously existing model was crucial to increase its performance. The automated process was validated against a set of studies extracted from the US and XC DICOM modalities to contemplate different modality scenarios. The system’s overall performance was measured based on the accuracy obtained when removing all sensitive annotations from images stored in the PACS. The results consisted on the analysis of 500 studies with different PHI burnt in the images. These studies were obtained from a dataset captured in a representative real-world environment which includes studies from two different modalities and several equipment models, presented in Table 1 . Validation of proposed system was performed with real-world data, collected from production environments. It was captured from 7 distinct equipment, representing the two main medical imaging modalities that have PHI burned in pixel data. This diversity of equipment was a main requirement in the validation process because they have distinct characteristics (image resolution, PHI character font and location in the image pixel, etc.) that influence the performance of proposed system, namely the visual anonymizer. The results obtained show that visual anonymization process provides good results, reporting a 97.2% success rate. Performance evaluation measures the correctness of the system since it evaluates the number of cases in which the exams had their pixel data completely anonymized, in an automatic way. Moreover, proposed pipeline also identifies automatically the cases that were not anonymized (2.8%). During the visual anonymization process, the PHI metadata not found on the image is considered as not well anonymized (2.8%). The physician is required to analyze those remaining cases, which still contain DICOM PHI burned in the pixel data, through visual inspection. This occurs prior to the reversible anonymization step of the system. This way, we can ensure that studies made available to regular users of the system are 100% anonymized. As stated previously, Newhauser et al.[23] automated the process of using commercial optical character recognition (OCR) solutions for detecting burned-in annotations on pixel data. Although very efficient, this algorithm removes all alpha-numerical annotations. This is not desirable in some situations where some important annotations must be preserved. On the other hand, Huang et al.[24] reported a 65% success rate for pixel data de-identification. However, the pixel data de-identification was not reliable due to errors when recognizing ambiguous characters and due to the medical image background. Obtained results are significantly better than the ones obtained in [25] which reported a 78.6% success rate and in [26] which reported a improvement to 89.2%. To assess the impact of the improvements made on the model, the previous system reported in [26] was tested with the same dataset used in this article. It obtained a success rate in the anonymization procedure of 52.9%, being significantly lower that the success rate obtained by our improved version of the model. It should be noted that the resolution of the characters and font size in the images may lead to a poor performance [25]. Regarding the final results, we may state that it was possible to use the automated process for pixel data de-identification with excellent performance. Moreover, after analyzing the images that were anonymized, we could conclude that the anonymization process has a diminished impact on the image quality and clinical analysis because it has a relatively low occurrence rate of mistakenly removed areas. 4.2 Reversible meta anonymization In order to validate the meta anonymizer’s reversibility, a set of queries was designed and performed over a common DICOM file collection. For this purpose, 400 clinical cases were collected. The meta-data contained in these studies was adjusted, so that each clinical case would refer to a single patient (hence with a unique patient ID). Finally, the accession number attribute was set to uniquely identify the study in the data set. A total of six queries were constructed in order to cover all relevant cases of reversibility: • Query for an anonymized field. • Query with multiple anonymized fields. • Query with a non-anonymized field. • Query with a range of a non-anonymized field. • Query with a non-anonymized and an anonymized field. • Free-text query. In pursuance of performing the queries in an isolated environment, two separate PACS archives (Dicoogle 11 Dicoogle: 11 ) instances were created, where each one had its set of plug-ins. One instance had the DIM Query provider and the file storage, whereas the second instance also had the meta anonymizer installed. The procedure for both instances was to first store and index the entire dataset, independently of one another. The queries were performed and evaluated using a script that communicates with Dicoogle via the official client API in JavaScript. In the first instance of Dicoogle, queries were requested directly to the source DICOM content index. On the other hand, the actual query provider in the second instance was the anonymizer, which performs the necessary two-way transformations. These queries were built to cover the searching capabilities of the de-anonymizer, including cases where there are only one or multiple anonymized fields, and even free text search. The queries were performed with a priori knowledge of the database, and the literal values mentioned in the searched fields exist in the database. Each of the searched fields had a multiple distribution of results. For instance, if the searched name was “PatientName:Leite”, there would be several Patient Name results with that name. On the other hand, if the search had a unique identifier like PatientID, there would be several studies for only that specific patient. In the particular case of the query “PatientName:“CarneirôSilvia” AND AccessionNumber:001”, there was only one result. These queries (Q) were: • Q1:“PatientID:4” • Q2:“PatientName:‘‘CarneirôSilvia\" AND AccessionNumber:001 • Q3:“StudyDate:20130301” • Q4:“StudyDate:[20101029 TO 20110329]” • Q5:“PatientID:376 AND Modality:CR” • Q6:“Leite M” The search results obtained were aggregated by study, with the structure “Patient Name, Patient Gender, Patient ID, Study Accession Number, Modality, Study Date”. Afterwards, string comparison was performed between the obtained results in each of the Dicoogle instances. Results of the string comparison revealed that the search results obtained from performing the same queries on both instances were the same, i.e. achieved a performance of 100%. This demonstrates that the reversibility of the proposed anonymizer retains search capabilities from the original DICOM data index. In order to compare the performance of storage and indexing processes, 400 clinical cases were used and the time that each system took to perform these tasks was measured. The results can be observed in Table 2 . It is worth mentioning that the measurements taken were only relative to the process of Storage and Indexing, since the data transfer time was not taken into account. As expected, the storage process is more time consuming in the system with the Meta Anonymizer, since it involves the removal of protected data and the storage and indexing of all identification data in secure premises. However, this is an offline process and the duration of data transfer should be much longer than the Storage process, making this process not significant for the end user. Regarding the Indexing process, there is a 7% increase in time in the system with the Meta Anonymizer, but once again, it is an offline process and the difference is negligible for the average user. To assess performance regarding query retrieval, we executed each of the 6 queries 500 times and measured the average as well as the standard deviation of the time taken from the moment the request is made until the information is received. The results can be observed in Fig. 5 . Although the majority of the queries take more time when the system has the Meta Anonymization, on average the difference is 0,030 ms which should not be noticeable for a common user of this solution. 5 Use cases results 5.1 PACS integration In practice, the proposed solution was developed as an extension to Dicoogle, an open source PACS archive [27] with a plug-in-based architecture. All storage, indexing and retrieval capabilities are delegated to independently deployed components. This architecture has enabled Dicoogle to support a multitude of use cases. Typical storage plug-ins in Dicoogle define a binding between file storage and retrieval operations in the system to a storage provider, which can be a remote data source (for instance, a cloud storage provider), or simply the local file system. On the other hand, by relying on further delegation stages, other plug-ins can be made for the sole purpose of processing files as they are stored or retrieved. Furthermore, indexing and query plug-ins drive the system’s search capabilities by providing indexing and querying mechanisms, respectively. For our use case, we have developed two plug-ins, as depicted in Fig. 6 . The storage anonymizer and the query provider assume the respective roles of storage and query plug-ins. Since the anonymization pipeline was integrated into the Dicoogle platform, control access is delegated to Dicoogle, which specifies all users’ privileges of access to private information. Furthermore, were the identification database’s storage outsourced to a cloud provider, additional mechanisms for safe searching would have to be employed to ensure confidentiality in its premises [28]. 5.2 SCREEN-DR SCREEN-DR is a multidisciplinary collaborative platform to study Diabetic Retinopathy (DR). UIn this context, a distributed and automated screening platform for this pathology is being developed, based on state-of-the-art Information and Communication Technologies (ICT). This includes advanced PACS management and the use of machine learning and image analysis technologies, enabling an immediate response from health carers, and thus allowing accurate follow-up strategies and fostering technological innovation. The SCREEN-DR platform is one of the use cases where this type of anonymization system will be implemented. Since it is a multidisciplinary collaborative platform for doctors, researchers and students, it is extremely important to keep the data interconnected and at the same time with a certain level of privacy. Moreover, the datasets available for the SCREEN-DR project may contain images with burned PHI in the pixel data due to the equipment used, and the meta-data present was not anonymized. On the other hand, in this use case depicted in Fig. 7 , a blinded annotation scenario exists as well as the need to signal the image so that the authorized personnel can obtain additional information about the patient to perform a correct diagnosis. Furthermore, a statistical analysis of the prevalence of retinopathy, maculopathy and photo-coagulation will be performed, and therefore some of the PHI such as age and gender will need to be accessed by authorized personnel. 6 Conclusion As a consequence of the emergence of collaborative platforms such as the SCREEN-DR project, the usage of medical image beyond the normal institutional borders has increased significantly. The reason for the creation of such programs is intertwined with research purposes, construction of phenotype-specific databases and even teaching. In this context, there is an even clearer need to maintain patient privacy as well as provide an easy and secure mechanism for privileged access personnel to consult that information. This article proposes a solution for this problem in the form of a reversible de-identifier of medical imaging data that retains search capabilities from the original DICOM data index, as well as a visual anonymizer, integrated into an open source PACS archive. The Visual Anonymizer uses image processing functions and a machine learning model to bring about an automatic system for medical image anonymization, allowing the removal of only protected health information annotations while preserving some textual information burned into the image. On the other hand, the proposed meta-data de-identification system used a modular combination of algorithms and databases for de-identification from the stored files and a seamless query and result reconstruction. Validation of the software proved that developed content discovery mechanisms can be used by authorized users over anonymized data, keeping the search capabilities available in the original repository (in clear text) with a very small impact on system performance. Validation was already performed with real data from various equipments both for the reversible de-identifier and for the visual data anonymizer, therefore it is representative of a real scenario where the proposed solution can be deployed. Using the presented solution with data from more equipments, that have distinct characteristics, might have an impact in obtained performance specifically concerning the visual anonymizer, where certain font types might prove more difficult for the model to detect and classify. Conflict of interest The authors declare that there is no conflict of interest. Acknowledgments Jorge Miguel Silva is funded by the Research grant Project CMUP-ERI/ICT/0028/2014-SCREEN-DR, Research Unit 127/94-IEETA. Eduardo Pinho is funded by Fundação para a Ciência e Tecnologia (FCT) under the grant PD/BD/105806/2014. João Figueira Silva is funded by the research grant of PTDC/EEI-ESS/6815/2014 project. References [1] Frost & Sullivan, Image Management Today Fragmentation and Related Challenges, 2011, pp. 20. [2] B. Myers, U.S. medical imaging informatics industry reconnects with growth in the enterprise image archiving market, 2012. [3] K. Marconi, H. P. Lehmann, Big data and health analytics, 2014. [4] K.J. Dreyer J.H. Thrall D.S. Hirschorn A. Mehta PACS 2006 springer-Verlag New York [5] H.K. Huang PACS and Imaging Informatics: Basic Principles and Applications second ed. 2010 Wiley [6] O.S. Pianykh Digital Imaging and Communications in Medicine (DICOM) – A Practical Introduction and Survival Guide 2012 Springer [7] C. Costa F. Freitas M. Pereira A. Silva J.L. Oliveira Indexing and retrieving DICOM data in disperse and unstructured archives Int. J. Comput. Assist. Radiol. Surg. 4 2009 71 77 [8] J.L. Fernndez-Alemn I.C. Seor P. Ángel Oliver Lozoya A. Toval Security and privacy in electronic health records: a systematic literature review J. Biomed. Infor. 46 2013 541 562 [9] T. Douraki, Ethical and legal dimensions of medical confidentiality in European law of human rights, in: Manage or Perish? Springer US, Boston, MA, 1999, pp. 421–427. [10] A. Gkoulalas-Divanis G. Loukides L. Xiong J. Sun Informatics methods in medical privacy J. Biomed. Infor. 50 2014 1 3 (Special Issue on Informatics Methods in Medical Privacy) [11] L.R. Knudsen, G. Leander, F.L. Bauer, C. De Cannière, C.D. Cannière, C. Petit, J.-J. Quisquater, B. Preneel, F.L. Bauer, C. Adams, C. Adams, A. Stiglic, C. Adams, A.W. Dent, C. Adams, R. Housley, S. Turner, M. Schunter, G. Bleumer, M. Just, G. Bleumer, D. Naccache, H.C.A. van Tilborg, S. Vimercati, P. Samarati, E. Celikel Cankaya, A. Biryukov, A. Biryukov, A. Biryukov, L.D. McFearin, A. Biryukov, S. De Capitani di Vimercati, P. Samarati, B. Kaliski, L.R. Knudsen, C. Fontaine, D. Micciancio, B. Preneel, N. Sendrier, A. Biryukov, N. Heninger, J. Mirkovic, B. Preneel, B. Preneel, A. Canteaut, C. Crépeau, T. Caddy, P. Salvaneschi, G. Bleumer, M. Kuhn, S. Vadhan, I. Shparlinski, X. Wang, G. Dr., M. Riesner, M. Vauclair, A. Rosenthal, E. Sciore, M. Schunter, M.D. Soete, M.T. Hunter, G. Bleumer, B. Preneel, A. Canteaut, C. Carlet, F. Cuppens, N. Cuppens-Boulahia, Y. Desmedt, T. Pedersen, M.E. Locasto, D. Boneh, A.J. Lee, G. Bleumer, E. Kirda, T. Helleseth, F.L. Bauer, F.L. Bauer, F.L. Bauer, D. Accache, F.L. Bauer, H. Imai, A. Yamagishi, M. Videau, C. Crépeau, P. Charpin, Ciphertext-only attack, in: Encyclopedia of Cryptography and Security, springer US, Boston, MA, 2011, pp. 207–207. [12] B. Schneier Secrets and Lies: Digital Security in a Networked World 2004 Wiley Indianapolis, Ind. ISBN-10: 0471253111, 0471453803, ISBN-13:9780470226261, 9780471453802, Url: [13] M. Peleg D. Beimel D. Dori Y. Denekamp Situation-based access control: privacy management via modeling of patient data access scenarios J. Biomed. Infor. 41 2008 1028 1040 [14] R. Cushman A.M. Froomkin A. Cava P. Abril K.W. Goodman Ethical, legal and social issues for personal health records and applications J. Biomed. Infor. 43 2010 S51 S55 (Project HealthDesign) [15] M. Onken, J. Riesmeier, M. Engel, A. Yabanci, B. Zabel, S. Després, Reversible anonymization of DICOM images using automatically generated policies. [16] H. Brenner M. Hoffmeister V. Arndt U. Haug Gender differences in colorectal cancer: implications for age at initiation of screening Br. J. Cancer 96 2007 828 831 [17] NEMA, Digital Imaging and Communications in Medicine ( DICOM ) Part 5: Data Structures and Encoding, 2016. [18] K.Y.E. Aryanto M. Oudkerk P.M.A. van Ooijen Free DICOM de-identification tools in clinical research: functioning and safety of patient privacy Eur. Radiol. 25 2015 3685 3695 [19] J.B. Freymann J.S. Kirby J.H. Perry D.A. Clunie C.C. Jaffe Image data sharing for biomedical research meeting HIPAA requirements for de-identification J. Digit. Imag. 25 2012 14 24 [20] NEMA, DICOM PS3.15 2016e - Security and System Management Profiles, 2016. [21] T. Carpenter J.I.V. Hemert J. Wardlaw An open source toolkit for medical imaging de-identification Eur. Radiol. 2010 1896 1904 [22] PixelMed PublishingTM Java DICOM Toolkit, 2016. [23] W. Newhauser T. Jones S. Swerdloff W. Newhauser M. Cilia R. Carver A. Halloran R. Zhang Anonymization of DICOM electronic medical records for radiation therapy Comput. Biol. Med. 53 2014 134 140 [24] L.-C. Huang H.-C. Chu C.-Y. Lien C.-H. Hsiao T. Kao Privacy preservation and information security protection for patients’ portable electronic health records Comput. Biol. Med. 39 2009 743 750 [25] E. Monteiro, C. Costa, J.L. Oliveira, A machine learning methodology for medical imaging anonymization, in: 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 1381–1384. [26] E. Monteiro C. Costa J.L. Oliveira A de-identification pipeline for ultrasound medical images in DICOM format J. Med. Syst. 41 2017 89 [27] F. Valente L.A.B. Silva T.M. Godinho C. Costa Anatomy of an extensible open source PACS J. Digit. Imag. 29 2016 284 296 [28] L.S. Ribeiro C. Viana-Ferreira J.L. Oliveira C. Costa XDS-I outsourcing proxy: ensuring confidentiality while preserving interoperability, biomedical and health informatics J. IEEE 18 2014 1404 1412 "
    },
    {
        "doc_title": "An Intelligent Cloud Storage Gateway for Medical Imaging",
        "doc_scopus_id": "85027055714",
        "doc_doi": "10.1007/s10916-017-0790-8",
        "doc_eid": "2-s2.0-85027055714",
        "doc_date": "2017-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Cloud Computing",
            "Diagnostic Imaging",
            "Information Storage and Retrieval",
            "Internet",
            "Outsourced Services"
        ],
        "doc_abstract": "© 2017, Springer Science+Business Media, LLC.Historically, medical imaging repositories have been supported by indoor infrastructures. However, the amount of diagnostic imaging procedures has continuously increased over the last decades, imposing several challenges associated with the storage volume, data redundancy and availability. Cloud platforms are focused on delivering hardware and software services over the Internet, becoming an appealing solution for repository outsourcing. Although this option may bring financial and technological benefits, it also presents new challenges. In medical imaging scenarios, communication latency is a critical issue that still hinders the adoption of this paradigm. This paper proposes an intelligent Cloud storage gateway that optimizes data access times. This is achieved through a new cache architecture that combines static rules and pattern recognition for eviction and prefetching. The evaluation results, obtained from experiments over a real-world dataset, show that cache hit ratios can reach around 80%, leading to reductions of image retrieval times by over 60%. The combined use of eviction and prefetching policies proposed can significantly reduce communication latency, even when using a small cache in comparison to the total size of the repository. Apart from the performance gains, the proposed system is capable of adjusting to specific workflows of different institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An efficient architecture to support digital pathology in standard medical imaging repositories",
        "doc_scopus_id": "85020429778",
        "doc_doi": "10.1016/j.jbi.2017.06.009",
        "doc_eid": "2-s2.0-85020429778",
        "doc_date": "2017-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Communication interface",
            "Communications systems",
            "DICOM",
            "Efficient architecture",
            "Hospital information systems",
            "Research applications",
            "Viewer",
            "Whole slide imaging (WSI)",
            "Computer Systems",
            "Diagnostic Imaging",
            "Hospital Information Systems",
            "Humans",
            "Radiology Information Systems",
            "Telepathology",
            "Workflow"
        ],
        "doc_abstract": "© 2017 Elsevier Inc.In the past decade, digital pathology and whole-slide imaging (WSI) have been gaining momentum with the proliferation of digital scanners from different manufacturers. The literature reports significant advantages associated with the adoption of digital images in pathology, namely, improvements in diagnostic accuracy and better support for telepathology. Moreover, it also offers new clinical and research applications. However, numerous barriers have been slowing the adoption of WSI, among which the most important are performance issues associated with storage and distribution of huge volumes of data, and lack of interoperability with other hospital information systems, most notably Picture Archive and Communications Systems (PACS) based on the DICOM standard. This article proposes an architecture of a Web Pathology PACS fully compliant with DICOM standard communications and data formats. The solution includes a PACS Archive responsible for storing whole-slide imaging data in DICOM WSI format and offers a communication interface based on the most recent DICOM Web services. The second component is a zero-footprint viewer that runs in any web-browser. It consumes data using the PACS archive standard web services. Moreover, it features a tiling engine especially suited to deal with the WSI image pyramids. These components were designed with special focus on efficiency and usability. The performance of our system was assessed through a comparative analysis of the state-of-the-art solutions. The results demonstrate that it is possible to have a very competitive solution based on standard workflows.",
        "available": true,
        "clean_text": "serial JL 272371 291210 291682 291870 291901 31 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2017-06-07 2017-06-07 2017-06-12 2017-06-12 2017-06-28T10:50:43 S1532-0464(17)30132-6 S1532046417301326 10.1016/j.jbi.2017.06.009 S300 S300.1 FULL-TEXT 2021-03-09T21:34:14.229334Z 0 0 20170701 20170731 2017 2017-06-07T16:14:26.812606Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref specialabst 1532-0464 15320464 true 71 71 C Volume 71 19 190 197 190 197 201707 July 2017 2017-07-01 2017-07-31 2017 Original research papers article fla © 2017 Elsevier Inc. EFFICIENTARCHITECTURESUPPORTDIGITALPATHOLOGYINSTANDARDMEDICALIMAGINGREPOSITORIES MARQUESGODINHO T 1 Introduction 2 Background 2.1 PACS and DICOM 2.2 Whole-slide imaging visualization 2.3 Content discovery and retrieval 2.4 Related work 2.4.1 Repositories and data formats 2.4.2 Content distribution and visualization 3 Proposed architecture 3.1 Components 3.2 Methods 3.2.1 Image acquisition 3.2.2 Content discovery and visualization 4 Validation 4.1 Storage 4.2 Visualization 5 Conclusion Conflict of interest Ethical approval Acknowledgements References HUISMAN 2010 751 757 A HO 2006 322 331 J WANG 2012 F PARK 2012 557 584 S FARAHANI 2015 23 33 N PUNYS 2009 856 860 V GARCIAROJO 2016 M PATTERSON 2011 1 E PANTANOWITZ 2011 36 L CAICEDO 2011 519 528 J BUENO 2016 61 69 G BRUNYE 2017 171 179 T CORREDOR 2015 39 49 G BECKWITH 2016 87 97 B DIGITALPATHOLOGYHISTORICALPERSPECTIVESCURRENTCONCEPTSFUTUREAPPLICATIONS STANDARDSFORDIGITALPATHOLOGYWHOLESLIDEIMAGING DANIEL 2009 1841 1849 C HAAK 2014 415 420 D BILDVERARBEITUNGFURDIEMEDIZIN2014 IMAGEJPLUGINFORWHOLESLIDEIMAGING KALINSKI 2009 998 1005 T LIPTON 2012 646 652 P DRAGAN 2012 111 121 D PAMBRUN 2012 135 148 J ROJO 2006 285 305 M ZWONITZER 2010 457 458 R TUOMINEN 2009 250 258 V GOODE 2013 A SCHULER 2012 29 38 R TUOMINEN 2010 454 462 V COSTA 2011 848 856 C VALENTE 2015 1 13 F ROJO 2015 57 M MARQUESGODINHOX2017X190 MARQUESGODINHOX2017X190X197 MARQUESGODINHOX2017X190XT MARQUESGODINHOX2017X190X197XT Full 2018-07-01T01:14:31Z OA-Window ElsevierBranded CHU_DOE publishAcceptedManuscriptIndexable 2018-06-12T00:00:00Z 2018-06-12T00:00:00Z UnderEmbargo © 2017 Elsevier Inc. This article is made available under the Elsevier license. item S1532-0464(17)30132-6 S1532046417301326 10.1016/j.jbi.2017.06.009 272371 2020-10-23T16:27:00.782972Z 2017-07-01 2017-07-31 true 433322 MAIN 8 56585 849 656 IMAGE-WEB-PDF 1 gr2 3042 31 219 gr3 6521 110 219 fx1 true 11673 83 219 gr1 7672 64 219 gr2 11934 75 533 gr3 57626 358 712 fx1 true 30660 190 500 gr1 36550 155 534 gr2 93906 333 2362 gr3 433931 1585 3152 fx1 true 190874 841 2213 gr1 281478 687 2364 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P14 560653 14 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P07 699608 07 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P16 588751 16 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P19 792034 19 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P03 711552 03 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P13 722099 13 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P17 642995 17 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P02 698707 02 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P08 626444 08 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P06 698453 06 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P21 322099 21 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P09 729668 09 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P11 620953 11 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P20 306278 20 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P12 746351 12 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P04 715614 04 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P15 680203 15 Tiago_Marques_Godinho_Whole_slide_imaging_2017 936981 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P18 748970 18 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P05 709083 05 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P10 670316 10 Tiago_Marques_Godinho_Whole_slide_imaging_2017_P01 577254 01 am 905410 YJBIN 2805 S1532-0464(17)30132-6 10.1016/j.jbi.2017.06.009 Elsevier Inc. Fig. 1 Single frame (left) format vs Tiled format (middle), and image pyramid example (right). Fig. 2 Overview of the proposed architecture. Fig. 3 Information flows in the proposed Pathology PACS architecture. Table 1 Storage size of different WSI formats. Samples No Max resolution (GP) Pyramid levels DICOM&TIFF Image size DICOM (GB) Image size TIFF (GB) vsDICOM Image size JP2 Reversible (GB) vsDICOM Image size JP2 Irreversible (GB) vsDICOM 21 14.8 10 2.874 2.648 0.92 10,907 3.80 1.558 0.54 99 25.8 10 2.663 2.520 0.95 12,400 4.66 2.720 1.02 61 4.5 9 1.276 1.205 0.94 4277 3.35 0.480 0.38 23 5.1 9 1.152 1.068 0.93 4224 3.67 0.542 0.47 64 5.4 9 1.075 0.984 0.92 3661 3.41 0.566 0.53 Average 0.94±0.02 3.29±0.41 0.43±0.26 Table 2 Trial results. No 21 99 61 23 64 Average Task JVSView JPIP (s) Proposal DICOM (s) Speedup JVSView JPIP (s) Proposal DICOM (s) Speedup JVSView JPIP (s) Proposal DICOM (s) Speedup JVSView JPIP (s) Proposal DICOM (s) Speedup JVSView JPIP (s) Proposal DICOM (s) Speedup 1 2.91 1.87 1.56 3.42 1.92 1.78 4.08 1.65 2.47 4.47 1.56 2.87 3.56 1.58 2.26 2.19 2 22.94 1.14 20.09 23.12 1.12 20.73 27.24 0.92 29.71 27.43 1.07 25.68 31.85 1.27 25.02 24.25 3 30.68 1.36 22.58 18.55 1.59 11.70 26.27 0.97 26.97 29.53 1.06 27.88 31.05 1.16 26.69 23.16 4 15.83 0.47 33.69 10.14 0.82 12.43 12.83 0.47 27.14 13.81 0.40 34.38 19.09 0.42 45.30 30.59 5 29.07 1.46 19.86 16.70 2.69 6.21 29.95 1.03 29.13 23.96 1.15 20.83 35.59 1.01 35.09 22.22 6 12.08 0.04 283.78 10.29 0.03 338.90 13.82 0.80 17.18 10.00 0.03 317.09 17.35 0.43 39.99 199.39 7 19.90 1.09 18.19 22.14 1.87 11.86 26.69 0.91 29.25 19.22 0.91 21.04 29.61 1.13 26.11 21.29 All 133.41 7.44 17.93 104.37 10.03 10.41 140.87 6.76 20.84 128.42 6.18 20.77 168.08 7.02 23.96 18.78 An efficient architecture to support digital pathology in standard medical imaging repositories Tiago Marques Godinho a ⁎ Rui Lebre a Luís Bastião Silva b Carlos Costa a a University of Aveiro, Portugal University of Aveiro Portugal b BMD Software, Portugal BMD Software Portugal ⁎ Corresponding author. Graphical abstract In the past decade, digital pathology and whole-slide imaging (WSI) have been gaining momentum with the proliferation of digital scanners from different manufacturers. The literature reports significant advantages associated with the adoption of digital images in pathology, namely, improvements in diagnostic accuracy and better support for telepathology. Moreover, it also offers new clinical and research applications. However, numerous barriers have been slowing the adoption of WSI, among which the most important are performance issues associated with storage and distribution of huge volumes of data, and lack of interoperability with other hospital information systems, most notably Picture Archive and Communications Systems (PACS) based on the DICOM standard. This article proposes an architecture of a Web Pathology PACS fully compliant with DICOM standard communications and data formats. The solution includes a PACS Archive responsible for storing whole-slide imaging data in DICOM WSI format and offers a communication interface based on the most recent DICOM Web services. The second component is a zero-footprint viewer that runs in any web-browser. It consumes data using the PACS archive standard web services. Moreover, it features a tiling engine especially suited to deal with the WSI image pyramids. These components were designed with special focus on efficiency and usability. The performance of our system was assessed through a comparative analysis of the state-of-the-art solutions. The results demonstrate that it is possible to have a very competitive solution based on standard workflows. Keywords Whole-slide imaging DICOM PACS Pathology Web Viewer 1 Introduction Pathology focuses mainly on the identification structural anomalies, through the naked eye or a microscope, and on the detection of possible relationships with functional disorders of tissues, therefore, identifying diseases. The aim of pathology has remained unchanged over time; focused on the analysis and comparison of tissue specimens on specific glass slides. For this, the use of optical microscopes has been fundamental since it was the only available instrumentation for centuries [1]. Despite using very methodical analysis workflows, it is possible for the same professional to draw different conclusions about the same specimen at different times. Moreover, asking for second opinions is common practice, and specific cases could be part of conferences or external quality assurance programs [2]. Consequently, there is the requirement for glass slides storage and delivery infrastructures. However, the specimen storage process is expensive, requiring accessibility, cleaning, and protection, which entails greater care by specialized staff. In contrast, digital storage and distribution cuts these costs and increases the throughput of pathology laboratories. In this context, the branch of digital pathology and whole-slide imaging arises. These recent concepts refer to the digital capture of an image from a classic glass slide as well as to the field of information systems for managing the associated data. The arrival of Digital Slide Scanners (DSS) has introduced the concept of whole-slide images (WSI). As the name suggests, these images capture the slide as a whole, rather than specific artifacts found by pathologists. As such, they can be captured unattendedly and screened later. These large images have characteristics, forms of handling and operation similar to images produced by optical microscopes [3]. It is worth mentioning that a typical slide scanned at ×40 (approx. 1600 megapixels) produces a file with several gigabytes [4,5] and requires a viewer application with special functionality to fulfill the pathologist’s needs [6]. Digital pathology and whole-slide imaging have been gaining momentum with the proliferation of digital scanners from different manufacturers [5,7]. The literature reports significant advantages associated with the adoption of digital images in pathology [8–11], namely, improvements in diagnostic accuracy, promotion of distributed work processes (e.g. telepathology), integration of images with hospital information systems and economic efficiency gains. Moreover, WSI offers new clinical and research applications to the pathology community [5,12]. However, numerous barriers have been slowing this process, including performance issues, workflow efficiency, infrastructure, and integration with other software [8,9]. Despite being a medical imaging modality, pathology studies have been kept away from standard Picture Archive and Communications Systems (PACS), in proprietary formats and information systems [5,7]. In part because the DICOM standard did not address all the necessary requirements, but also due to the technical challenges raised by whole-slide imaging. As a result, major DDS vendors preferred to use proprietary archives and image formats (e.g. NDPI, SVS, and TIFF) [5,13]. However, in the last few years, whole-slide imaging has been incorporated in the DICOM standard, and vendors are slowly starting to support DICOM in their scanners. This opens the door to the use of vendor-neutral DICOM-compliant archives and services for storage processes and for exchanging data with viewer applications [6]. Moreover, such a system would support third-party applications to access the WSI images directly. Nonetheless, WSI in general purpose PACS is still a mirage, because traditional solutions are not ready to handle the remarkable image resolution and volumes of data generated from each study, requiring new architectures for storage, efficient distribution, and visualization across heterogeneous systems [3]. In this paper, we propose an architecture to include digital pathology workflows in a general purpose PACS using exclusively DICOM standard data formats and communications. The PACS archive provides storage and access to the WSI data via the most recent DICOM Web services which follow the RESTful Web services principles. This flexible interface allows a specific region (i.e. tile) of the large WSI matrix to be requested. A web viewer application that includes a tiling engine specially designed to fulfill the WSI navigation requirements was also developed. It consumes the WSI data directly from the PACS via its DICOM services. This viewer application features all typical tools available in proprietary solutions. The whole solution usability, performance, and efficiency were evaluated. The performance implication of serving WSI was assessed and the results demonstrate an architecture capable of efficiently supporting pathology imaging workflows. The main goal was to demonstrate that it is possible to support a digital pathology laboratory in a common hospital PACS, opening doors to the proliferation of vendor-neutral equipment and application to digital pathology. 2 Background 2.1 PACS and DICOM The DICOM Standard has been the principle driving force behind the adoption of Picture Archive and Communication Systems [14]. DICOM defines not only the storage format for medical images but also the communication protocol for exchanging images and related meta-data among different PACS applications. DICOM objects come coupled with a wide-range of meta-data related to the image itself, the acquisition procedure, and the different stakeholders involved, such as patients, physicians and institutions. The standard defines thoroughly which information should be included in each modality, as well as the image organization in the DICOM Files. This allows DICOM to address the needs of medical imaging workflows and provide general purpose and vendor image formats [14]. Initially, DICOM was developed as a standard for radiology modalities. Nonetheless, it has rapidly spread its coverage to other medical specialties, such as cardiology or ophthalmology [14], due to the benefit of aggregating all patient imaging history in the PACS. The first attempt to introduce microscopy into DICOM dates to 1999, with the addition of the visible light supplement 15 [15]. As stated in [15], the scope of this supplement was to support visible light images acquired by endoscopes, microscopes or photographic cameras. This extension to the standard introduced four new modalities: Endoscopy (ES), Microscopy (GM), Automated-Stage Microscopy (SM), and Photography (XC). WSI was not included in this supplement since the technology was in its early stages. Therefore, the requirements of pathologists were not fulfilled and they continued to work without standard digital imaging end systems. Nevertheless, the automated stage microscopy modality opened the door to automated whole-slide scanners in DICOM. The rising interest in WSI motivated the constitution of DICOM Workgroup 26 with the intention of supporting whole-slide imaging in DICOM. This required alterations to the standard, not only at the image storage level, to handle images with very high resolutions, but also to the information model [16]. The traditional DICOM information model is patient-centric as opposed to the microscopy environment where the specimen is the most relevant subject [16]. The DICOM supplement 122 [17] was the first contribution of this workgroup and introduces the concept of a specimen and container. Moreover, it also includes new data elements necessary to more accurately describe the workflows associated with WSI preparation and acquisition [14]. 2.2 Whole-slide imaging visualization WSI presented major challenges for medical imaging visualization. These images cannot be fully displayed in their maximum resolution, not only because there are no displays supporting such resolutions, but also because their size is much greater than the amount of memory available in most computers [13,18]. As a result, pathologists screening these images use a pan & zoom strategy. This involves browsing and selecting sub-regions with diagnostic interest in a lower resolution image and then zooming into a higher resolution whenever more detail is needed. Nowadays, most Internet users are accustomed to this process because of the map browsers available online, such as Google Maps. The arrangement of the image pixel data in the storage data structure has significant performance implications for the visualization processes. The simplest, and most common, way of storing two-dimensional images is in a single frame/page, where the image pixels are stored in a sequential array. Normally, pixels are oriented horizontally, by rows, although they could follow a vertical orientation just as easily. However, this organization has limitations in the WSI screening paradigm, because it does not provide direct access to 2D sub-regions. As a result, all the regions’ overlapping rows must be loaded completely, which due to the gigantic size of WSI may not fit into the system capabilities [18]. An illustration of this limitation is provided on the left of Fig. 1 . In order to retrieve the green region, all the pixels in red must be retrieved because they are arranged sequentially. A more efficient way of storing high-resolution WSI is using a tiled organization, illustrated in Fig. 1(middle), where the entire image is stored in rectangular regions sequentially. Despite being a more complex organization, it enables direct access to these sub-regions. As a result, in order to load the green region, it is only necessary to retrieve a smaller image subset composed only of the overlapping tiles [3], as shown in Fig. 1. The tile size has a significant effect on the retrieval performance and its optimum value is influenced not only by the physical storage medium but also by the viewer display resolution. While a tiled organization facilitates the panning processes, the zooming process on the other hand is still a challenge. At lower resolutions, larger areas of the image must be accessed in order to display the requested region, as shown in Fig. 1(middle). In the extreme case, a thumbnail requires the entire image data to be processed. This would probably be a very intensive task. In order to optimize zooming, a lower resolution version of the full image can be pre-calculated and stored alongside the full resolution image. Thus, the typical image pyramid organization arises, as shown in Fig. 1(right). According to this scheme, the WSI consists of multiple images at different magnifications where the pyramid provides distinct zoom level. The base of the pyramid contains the highest resolution, while the top contains the lowest resolution image, typically a thumbnail. The thumbnail is a very low-resolution version of the image, making it easy to see the entire image. One or more levels may be created, at intermediate resolutions, to facilitate the loading of arbitrary magnification levels. Each pyramid level follows the tiled organization described above. These strategies are valid for open and proprietary image formats. DICOM Workgroup 26 also adopted the image pyramid concept and introduced it in the standard. The DICOM Supplement 145 [19] takes advantage of the existing multi-frame objects where each sub-resolution image is stored in a separate multi-frame object, while the individual tiles are stored as separated frames. Each resolution level is assigned a different DICOM series. The standard also supports 3D microscopy, a technique that acquires multiple planes of the slide at different depths [20]. Therefore, each resolution level may contain several Z-planes, which represent acquisition at different focal points. The Z-planes can be stored as separate objects within the DICOM series or, alternatively, in the image object with the corresponding magnification level. 2.3 Content discovery and retrieval DICOM defines a set of content discovery and retrieval services, the basis of PACS communications. The most typical services, C-Store, C-Find, and C-Move, allow data, images and/or metadata to be stored, searched and retrieved respectively. These services are built on top of a customized application layer protocol, which is defined in the standard. This has led to multiple problems when using DICOM outside institutional Intranets. The advent of web-based technologies and the necessity of interconnecting multiple medical imaging laboratories across the Internet has led to the addition of several content discovery and retrieval services supported by web-services. Initially, DICOM WADO via Web-Services was defined in supplement 148 [21]. It enabled access to the attributes of DICOM Objects using SOAP web-services. These services mapped the functionality provided by the traditional DICOM C-Move [22]. However, SOAP is considered a heavyweight technology for web-services, because the verbose messages make them difficult to include in lightweight applications, such as websites. Therefore, the community demanded the inclusion of simpler services based on the Restful technology. Moreover, web services for searching and storage were also requested. So in 2014, the standard introduced three REST web services: STOW-RS, 1 URL: service-endpoint/studies; HTTP – POST. 1 QIDO-RS, 2 URL: service-endpoint/studies/{StudyInstanceUID}/series/{SeriesInstanceUID}/instances; HTTP – GET. 2 and WADO-RS. 3 URL: service-endpoint/studies/{StudyInstanceUID}/series/{SeriesInstanceUID}/instances/{SOPInstanceUID}/frames/{FrameList}; HTTP – GET. 3 Besides the traditional DICOM Services and web-services, DICOM also contemplates storage and streaming of JPEG2000 images. Currently, JPEG2000 is the most advanced format for general purpose images. The standard defines a lossless compression algorithm, as well as a protocol for “interacting with JPEG2000 based images in an efficient and effective manner” [23]. In fact, the JPEG2000 standard has a special focus towards image interactivity, supporting interesting features such as resolution scalability, progressive refinement and spatial randomness [23,24]. The JPIP protocol allows viewer application to interact with JPEG2000 images over networks. By using JPIP, viewer applications do not have to download the whole image, and can instead request from the JPIP server a particular region that best fits their visualization purposes [23,25]. This image streaming strategy allows the storage and consistency of image data to be optimized, as well as reducing the required bandwidth to support visualization of remote images [25]. The DICOM Standard supports both JPEG2000 images and the JPIP protocol. Supplement 61 [26] introduced the JPEG2000 format into regular DICOM images. The JPEG2000 pixel data is stored in the DICOM image pixel data attribute, just like any other compression format such as JPEG Baseline or JPEG-LS. As such, this supplement introduced two new transfer syntaxes: 1.2.840.10008.1.2.4.90, and 1.2.840.10008.1.2.4.91. The support for JPIP was introduced by supplement 106 [27]. In this case, the image pixel data is replaced by the JPIP server URL. As a result, the viewer application can request the image directly from the JPIP server, without mediation from the PACS Archive. This behavior is supported by the transfer syntaxes: 1.2.840.10008.1.2.4.94, and 1.2.840.10008.1.2.4.95. 2.4 Related work In the scope of this work, an exhaustive search was performed for PACS/DICOM solutions that support digital pathology workflows, not only in the scientific literature but also in commercial applications. We searched in Scopus using the following keywords: DICOM Pathology Viewer, Web Pathology Viewer and DICOM Whole Slide Imaging. The queries matched 144 articles, of which 33 were found relevant to the topic. A detailed examination of these documents allowed us to conclude that there is no solution, commercial or not, for supporting DICOM WSI in a general purpose PACS resorting only to DICOM compliant communications. Moreover, only a much-reduced number of solutions provided a centralized repository compliant with Cloud Computing requirements, i.e. a network archive and a zero footprint web-viewer ready to run in any common web-browser without third-party software requirements or setup processes. Another important aspect is that many of these solutions required very complex server-side setups in order to achieve good performance [13]. However, the most important conclusion is that, despite being specified in the DICOM Standard, WSI is not currently efficiently supported by vendors [18]. 2.4.1 Repositories and data formats Currently, there is a multitude of image formats used by whole-slide imaging equipment vendors, most notably TIFF, JPEG, JPEG2000, SVS or ndpi [5,28]. These formats represent the most commonly accepted way of storing large images using a pyramidal and tiled structure. In this picture, the TIFF format comes as a convenient container, since it supports multiple tiles and multiple compression algorithms. Moreover, it allows storing multiple images in a single file, which is useful for storing multiple pyramid levels for faster browsing. TIFF also enables semi-structured meta-data to be recorded within the image. Another promising format for whole-slide imaging is JPEG2000. In [6], the authors report that previously to the DICOM Standard, encoding images in JPEG2000 and serving them as JPIP was a workaround for supporting WSI in standard PACS. Such an approach was applied in a solution for browsing mammography images [25]. In [29], a solution with a JPIP viewer is used for supporting education in virtual microscopy. A study addressing the use of virtual microscopy in mobile devices also applied the JPEG2000 image format along with several optimization strategies [13]. A study assessing the performance of JPEG2000 and JPIP protocol for whole-slide imaging was presented in [30]. An optimal combination of JPEG2000 compression algorithm parameters was evaluated for fulfilling the WSI usage requirements. However, there is no discussion about the method that enabled them to draw such conclusions. An empirical process, rather than a quantifiable one, may compromise the validity of such conclusions. The authors also compared their solution performance against a Zoomify 4 4 server. The JPIP Server reportedly managed 33% more users than the Zoomify server. In order to overcome the lack of a de facto standard format for WSI images and as an effort to manage the wide variety of proprietary protocols, the OpenSlide project [31] provides a C library capable of reading and manipulating slides from different vendor formats. Currently, it supports Aperio, Leica, and Hamamatsu among others but, unfortunately, DICOM is not supported. This library provides an abstraction to the applications developer, who may shift his focus from the multitude of vendor formats to the OpenSlide’s unified API. However, a unified API reduces the extractable information to a subset of common information elements. While this does not hinder the extraction of pixel data itself, it affects the metadata related to the image, which in fact has been crucial for the success of the DICOM standard. OpenSlide also provides a web-based viewer that consumes WSIs converted to Microsoft’s Deep Zoom format. In contrast, our architecture resorts exclusively to the DICOM standard for data format and communications. We provide a full-featured Web viewer platform compliant with DICOM. As a result, third-party applications can access to DICOM WSIs, including their metadata. The adoption of DICOM by the major scanner vendors has been delayed due to various reasons, most notably patent ownerships and concerns about the DICOM standard not being able of delivering sufficient performance for use in pathology laboratories. Initiatives, such as described in this article, may contribute to foster the adoption of the standard by demystifying the supposedly negative impact of DICOM format and communications on delivering WSI services. Meanwhile, while the standard is not widely adopted by vendors, a compromise solution is to “DICOMize”, i.e. convert proprietary formats into DICOM, for instance leveraging the OpenSlide library to read the proprietary formats. However, this approach has a major drawback when it comes to fulfilling the DICOM requirements in terms of metadata, namely because it cannot be completely extracted from proprietary formats by third-party applications or simply because these formats do not contain that information. 2.4.2 Content distribution and visualization In [32], a distributed collaborative system for pathologists is described. The article refers to [19], stating that their system is able to export lower-resolution pixel data into DICOM files. The system is supported by a Zoomify web viewer that uses its own file format rather than DICOM. Moreover, the database model described in the article does not include the necessary information to build DICOM compliant WSI. The PAIS project, described in [3], hosts a database for supporting research using WSI. The article describes a comprehensive data model that is considered suitable for supporting a Pathology PACS system. However, the system's interfaces are not DICOM, but custom-made web-services. Moreover, the images seem to be stored in different TIFF files, separated by regions. In their web-site, at least some images are in Asperio's SVS format. A distribution platform based on JPEG2000 and JPIP is presented in [33]. It includes a JVS DICOM Workstation that interacts with the PACS Archive using traditional DICOM Query/Retrieve services. The images are provided to the workstations by a JPIP Server deployed alongside the PACS Archive. These workstations receive a JPIP reference and open an external JPIP viewer to display the WSI images. This article also presents the compressor application that converts the proprietary WSI format to DICOM-compatible JPEG2000 transfer syntax. However, this platform is only compatible with Windows OS and is no longer supported. Dcm-Ar is a web solution based on Adobe Flash [34]. It uses a proprietary protocol that implements progressive image transition, a process similar to JPIP. Because it does not use DICOM services for image distribution, a proxy server is required to support DICOM communications with the PACS archive. This viewer is claimed as being able to display large DICOM images. However, the images used in the validation study only ranged from 10 to 30MB. The article does not refer to any specific modality. In [12], the authors developed a browser-based WSI viewer using Microsoft Silverlight. Similarly, there are many other visualization solutions reported in the literature built entirely using HTML and JavaScript technologies, such as [35], but they do not seem to support whole-slide microscopy. 3 Proposed architecture This section describes an architecture for a fully DICOM compliant PACS for digital pathology. It supports storage, discovery, retrieval and interactive visualization of whole-slide images and fulfills the major requirement of using only DICOM compliant communications and data formats. Pure Web technologies are exploited, leading to a solution that is well suited to Cloud Computing environments. 3.1 Components Apart from acquisition equipment, the proposed architecture has two major components: a PACS Archive and a Web viewer application (Fig. 2 ). The PACS Archive is the central component that provides services for storage, content discovery and retrieval of medical images. It is based on Dicoogle, an open source project offering a plugin-based architecture and a software development kit [36,37] and intended to support the research and development of new PACS components and applications. In this case, an efficient DICOM WSI pyramid generator and a DICOM Web services API were developed, capable of providing tiles at distinct zoom levels. A zero-footprint viewer application, with its own tiling engine, was developed exclusively based on HTML5 and JavaScript technologies. This client is a multi-platform web application that allows pathologists to search over the Pathology PACS archive, retrieve WSI studies and manipulate them in a general-purpose web-browser (e.g. Chrome or Safari). Its tiling engine uses the streaming paradigm to avoid downloading whole images and makes use exclusively of the DICOM Standard for communications with the PACS archive, namely through its DICOM Web services interface. The viewer workstation is a fundamental component of the proposed architecture. To be useful in real-world environments, a set of common tools required by pathologists to review the slide images is provided, such as annotations and filters, for example. Our viewer uses the pan & zoom navigation paradigm described in the background section. In order to be efficient, it makes use of the image pyramids available in the PACS archive and only downloads and maintains in memory portions of the WSI required to display the current viewport region of interest. To do this, it must retrieve some information about the image, namely, its dimension, resolution, z-planes, and the image pyramid configuration. Additionally, for each sub-resolution level, it needs information about its dimension, resolution and tile configuration. 3.2 Methods 3.2.1 Image acquisition The journey to support WSI in general purpose PACS begins with the image acquisition itself. Digital scanners are able to capture digital images directly from glass slides. After the acquisition, images must be moved to the PACS Archive for long-term storage. This procedure is analogous to other medical imaging modalities. Our architecture contemplates two services to accomplish this task. The images can be uploaded to the PACS Archive via the traditional DICOM C-Store service or by its web-service counterpart, the STOW-RS. As digital scanner manufacturers gradually start to support DICOM in their equipment, exporting the DICOM WSI directly from the acquisition equipment to the archive should be straightforward. In fact, we have already sent images to our Pathology PACS directly from a whole-slide scanner of a major vendor. This stage is illustrated in Fig. 3 , step 1. Nonetheless, this process can raise two major issues related to image compression and to the configuration of the image pyramid. These factors have a considerable impact both on the PACS long-term performance, and on the image visualization procedure itself. However, they are subject to the acquisition equipment capabilities and configurations. Regarding image compression, the algorithm used by the acquisition equipment may not be suited to fulfill the PACS requirements, thus creating larger images that would ruin the PACS long-term storage capacity. Conversely, the acquisition equipment could apply an irreversible compression algorithm that produces images unfit for medical diagnosis. As a result, adequately configuring the acquisition equipment is a major requirement for an efficient PACS. Our archive can be configured to recompress the incoming images with a fitter algorithm according to the PACS administrator’s preference. This stage is illustrated in Fig. 3, step 2. Regarding the image pyramid configuration, this article already discussed its major importance in the screening procedure. The acquisition equipment could be configured to produce only the larger resolution image or, alternatively, to create the image pyramid and send several sub-resolution images to the archive. Since the image pyramid configuration influences the performance of the visualization procedure, we argue that it should be generated in the PACS to achieve an optimal compromise between storage space and efficient access. In fact, our PACS Archive is able to generate the sub-resolution images solely based on the largest resolution image. When new images arrive at our PACS, the procedure for generating the image pyramid is scheduled. Depending on the image dimensions and compression used, it could be an intensive process, therefore it is performed as a background process to avoid overloading. This stage is illustrated in Fig. 3, step 3. 3.2.2 Content discovery and visualization The next step in the workflow requires finding a certain image, or specimen, in the archive. In order to discover DICOM WSI in the Pathology PACS, the traditional DICOM Query and Retrieve services (i.e. C-FIND and C-MOVE), as well as the more recent DICOM QIDO-RS service [21], can be used. These services allow searching in the DICOM images metadata. As such, information about the available images can be retrieved, as well as objects of interest described within it, such as Patients or Specimens. Orchestration of these services enables the development of advanced query interfaces that could be implemented by multiple vendors. We have tested our architecture using Dicoogle’s own search interface, as well as BMD’s software PACS Center, which is an enterprise solution. Our viewer makes use of existing image pyramids in the PACS archive because this method is more efficient than generating the image pyramids on demand. In order to do this, it has to retrieve some information about every resolution level of the image, namely, its dimension, resolution, z-planes, and last but not the least, the image pyramid configuration. This information is retrieved from the PACS archive using the DICOM QIDO-RS, i.e. the standard DICOM Web service for querying. The different sub-resolution images can be found by submitting a query for “SeriesInstanceUID”, for which the archive returns a list of instances that belong to the specified series. In order to organize the image pyramid in terms of resolution, the viewer firstly looks for the instance with the attribute “Image Type (0008,0008)” equal to “ORIGINAL/PRIMARY/VOLUME”. This instance has the maximum resolution. In contrast, instances of this attribute ending in “../VOLUME/DERIVED” are sub-resolution images. For each of these images, the viewer computes their magnification rate based on the image size (cols x rows). As a result, the visualization platform builds an internal representation of the image pyramid, as illustrated in Fig. 3, step 4. At this point, the viewer is ready to download the required image tiles and start displaying them. Initially, it identifies the resolution level in the image pyramid’s that is closest to the viewport window resolution. This minimizes the amount of pixel data exchanged between the PACS archive and the remote viewer, as well as the complexity of the resizing operation required to fit the pixel data to the viewport window. The image pixel data is retrieved from the archive using the DICOM WADO-RS service. The viewer specifies the image SOP, Study, and Series UIDs, as well as the list of frames to be retrieved. Remember that the image tiles are stored in individual frames, as described in the Background section. After receiving the frames, the viewer copies their pixel data into the window viewport, thus displaying the image. If the retrieved frames do not fit the viewport requirements exactly, the viewer also performs a resizing and cropping operation on the tiles’ pixel data. This is completely performed client-side, to avoid loading the PACS archive with additional image operations which could compromise the platform’s performance and scalability. Panning and zooming operations are resolved analogously by the viewer. In each event the user performs on the viewport, the tiling engine recalculates the closest resolution level and the underlying image tiles. These tiles are requested via DICOM WADO-RS and displayed on the window’s viewport. As expressed, the application supports several filters to the original image that are performed client-side and therefore have no additional impact on the solution’s server-side performance. 4 Validation This section provides several controlled laboratory trials aiming to validate the proposed architecture. The goal is to show that it is possible to support digital pathology efficiently in a general purpose PACS with fully DICOM compliant communications, and without compromising the system’s overall quality of service. To do this, several trials were made targeting the challenges faced by our architecture, namely the storage space required and the content retrieval performance. When possible, the trials included the comparison of our architecture with the current state of the art alternatives. We also deployed an online demonstration with publicly available studies. 5 Available at: in the pathology menu. 5 4.1 Storage Regarding the required storage space to hold WSI studies, our interest lays in knowing how the DICOM format influences this variable, especially when compared to other formats. We used a dataset composed of 100 WSI studies collected by a major vendor scanner. The images were stored in DICOM WSI format, encoded with JPEG lossy compression (factor 85). Sub-resolution images were also supplied by the scanner. The maximum resolution images of this dataset have a combined size of 13.9GB. Together with the sub-resolution images, the dataset amounts to 19.4GB. Regarding the image dimensions, the largest images have 26 GPixels, while the lowest image resolution is 16 MPixels. For validation purposes, the studies were replicated in other formats, namely Pyramidal TIFF, JPEG2000 Lossy (factor 85) and JPEG2000 Lossless. The JPEG2000 images were generated using JVSDicom Compressor with parameters recommended in [30]. These parameters are supposed to enable better visualization of the WSIs. Table 1 shows a comparison of the required storage space for the multiple image formats. For simplicity reasons, only the values of the five largest images are shown, together with the mean and standard deviation values for the whole dataset. By analyzing Table 1, it is possible to conclude that TIFF pyramids are on average 94% the size of the same image stored in DICOM. This was expected due to the amount of important metadata stored alongside the DICOM images, which is not shipped in the TIFF files. Another conclusion drawn from this table is that JPEG2000 reversible compression tends to generate very large files when compared with irreversible JPEG compression (factor=85) of the DICOM pyramids. These images are on average 3 times the size of the reference for this study. Finally, comparing the DICOM image pyramids with the best JPEG2000 irreversible compression parameters, we found that on average the images produced are 43% the size of our reference. We think that such a figure is relevant in the context of an institutional PACS, since it represents meaningful storage savings. However, as will become clear in this document, this saving in storage space will come at the expense of performance in the visualization process, which is a negative effect. 4.2 Visualization Performance in image visualization is of paramount importance for any digital pathology laboratory since it influences directly the quality of service perceived by pathologists. Therefore, it is crucial to validate the proposed architecture on this point, as well as comparing its performance with the previous alternatives described in this document. In order to assess our architecture’s content discovery and retrieval capabilities, we elaborated a trial to evaluate how quickly images could be presented to the user. This consists of executing a predefined workflow on the viewer application. A workflow with seven tasks was designed for each image. The tasks, shown below, reflect common operations performed by users and require additional data to be retrieved from the image server. 1. Open the image. 2. Zoom to 10x in the current position (center). 3. Pan to a point ‘a’. 4. Zoom to 20x centered on point ‘a’. 5. Pan to point ‘b’. 6. Zoom to 40x centered on point ‘b’. 7. Pan to point ‘c’. The time required for the application to fulfill each task was measured. This process involved displaying the image regions in their highest quality. We submitted two viewers to this trial, the solution proposed here and JVSView, which was selected because it was presented in the literature as the best solution using JPEG2000 and JPIP, namely in [30]. Consequently, we can establish a comparison between the performance of a fully DICOM architecture, and its JPEG2000 & JPIP counterpart. Both applications and servers were deployed on the same computer (Intel Core i7-3770 CPU, 16GB DDR3 RAM, and a 7200rpm hard-drive), and the same monitor and resolution were used by both applications. Table 2 provides the output of this trial as well as the speed-up provided by our architecture. The results show that the proposed solution has significant performance gains. For anyone using both applications, this perception is immediate due to the usability discrepancy between them. The proposed architecture managed to complete the workflow on average 18x faster than its counterpart. A key factor is that it managed to complete every task in less than 2s, which we think is a manageable waiting period for a reviewing pathologist. In the previous experiments, the time required for retrieving each tile from the proposed Pathology PACS archive was also recorded. On average, each tile was loaded in 20ms, with a standard deviation of 15ms. We estimate that the proposed viewer is capable of loading around 50 tiles per second, which corresponds to an area of 3620 square pixels. This area is larger than the high-resolution 4K displays which are expected to improve the WSI screening efficiency [38]. Those are interesting values that validate the content retrieval capabilities of a solution based on standard DICOM services. Moreover, the improvements in the visualization process and image distribution completely compensate for the greater storage space requirements. 5 Conclusion This article proposes, describes and validates a PACS architecture for supporting digital pathology and whole-slide imaging along with the remaining modalities of medical imaging. The architecture resorts uniquely to the DICOM standard, both for communications and data format, and the proposed components were designed with a special focus on efficiency and usability. To our knowledge, this is the first attempt at building such a functional architecture. The performance challenges raised by whole-slide imaging and the idea that DICOM services could not be used to efficiently support these scenarios were the most likely causes for this gap in the state of the art. However, this article demonstrates that a standard PACS-DICOM solution is capable of achieving better performance than common proprietary solutions based on JPEG2000. Moreover, the solution is also compliant with current web application requirements, using pure Web technologies and paradigms that make it suitable to support tele-pathology services. Conflict of interest All authors declare that there are no conflicts of interest in this work. Ethical approval This article does not contain any studies with human participants or animals performed by any of the authors. Acknowledgements This work is financed by the ERDF – European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation – COMPETE 2020 Programme, and by National Funds through the FCT – Portugal (Fundação para a Ciência e a Tecnologia) within project CMUP-ERI/TIC/0028/2014. Tiago Marques Godinho is funded by FCT under the grant agreement SFRH/BD/104647/2014. References [1] A. Huisman A. Looijen S.M. van den Brink P.J. van Diest Creation of a fully digital pathology slide archive by high-volume tissue slide scanning Hum. Pathol. 41 5 2010 751 757 [2] J. Ho A.V. Parwani D.M. Jukic Y. Yagi L. Anthony J.R. Gilbertson Use of whole slide imaging in surgical pathology quality assurance: design and pilot validation studies Hum. Pathol. 37 3 2006 322 331 [3] F. Wang T.W. Oh C. Vergara-Niedermayr T. Kurc J. Saltz Managing and querying whole slide images Proceed. SPIE 8319 2012 [4] S. Park L. Pantanowitz A.V. Parwani Digital imaging in pathology Clin. Lab. Med. 32 12 2012 557 584 [5] N. Farahani A.V. Parwani L. Pantanowitz Whole slide imaging in pathology: advantages, limitations, and emerging perspectives Pathol. Labor. Med. Int. 7 2015 23 33 [6] V. Punys A. Laurinavicius J. Puniene A data model for handling whole slide microscopy images in picture archiving and communications systems Stud. Health Technol. Inform. 150 2009 856 860 [7] M. Garcia-Rojo A. Sanchez G. Bueno D.d. Mena Standardization of pathology whole slide images according To DICOM 145 supplement and storage in PACs Diagn. Pathol. 1 2016 [8] E.S. Patterson M. Rayo C. Gill M.N. Gurcan Barriers and facilitators to adoption of soft copy interpretation from the user perspective: lessons learned from filmless radiology for slideless pathology J. Pathol. Inform. 2 2011 1 [9] L. Pantanowitz P.N. Valenstein A.J. Evans K.J. Kaplan J.D. Pfeifer D.C. Wilbur Review of the current state of whole slide imaging in pathology J. Pathol. Inform. 2 2011 36 [10] J.C. Caicedo F.A. González E. Romero Content-based histopathology image retrieval using a kernel-based semantic annotation framework J. Biomed. Inform. 44 8 2011 519 528 [11] G. Bueno M.M. Fernandez-Carrobles O. Deniz M. Garcia-Rojo New trends of emerging technologies in digital pathology Pathobiology 83 2016 61 69 [12] T.T. Brunyé E. Mercan D.L. Weaver J.G. Elmore Accuracy is in the eyes of the pathologist: the visual interpretive process and diagnostic accuracy with digital whole slide images J. Biomed. Inform. 66 2 2017 171 179 [13] G. Corredor E. Romero M. Iregui An adaptable navigation strategy for Virtual Microscopy from mobile platforms J. Biomed. Inform. 54 4 2015 39 49 [14] B.A. Beckwith Standards for digital pathology and whole slide imaging K.J. Kaplan L.K.F. Rao Digital Pathology: Historical Perspectives, Current Concepts & Future Applications 2016 Springer International Publishing Cham 87 97 [15] A.C.R. Nema, Digital imaging and communications in medicine (DICOM) Supplement 15: Visible Light Image for Endoscopy, Microscopy, and Photography, ed: Part, 1999. [16] C. Daniel M. Garcia Rojo K. Bourquard D. Henin T. Schrader V. Della Mea Standards to support information systems integration in anatomic pathology Arch. Pathol. Lab. Med. 133 11 2009 1841 1849 [17] A.C.R. Nema, Digital imaging and communications in medicine (DICOM) Supplement 122: Specimen Identification and Revised Pathology, ed.: Part, 2008. [18] D. Haak Y.Z. Filmwala E. Heder S. Jonas P. Boor T.M. Deserno An ImageJ plugin for whole slide imaging T.M. Deserno H. Handels H.-P. Meinzer T. Tolxdorff Bildverarbeitung für die Medizin 2014 2014 Springer Berlin Heidelberg Berlin, Heidelberg 415 420 [19] A.C.R. Nema, Digital imaging and communications in medicine (DICOM) Supplement 145: Whole Slide Imaging in Pathology, ed.: Part, 2009. [20] T. Kalinski R. Zwönitzer F. Grabellus S.-Y. Sheu S. Sel H. Hofmann Lossy compression in diagnostic virtual 3-dimensional microscopy—where is the limit? Hum. Pathol. 40 7 2009 998 1005 [21] A.C.R. Nema, Digital imaging and communications in medicine (DICOM) Supplement 148: Web Access to DICOM Persistent Objects by Means of Web Services, ed.: Part, 2011. [22] P. Lipton P. Nagy G. Sevinc Leveraging internet technologies with DICOM WADO J. Digit. Imaging 25 2012 646 652 [23] D.S. Taubman, R. Prandolini, Architecture, philosophy, and performance of JPIP: internet protocol standard for JPEG2000, 2003, pp. 791–805. [24] D. Dragan D. Ivetić Request redirection paradigm in medical image archive implementation Comput. Methods Programs Biomed. 107 8 2012 111 121 [25] J.-F. Pambrun R. Noumeir Streaming of medical images using JPEG2000 interactive protocol Int. J. Innov. Comput. Appl. 4 11 2012 135 148 [26] A.C.R. Nema, Digital imaging and communications in medicine (DICOM) Supplement 61: JPEG 2000 Transfer Syntaxes, ed.: Part, 2002. [27] A.C.R. Nema, Digital imaging and communications in medicine (DICOM) Supplement 106: JPEG 2000 Interactive Protocol, ed.: Part, 2006. [28] M.G. Rojo G.B. García C.P. Mateos J.G. García M.C. Vicente Critical comparison of 31 commercially available digital slide systems in pathology Int. J. Surg. Pathol. 14 11 2006 285 305 [29] R. Zwönitzer H. Hofmann A. Roessner T. Kalinski Virtual 3D microscopy in pathology education Hum. Pathol. 41 3 2010 457 458 [30] V.J. Tuominen J. Isola The Application of JPEG2000 in virtual microscopy J. Digit. Imaging 22 2009 250 258 [31] A. Goode B. Gilbert J. Harkes D. Jukic M. Satyanarayanan OpenSlide: A vendor-neutral software foundation for digital pathology J. Pathol. Inform. 4 2013 pp. 27-27 [32] R. Schuler D.E. Smith G. Kumaraguruparan A. Chervenak A.D. Lewis D.M. Hyde A flexible, open, decentralized system for digital pathology networks Stud. Health Technol. Inform. 175 2012 29 38 [33] V.J. Tuominen J. Isola Linking whole-slide microscope images with DICOM by using JPEG2000 interactive protocol J. Digit. Imaging 23 8 2010 454 462 [34] E.J. Arguinarena, J.E. Macchi, P.P. Escobar, M. Del Fresno, J.M. Massa, M.A. Santiago, Dcm-ar: A Fast Flash-based Web-PACS viewer for Displaying Large DICOM Images, in: Conf. Proc. IEEE Eng. Med. Biol. Soc. 2010, 2010, pp. 3463–3466. [35] E.J.M. Monteiro, C. Costa, J.L. Oliveira, A DICOM viewer based on web technology, in: 2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013), 2013, pp. 167–171. [36] C. Costa C. Ferreira L. Bastião L. Ribeiro A. Silva J. Oliveira Dicoogle - an Open Source Peer-to-Peer PACS J. Digit. Imaging 24 2011 848 856 [37] F. Valente L.B. Silva T. Godinho C. Costa Anatomy of an extensible open source PACS J. Digit. Imaging 2015 1 13 [38] M.G. Rojo G. Bueno Analysis of the impact of high-resolution monitors in digital pathology J. Pathol. Inform. 6 2015 57 "
    },
    {
        "doc_title": "Intelligent generator of big data medical imaging repositories",
        "doc_scopus_id": "85022227696",
        "doc_doi": "10.1049/iet-sen.2016.0191",
        "doc_eid": "2-s2.0-85022227696",
        "doc_date": "2017-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Design and implementations",
            "Imaging modality",
            "Performance and scalabilities",
            "Real environments",
            "Research purpose",
            "Test bed environment",
            "Validation process"
        ],
        "doc_abstract": "© The Institution of Engineering and Technology 2017.The production of medical imaging data has grown tremendously in the last decades. Nowadays, even small institutions produce a considerable amount of studies. Furthermore, the general trend in new imaging modalities is to produce more data per examination. As a result, the design and implementation of tomorrow's storage and communication systems must deal with big data issues. The research on technologies to cope with big data issues in large scale medical imaging environments is still in its early stages. This is mostly due to the difficulty of implementing and validating new technological approaches in real environments, without interfering with clinical practice. Therefore, it is crucial to create test bed environments for research purposes. This study proposes a methodology for creating simulated medical imaging repositories, based on the indexing of model datasets, extraction of patterns and modelling of study production. The system creates a model from a real-world repository's representative time window and expands it according to on-going research needs. In addition, the solution provides distinct approaches to reducing the size of the generated datasets. The proposed system has already been used by other research projects in validation processes that aim to assess the performance and scalability of developed systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A De-Identification Pipeline for Ultrasound Medical Images in DICOM Format",
        "doc_scopus_id": "85017457466",
        "doc_doi": "10.1007/s10916-017-0736-1",
        "doc_eid": "2-s2.0-85017457466",
        "doc_date": "2017-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Confidentiality",
            "Data Anonymization",
            "Image Processing, Computer-Assisted",
            "Information Dissemination",
            "Privacy",
            "Software",
            "Ultrasonography"
        ],
        "doc_abstract": "© 2017, Springer Science+Business Media New York.Clinical data sharing between healthcare institutions, and between practitioners is often hindered by privacy protection requirements. This problem is critical in collaborative scenarios where data sharing is fundamental for establishing a workflow among parties. The anonymization of patient information burned in DICOM images requires elaborate processes somewhat more complex than simple de-identification of textual information. Usually, before sharing, there is a need for manual removal of specific areas containing sensitive information in the images. In this paper, we present a pipeline for ultrasound medical image de-identification, provided as a free anonymization REST service for medical image applications, and a Software-as-a-Service to streamline automatic de-identification of medical images, which is freely available for end-users. The proposed approach applies image processing functions and machine-learning models to bring about an automatic system to anonymize medical images. To perform character recognition, we evaluated several machine-learning models, being Convolutional Neural Networks (CNN) selected as the best approach. For accessing the system quality, 500 processed images were manually inspected showing an anonymization rate of 89.2%. The tool can be accessed at https://bioinformatics.ua.pt/dicom/anonymizer and it is available with the most recent version of Google Chrome, Mozilla Firefox and Safari. A Docker image containing the proposed service is also publicly available for the community.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extensible Architecture for Multimodal Information Retrieval in Medical Imaging Archives",
        "doc_scopus_id": "85019197806",
        "doc_doi": "10.1109/SITIS.2016.58",
        "doc_eid": "2-s2.0-85019197806",
        "doc_date": "2017-04-21",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Complete solutions",
            "Content based image retrieval",
            "Digital imaging",
            "Distributed systems",
            "Health facilities",
            "Multimodal information retrieval",
            "Open sources",
            "Scientists and engineers"
        ],
        "doc_abstract": "© 2016 IEEE.The challenge of medical information retrieval has attracted significant attention since the introduction of digital imaging in hospitals and other health facilities. Given the huge growth of medical imaging data produced in the past few years, studying new ways to index, process and retrieve medical images becomes an important subject to be addressed by the wider community of radiologists, scientists and engineers. At the moment, content-based image retrieval in medical imaging archives, although known to be beneficial to practitioners and researchers, is still rarely integrated into these archives. The aim of this paper is to present an overview of multimodal information retrieval for medical imaging studies, as well as the architecture of a solution for automatic medical image classification and retrieval using combinations of text and image queries. The complete solution was designed and implemented over an extensible open-source medical imaging archive software.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Multimodal Search Engine for Medical Imaging Studies",
        "doc_scopus_id": "84983390819",
        "doc_doi": "10.1007/s10278-016-9903-z",
        "doc_eid": "2-s2.0-84983390819",
        "doc_date": "2017-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Content based image retrieval",
            "Graphical user interfaces (GUI)",
            "Information storage and retrieval",
            "Multimodal information retrieval",
            "Reproducibilities",
            "Diagnosis, Computer-Assisted",
            "Diagnostic Imaging",
            "Humans",
            "Information Storage and Retrieval",
            "Radiology Information Systems",
            "Search Engine",
            "Software",
            "User-Computer Interface"
        ],
        "doc_abstract": "© 2016, Society for Imaging Informatics in Medicine.The use of digital medical imaging systems in healthcare institutions has increased significantly, and the large amounts of data in these systems have led to the conception of powerful support tools: recent studies on content-based image retrieval (CBIR) and multimodal information retrieval in the field hold great potential in decision support, as well as for addressing multiple challenges in healthcare systems, such as computer-aided diagnosis (CAD). However, the subject is still under heavy research, and very few solutions have become part of Picture Archiving and Communication Systems (PACS) in hospitals and clinics. This paper proposes an extensible platform for multimodal medical image retrieval, integrated in an open-source PACS software with profile-based CBIR capabilities. In this article, we detail a technical approach to the problem by describing its main architecture and each sub-component, as well as the available web interfaces and the multimodal query techniques applied. Finally, we assess our implementation of the engine with computational performance benchmarks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards representation learning for biomedical concept detection in medical images: UA.PT bioinformatics in ImageCLEF 2017",
        "doc_scopus_id": "85034768395",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85034768395",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic information extraction",
            "ImageCLEF",
            "K-means clustering",
            "Learning approach",
            "Medical decision support system",
            "Multimedia Retrieval",
            "Representation learning",
            "Representation type"
        ],
        "doc_abstract": "Representation learning is a field that has rapidly evolved during the last decade, with much of this progress being driven by the latest breakthroughs in deep learning. Digital medical imaging is a particularly interesting application since representation learning may enable better medical decision support systems. ImageCLEFcaption focuses on automatic information extraction from biomedical images. This paper describes two representation learning approaches for the concept detection sub-task. The first approach consists of k-means clustering to create bags of words from SIFT descriptors. The second approach is based on a custom deep denoising convolutional autoencoder. A set of perceptron classifiers were trained and evaluated for each representation type. Test results showed a mean F1 score of 0.0488 and 0.0414 for the best run using bags of words and the autoencoder, respectively.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D-CNN in drug resistance detection and tuberculosis classification",
        "doc_scopus_id": "85034743218",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85034743218",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D-CNN",
            "Classification tasks",
            "Convolutional neural network",
            "ImageCLEF",
            "Object classification",
            "Over fitting problem",
            "Tuberculosis",
            "Unbalanced distribution"
        ],
        "doc_abstract": "Object classification is a very demanding field in computer vision, especially when dealing with medical imaging datasets, which are often small and have unbalanced distributions. Deep learning (DL) methods have proven to be effective in dealing with such problems and have established themselves as the state-of-the-art. ImageCLEFtuberculosis is a challenge that encompasses the classification problem on medical images, and is divided into two subtasks: Drug Resistance Detection and Tuberculosis classification. For both subtasks, provided images were pre-processed to segment the lungs from the CT volumes. Afterwards, pre-processed CT volumes were fed in batches to a 3D convolutional neural network. Test results for the Drug Resistance detection task scored an accuracy of 46.5% and AUC of 0.46, while in the Tuberculosis classification task an accuracy of 24% and Cohen's Kappa value of 0.022 were obtained. Using data augmentation and weight normalization, the overfitting problem could be reduced, and submitted models' performance improved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "User riented latform for ata nalytics in",
        "doc_scopus_id": "85033482382",
        "doc_doi": "10.3233/978-1-61499-678-1-717",
        "doc_eid": "2-s2.0-85033482382",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Continuous production",
            "Conventional approach",
            "Data analytic",
            "DICOM",
            "Graphical interface",
            "Institutional repositories",
            "Treatment process",
            "Web-based solutions"
        ],
        "doc_abstract": "©2016 European Federation for Medical Informatics (EFMI) and IOS Press.The production of medical imaging studies and associated data has been growing in the last decades. Their primary use is to support medical diagnosis and treatment processes. However, the secondary use of the tremendous amount of stored data is generally more limited. Nowadays, medical imaging repositories have turned into rich databanks holding not only the images themselves, but also a wide range of metadata related to the medical practice. Exploring these repositories through data analysis and business intelligence techniques has the potential of increasing the efficiency and quality of the medical practice. Nevertheless, the continuous production of tremendous amounts of data makes their analysis difficult by conventional approaches. This article proposes a novel automated methodology to derive knowledge from medical imaging repositories that does not disrupt the regular medical practice. Our method is able to apply statistical analysis and business intelligence techniques directly on top of live institutional repositories. It is a Web-based solution that provides extensive dashboard capabilities, including complete charting and reporting options, combined with data mining components. Moreover, it enables the operator to set a wide multitude of query parameters and operators through the use of an intuitive graphical interface.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A dynamic approach to support interoperability for medical reports using DICOM SR",
        "doc_scopus_id": "85018848400",
        "doc_doi": "10.3233/978-1-61499-678-1-461",
        "doc_eid": "2-s2.0-85018848400",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Clinical observation",
            "DICOM",
            "Dynamic approaches",
            "Integrating the healthcare enterprise",
            "Medical institutions",
            "Profile management",
            "Radiology reports",
            "Structured reports",
            "Diagnostic Imaging",
            "Electronic Health Records",
            "Radiology Information Systems",
            "Systems Integration"
        ],
        "doc_abstract": "© 2016 European Federation for Medical Informatics (EFMI) and IOS Press.The standardization of data structures for clinical observations in medical imaging environments is a relatively recent effort. DICOM standard defines a set of supplements for different medical reports denominated as Structured Reports (SR). In 2013, Integrating the Healthcare Enterprise (IHE) also followed this trend by publishing the profile Management of Radiology Report Templates (MRRT). However, the generalized adoption of these normalized reports has been delayed due to several factors. In fact, numerous medical institutions still use proprietary formats that do not promote sharing and remote access. New strategies to incentivise the adoption of normalized report templates are needed to make them interoperable between distinct applications. This article proposes a new method to automatically generate DICOM SR from distinct data sources. It encompasses a flexible mapping schema that can be used with distinct medical imaging modalities. Our ultimate goal is to encourage the usage of DICOM SR by providing an effortless method to convert proprietary formats into standard ones. Moreover, the developed methods can be also used for supporting IHE MRRT profiles, making the reports accessible across different information systems and institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web Validation Service for Ensuring Adherence to the DICOM Standard",
        "doc_scopus_id": "85018844427",
        "doc_doi": "10.3233/978-1-61499-753-5-38",
        "doc_eid": "2-s2.0-85018844427",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Diagnostic Imaging",
            "Internet",
            "Management Information Systems",
            "Workflow"
        ],
        "doc_abstract": "© 2017 European Federation for Medical Informatics (EFMI) and IOS Press.The DICOM Standard has been fundamental for ensuring the interoperability of Picture Archive and Communications Systems (PACS). By compiling rigorously to the standard, medical imaging equipment and applications from different vendors can share their data, and create integrated workflows which contributes to better quality healthcare services. However, DICOM is a complex, flexible and very extensive standard. Thus, it is difficult to attest the conformity of data structures produced by DICOM applications resulting in unexpected behaviors, errors and malfunctions. Those situations may be critical for regular PACS operation, resulting in serious losses to the healthcare enterprise. Therefore, it is of paramount importance that application vendors and PACS administrators are confident that their applications follow the standard correctly. In this regard, we propose a method for validating the compliance of PACS application with the DICOM Standard. It can capture the intricate dependency structure of DICOM modules and data elements using a relatively simple description language. The modular nature of our method allows describing each DICOM module, their attributes, and dependencies on a re-usable basis. As a result, our validator is able to encompass the numerous modules present in DICOM, as well as keep up with the emergence of new ones.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating multiple data sources in a cardiology imaging laboratory",
        "doc_scopus_id": "85006493479",
        "doc_doi": "10.1109/HealthCom.2016.7749524",
        "doc_eid": "2-s2.0-85006493479",
        "doc_date": "2016-11-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health (social science)",
                "area_abbreviation": "SOCI",
                "area_code": "3306"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Business analytics",
            "dicom",
            "Heterogeneous systems",
            "Information elements",
            "Multiple data sources",
            "Proprietary information",
            "Services management",
            "Structured reports"
        ],
        "doc_abstract": "© 2016 IEEE.Nowadays, medical imaging laboratories are supported by heterogeneous systems, including image repositories, acquisition devices, viewer workstations and other administrative information systems. They hold tremendous amounts of data resulting not only from imaging modalities, but also from patient diagnosis, treatment, and services management. Unfortunately, the interoperability between the different medical information systems is still a major limitation. Despite the existence of standards to support the distinct RIS and PACS applications, such as DICOM and HL7, the interoperability between them is, in most cases, limited to a few sets of information elements. As a result, the establishment of cooperative workflows or the integrated visualization of patient data is still compromised. Moreover, this scenario severely constraints the usage of these data for research and business analytics purposes, commonly referred as secondary uses of data. In this document, we propose a method for transforming echocardiography reports held by proprietary information systems into DICOM Structured Reports (SR), the gold standard for interoperability in medical imaging. As a result, reports, images, and associated metadata can be accessed and shared by all PACS applications in an integrated and structured manner. Furthermore, the large-scale federation of those elements has a tremendous interest for data analytics and secondary uses of data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessing the relational database model for optimization of content discovery services in medical imaging repositories",
        "doc_scopus_id": "85006384872",
        "doc_doi": "10.1109/HealthCom.2016.7749484",
        "doc_eid": "2-s2.0-85006384872",
        "doc_date": "2016-11-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health (social science)",
                "area_abbreviation": "SOCI",
                "area_code": "3306"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Acceptable performance",
            "Content discoveries",
            "Controlled experiment",
            "DICOM",
            "Medical decision making",
            "Performance optimizations",
            "Picture archiving and communication systems (PACS)",
            "Relational database model"
        ],
        "doc_abstract": "© 2016 IEEE.Medical imaging has been an essential contributor to high-quality medical decisions. In the past few years, the production of medical imaging data has grown impressively, thanks to the increasing number of imaging centers and higher resolution modalities. Keeping high availability and acceptable performance in this scenario raises new challenges related to storage, discovery and distribution of imaging data. Nowadays Picture Archiving and Communication System (PACS) must optimize these processes to the limit to cope with Big Data usage scenarios. In this regard, this work explores novel technologies to improve the performance of query and retrieve services in medical imaging context, ensuring always the compatibility with Digital Imaging and Communications in Medicine (DICOM) standard. The focus is the optimization of querying services. Namely, we conducted several controlled experiments to determine the best database model to support these services. More precisely, we studied the performance of a traditional PACS archive, based on a relational database, against a more recent NoSQL database. We used large datasets with 7 million medical images that represent accurately a year of medical practice. The result of this work is a set of guidelines for the correct usage of analyzed databases in big data medical imaging scenarios, including the advantages and limitations of each model.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Caching and prefetching images in a web-based DICOM viewer",
        "doc_scopus_id": "84987624074",
        "doc_doi": "10.1109/CBMS.2016.68",
        "doc_eid": "2-s2.0-84987624074",
        "doc_date": "2016-08-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Caching and prefetching",
            "DICOM viewer",
            "End-user experience",
            "Imaging applications",
            "Information access",
            "Mammography screening",
            "Prefetching",
            "Web caching"
        ],
        "doc_abstract": "© 2016 IEEE.The general trend of information access anywhere and anytime is also leading to the emergence of innovative medical imaging systems, adapted to this new reality. The HTML5 standard led Web applications to another software level, providing a set of features that allows developing professional Web-based medical imaging applications. However, despite the visualization quality that is already possible in HTML5 browsers, the performance is still an issue, due to the typical size of image studies. In this paper, we present a caching and prefetching solution that enriches the end-user experience in a Web-based DICOM viewer, by reducing data access latency of examinations under revision. We deployed the system in a radiology center for mammography screening, at a national level, and the results show that this technique significantly reduces the average examination access latency, during the reviewing process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Generating Big Data repositories for research in medical imaging",
        "doc_scopus_id": "84982156746",
        "doc_doi": "10.1109/CISTI.2016.7521382",
        "doc_eid": "2-s2.0-84982156746",
        "doc_date": "2016-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Design and implementations",
            "dicom",
            "Digital acquisition",
            "Real environments",
            "repositories",
            "Statistical summary",
            "Test bed environment"
        ],
        "doc_abstract": "© 2016 AISTI.The production of medical imaging data has grown tremendously in the last decades. The proliferation of digital acquisition equipment has enabled even small institutions to produce considerable amounts of studies. Furthermore, the general trend for new imaging modalities is to produce more data per examinations. As a result, the design and implementation of tomorrow's storage and communication systems must deal with Big Data issues. Moreover, new realities such as the outsourcing of medical imaging infrastructures to the Cloud impose additional pressure on these systems. The research on technologies for coping with Big Data issues on large scale medical imaging environments is still in its early stages. This is mostly due to the difficulty of implementing and validating new technological approaches in real environments, without interfering with clinical practice. Therefore, it is crucial to create test bed environments for research purposes. This article proposes a methodology for creating simulated Big Data repositories. The system is able to use a real world repository to collect data from a representative time window and expand it according to research needs. In addition, the solution provides anonymization tools and allows exporting two types of simulated data: A structured file repository containing DICOM objects and a statistical summary of the archive content based on its characteristics, such as the number of produced studies per day, their modality and their associated data volumes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Anatomy of an Extensible Open Source PACS",
        "doc_scopus_id": "84945156932",
        "doc_doi": "10.1007/s10278-015-9834-0",
        "doc_eid": "2-s2.0-84945156932",
        "doc_date": "2016-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Content-Based Image Retrieval",
            "Deployed applications",
            "Financial constraints",
            "Healthcare information technology",
            "Healthcare institutions",
            "PACS implementation",
            "PACS integration",
            "Picture archiving and communication systems (PACS)",
            "Computer Communication Networks",
            "Hospital Information Systems",
            "Humans",
            "Information Storage and Retrieval",
            "Radiology Information Systems",
            "Sensitivity and Specificity",
            "Software"
        ],
        "doc_abstract": "© 2015, Society for Imaging Informatics in Medicine.The conception and deployment of cost effective Picture Archiving and Communication Systems (PACS) is a concern for small to medium medical imaging facilities, research environments, and developing countries’ healthcare institutions. Financial constraints and the specificity of these scenarios contribute to a low adoption rate of PACS in those environments. Furthermore, with the advent of ubiquitous computing and new initiatives to improve healthcare information technologies and data sharing, such as IHE and XDS-i, a PACS must adapt quickly to changes. This paper describes Dicoogle, a software framework that enables developers and researchers to quickly prototype and deploy new functionality taking advantage of the embedded Digital Imaging and Communications in Medicine (DICOM) services. This full-fledged implementation of a PACS archive is very amenable to extension due to its plugin-based architecture and out-of-the-box functionality, which enables the exploration of large DICOM datasets and associated metadata. These characteristics make the proposed solution very interesting for prototyping, experimentation, and bridging functionality with deployed applications. Besides being an advanced mechanism for data discovery and retrieval based on DICOM object indexing, it enables the detection of inconsistencies in an institution’s data and processes. Several use cases have benefited from this approach such as radiation dosage monitoring, Content-Based Image Retrieval (CBIR), and the use of the framework as support for classes targeting software engineering for clinical contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pattern recognition for cache management in distributed medical imaging environments",
        "doc_scopus_id": "84957948635",
        "doc_doi": "10.1007/s11548-015-1272-4",
        "doc_eid": "2-s2.0-84957948635",
        "doc_date": "2016-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Diagnostic Imaging",
            "Humans",
            "Models, Theoretical",
            "Neural Networks (Computer)",
            "Pattern Recognition, Automated"
        ],
        "doc_abstract": "© 2015, CARS.Purpose: Traditionally, medical imaging repositories have been supported by indoor infrastructures with huge operational costs. This paradigm is changing thanks to cloud outsourcing which not only brings technological advantages but also facilitates inter-institutional workflows. However, communication latency is one main problem in this kind of approaches, since we are dealing with tremendous volumes of data. To minimize the impact of this issue, cache and prefetching are commonly used. The effectiveness of these mechanisms is highly dependent on their capability of accurately selecting the objects that will be needed soon. Methods: This paper describes a pattern recognition system based on artificial neural networks with incremental learning to evaluate, from a set of usage pattern, which one fits the user behavior at a given time. The accuracy of the pattern recognition model in distinct training conditions was also evaluated. Results: The solution was tested with a real-world dataset and a synthesized dataset, showing that incremental learning is advantageous. Even with very immature initial models, trained with just 1 week of data samples, the overall accuracy was very similar to the value obtained when using 75 % of the long-term data for training the models. Preliminary results demonstrate an effective reduction in communication latency when using the proposed solution to feed a prefetching mechanism. Conclusions: The proposed approach is very interesting for cache replacement and prefetching policies due to the good results obtained since the first deployment moments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "User Oriented Platform for Data Analytics in Medical Imaging Repositories",
        "doc_scopus_id": "85044844972",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85044844972",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Databases, Factual",
            "Diagnostic Imaging",
            "Humans",
            "Internet",
            "User-Computer Interface"
        ],
        "doc_abstract": "The production of medical imaging studies and associated data has been growing in the last decades. Their primary use is to support medical diagnosis and treatment processes. However, the secondary use of the tremendous amount of stored data is generally more limited. Nowadays, medical imaging repositories have turned into rich databanks holding not only the images themselves, but also a wide range of metadata related to the medical practice. Exploring these repositories through data analysis and business intelligence techniques has the potential of increasing the efficiency and quality of the medical practice. Nevertheless, the continuous production of tremendous amounts of data makes their analysis difficult by conventional approaches. This article proposes a novel automated methodology to derive knowledge from medical imaging repositories that does not disrupt the regular medical practice. Our method is able to apply statistical analysis and business intelligence techniques directly on top of live institutional repositories. It is a Web-based solution that provides extensive dashboard capabilities, including complete charting and reporting options, combined with data mining components. Moreover, it enables the operator to set a wide multitude of query parameters and operators through the use of an intuitive graphical interface.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A routing mechanism for cloud outsourcing of medical imaging repositories",
        "doc_scopus_id": "84971592176",
        "doc_doi": "10.1109/JBHI.2014.2361633",
        "doc_eid": "2-s2.0-84971592176",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Bandwidth requirement",
            "Cache",
            "Conventional approach",
            "Dynamic management",
            "Healthcare institutions",
            "Picture archive and communication systems",
            "routing",
            "Web-based technologies",
            "Algorithms",
            "Cloud Computing",
            "Computer Communication Networks",
            "Database Management Systems",
            "Diagnostic Imaging",
            "Humans",
            "Information Storage and Retrieval",
            "Internet",
            "Outsourced Services"
        ],
        "doc_abstract": "© 2014 IEEE.Web-based technologies have been increasingly used in picture archive and communication systems (PACS), in services related to storage, distribution, and visualization of medical images. Nowadays,many healthcare institutions are outsourcing their repositories to the cloud. However, managing communications between multiple geo-distributed locations is still challenging due to the complexity of dealingwith huge volumes of data and bandwidth requirements. Moreover, standard methodologies still do not take full advantage of outsourced archives, namely because their integration with other in-house solutions is troublesome. In order to improve the performance of distributed medical imaging networks, a smart routing mechanism was developed. This includes an innovative cache system based on splitting and dynamic management of digital imaging and communications in medicine objects. The proposed solution was successfully deployed in a regional PACS archive. The results obtained proved that it is better than conventional approaches, as it reduces remote access latency and also the required cache storage space.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic knowledge base construction from radiology reports",
        "doc_scopus_id": "84969263661",
        "doc_doi": "10.5220/0005709503450352",
        "doc_eid": "2-s2.0-84969263661",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical reports",
            "Health care information system",
            "Healthcare institutions",
            "Natural languages",
            "Radiology reports",
            "Semantic knowledge",
            "Text mining"
        ],
        "doc_abstract": "Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.The tremendous quantity of data stored daily in healthcare institutions demands the development of new methods to summarize and reuse available information in clinical practice. In order to leverage modern healthcare information systems, new strategies must be developed that address challenges such as extraction of relevant information, data redundancy, and the lack of associations within the data. This article proposes a pipeline to overcome these challenges in the context of medical imaging reports, by automatically extracting and linking information, and summarizing natural language reports into an ontology model. Using data from the Physionet MIMIC II database, we created a semantic knowledge base with more than 6.5 millions of triples obtained from a collection of 16,000 radiology reports.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A cloud architecture for teleradiology-as-a-service",
        "doc_scopus_id": "84968867602",
        "doc_doi": "10.3414/ME14-01-0052",
        "doc_eid": "2-s2.0-84968867602",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Advanced and Specialized Nursing",
                "area_abbreviation": "NURS",
                "area_code": "2902"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Cloud Computing",
            "Internet",
            "Radiology Information Systems",
            "Social Media",
            "Teleradiology",
            "Time Factors"
        ],
        "doc_abstract": "© Schattauer 2016.Background: Telemedicine has been promoted by healthcare professionals as an efficient way to obtain remote assistance from specialised centres, to get a second opinion about complex diagnosis or even to share knowledge among practitioners. The current economic restrictions in many countries are increasing the demand for these solutions even more, in order to optimize processes and reduce costs. However, despite some technological solutions already in place, their adoption has been hindered by the lack of usability, especially in the set-up process. Objectives: In this article we propose a telemedicine platform that relies on a cloud computing infrastructure and social media principles to simplify the creation of dynamic user-based groups, opening up opportunities for the establishment of teleradiology trust domains. Methods: The collaborative platform is provided as a Software-as-a-Service solution, supporting real time and asynchronous collaboration between users. To evaluate the solution, we have deployed the platform in a private cloud infrastructure. The system is made up of three main components - the collaborative framework, the Medical Management Information System (MMIS) and the HTML5 (Hyper Text Markup Language) Web client application - connected by a message-oriented middleware. Results: The solution allows physicians to create easily dynamic network groups for synchronous or asynchronous cooperation. The network created improves dataflow between colleagues and also knowledge sharing and cooperation through social media tools. The platform was implemented and it has already been used in two distinct scenarios: teaching of radiology and tele-reporting. Conclusions: Collaborative systems can simplify the establishment of telemedicine expert groups with tools that enable physicians to improve their clinical practice. Stream - lining the usage of this kind of systems through the adoption of Web technologies that are common in social media will increase the quality of current solutions, facilitating the sharing of clinical information, medical imaging studies and patient diagnostics among collaborators.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A machine learning methodology for medical imaging anonymization",
        "doc_scopus_id": "84953281406",
        "doc_doi": "10.1109/EMBC.2015.7318626",
        "doc_eid": "2-s2.0-84953281406",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Diagnostic Imaging",
            "Machine Learning",
            "Privacy"
        ],
        "doc_abstract": "© 2015 IEEE.Privacy protection is a major requirement for the complete success of EHR systems, becoming even more critical in collaborative scenarios, where data is shared among institutions and practitioners. While textual data can be easily de-identified, patient data in medical images implies a more elaborate approach. In this work we present a solution for sensitive word identification in medical images based on a combination of two machine-learning models, achieving a F1-score of 0.94. Three experts evaluated the system performance. They analyzed the output of the present methodology and categorized the studies in three groups: studies that had their sensitive words removed (true positive), studies with complete patient identity (false negative) and studies with mistakenly removed data (false positive). The experts were unanimous regarding the relevance of the present tool in collaborative medical environments, as it may improve the exchange of anonymized patient data between institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An automation framework for PACS workflows optimization in shared environments",
        "doc_scopus_id": "84943329444",
        "doc_doi": "10.1109/CISTI.2015.7170422",
        "doc_eid": "2-s2.0-84943329444",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Communications systems",
            "DICOM",
            "distributed",
            "Human interactions",
            "Multiple indicators",
            "Remote environment",
            "Technical challenges",
            "workflow"
        ],
        "doc_abstract": "© 2015 AISTI.The usage paradigm of PACS (Picture Archive and Communications Systems) has been suffering significant alterations throughout the years. Recently, pushed by new services related to Cloud and Web technologies, the usage of PACS in geographically distributed medical environments has promised significant productivity gains as well as increased quality medical services. However, the usage of PACS to support radiology workflows, previously local to institutions, in remote environments still raises great technological and technical challenges to PACS administrators. The following article describes the most common workflows associated with the radiology service and proposes a computer method for their optimization. The proposed method provides an automatic framework for scheduling and performing repetitive tasks without human interaction. Moreover, it was designed to optimize the workflows associated with distributed PACS scenarios by enabling prefetching rules to be applied to medical image studies. The idea is to leverage multiple indicators associated with the radiology workflows in order to predict whether a given study is likely to be requested, and anticipate it's transference procedure so that the study is already available when requested.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Normalizing Heterogeneous Medical Imaging Data to Measure the Impact of Radiation Dose",
        "doc_scopus_id": "84946490879",
        "doc_doi": "10.1007/s10278-015-9805-5",
        "doc_eid": "2-s2.0-84946490879",
        "doc_date": "2015-05-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "ALARA",
            "Automatic monitoring",
            "Body regions",
            "DICOM",
            "Healthcare institutions",
            "PACS",
            "Radiation Exposure",
            "Set-ups",
            "Humans",
            "Quality Assurance, Health Care",
            "Radiation Dosage",
            "Radiology Information Systems",
            "Tomography, X-Ray Computed"
        ],
        "doc_abstract": "© 2015, Society for Imaging Informatics in Medicine.The production of medical imaging is a continuing trend in healthcare institutions. Quality assurance for planned radiation exposure situations (e.g. X-ray, computer tomography) requires examination-specific set-ups according to several parameters, such as patient’s age and weight, body region and clinical indication. These data are normally stored in several formats and with different nomenclatures, which hinder the continuous and automatic monitoring of these indicators and the comparison between several institutions and equipment. This article proposes a framework that aggregates, normalizes and provides different views over collected indicators. The developed tool can be used to improve the quality of radiologic procedures and also for benchmarking and auditing purposes. Finally, a case study and several experimental results related to radiation exposure and productivity are presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Incremental learning versus batch learning for classification of user's behaviour in medical imaging",
        "doc_scopus_id": "84938871846",
        "doc_doi": "10.5220/0005219704310438",
        "doc_eid": "2-s2.0-84938871846",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Batch learning",
            "Cache mechanism",
            "Communication latency",
            "DICOM",
            "Incremental learning",
            "Outsourcing solution",
            "Prefetching",
            "Storage capacity"
        ],
        "doc_abstract": "Communication latency still hinders the adoption of Cloud computing paradigms in medical imaging environments where it could serve as a reliable technology to support repository outsourcing solutions or inter-institutional workflows, for instance. One way to overcome this is by implementing cache repositories and prefetching mechanisms. Nevertheless, such solutions are usually based on static rules that may inefficiently manage the cache storage capacity. For that reason, this paper compares a pattern recognition system using incremental learning versus batch learning, in order to assess which one could be more appropriately used in a medical imaging cache mechanism.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Recommender System for Medical Imaging Diagnostic",
        "doc_scopus_id": "84937510681",
        "doc_doi": "10.3233/978-1-61499-512-8-461",
        "doc_eid": "2-s2.0-84937510681",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "CBIR",
            "Clinical practices",
            "Context based retrieval",
            "Context-based",
            "Diagnostic decisions",
            "Healthcare institutions",
            "Imaging diagnostics",
            "Large volumes",
            "Data Mining",
            "Decision Support Systems, Clinical",
            "Diagnostic Imaging",
            "Expert Systems",
            "Image Interpretation, Computer-Assisted",
            "Natural Language Processing",
            "Radiology Information Systems",
            "Search Engine"
        ],
        "doc_abstract": "© 2015 European Federation for Medical Informatics (EFMI).The large volume of data captured daily in healthcare institutions is opening new and great perspectives about the best ways to use it towards improving clinical practice. In this paper we present a context-based recommender system to support medical imaging diagnostic. The system relies on data mining and context-based retrieval techniques to automatically lookup for relevant information that may help physicians in the diagnostic decision.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Combining multiple machine learning algorithms to predict taxa under reference conditions for streams bioassessment",
        "doc_scopus_id": "84911414909",
        "doc_doi": "10.1002/rra.2707",
        "doc_eid": "2-s2.0-84911414909",
        "doc_date": "2014-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Environmental Chemistry",
                "area_abbreviation": "ENVI",
                "area_code": "2304"
            },
            {
                "area_name": "Water Science and Technology",
                "area_abbreviation": "ENVI",
                "area_code": "2312"
            },
            {
                "area_name": "Environmental Science (all)",
                "area_abbreviation": "ENVI",
                "area_code": "2300"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2013 John Wiley & Sons, Ltd.In the present study, we tested the potential of combining three machine learning techniques in a bioassessment tool to more accurately predict the pool of expected taxa at a site. This tool, the Hydra, uses the best performing technique from Support Vector Machines (SVM), Multi-layer Perceptron and K-Nearest Neighbour (KNN), to predict the taxa expected at a stream site, and further evaluates the quality of a site, though a classification system based on observed/expected values, similar to that used in River Invertebrate Prediction and Classification System (RIVPACS) models. To test the procedure, we used a dataset composed of 137 training sites, 15 validation sites and 174 test sites (potentially disturbed) from Portuguese streams. The combined use of three machine learning techniques was more effective in the prediction of invertebrate taxa at a site than their individual use. The three methods were always tested for all invertebrate taxa, but from the three techniques tested, SVM and KNN were most often the best performing techniques (the most accurate among the three for a higher number of taxa) in the prediction of invertebrate taxa with the present dataset. The combination of all algorithms implemented in Hydra resulted in good models for stream bioassessment (e.g. SD OE50<0.2, regression of O vs E: R2>0.6, Spearman correlations with global degradation >0.7). We also found no advantage in removing rare taxa from the training dataset, and 50% accuracy is the most adequate accuracy level for calculation of OE ratios through Hydra. Future work should consist of comparing the performance of this technique with others, such as the RIVPACS models, using the same datasets. Considering the flexibility of this technique, self-adjustment and easy implementation through a website (aquaweb.uc.pt), we expect it to be also useful in the prediction of other aquatic elements such as fishes and algae.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A cloud service integration platform for web applications",
        "doc_scopus_id": "84908631977",
        "doc_doi": "10.1109/HPCSim.2014.6903709",
        "doc_eid": "2-s2.0-84908631977",
        "doc_date": "2014-09-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Access control policies",
            "Cloud storages",
            "Integration frameworks",
            "Interoperability framework",
            "Open Standards",
            "Platform architecture",
            "Usage patterns",
            "WEB application"
        ],
        "doc_abstract": "© 2014 IEEE.Due to the latest trends on cloud and multi-cloud computing, the lack of interoperability raised a few issues that have been tackled with open standards and integration frameworks. However, the development of web applications adds a few more issues when accessing, managing, combining and orchestrating cloud resources in the application's logic. This paper proposes an extensible platform architecture for portable cloud service integration. It was designed to satisfy requirements and usage patterns of web applications. Moreover, it implements access control policies and mechanisms for sharing and delegation of resources. The article also explains how the platform can be implemented over existent interoperability frameworks, concretely the mOSAIC platform. Finally, some use-cases and implications of the proposed platform are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clinical data mining in small hospital PACS: Contributions for radiology department improvement",
        "doc_scopus_id": "84945432008",
        "doc_doi": "10.4018/978-1-4666-6339-8.ch003",
        "doc_eid": "2-s2.0-84945432008",
        "doc_date": "2014-08-31",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015 by IGI Global. All rights reserved.Technological developments in the medical imaging acquisition and storage process have triggered the use of PACS (Picture Archiving and Communication Systems) with gradually larger archives. Nowadays, there is data stored in the DICOM (Digital Imaging and Communication in Medicine) file that is not searchable using the traditional PACS database. However, it may represent an important source of information for continuous professional practice improvement. The use of DICOM Data Mining tools has been a valuable asset to analyze the information stored in the DICOM file and can result in gathering important data for the professional practice improvement. These tools can also contribute to the PACS information audit and facilitate access to relevant clinical data within programs for quality continuous improvement. By allowing the construction of multiple views over data repository in a flexible and quickly way and with the possibility to export data for further statistical analysis, Dicoogle permits the identification of data and process inconsistencies that can contribute to radiology department improvement, such as in dose surveillance and patient safety programs and image quality control initiatives. However, the assessment of relevant data for practice improvement must take into account several factors related to the informational environment, professional reality, and healthcare goals and mission. This chapter describes a method to examine and perform studies over a medical imaging repository. Moreover, a case study of a small hospital where the obtained results are discussed is shown.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic search over DICOM Repositories",
        "doc_scopus_id": "84949924636",
        "doc_doi": "10.1109/ICHI.2014.41",
        "doc_eid": "2-s2.0-84949924636",
        "doc_date": "2014-03-02",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "PACS",
            "Semantic data",
            "Semantic representation",
            "Semantic search",
            "Software implementation",
            "Standard architecture",
            "Technical solutions"
        ],
        "doc_abstract": "© 2014 IEEE.The integration of semantic representation in medical information systems has been seen as a key opportunity to achieve full interoperability between distinct technical solutions and to extract knowledge from existing repositories. In this paper, we propose an ontology-based medical imaging archive that provides a generic, dynamic and standard architecture to interrogate the repository. Moreover, the solution flexibility makes possible to improve the search results only by updating the ontologies used, without any changes in the software implementation. This archive is based on a triple store that follows the Rad Lex ontology. To validate the proposed system, the performance of storage and search queries were compared against the results obtained in a traditional PACS archive.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Testing a Multiple Machine Learning Tool (HYDRA) for the Bioassessment of Fresh Waters",
        "doc_scopus_id": "84940052854",
        "doc_doi": "10.1086/678768",
        "doc_eid": "2-s2.0-84940052854",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Ecology, Evolution, Behavior and Systematics",
                "area_abbreviation": "AGRI",
                "area_code": "1105"
            },
            {
                "area_name": "Ecology",
                "area_abbreviation": "ENVI",
                "area_code": "2303"
            },
            {
                "area_name": "Aquatic Science",
                "area_abbreviation": "AGRI",
                "area_code": "1104"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 by The Society for Freshwater Science.We developed a bioassessment tool (HYDRA) to predict the taxa present at a site based on the best performing machine learning tool (Support Vector Machines [SVM], Multi-Layer Perceptron [MLP], K-Nearest Neighbour [KNN]) for each taxon. HYDRA differs from standard models based on discriminant function analysis (DFA) in 2 main ways: 1) HYDRA predicts taxa directly without a priori reference-site classification, and 2) all environmental variables provided for model building contribute to predictions instead of only those that best explain differences among groups. Probabilities of taxon occurrence were used to calculate the Observed/Expected index (O/E50), based on taxa predicted with >50% probability of occurrence. We tested the hypothesis that a combination of models (HYDRA) would perform better than each model alone (SVM, MLP, KNN). We measured performance as: 1) taxon prediction accuracy, 2) precision given by the O/E50 standard deviation (SD O/E50), 3) accuracy of the validation O vs E linear regression, and 4) sensitivity to impairment. We used 3 data sets covering a wide range of environmental conditions (Yukon Territory, Great Lakes, Australian Capital Territory) and calculated O/E50 values for reference, validation, and sites with 3 known levels of simulated impairment. We created 4 quality classes (Good-Severe) and used the 10th percentile O/E50 values of training sites to define the boundary between Good (= reference) and Moderate classes. HYDRA was the best solution for all data sets and was able to distinguish levels of impairment. Taxon prediction accuracy was not related to taxonomic group. Models (SVM, MLP, KNN) varied in accuracy among data sets, and accuracy seemed to depend on the distribution of the taxa across training sites. SVM provided good models, but showed poor sensitivity with 1 data set, which indicated inability to deal with low-richness communities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Challenges of using cloud computing in medical imaging",
        "doc_scopus_id": "84938876726",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84938876726",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Acquisition process",
            "Communication latency",
            "Computer technology",
            "Distributed environments",
            "Healthcare institutions",
            "Imaging services",
            "Picture archiving and communication systems (PACS)",
            "System architectures"
        ],
        "doc_abstract": "© 2014 Nova Science Publishers, Inc.Medical imaging is an important tool for medical diagnosis and treatment. Historically, this tool has been taking advantage of the computer technology advances. In fact, medical imaging has been more and more supported by computers. An example is the creation of image acquisition processes based on computational methods, like magnetic resonance imaging and positron emission tomography. Another example is the appearance of Picture Archiving and Communication System (PACS) concept, which is an umbrella term that embraces the systems responsible for the medical imaging acquisition, archive, distribution, visualization and management. In general, public cloud computing services have many advantages. They have potential to support medical imaging services and processes in distributed environments. For instance, they can be used to support centralized storage of medical imaging data, including the establishment of regional archives, releasing the healthcare institutions from the duties of maintaining an infrastructure capable of storing a huge amount of data. Another scenario is to support a broker of communications between institutions, taking advantage of cloud services availability and scalability. Nevertheless, such technologic choice has several risks, due to specificities of medical imaging scenario, especially about security and data access latency. This chapter identifies the challenges of using cloud in demanding medical imaging scenario and consequent strategies to solve or minimize them. Furthermore, it has proposed an architecture that combines distinct technological approaches to deal with data privacy, communication latency and protocol interoperability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-provider Architecture for Cloud Outsourcing of Medical Imaging Repositories",
        "doc_scopus_id": "84929511107",
        "doc_doi": "10.3233/978-1-61499-432-9-146",
        "doc_eid": "2-s2.0-84929511107",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data availability",
            "DICOM",
            "Improving performance",
            "Maintenance cost",
            "Medical community",
            "Medical institutions",
            "Quality healthcare",
            "Resource management",
            "Computer Security",
            "Information Storage and Retrieval",
            "Internet",
            "Medical Record Linkage",
            "Models, Organizational",
            "Outsourced Services",
            "Radiology Information Systems",
            "Systems Integration"
        ],
        "doc_abstract": "© 2014 European Federation for Medical Informatics and IOS Press.Over the last few years, the extended usage of medical imaging procedures has raised the medical community attention towards the optimization of their workflows. More recently, the federation of multiple institutions into a seamless distribution network has brought hope of increased quality healthcare services along with more efficient resource management. As a result, medical institutions are constantly looking for the best infrastructure to deploy their imaging archives. In this scenario, public cloud infrastructures arise as major candidates, as they offer elastic storage space, optimal data availability without great requirements of maintenance costs or IT personnel, in a pay-as-you-go model. However, standard methodologies still do not take full advantage of outsourced archives, namely because their integration with other in-house solutions is troublesome. This document proposes a multi-provider architecture for integration of outsourced archives with in-house PACS resources, taking advantage of foreign providers to store medical imaging studies, without disregarding security. It enables the retrieval of images from multiple archives simultaneously, improving performance, data availability and avoiding the vendor-locking problem. Moreover it enables load balancing and cache techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Screening Radiation Exposure for Quality Assurance",
        "doc_scopus_id": "84929507627",
        "doc_doi": "10.3233/978-1-61499-432-9-622",
        "doc_eid": "2-s2.0-84929507627",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "As low as reasonably achievable",
            "DICOM",
            "Digital radiography",
            "Distributed systems",
            "IHE REM",
            "Medical Exposure Directive",
            "Radiation Exposure",
            "Radiological protection",
            "Environmental Exposure",
            "Europe",
            "Information Storage and Retrieval",
            "Quality Assurance, Health Care",
            "Radiation Dosage",
            "Radiography",
            "Radiometry"
        ],
        "doc_abstract": "© 2014 European Federation for Medical Informatics and IOS Press.Quality assurance for planned radiation exposure situations (e.g. Digital Radiography, Computed Tomography or Radio Fluoroscopic studies) requires the application of examination-specific scans protocols. These are tailored to patient age or size, body region and clinical indication for ensuring that the dose applied to each patient is as low as reasonably achievable for the clinical purpose of the image acquisition (ALARA principle). The European legal framework-2013/59/EURATOM-points that health authorities will be more pervasive on inspecting the dosimetry applied to patients. This paper discusses these legal alterations and presents an interoperable distributed system for dose monitoring, which is compliant with legal procedures and the IHE Radiation Exposure Monitoring profile (REM). The system combines the most representative stakeholders affected and directly interested in the patient radiological protection: patients, radiologists, practitioners, health authorities, and ethics committee. The system is capable of gathering, in real-time, dose information applied to the patient and storing it in a regional or national wide dose registry. The paper addresses which information should such systems hold and which should be accessed, from each stakeholder perspective. Furthermore, the system may detect irregular dose patterns, which could indicate dose abuses, and signal those findings to the appropriate stakeholders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Normalizing medical imaging archives for dose quality assurance and productivity auditing",
        "doc_scopus_id": "84906920699",
        "doc_doi": "10.1109/MeMeA.2014.6860112",
        "doc_eid": "2-s2.0-84906920699",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "As low as reasonably achievable",
            "Continuous monitoring",
            "DICOM",
            "Distinct points",
            "Medical institutions",
            "PACS",
            "Quality indicators",
            "Radiation Exposure"
        ],
        "doc_abstract": "Quality assurance for planned radiation exposure situations (e.g. x-ray, CT) requires the application of examination-specific scans tailored to patient age or size, body region and clinical indication for ensuring that the dose to each patient is as low as reasonably achievable for the clinical purpose of the image acquisition. Nevertheless, assuring quality implies a heavy manual labor to measure and optimize care services flow. There are already several methodologies and protocols that can be used to achieve this objective. However, the continuous monitoring of quality indicators is still not performed in many healthcare centers. The challenge is to find a way to analyze these metrics in an efficient, effective and convenient manner. Moreover, these results are not often shared, due to confidentiality and privacy of patients and medical staff. In this paper, we propose a methodology and a software tool to aggregate and normalize the monitoring data from distinct points, in order to collect indicators, namely about productivity, efficiency and dose usage, factors that are crucial for benchmarking and for improving the quality of protocol procedures. To evaluate the effectiveness of our solution, several results were collected from two medical institutions. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Medical imaging archiving: A comparison between several NoSQL solutions",
        "doc_scopus_id": "84906883440",
        "doc_doi": "10.1109/BHI.2014.6864305",
        "doc_eid": "2-s2.0-84906883440",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Digital medical imaging systems",
            "Healthcare institutions",
            "High availability",
            "Medical decision making",
            "MongoDB",
            "Nosql database",
            "Relational Database"
        ],
        "doc_abstract": "The use of digital medical imaging systems has greatly increased in healthcare institutions and they are currently valuable tools supporting medical decision and treatment procedures. The proliferation of digital modalities led to an explosion on medical images production, increasing the need to have larger repositories to afford all this amount of data with high availability and performance. NoSQL databases have been replacing relational databases in some scenarios, due to their horizontal scalability and to their flexibility to adapt to dynamic requirements. In this paper, we present an implementation of a medical imaging archive supported in both MongoDB and CouchDB. This implementation is compliant with the medical imaging standards and the storage and query/retrieve performance of our different implementations were evaluated. We also discuss the strengths and weakness of the proposed implementations and present several scenarios that take advantage of the proposed solutions. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DICOM traffic generator based on behavior profiles",
        "doc_scopus_id": "84906880000",
        "doc_doi": "10.1109/BHI.2014.6864312",
        "doc_eid": "2-s2.0-84906880000",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Fall backs",
            "Message exchange",
            "Real environments",
            "Remote repositories",
            "Speed up",
            "Traffic generators",
            "Work-flows"
        ],
        "doc_abstract": "Recent advances in medical imaging systems demands solutions for minimization of latency of communications in the access of data stored in remote repositories. Such solutions are usually dependent on the flow of messages between the remote repositories and the data consumers, which leads to the need of testing under different scenarios the solutions to evaluate the speed up of them. Nevertheless, due to feasibility, ethical and legal reasons, developers not always have access to real data or the possibility of testing solutions in a real environment. Moreover, the network workflows can include distinct usage profiles, which can change between institutions. So, developers have to fall back on simulation, which leads to the problem of obtaining message exchange flow data. This paper presents a generator of Digital Imaging and Communications in Medicine (DICOM) traffic based on modulation of users' behavior that can be used on simulation and dimensioning of solutions for minimization of latency of communications. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "XDS-I outsourcing proxy: Ensuring confidentiality while preserving interoperability",
        "doc_scopus_id": "84904289764",
        "doc_doi": "10.1109/JBHI.2013.2292776",
        "doc_eid": "2-s2.0-84904289764",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Current limitation",
            "Document sharing",
            "Health professionals",
            "Integrating the healthcare enterprise",
            "Search patterns",
            "Searchable encryptions",
            "Security constraint",
            "Computer Security",
            "Confidentiality",
            "Diagnostic Imaging",
            "Electronic Health Records",
            "Humans",
            "Internet",
            "Medical Informatics Computing"
        ],
        "doc_abstract": "The interoperability of services and the sharing of health data have been a continuous goal for health professionals, patients, institutions, and policy makers. However, several issues have been hindering this goal, such as incompatible implementations of standards (e.g., HL7, DICOM), multiple ontologies, and security constraints. Cross-enterprise document sharing (XDS) workflows were proposed by Integrating the Healthcare Enterprise (IHE) to address current limitations in exchanging clinical data among organizations. To ensure data protection, XDS actors must be placed in trustworthy domains, which are normally inside such institutions. However, due to rapidly growing IT requirements, the outsourcing of resources in the Cloud is becoming very appealing. This paper presents a software proxy that enables the outsourcing of XDS architectural parts while preserving the interoperability, confidentiality, and searchability of clinical information. A key component in our architecture is a new searchable encryption (SE) scheme - Posterior Playfair Searchable Encryption (PPSE) - which, besides keeping the same confidentiality levels of the stored data, hides the search patterns to the adversary, bringing improvements when compared to the remaining practical state-of-the-art SE schemes. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sensor-based architecture for medical imaging workflow analysis systems-level quality improvement",
        "doc_scopus_id": "84904038647",
        "doc_doi": "10.1007/s10916-014-0063-8",
        "doc_eid": "2-s2.0-84904038647",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Hospital Information Systems",
            "Humans",
            "Radiology Information Systems",
            "Systems Integration",
            "User-Computer Interface",
            "Workflow"
        ],
        "doc_abstract": "The growing use of computer systems in medical institutions has been generating a tremendous quantity of data. While these data have a critical role in assisting physicians in the clinical practice, the information that can be extracted goes far beyond this utilization. This article proposes a platform capable of assembling multiple data sources within a medical imaging laboratory, through a network of intelligent sensors. The proposed integration framework follows a SOA hybrid architecture based on an information sensor network, capable of collecting information from several sources in medical imaging laboratories. Currently, the system supports three types of sensors: DICOM repository meta-data, network workflows and examination reports. Each sensor is responsible for converting unstructured information from data sources into a common format that will then be semantically indexed in the framework engine. The platform was deployed in the Cardiology department of a central hospital, allowing identification of processes' characteristics and users' behaviours that were unknown before the utilization of this solution. © 2014 Springer Science+Business Media New York.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A centralized platform for geo-distributed PACS management",
        "doc_scopus_id": "84897027876",
        "doc_doi": "10.1007/s10278-013-9650-3",
        "doc_eid": "2-s2.0-84897027876",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Geo-Distributed PACS",
            "Healthcare institutions",
            "Operation and management",
            "PACS administrator",
            "Patient care",
            "Picture archive and communication systems",
            "Remote monitoring"
        ],
        "doc_abstract": "Picture Archive and Communication System (PACS) is a globally adopted concept and plays a fundamental role in patient care flow within healthcare institutions. However, the deployment of medical imaging repositories over multiple sites still brings several practical challenges namely related to operation and management (O&M). This paper describes a Web-based centralized console that provides remote monitoring, testing, and management over multiple geo-distributed PACS. The system allows the PACS administrator to define any kind of service or operation, reducing the need for local technicians and providing a 24/7 monitoring solution. © 2013 Society for Imaging Informatics in Medicine.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating echocardiogram reports with medical imaging",
        "doc_scopus_id": "84889070590",
        "doc_doi": "10.1109/CBMS.2013.6627819",
        "doc_eid": "2-s2.0-84889070590",
        "doc_date": "2013-12-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical services",
            "Computational system",
            "Healthcare institutions",
            "Heterogeneous database",
            "Index information",
            "New approaches",
            "Ultrasound medical images"
        ],
        "doc_abstract": "Healthcare institutions are increasingly taking advantage of information and computational systems to enhance the efficiency and quality of their services. These IT systems are normally able to handle huge amounts of digital data, and to extract relevant fingerprints useful to improve the quality of clinical practice. However, building automatized processes to achieve this over multiple and heterogeneous databases is still a challenge. This paper presents a new approach able to collect and index information from distinct medical data sources, allowing us to identify important metrics to evaluate the performance and quality of clinical services. A case study combining information from ultrasound medical images and echocardiogram clinical reports is also presented. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-domain platform for medical imaging",
        "doc_scopus_id": "84888996180",
        "doc_doi": "10.1109/CBMS.2013.6627770",
        "doc_eid": "2-s2.0-84888996180",
        "doc_date": "2013-12-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cloud services",
            "Data repositories",
            "Health informations",
            "Imaging protocol",
            "Indexing engines",
            "Medical imaging equipment",
            "Peer-to-peer paradigm",
            "Security constraint"
        ],
        "doc_abstract": "The increasing adoption of medical imaging equipment in healthcare has been leading to a huge dispersion of data repositories and institutions. Although the quality of diagnostic and treatment is deeply dependent on the health information that is available for physicians, several legal and technological issues have hindered the integration of these data. One of such problems is because traditional medical imaging protocols do not perform well in inter-institutional scenarios. This paper describes a hybrid network platform for medical imaging systems that provides searching and retrieval over multiple centres. Three key components support the system: an indexing engine, a multicast framework and a cloud service. Using a peer-to-peer paradigm with security constraints, the platform gathers the information of medical imaging repositories hosted inside the institutions, allowing physicians to access data when and where they need it. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DICOM viewer based on web technology",
        "doc_scopus_id": "84894154830",
        "doc_doi": "10.1109/HealthCom.2013.6720660",
        "doc_eid": "2-s2.0-84894154830",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "During the last decade, medical imaging services have assumed a central role in healthcare institutions, and they are nowadays a decisive factor for the quality of diagnostic and treatments. Health stakeholders and policy makers have been steadily adopting PACS and DICOM standard, simplifying interoperability between distinct equipment and institutions. To assist images' interpretation, several visualization solutions emerged. However, these applications are targeted to specific operating systems, hindering its ubiquitous use, in increasing web-based working environments. In this paper we present a Web-based DICOM viewer that was entirely developed with web technology, namely HTML5 and JavaScript. The result is a visualization station that is already in use in two medical imaging centres and that can be accessed through a common web browser, from any computer, mobile device, or operation system. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A cloud based architecture for medical imaging services",
        "doc_scopus_id": "84894144135",
        "doc_doi": "10.1109/HealthCom.2013.6720767",
        "doc_eid": "2-s2.0-84894144135",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Medical imaging has been increasingly a computational matter, from image acquisition, to its storage, processing and visualization. The recent advances in cloud computing also represent a great opportunity to distribute and process imaging workflows. However, this combination brings a new set of challenges. In this paper we discuss why and how cloud technologies can become a future environment for the deployment of medical imaging services. Furthermore, we propose an architecture with technological approaches oriented to this demanding scenario, that deals well with critical issues such as security, communication latency and interoperability. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Leveraging XDS-I and PIX workflows for validating cross-enterprise patient identity linkage",
        "doc_scopus_id": "84894123103",
        "doc_doi": "10.1109/HealthCom.2013.6720686",
        "doc_eid": "2-s2.0-84894123103",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Document exchange communities set the ground for cross-organization cooperation. They enable the exchange of patient's documents across distinct health organizations. However, there are various challenges that must be overcame before deploying such communities, for instance the construction of the Enterprise Master Patient Index (EMPI) which maps the several patient identifiers of each domain. This paper describes the development of an interoperable distributed system that expedites the exchange of documents by taking care of the patient identities autonomously. The system automatically builds the EMPI leveraging the healthcare workflow (based on PIX and XDS-I) for validating the automatic linkages of the patient identifiers. The human validation is a consequence of user's interaction with cross-domain documents: distributing and attenuating the validation effort. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Content-based image retrieval for clinical applications: An overview of current approaches and challenges",
        "doc_scopus_id": "84893496321",
        "doc_doi": "10.2174/15734056091310281214",
        "doc_eid": "2-s2.0-84893496321",
        "doc_date": "2013-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Internal Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2724"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Digital medical imaging has become one of the most important tools for medical diagnosis. However, the ongoing evolution in both storage and modality devices has had as consequence that enormous amounts of imaging data are being produced. The existence of large sets of data coupled with the limited query capabilities provided by the standard tools and protocols poses problems for radiologists and has shifted the research focus from data availability towards data accessibility. Content-based image retrieval (CBIR) systems have been heralded as a solution that is able to cope with the increasingly larger volumes of information present in medical repositories and assist radiologists with decision support. While generic, extensible CBIR frameworks that work natively with Picture Archive and Communication Systems (PACS) are scarce, developments are happening at a fast pace and several systems have been implemented with some degree of success. Based on the literature available, we provide an overview of such CBIR systems, architecture and implementation techniques, with an emphasis on systems oriented towards usage in a clinical domain. We conclude with an analysis of some of the challenges that still need to be overcome in order to bring this technology to a medical audience. © 2013 Bentham Science Publishers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An agile framework to support distributed medical imaging scenarios",
        "doc_scopus_id": "84893447291",
        "doc_doi": "10.1109/ICHI.2013.48",
        "doc_eid": "2-s2.0-84893447291",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Collaborative workspace",
            "Communications platform",
            "Computational power",
            "Continuous production",
            "DICOM",
            "Healthcare institutions",
            "Imaging applications",
            "PACS"
        ],
        "doc_abstract": "Healthcare institutions are increasingly taking advantage of telemedicine to create collaborative workspaces. However, the continuous production of medical data requires higher performance from infrastructures, to maintain or even enhance the quality of service. On the other hand, a tremendous amount of ubiquitous computational power and an unprecedented number of Internet resources and services are used every day as a normal commodity. This paper presents a Cloud-based telemedicine framework that allows applications to store data and communicate easily, using any Cloud provider. From this framework, two medical imaging applications were developed to provide standard services with high abstraction level: a medical imaging repository and an inter-institutional communications platform. Furthermore, a telemedicine case study is presented, namely a regional repository, where the physicians can review medical studies from anywhere and at any time. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhanced regional network for medical imaging repositories",
        "doc_scopus_id": "84887947734",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887947734",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Healthcare institutions",
            "High level control",
            "Large amounts of data",
            "PACS",
            "Performance issues",
            "Picture archive and communication systems",
            "Web-based technologies"
        ],
        "doc_abstract": "Nowadays PACS (Picture Archive and Communication System) tends to integrate web-based technologies in processes associated to storage, distribution and visualization of medical images, creating the general abstraction of distributed PACS. Although PACS integration with web-based technologies offer significant advantages to healthcare institutions, it is crucial that information technology based issues do not impose constraints in the medical practice workflow. Therefore, distributed PACS have to deal with performance issues in both storing and retrieving of large amounts of data across distinct systems, possibly hosted on different locations. This paper proposes an approach to reduce data transference footprint in a distributed PACS environment with DICOM-Ready applications. The proposed method provides a high level control of transference parameters, such as, number of connections, number of fragments and size of each fragment. The result was a highly tunable data stream that can be used in any distributed PACS environment. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi vendor DICOM metadata access: A multi site hospital approach using Dicoogle",
        "doc_scopus_id": "84887886530",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887886530",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Digital imaging and communication in medicines",
            "Healthcare facility",
            "Heterogeneous database",
            "PACS",
            "Picture archiving and communication systems (PACS)",
            "Quality initiatives",
            "Radiology departments"
        ],
        "doc_abstract": "The Radiology departments are increasingly taking advantage of Information Technologies (IT) to enhance the efficiency and quality of patient care services. These IT systems are normally able to handle huge amounts of digital data and to extract relevant fingerprints useful to improve the quality of clinical practice. However, they provide multiple and heterogeneous databases and informational environments, namely regarding to the information stored in Picture Archiving and Communication Systems (PACS). The heterogeneous of environments make the access to the information a challenge to extract relevant data. This paper presents the deployment and validation process of the Dicoogle system in two health care facilities. This system represents a new approach able to collect and index information from distinct PACS archives, developed on top of Dicoogle, which allow the construction of multiple views over data repositories according to the standard Digital Imaging and Communication in Medicine (DICOM), in a flexible and fast way. The developed methodology can contribute to improve the use of DICOM metadata, stored over disperse PACS of radiology departments, which otherwise not be used in productivity, efficiency and quality initiatives at radiology departments. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A common API for delivering services over multi-vendor cloud resources",
        "doc_scopus_id": "84881477740",
        "doc_doi": "10.1016/j.jss.2013.04.037",
        "doc_eid": "2-s2.0-84881477740",
        "doc_date": "2013-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Business computing",
            "Cloud database",
            "Cloud providers",
            "Cloud services",
            "Cloud storages",
            "Computing markets",
            "Redundant services",
            "Secondary operations"
        ],
        "doc_abstract": "The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers' services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly. © 2013 Elsevier Inc.",
        "available": true,
        "clean_text": "serial JL 271629 291210 291773 291791 291869 291870 31 Journal of Systems and Software JOURNALSYSTEMSSOFTWARE 2013-04-24 2013-04-24 2014-08-17T12:57:20 S0164-1212(13)00105-2 S0164121213001052 10.1016/j.jss.2013.04.037 S300 S300.2 FULL-TEXT 2015-05-14T04:32:04.646695-04:00 0 0 20130901 20130930 2013 2013-04-24T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0164-1212 01641212 false 86 86 9 9 Volume 86, Issue 9 9 2309 2317 2309 2317 201309 September 2013 2013-09-01 2013-09-30 2013 SI :The Future of Software Engineering For/In the Cloud Edited by Rami Bahsoon, Ivan Mistrík, Nour Ali, TS Mohan & Nenad Medvidovic article fla Copyright © 2013 Elsevier Inc. All rights reserved. ACOMMONAPIFORDELIVERINGSERVICESOVERMULTIVENDORCLOUDRESOURCES BASTIAOSILVA L 1 Introduction 2 Background and related work 2.1 Cloud computing services 2.1.1 Storage-as-a-service 2.1.2 Database-as-a-service 2.1.3 Notification service 2.2 Interoperability and standardization 3 A Service Delivery Cloud Platform 3.1 Entities 3.2 Cloud services 3.2.1 Cloud streams 3.2.2 Columnar data abstraction 3.2.3 Notification abstraction 3.3 Cloud Controller 3.3.1 RESTful API 3.3.2 Dashboard panel 3.4 Cloud Gateway 3.5 SDCP-SDK 3.6 Privacy and confidenciality model 4 Case studies 4.1 Medical imaging repository 4.2 Sharing medical images “anytime and anywhere” 5 Discussion 5.1 Abstractions 5.2 Advantages and drawbacks 6 Conclusion Acknowledgements References BASTIAO 2011 1 10 L BASTIAO 2011 L APACSGATEWAYCLOUD BIEBERSTEIN 2006 N SERVICEORIENTEDARCHITECTURECOMPASSBUSINESSVALUEPLANNINGENTERPRISEROADMAP DIMARTINO 2011 571 578 B BUILDINGAMOSAICCLOUDS DILLON 2010 27 33 T 201024THIEEEINTERNATIONALCONFERENCEADVANCEDINFORMATIONNETWORKINGAPPLICATIONS CLOUDCOMPUTINGISSUESCHALLENGES DILLON 2010 27 33 T HAJJAT 2010 243 254 M KEAHEY 2009 43 51 K KUMBHARE 2012 510 517 A LEAVITT 2009 5 N LEE 2010 451 459 C PROCEEDINGS19THACMINTERNATIONALSYMPOSIUMHIGHPERFORMANCEDISTRIBUTEDCOMPUTINGACM APERSPECTIVESCIENTIFICCLOUDCOMPUTING PARAMESWARAN 2009 19 26 A PETCU 2011 405 411 D SHAN 2012 82 89 C SOMASUNDARAM 2012 162 167 T RECENTTRENDSININFORMATIONTECHNOLOGYICRTIT2012INTERNATIONALCONFERENCEIEEE ARCHITECTURALFRAMEWORKSOLVEINTEROPERABILITYISSUEBETWEENPRIVATECLOUDSUSINGSEMANTICTECHNOLOGY VECCHIOLA 2009 C ANEKAASOFTWAREPLATFORMFORNETBASEDCLOUDCOMPUTING ZENG 2009 1044 1048 W PROCEEDINGS2NDINTERNATIONALCONFERENCEINTERACTIONSCIENCESINFORMATIONTECHNOLOGYCULTUREHUMANACM RESEARCHCLOUDSTORAGEARCHITECTUREKEYTECHNOLOGIES BASTIAOSILVAX2013X2309 BASTIAOSILVAX2013X2309X2317 BASTIAOSILVAX2013X2309XL BASTIAOSILVAX2013X2309X2317XL item S0164-1212(13)00105-2 S0164121213001052 10.1016/j.jss.2013.04.037 271629 2014-08-17T10:09:15.119889-04:00 2013-09-01 2013-09-30 true 766252 MAIN 9 63627 849 656 IMAGE-WEB-PDF 1 gr8 21106 252 452 gr7 17583 149 339 gr6 19888 229 471 gr5 15950 149 271 gr4 16580 168 294 gr3 29588 282 471 gr2 10329 144 322 gr1 41472 525 386 gr8 4549 122 219 gr7 4814 96 219 gr6 3971 107 219 gr5 7116 120 219 gr4 5678 125 219 gr3 5262 131 219 gr2 3928 98 219 gr1 4398 163 120 JSS 9155 S0164-1212(13)00105-2 10.1016/j.jss.2013.04.037 Elsevier Inc. Fig. 1 SDCP general overview. Fig. 2 Entities of Service Delivery Cloud Platform. Fig. 3 Cloud input/output. Fig. 4 Abstraction columnar data. Fig. 5 Publish/subscribe abstraction. Fig. 6 Cloud controller – architecture. Fig. 7 Cloud controller dashboard. Fig. 8 Cloud gateway architecture. A common API for delivering services over multi-vendor cloud resources Luís A. Bastião Silva ⁎ Carlos Costa José Luís Oliveira University of Aveiro, DETI/IEETA, Portugal University of Aveiro, DETI/IEETA Portugal ⁎ Corresponding author. Tel.: +351 916427877. The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers’ services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly. Keywords Cloud services Cloud standardization Cloud storage Cloud databases 1 Introduction The increasing pace of evolution in business computing services leads enterprises to renew their way of operating. Business requirements have also changed and outsourcing has been adopted by many industries, allowing the enterprise to focus more on its core business (Bieberstein, 2006). The Cloud computing market has grown significantly over the past few years and, following the natural progress of business models, there is a great interest in the IT industry in migrating services to this kind of infrastructures (Hajjat et al., 2010; Leavitt, 2009). In order to respond to this demand, many cloud companies increasingly offer new features to their consumers–using a rich set of services on the server side, and/or providing a platform where users can more easily deploy their applications. For instance, Amazon Web Services (Amazon, 2011) has released services such as Simple Storage Service (S3), Amazon SQS, SimpleDB and many others. However, despite this evolution, most cloud companies have been creating distinct APIs for their services, which implies that the applications developed for the cloud can only be instantiated in one single provider, i.e. they are locked to each vendor. To achieve portability and interoperability, it is clear that stockholders, i.e. the user community and the marketplace, must adopt a common design (Lee, 2010). Some efforts have been made to create standards, in order to grant interoperability among the various cloud providers, e.g. SNIA (Association, 2011b) and CDMI (Association, 2011a). The desired scenario consists of creating specifications and interfaces that could be used by all cloud service providers. However, these standards are still drafts, with reduced practical impact. Furthermore, several efforts have been made to standardize Infrastructure-as-a-Service (IaaS), while the Platform-as-a-Service (PaaS) still does not have any consistent unified API. To tackle this issue, a notable effort has been made by several communities, such as jclouds (jclouds, 2010), libcloud (libcloud, 2011) and simplecloud (simplecloud, 2011), to provide a unique programmatic API to deal with multiple cloud solutions. Unfortunately, they do support a limited number of providers and the extension to new ones is not simple. Most of them still focus on IaaS, and there is still a gap in the standardization of other services like storage, database and notification services. The transparent combination of multiple cloud resources will allow applications to communicate with any cloud, even with different providers at same time, leading to the Sky computing concept (Keahey et al., 2009). Although this paradigm was initially used just for IaaS, recently several efforts have been made to extend it to PaaS (Petcu et al., 2011; Di Martino et al., 2011). However, Sky computing for PaaS is still not fully developed and only possible architecture has been briefly discussed until now. Standardization may solve many problems in Cloud computing, but there are also some challenges that must be considered during the process. Several usage scenarios imply dealing with critical information, and applications need to manage access to resources to avoid data tampering. Those privacy concerns are a real problem for some corporations as there is information that they intend to keep safeguarded (Kumbhare et al., 2012). Moreover, Amazon released a new service named AWS Storage Gateway, where the main idea is to cipher and decipher safeguarded data over the cloud, i.e. the data is secure encrypted/decrypted in house. However, it is an Amazon service and it does not work with other cloud providers. This paper presents a platform that allows client applications to easily interoperate with distinct cloud providers, combining and decorating services like, for instance, dynamic data storage across multiple and incompatible infrastructures. Throughout the paper, we will describe a set of APIs related to cloud services, focusing on storage, columnar databases and publishing/subscription, i.e. the principal resources consumed by Internet applications. Finally, a case study will be presented and a discussion of the advantages and drawbacks of the solution will be provided. The rest of the manuscript is organized as follows. The next section will give a background of Cloud computing, giving special emphasis to storage, columnar data and publishing/subscribe services. Also, standardization and interoperability will be discussed. Section 3 presents the system architecture and discusses the proposed abstractions. Section 4 presents a case study, i.e. a clinical solution instantiated with this platform. Section 5 presents a discussion of advantages and drawbacks of the proposed solution, as well as a comparative analysis of other solutions. Finally, the main conclusions of the paper will be presented in a summary in Section 6. 2 Background and related work 2.1 Cloud computing services IT solutions have been mostly supported by on-premise software, i.e. hosted on private enterprise datacenters. With the emergence of cloud computing, off-premise software has increased its applicability and nowadays applications are offered as services through cloud providers. Thus, cloud computing has been adapted to customers’ requirements, creating dissimilar development models among cloud providers. Clearly, there are self-services offered by these companies, where resources are available at anytime and anywhere for customers. Moreover, the resources can be distributed over multiple locations, in order to improve reliability. In the following sections, we will focus on the description of three of these services: Storage-as-a-Service, Database-as-a-Service and Notification Service. 2.1.1 Storage-as-a-service Storage-as-a-Service (SaaS) is the ability to offer remote storage in a local virtualized way, for any operating system and application. Nowadays, Cloud providers are offering storage using the Blobstore concept, which, per se, is not new. In the past, these concepts were used in Database Management Systems (DBMS) in the storage and movement of large data blocks. Blobstores are associative memories, i.e. key-value storage providers, where the blob is unstructured data (value) stored in a container and the lookup is performed through a text key. A container is a namespace or domain for the blobs. A blob is always uploaded to a container. The blobstores have a list of containers where the developer can create, remove or rename them. The container holds content, which can be blobs, folders or virtual path. Also, the blobstore in cloud services has an access control list to authorize people to access the data. In practice, blobstore service allows customers to store data in a container under the Cloud. For instance, Amazon S3, Microsoft Azure and OpenStack have their own blobstore APIs. These services are considered PaaS because they allow developers to take advantage of remote storage service to support their application data in a transparent way. There are many examples of SaaS usage, for instance, the Dropbox application which stores customers’ files in Amazon S3 or commercial web portals that store great quantities of pictures in cloud blobstores. 2.1.2 Database-as-a-service Database-as-a-Service (DaaS) is a new paradigm that outsources the burden effort to the cloud provider. Therefore, the database is hosted in a remote datacenter and can be shared between users, in a transparent way. For instance, Amazon AWS, Windows Azure and Rackspace (2011) offer a database as a service where customers pay for what they use. There is a new type of databases, called columnar data, which are organized in a key-value structure. These databases store the information by column, instead of the traditional ones that store the information by row. It has several advantages in computing large amounts of information and cloud providers are now offering these databases. All database operations are supported by these services, for instance, creating tables, loading and accessing data in the tables. Cloud players often supply an API to access the database, and execute operations through a web service API. Furthermore, database maintainers do not need to worry about the server's redundancy, upgrades, back-up plan and recovery from disaster. Nonetheless, some enterprises are concerned about ensuring data privacy. In fact, this is one of the weaknesses of DaaS. Despite the provision of Server Level Agreements (SLA) by Cloud Providers, there are legal issues that need a very high level of privacy and confidentiality and these organizations do not have clear data in any case. Also, store procedures and triggers might not be supported in the overwhelming majority of cloud providers supplying DaaS. Finally, performance might deteriorate because applications will access the data in remote datacenters when located in the public cloud provider. 2.1.3 Notification service Most information systems and computer applications rely heavily on updated and real time information. In addition, many different applications need to exchange information, i.e. the capability to publish a message from an application and immediately deliver it to other applications that subscribe the same channel. Nowadays, many applications provide this functionality supported on a polling strategy, i.e. checking periodically for new messages. They supply these features in a transparent way to the end-user, although there are extra computational resources requirements and in some cases without the expected efficiency. Cloud providers were influenced by this tendency and have also created services to communicate through a message-based model. Notification services refer to the ability to communicate with another remote entity through Cloud Services. There is some variety of communication services, but essentially they use the Publish/Subscribe model. Several companies are working on notification services, i.e. Amazon SQS (2011), PubNub (PubNub, 2011) and Azure Queue (Corporation). All of them have the same concept: a web service that sends notifications to hanging users through an event-driven workflow. 2.2 Interoperability and standardization Nowadays there is great competition in the industry to provide better and more services over the Cloud. However, the services provided by different players are not typically compatible (Petcu et al., 2011; Shan et al., 2012; Dillon et al., 2010b). Undoubtedly, interoperability and portability is required to allow applications to be ported more easily between different cloud providers. Recently, several groups have been formed to create standards and common interfaces that could allow interoperability between distinct cloud solutions (Parameswaran and Chaddha, 2009). For example, Storage Network Industry Association (SNIA) (Association, 2011b) has been working on a storage data standard in the cloud (Association, 2011a). This standard explores the features that vendors are offering and extracts the common domain, aiming for a high quality cloud storage interface in the future. Also, they focus on the financial paradigm of “pay-as-you-go”, considering that such attributes will interest many businesses. Cloud Computing Interoperability Forum is another group that aims to standardize cloud computing. They are an open and vendor-neutral organization, which intends their solutions to be rapid, potentiating successful industry adoption. There are also other committees with the same goals, such as Open Cloud Computing Interface, Open Grid Forum and Open Cloud Consortium. Most of this standardization work has been related to IaaS, while solutions for PaaS are still emerging (Dillon et al., 2010a). Also, little work has been done regarding standardization in multiple vendor clouds for services such as storage, database and communication. Aneka (Vecchiola et al., 2009), for instance, is a computational platform whose main goal is to support multiple programming models by using specific abstractions for virtual machine deployment. Although cloud services have been used, multi-vendor integration still needs unified specifications (Zeng et al., 2009). Nimbus is an open-source project that aims to offer a Sky computing framework. The main goal is to connect several nodes from different providers, allowing communication between them, transparent migration of machines and ubiquitous management. Nimbus raises the concept of IaaS gateway, working like a deployment orchestrator that allows interoperability to be maintained between several clouds, and creating federated cloud computing, known as Sky computing. Nevertheless, this architecture does not have PaaS services such as databases or communication APIs. mOSAIC (mOSAIC, 2011) is another group that intends to propose a standard API for using Sky-computing. They intend to allow interoperability and portability to support IaaS and also PaaS. However, the project has a wide scope and efforts have been made in the IaaS and processing. They are developing an outstanding architecture that aims to support a Sky in the Cloud computing universe. Moreover, they are designing a language and an API for using multi-cloud resources. However, the project is still ongoing and there are as yet no concerns about the privacy of data. Orleans Bykov et al., 2010 is a programming framework that intends to create a model to outsource the processing, storage and a few other services to the cloud. This framework is quite significant, and is a major contribution to PaaS standardization. However, it does not address interoperability and privacy issues. Cloud4SOA is another European project focusing on achieving semantic interoperability between clouds. They are tackling challenges related with the development of Cloud applications, as well as deployment and migration. This project promises to be very relevant for cloud computing interoperability in the coming years. However, it is still a work in progress. There are other proposals focusing on achieving interoperability through semantic technology. Somasundaram et al. (Somasundaram et al., 2012) created a framework that aims to grant interoperability between Eucalyptus and OpenNebula, proposing a resource description, discovery and submission based on a broker that performs this translation. Although several organizations have come together to constitute interoperability groups, services over the cloud are not yet compatible. Clients from one cloud storage service cannot easily migrate their data to others. For instance, an application developed to use Amazon SimpleDB would not work with Azure Table. It is not flexible and may be expensive if the application needs data stored on different cloud computing platforms. In the following section we propose an architecture that solves this restriction and is able to deal with three cloud services. Moreover, the platform model copes well with upcoming standards and provides support to service combination, decoration and orchestration. For instance, secure and redundant services allocation. 3 A Service Delivery Cloud Platform We have developed a common API for delivering services over multi-vendor cloud resources, entitled Service Delivery Cloud Platform (SDCP). This platform has three main goals: (1) grant interoperability between different cloud providers, creating an abstract layer for three cloud services; (2) deliver services using multiple cloud resources, including storage, database management and notification systems; (3) provide service combination, decoration and orchestration. The first goal (1) consists of granting interoperability between cloud players in a transparent way. Basically, an application can work with as many vendors as is desired, taking advantage of existing cloud providers. The SDCP allows creation of cloud provider poll. For instance, it can store data in multiple cloud vendors or cloud free services, creating a federate view of all containers. In addition, it enables the developer to have interoperability with other protocols (2) inside private networks. Cloud services of distinct providers can bundled and decorated with extra functionalities like, for instance, data ciphering on-the-fly (3). Moreover, cache and pre-fetching mechanisms are other examples of value-added SDCP services, extremely important to reduce latency (3). The presented architecture consists of a hybrid infrastructure that allows “Enterprise to the Cloud” and “Enterprise to the Cloud to Enterprise” applications, i.e. communication between two or more different enterprises, using multiple resources from different cloud vendors. The architecture has basically two main components: the Cloud Controller and the Cloud Gateway (Fig. 1 ). The Controller contains sensitive information and must therefore be deployed in a trustable provider. This separation in the architecture was necessary to support critical use cases. For instance, some information should not be held in the public cloud providers, as discussed further in Section 4. Within the SDCP architecture we have also built a SDK (Software Development Toolkit) that simplifies the development of SDCP-based applications. The SDCP was designed to make it easier to develop and load new application modules using a plugin approach or web service API. As it is possible to see in Fig. 1, the applications are on top of SDCP. The platform is able to deliver new services using the cloud facilities: data store, databases and communication using cloud providers. In order to extend the platform to support different providers and services (e.g. Google, Amazon, Azure, Rakespace, etc.), we have built a specific model whose structural design sustains the use of different modules under the same interface. We have considered a distributed architecture to support multiple accesses to this data, from distinct points. We will describe each component of this architecture in the following sub-sections. 3.1 Entities The platform has its own entities that model the system architecture and describe how it is structured. The fundamental entities and associations of the infrastructure are described in Fig. 2 : • Agent – each gateway has to login using an agent account. Basically, agents are the entities situated inside the enterprise that relay the information to the cloud. • Domain – is a group of agents belonging to the same enterprise or the same trustable group/enterprise group. Thus, only agents of the same domain can communicate or access the data belonging to its domain. • Provider – defines a cloud provider and credentials to access them. It can be a storage, database or communication provider. These providers also belong to a domain. • Private Service – external services that can take advantage of Cloud Controller agents and cloud providers. This service will extend the functionality of the Cloud Controller. These entities are actors and concepts of the SDCP. The domain is a very important concept because it characterizes the trustable model, i.e. models the relationships and the manages the control of the resources. 3.2 Cloud services This section describes the implemented Cloud services. We will describe the three implemented cloud services and how the abstraction for these services was applied. 3.2.1 Cloud streams As expressed, the goal of our platform was to use any resources of the cloud without being locked to a specific provider. To implement this feature for storage services, we used an abstraction to write a set of bytes (i.e. blob) into the Cloud storage using typical Input/Output (I/O) streams. The designed abstraction assures provider independency but also makes it easier to extend to other cloud solutions. Two new I/O entities were implemented: CloudInputStream and CloudOutputStream, Fig. 3 . These entities are used to read/write in the storage services as a common Java stream mechanism. An important aspect regarding the writing of a blob is the access policy. By default, we assume that the blob is private, although the user can specify an ACL (Access Control List) to give permission to the blob. A blobstore API has different features implemented in different cloud providers. Although several features are presented in the blobstore API, others are not often presented. Our abstraction will not consider these features, and in that case an extension to the platform will be necessary. Nonetheless, we take into account that several features are just used occasionally, and a trade-off was necessary. At present, most cloud storage solutions do not offer an option to encrypt data when it is uploaded to the cloud. Our platform has an encryption/decryption layer on the client side, i.e. the cipher and decipher operations are executed on-the-fly on the enterprise side, through our abstraction. In that case, it is ciphered with AES (Advance Encryption Standard) algorithm and the key is stored in the Cloud Controller. On the other hand, multiple cloud providers can be supplied with a list of CloudSocket being blobs written in both and read from the first one that is available. The developed Cloud Streams extend the IO Java streams. The Cloud socket contains the identifier of the implementation that will be used to call the most appropriate one for a specific service. JClouds (jclouds, 2010) is an open source framework for cloud development that already provides several cloud players, and as such we decided to build the Cloud Streams as an instance of JClouds blobstores. In addition, we implemented our local storage, following the proposed abstraction. Furthermore, new APIs of different blobstore cloud vendors can be easily implemented using the proposed abstraction. 3.2.2 Columnar data abstraction As in the previous storage service, we have also developed the same generic API upon cloud databases. This aims to create an abstraction to columnar data, for instance, SimpleDB, Azure Table and other cloud databases publicly available. Nowadays there is a new trend to store information in columnar data instead of the traditional relational system. These tables are very dynamic and the developer does not need to pre-define a model, because the structure auto-fits the data. There were several problems regarding scalability, which have to be solved in this abstraction. For instance, Amazon Simple DB uses a mainly horizontal scalability, in opposition to Azure Table, which allows control of the vertical partition. Each partition key represents a different node to have the information. This issue was solved through the Table ID, which identifies the Table name, together with the node label or the location label. The idea was to contain generic features that can be applied in many database services. We implemented two of the available APIs, but it will work for other databases. The Java SDK already uses a high-level abstraction for databases, named JPA (Java Persistence API). Although it is widely used with Object-Oriented databases, we decided to follow this standard for two main reasons: JPA is often used by Java developers to abstract the access to databases, and it is easy to keep compatibility with these applications and the chosen an API that fits the JPA abstraction. Thus, it was decided to use the same JPA methods and also add other methods that are specific to the Cloud databases, such as create/remove tables (Fig. 4 ). Also, for representation of the results, we use a library named Guava (Google Collections), which provides very generic Java collections, e.g. the Table collection. Table is a triple values class <R, C, V> data structure, i.e. Row, Column and Value. This representation is perfect to retrieve the results of queries and also to insert new data into the columnar tables. Fig. 4 shows the architecture of the columnar data abstraction. A Select action is executed quite similarly to the JPA method. It uses a small set of the SQL and just conditions are considered. Complex queries with joins will not be considered in this abstraction, since the columnar tables do not support such a feature. The data columnar abstraction was deployed on Amazon SimpleDB. The API has a different representation from the JPA abstraction. For instance, each row is called Item, and each item has several attributes that are not structured and can change dynamically for each item. That is why it is called columnar data, because it can be different for each row, i.e. each record can contain different fields. Thus, when creating the table, we do not define a structure as in a common database. The SimpleDB uses a REST to supply the programmatic interaction with the developer and the results are retrieved in XML files with the responses. So the first step was to create the XML parsers and client communication with the REST AWS interface, as described in their specification (2011). The abstraction for this service is quite similar to the Cloud Streams, and any specific implementation has to be compliant with interfaces described in Fig. 4. In the SimpleDB case, we implemented all functions documented in the abstraction. The model copes with the common API with minor conversions. 3.2.3 Notification abstraction The notification abstraction aims to dynamically create a message-based communication, based on the Publish/Subscribe mechanism. It is asynchronous, allowing application delivery using this platform to tackle the polling issue often implemented in many applications to simulate an asynchronous system. However, not many Publish/Subscribe public services use only HTTP. We will take the example of PubNub (2011), although a new instance can be implemented, for example using other public services such as Channel API of Google AppEngine, or other protocols like XMPP which support the Publish/Subscribe mechanism. Moreover, the polling approach can also fit the abstraction, and in that case the subscriber has to poll the server until it has a signal message, and then it will call the Receiver callback. Also, for instance, Ajax Push Engine (APE) can be installed in a public cloud provider like Amazon EC2, and the service can be used with quite similar behavior to PubNub. Nevertheless, there are also very similar services based on the Publish/Subscribe model, for instance, Amazon SQS and Azure Queue. In this service abstraction, we used an Observer Pattern, and in the current implementation we created two entities: Publish and Subscribe (see Fig. 5 ). The channel represents the domain of each agent, and it assures that the communication can only be established between agents of the same domain. It is important to mention that PubNub specific implementation is quite analogous to the one proposed. So the abstraction classes will call the implementation of PubNub directly using the adapter pattern, similarly to the other previously presented abstractions. 3.3 Cloud Controller The Cloud Controller is a major component of our architecture responsible for functionalities such as: aggregating providers’ credentials; controlling access to cloud resources; managing authentication processes with Cloud Gateways; and addition of new services. This controller provides an API that can be used by third party applications to access their services. The Controller communicates through HTTP, using RESTful specification; thereby it will be much easier for other entities to access services. The Cloud Controller allows us to store credentials of cloud providers for different services, such as blobstore, database and communication (Fig. 6 ). Also, the ciphered keys used to cipher and decipher the blobs are stored in the Cloud controller, unless the developer explicitly denies the action. Moreover, it also supports addition of external services used by third party applications, extending in this way the Cloud Controller functionality. This platform was instanced with several end-user services associated with Medical Imaging (Repository Data Privacy), particularly the safe storage of medical data in multiple cloud players as described in Section 4. There is critical information in diverse scenarios. In such cases, the developer can create a new service in a private cloud to keep the more restricted access data. Our platform will be compatible with public or private clouds. Moreover, the Cloud Gateway can cipher the data before sending it to the cloud, and store the keys in these private services. 3.3.1 RESTful API The interface to external applications is issued as a RESTful web service that provides several interfaces, starting with an authentication mechanism. User validation is based on username and password and if the login is valid, the web service returns a token that will be used to validate subsequent operations. We created functions to get cloud provider and services information. 3.3.2 Dashboard panel In addition to the web services API, the Cloud Controller also provides a web portal interface (Fig. 7 ), whereby administrators can add or remove new cloud providers (storage, database, services, etc.) and also check the operation's logs. This portal was implemented through GWT (Google Web Toolkit) technologies. Also, they can create new domains, add/remove/ban agents and add new services. This dashboard also allows the user to setup a threshold of cloud provider requests because the actions of gateways cloud interactions are sent to the Cloud Controller. 3.4 Cloud Gateway The Cloud Gateway is a very important component of the architecture. Basically it is an application that loads new services dynamically. It grants authentication from the Cloud Controller and automatically loads the services that are uploaded by the user. Cloud Gateway can run as a daemon. Also, it has an optional external GUI that allows the user to load new plugins/applications or see operation logs. For instance, new adapters for new cloud providers are loaded in the Cloud Gateway. The architecture of Cloud Gateway (Fig. 8 ) also uses the SDCP-SDK. Namely, it has access to the plugin core mechanism to load new plugins. Moreover, the interfaces used in API plugins will be instantiated automatically using the Inversion of Control pattern. The plugins to the Cloud Gateway can be services programmed in Java, directly using the SDCP-SDK, but we offer the possibility of external applications, sending information to the cloud through a web service interface. This raises a question: what is the advantage of using the web service API? Third-party application will be allowed to store, access, and use resources from multiple public clouds, using a normalized interface. Thus, third-party applications do not need to be coupled as a Cloud Gateway Java plugin. Nearly every web application requires an authentication system. The Cloud Gateway is the middleware layer that allows access to cloud resources, and thereby it requires a user validation system. The Cloud Gateway authentication is used through the RESTful web services that access the Cloud Controller web services, previously described. When the gateway application starts, it requires a username and password for the end-user. Next, Cloud Gateway executes authentication and saves the token. 3.5 SDCP-SDK The end users of the SDCP are allowed to develop applications that use the cloud resources, as well as new plugins to new cloud providers. Thus, the applications can also take advantage of other cloud providers that the developer wants to support. To create these new applications, the developer will use the SDCP-SDK. The SDCP-SDK defines contracts and specification of the platform, including the communication between the Cloud Controller and the Cloud Gateway. The platform was developed in Java through a set of interfaces. The main idea is that the developer can take advantage of SDCP-SDK to delegate the authorization process to the platform. Also, the access to the cloud resources is provided by the SDK. The new application will be deployed in Cloud Gateway, the entity responsible for loading the applications. On the other hand, the abstractions of blobstore, columnar data and notification systems are also possible to extend using the SDCP-SDK. For instance, it is possible to the developer write a new plugin for a specific provider based on the SDCP-SDK, only implementing the methods described earlier in Section 3.2. We developed a plugin for notification system based on PubNub in 8h and now all developed applications with SDCP will benefit of this provider. 3.6 Privacy and confidenciality model Undoubtedly, cloud computing has several advantages for enterprises, but two major issues need to be addressed: the cost/benefits of the solution and the privacy and confidentiality of the data stored over the cloud. The first issue depends on the business, and several studies (Armbrust et al., 2009) have been addressing the financial impact of cloud computing. Often associated with data tampering, privacy aspects are still a challenge in these scenarios. Our platform takes those two aspects into consideration, because we can store the information in multiple cloud players and, at the same time, we also tackle the privacy and confidentiality issue. The solution architecture was built taking into account that particular requirement. Our cloud has two main components: Cloud Controller and the cloud players. Thus, for instance, in storage service, we have the opportunity to store the information in a ciphered way. At present, most cloud solutions do not offer an option to encrypt data when it is uploaded to the cloud. Some companies are already offering this service, for instance, AWS Storage Gateway, but we believe this should be a client service to give more confidence in the cloud solution. Our proposed platform has an encryption/decryption layer on the client side, i.e. the cipher and decipher operations are executed on-the-fly on the enterprise side. Moreover, this privacy issue is independent of the cloud vendor and the data can be easily sent and accessed in multiple cloud players at the same time. The end-user can do that more easily when writing, specifying a list of cloud providers they intend to use. Use of a common interface should be adopted by services and this will be a contribution to Cloud resources standardization. Another important issue regarding the architecture is that it can be deployed in a hybrid infrastructure. Nonetheless, the Cloud Controller can be deployed in a public or private cloud. Several applications may want to extend the functionality of the Cloud Controller because it may be relevant to host some information in the public cloud. 4 Case studies 4.1 Medical imaging repository Several use case scenarios that will benefit from the proposed solution can be pointed out. In this article, we will describe one such example: medical imaging repositories. Over the past two decades, the healthcare sector has increasingly adopted ICT to support diagnoses, treatment and patient care. Medical imaging, for instance, produces a huge amount of information and takes advantage of these technologies in daily diagnostic procedures. PACS (Picture Archive Communication System) encompasses several hardware and software technologies for acquisition, distribution, storage and analysis of digital images in distributed environments. The main components are: image acquisition devices, storage archive units, display workstations and databases. The amount of medical images has increased significantly over the last decade as result of the increase in the number and quality of studies. According to some researchers, this trend will continue over the next years. A common PACS archive has two major components: the DICOM (Digital Imaging and Communication in Medicine) object repository and the database system. The object repository typically demands an infrastructure with huge storage capacity to support all DICOM studies. The database module is normally a relational database management system (RDBMS) that supports the DICOM Information Model (DIM) (DICOM-P3, 2001), containing mandatory metadata related to patients, studies, series and images. Medical institutions have to store a large number of medical studies/images, i.e. DICOM object files. Thus, they need to have large datacenters inside the hospital, a major source of problems for systems’ administrators. The Cloud Computing model fits this scenario. Moreover, healthcare is a critical service and the information must always be available. Therefore, the approach described in this paper is crucial to create solutions that can be instanced in more than one cloud provider. Based on the SDCP-SDK presented, a PACS Cloud archive was developed supporting medical image storage and database services. A PACS Cloud Gateway to the Cloud (Bastião et al., 2011) was developed, which provides outsourcing of these two components to the cloud, namely using the new concepts of blobstore and database accessible through web services. Internally, the Gateway is seeded as a common Intranet PACS repository supporting DICOM standard services. A Cloud Gateway plugin was instantiated and also another component named PACS Cloud Controller to keep sensitive medical information. Those PACS modules were both deployed to interact with two distinct cloud providers, namely Amazon S3 (S3) and Google Storage (Bastiao et al., 2011). This solution is used to support a distributed archive over a medical institution with multiple centers. The medical repository is deployed over a private cloud, with two distinct gateways supporting the access to the archive. There are an average of 250 exams daily for this archive. Our solution will also replicate a given percentage of the archive to a public cloud provider, in a ciphered way, in order to achieve higher availability taking benefit of the SDCP features, i.e. writing in two distinct cloud providers using our common API. Due to some technical reasons or natural disasters the private cloud might be offline for a certain amount of time. If the solution is also deployed in a public cloud, medical data will be not lost and querying the repository will still be possible. Moreover, since we are working with huge volume of data the latency is one of the main barriers to the adoption of cloud services in medical imaging environments. The SDCP gateway plugin includes cache and pre-fetching mechanisms increasing considerably the quality of service provided. 4.2 Sharing medical images “anytime and anywhere” Cloud computing is largely used to share files over the Internet, and many examples can be pointed out, such as Dropbox (Dropbox, 2011) and Gmail (Google, 2011). Moreover, Cloud providers offer high availability and scalability of their services. In this case study, we do not outsource the medical repositories to the cloud. Instead, we keep them in the healthcare enterprises, but the goal is to grant external and/or inter-institutional access over organization boundaries. Our DICOM relay architecture takes advantage of the SDCP platform to provide transparent exchange of imagiologic information between several locations. Communication between the components of the digital medical laboratories is mainly carried out through DICOM. An application deployed in the Cloud Gateway was developed to effect a DICOM protocol forwarder via cloud SDCP. This DICOM relay implements the DICOM protocol and waits for new requests. For instance, when it receives storage requests for medical exams, it forwards the messages to another application that is running in another institution. In order to achieve this, the application relies on storage and notification services offered by the SDCP-SDK. Currently, this solution is supporting teleradiology in a medical institution, with two distinct locations. Physicians are reporting from several places, using the same central medical repository and the DICOM Router to access medical imaging. The solution is online 24/7 and is supporting transferring around 50 exams daily. Our Bridge was deployed in a local private cloud. The direct connection is frequently blocked or needs to be set up in the network, unlike our transparent Web 2.0 approaches. 5 Discussion 5.1 Abstractions The SNIA project has developed notable work in cloud storage standardization. A large set of important features are supported in the blobstore, including the methodologies to transfer objects to the cloud provider, the mechanism to request and get information and what kind of metadata can be supported. Furthermore, OpenStack has been adopting the SNIA design principles and companies like HP Cloud Services and Rackspace are using OpenStack to support their API infrastructure. However, there are still other players not offering SNIA compatible, which makes the presented platform and the proposed abstractions worthwhile. “Database-as-a-Service” is also a very valuable service offered by cloud players. However, standardization at this level has not yet been developed. Nonetheless, much effort has been made over the years to make interoperability between databases possible. In turn, implementing several drivers for each provider could solve the problem, using solutions like JPA, which we proposed in SDCP. Although this may be true, we made several contributions to the standardization of these interfaces. However, if organizations like SNIA extend their work to databases, interoperability will become much easier. Regarding the asynchronous systems for notification, there are no standards yet. Similar to database abstraction, we present a contribution toward standardization, as well as creating an abstraction for it. XMPP is a good standard for Publish/Subscribe and many applications use XMPP as a communication service. Nevertheless, the protocol was written to be a chat and not for notification proposes. Moreover, although the XMPP runs over HTTP, the service is not commonly offered as a web service. Thus, a possible approach to standardization in this service will be achieved by using the Publish/Subscribe pattern with multiple channels to communicate. 5.2 Advantages and drawbacks Despite the decoupling of the cloud players, the platform still has some weaknesses. Introduction of an abstraction, as a middleware, does not allow support of all features of all cloud providers. Cloud solution vendors are continuously competing for new and distinct features, so they can build a solid position in this growing market. Despite this competition is quite positive for customers, it also raises several problems mostly related with the lack of interoperability. This lack of normalization between interfaces can be easily overpassed with SDCP, which takes a conservative view of available services–keeping the maximum common set. However, if these services are not enough, and developers want to take advantage of a specific service for on provider, they can use the service directly through the vendor API, or they can create a plugin through SDCP. For instance, common features presented in blob structures, follow a key-value strategy inside a bucket. If a particular provider decided to support blob compression, since the SDCP API do not include that feature, it will be necessary to use the original service API or specific. Our approach explores several APIs and proposes an implemented platform that is able to support many vendors and easier to extend to other new providers. Using cloud computing, the risk of incidents is reduced because cloud providers have the data in multiple locations. However, there is another risk that developers have to consider: what happens if cloud-computing providers stop supplying their services? Such a situation will certainly harm cloud clients. The proposed approach will greatly minimize those risks, because the data can be redundantly stored in multiple cloud providers, without impact on SDCP API client applications. Moreover, it can automatically forward the resource to another provider, if a cloud provider starts failing. SDCP normalized interface over services of distinct providers, is an important advantage but not the unique one. Providers can be selected on-demand according to client predefined rules or quality parameters. The multi-provider services can be combined, orchestrated and decorated with new functionalities. The result is a value-added service with a high abstraction level. 6 Conclusion Despite many cloud service applications having been developed, the interoperability and portability of cloud resources is still a major challenge, and common specifications and standards are needed to enable multi-vendor integration. In this paper we have presented a solution targeting this goal. The proposed Service Delivery Cloud Platform (SDCP) is a cloud middleware infrastructure that provides a rich set of services using resources from multiple cloud providers. The presented architecture allows decoupling the specificities of each cloud provider API into a unique abstraction. Throughout the article we have extensively detailed the system architecture, and discussed the advantages and drawbacks of the presented solution. The platform was validated with private and public clouds, using three common services: blobstore, columnar data and notification service. In addition, the platform was successfully used in several medical image scenarios that exploit storage redundancy using two databases, in private and public clouds, and the notification service to communicate between multiple data access points. Acknowledgements The research leading to these results has received funding from FEDER under the COMPETE programme and by FCT – Fundação para a Ciência e a Tecnologia under grant agreement PTDC/EIA-EIA/104428/2008 and SFRH/BD/79389/2011. References Amazon SimpleDB Documentation, 2011 Amazon SimpleDB Documentation, 2011. Secondary, Secondary Amazon SimpleDB Documentation. Amazon SQS, 2011 Amazon SQS, 2011. Secondary, Secondary Amazon SQS, Amazon, 2011 Amazon, 2011. Amazon webservices (AWS). In: Secondary Amazon (Ed.), Secondary Amazon Webservices (AWS). Armbrust et al., 2009 Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R.H., Konwinski, A., Lee, G., Patterson, D.A., Rabkin, A., Stoica, I., 2009. Above the Clouds: A Berkeley View of Cloud Computing. Technical Report UCB/EECS-2009-28, EECS Department, University of California, Berkeley. Association S.N.I., 2011a Association, S.N.I., 2011a. Cloud data management interface v.1.0.1h, Overview of cloud storage. Association S.N.I., 2011b Association, S.N.I., 2011b. Cloud storage initiative. In: Secondary Association, S.N.I. (Ed.), Secondary Cloud Storage Initative. Bastiao et al., 2011 L. Bastiao C. Costa J.L. Oliveira A PACS archive architecture supported on cloud services International Journal of Computer Assisted Radiology and Surgery 2011 1 10 Bastião et al., 2011 L.A. Bastião C. Costa J.L. Oliveira A PACS Gateway to the Cloud 2011 CISTI Chaves Bieberstein, 2006 N. Bieberstein Service-oriented architecture compass: business value, planning, and enterprise roadmap 2006 Prentice Hall Bykov et al., 2010 Bykov, S.a.G., A., Kliot, G., Larus, J., Pandya, R., Thelin, J., 2010. Orleans: a framework for cloud computing. Corporation, M, Windows Azure Platform. Cloud Computing Interoperability Forum (CCIF), 2011 Cloud Computing Interoperability Forum (CCIF), 2011. In: Secondary (Ed.), Secondary Cloud Computing Interoperability Forum (CCIF), Di Martino et al., 2011 B. Di Martino D. Petcu R. Cossu P. Goncalves T. Máhr M. Loichate Building a Mosaic of Clouds 2011 Springer Ischia, Italy 571 578 DICOM-P3, 2001 DICOM-P3, 2001. Digital Imaging and Communications in Medicine (DICOM), Part 3: Information Object Definitions. National Electrical Manufacturers Association. Dillon et al., 2010a T. Dillon C. Wu E. Chang Cloud computing: issues and challenges 2010 24th IEEE International Conference on Advanced Information Networking and Applications 2010 27 33 Dillon et al., 2010b T. Dillon C. Wu E. Chang Cloud computing: issues and challenges IEEE 2010 27 33 Dropbox, 2011 Dropbox, 2011. Dropbox service, In: Secondary Dropbox (Ed.), Secondary Dropbox Service, Google Storage for Developer, in press Google Storage for Developers, Accessed in April 2013. Google, 2011 Google, 2011. Gmail, In: Secondary Google (Ed.), Secondary Gmail, Hajjat et al., 2010 M. Hajjat X. Sun Y.-W.E. Sung D. Maltz S. Rao K. Sripanidkulchai M. Tawarmalani Cloudward bound: planning for beneficial migration of enterprise applications to the cloud SIGCOMM: Computer Communication Review 40 2010 243 254 jclouds, 2010 jclouds, 2010. jclouds: multi-cloud library, In: Secondary jclouds (Ed.), Secondary jclouds: Multi-Cloud Library, Keahey et al., 2009 K. Keahey M. Tsugawa A. Matsunaga J. Fortes Sky computing Internet Computing, IEEE 13 2009 43 51 Kumbhare et al., 2012 A. Kumbhare Y. Simmhan V. Prasanna Cryptonite: a secure and performant data repository on public clouds, Cloud Computing (CLOUD) 2012 IEEE 5th International Conference on IEEE 2012 510 517 Leavitt, 2009 N. Leavitt Is cloud computing really ready for prime time? Growth 27 2009 5 Lee, 2010 C.A. Lee A perspective on scientific cloud computing Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing. ACM Chicago, IL 2010 451 459 libcloud, 2011 libcloud, 2011. libcloud, In: Secondary libcloud (Ed.), Secondary libcloud, mOSAIC, 2011 mOSAIC, 2011. mOSAIC platform, In: Secondary mOSAIC (Ed.), Secondary mOSAIC Platform, Open Cloud Consortium (OCC), 2011 Open Cloud Consortium (OCC), 2011. In: Secondary, Secondary Open Cloud Consortium (OCC), OGF, 2011 OGF, 2011. Open Grid Forum (OGF), In: Secondary (OGF), O.G.F. (Ed.), Secondary Open Grid Forum (OGF), Parameswaran and Chaddha, 2009 A.V. Parameswaran A. Chaddha Cloud interoperability and standardization SETLabs Briefings 7 2009 19 26 Petcu et al., 2011 D. Petcu C. Craciun M. Neagul I. Lazcanotegui M. Rak Building an interoperability API for sky computing IEEE 2011 405 411 PubNub, 2011 PubNub, 2011. PubNub, In: Secondary PubNub (Ed.), Secondary PubNub, S3, A., Amazon Simple Storage Service. Shan et al., 2012 C. Shan C. Heng Z. Xianjun Inter-cloud operations via NGSON. Communications Magazine IEEE 50 2012 82 89 simplecloud, 2011 simplecloud, 2011. Simple cloud API, In: Secondary simplecloud (Ed.), Secondary Simple Cloud API, Somasundaram et al., 2012 T. Somasundaram K. Govindarajan M. Rajagopalan S. Rao An architectural framework to solve the interoperability issue between private clouds using semantic technology Recent Trends in Information Technology (ICRTIT), 2012 International Conference on IEEE 2012 162 167 Vecchiola et al., 2009 C. Vecchiola X. Chu R. Buyya Aneka: A Software Platform for Net-Based Cloud Computing 2009 arXiv:0907.4622 Zeng et al., 2009 W. Zeng Y. Zhao K. Ou W. Song Research on cloud storage architecture and key technologies Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human ACM 2009 1044 1048 Luís A. Bastião Silva is a PhD student of MAP Doctoral Program in Computer Science of University of Aveiro. He received his Master of Science on Computer and Telematics Engineering in 2011. He worked on medical informatics research with partnerships with several medical institutions. His current research interests include healthcare records, medical imaging repositories, health records data federation, integration and cloud computing. Carlos Manuel Azevedo Costa is Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the Aveiro University. He is also a researcher at the Institute of Electronics and Telematics Engineering of Aveiro (IEETA) and member of the Bioinformatics group at the University of Aveiro. He holds a PhD in Medical Informatics and he is author or co-author of more than 80 publications in this area. His main research activity is in the area of PACS-DICOM (medical imaging systems and networks) and Healthcare Information Systems. He has also interests in other areas of research such as telemedicine, security and access control. José Luis Oliveira is an associate professor at the Electronics, Telecommunications and Informatics Department, University of Aveiro, Portugal. His research interests include text mining, information retrieval, distributed systems and computational methods in biomedical informatics. He was involved in more than 20 international projects, such as InfoGenMed, Daidalos, EuroNGI, InfoBioMed, EU-ADR (FP7), GEN2PHEN (FP7), RD-CONNECT (FP7) and EMIF (IMI). He has more than 250 publications in book chapters, journals and international conferences. "
    },
    {
        "doc_title": "Dicoogle, a Pacs Featuring Profiled Content Based Image Retrieval",
        "doc_scopus_id": "84877095473",
        "doc_doi": "10.1371/journal.pone.0061888",
        "doc_eid": "2-s2.0-84877095473",
        "doc_date": "2013-05-06",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Content-based image retrieval (CBIR) has been heralded as a mechanism to cope with the increasingly larger volumes of information present in medical imaging repositories. However, generic, extensible CBIR frameworks that work natively with Picture Archive and Communication Systems (PACS) are scarce. In this article we propose a methodology for parametric CBIR based on similarity profiles. The architecture and implementation of a profiled CBIR system, based on query by example, atop Dicoogle, an open-source, full-fletched PACS is also presented and discussed. In this solution, CBIR profiles allow the specification of both a distance function to be applied and the feature set that must be present for that function to operate. The presented framework provides the basis for a CBIR expansion mechanism and the solution developed integrates with DICOM based PACS networks where it provides CBIR functionality in a seamless manner. © 2013 Valente et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clinical data mining in small hospital PACS: Contributions for radiology department improvement",
        "doc_scopus_id": "84898329665",
        "doc_doi": "10.4018/978-1-4666-3667-5.ch016",
        "doc_eid": "2-s2.0-84898329665",
        "doc_date": "2013-03-31",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            }
        ],
        "doc_keywords": [
            "Clinical data minings",
            "Continuous improvements",
            "Data-mining tools",
            "Digital imaging and communication in medicines",
            "Information audit",
            "Professional practices",
            "Radiology departments",
            "Technological development"
        ],
        "doc_abstract": "© 2013, IGI Global.Technological developments in the medical imaging acquisition and storage process have triggered the use of PACS (Picture Archiving and Communication Systems) with gradually larger archives. Nowadays, there is data stored in the DICOM (Digital Imaging and Communication in Medicine) file that is not searchable using the traditional PACS database. However, it may represent an important source of information for continuous professional practice improvement. The use of DICOM Data Mining tools has been a valuable asset to analyze the information stored in the DICOM file and can result in gathering important data for the professional practice improvement. These tools can also contribute to the PACS information audit and facilitate access to relevant clinical data within programs for quality continuous improvement. By allowing the construction of multiple views over data repository in a flexible and quickly way and with the possibility to export data for further statistical analysis, Dicoogle permits the identification of data and process inconsistencies that can contribute to radiology department improvement, such as in dose surveillance and patient safety programs and image quality control initiatives. However, the assessment of relevant data for practice improvement must take into account several factors related to the informational environment, professional reality, and healthcare goals and mission. This chapter describes a method to examine and perform studies over a medical imaging repository. Moreover, a case study of a small hospital where the obtained results are discussed is shown.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating echocardiogram reports with medical imaging",
        "doc_scopus_id": "84897085332",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84897085332",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical services",
            "Computational system",
            "Healthcare institutions",
            "Heterogeneous database",
            "Index information",
            "New approaches",
            "Ultrasound medical images"
        ],
        "doc_abstract": "Healthcare institutions are increasingly taking advantage of information and computational systems to enhance the efficiency and quality of their services. These IT systems are normally able to handle huge amounts of digital data, and to extract relevant fingerprints useful to improve the quality of clinical practice. However, building automatized processes to achieve this over multiple and heterogeneous databases is still a challenge. This paper presents a new approach able to collect and index information from distinct medical data sources, allowing us to identify important metrics to evaluate the performance and quality of clinical services. A case study combining information from ultrasound medical images and echocardiogram clinical reports is also presented. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-domain platform for medical imaging",
        "doc_scopus_id": "84897064805",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84897064805",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Cloud services",
            "Data repositories",
            "Health informations",
            "Imaging protocol",
            "Indexing engines",
            "Medical imaging equipment",
            "Peer-to-peer paradigm",
            "Security constraint"
        ],
        "doc_abstract": "The increasing adoption of medical imaging equipment in healthcare has been leading to a huge dispersion of data repositories and institutions. Although the quality of diagnostic and treatment is deeply dependent on the health information that is available for physicians, several legal and technological issues have hindered the integration of these data. One of such problems is because traditional medical imaging protocols do not perform well in inter-institutional scenarios. This paper describes a hybrid network platform for medical imaging systems that provides searching and retrieval over multiple centres. Three key components support the system: an indexing engine, a multicast framework and a cloud service. Using a peer-to-peer paradigm with security constraints, the platform gathers the information of medical imaging repositories hosted inside the institutions, allowing physicians to access data when and where they need it. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enabling outsourcing XDS for imaging on the public cloud",
        "doc_scopus_id": "84894384235",
        "doc_doi": "10.3233/978-1-61499-289-9-33",
        "doc_eid": "2-s2.0-84894384235",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data confidentiality",
            "Document sharing",
            "Legal concern",
            "Medical information",
            "New approaches",
            "Patient privacies",
            "Picture archiving and communication systems (PACS)",
            "Public clouds",
            "Computer Security",
            "Database Management Systems",
            "Information Dissemination",
            "Information Storage and Retrieval",
            "Internet",
            "Medical Record Linkage",
            "Portugal",
            "Radiology Information Systems"
        ],
        "doc_abstract": "Picture Archiving and Communication System (PACS) has been the main paradigm in supporting medical imaging workflows during the last decades. Despite its consolidation, the appearance of Cross-Enterprise Document Sharing for imaging (XDS-I), within IHE initiative, constitutes a great opportunity to readapt PACS workflow for inter-institutional data exchange. XDS-I provides a centralized discovery of medical imaging and associated reports. However, the centralized XDS-I actors (document registry and repository) must be deployed in a trustworthy node in order to safeguard patient privacy, data confidentiality and integrity. This paper presents XDS for Protected Imaging (XDS-p), a new approach to XDS-I that is capable of being outsourced (e.g. Cloud Computing) while maintaining privacy, confidentiality, integrity and legal concerns about patients' medical information. © 2013 IMIA and IOS Press.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DICOM relay over the cloud",
        "doc_scopus_id": "84878019584",
        "doc_doi": "10.1007/s11548-012-0785-3",
        "doc_eid": "2-s2.0-84878019584",
        "doc_date": "2013-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Purpose: Healthcare institutions worldwide have adopted picture archiving and communication system (PACS) for enterprise access to images, relying on Digital Imaging Communication in Medicine (DICOM) standards for data exchange. However, communication over a wider domain of independent medical institutions is not well standardized. A DICOM-compliant bridge was developed for extending and sharing DICOM services across healthcare institutions without requiring complex network setups or dedicated communication channels. Methods: A set of DICOM routers interconnected through a public cloud infrastructure was implemented to support medical image exchange among institutions. Despite the advantages of cloud computing, new challenges were encountered regarding data privacy, particularly when medical data are transmitted over different domains. To address this issue, a solution was introduced by creating a ciphered data channel between the entities sharing DICOM services. Results: Two main DICOM services were implemented in the bridge: Storage and Query/Retrieve. The performance measures demonstrated it is quite simple to exchange information and processes between several institutions. The solution can be integrated with any currently installed PACS-DICOM infrastructure. This method works transparently with well-known cloud service providers. Conclusions: Cloud computing was introduced to augment enterprise PACS by providing standard medical imaging services across different institutions, offering communication privacy and enabling creation of wider PACS scenarios with suitable technical solutions. © 2012 CARS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clustering of distinct PACS archives using a cooperative peer-to-peer network",
        "doc_scopus_id": "84868521138",
        "doc_doi": "10.1016/j.cmpb.2012.05.013",
        "doc_eid": "2-s2.0-84868521138",
        "doc_date": "2012-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical environments",
            "Data loss",
            "DICOM",
            "Distributed data",
            "Exchange rates",
            "Health care professionals",
            "High availability",
            "Horizontal scaling",
            "Image fragments",
            "Medical images",
            "PACS",
            "PACS archive",
            "Peer to peer",
            "Scale-out",
            "Transfer rates"
        ],
        "doc_abstract": "To face the demanding requirements of the clinical environment, PACS archives need to be resilient and reliable, supporting high availability and fault tolerance. Often, to ensure no data loss, PACS archives retain two copies of images on separate physical machines, using distributed data storage facilities. However, PACS do not take advantage of the various replicas to improve the transfer rates of medical images. This happens mostly because the DICOM standard does not comply with distributed fetching of image fragments while performing a store. Inspired by this unexplored opportunity, we designed and implemented a new solution that takes advantage of the distributed image replicas and, at the same time, respects the DICOM standard. Our strategy brought significant improvements in the exchange rates, load balancing and availability of installed PACS archives. Moreover, the adopted strategy forms a cluster of PACS archives that transparently enables horizontal scaling, facilitates the creation of backups, and gives to healthcare professionals a unified view of the distributed repositories. © 2012 Elsevier Ireland Ltd.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2012-06-15 2012-06-15 2014-09-30T23:42:41 S0169-2607(12)00152-6 S0169260712001526 10.1016/j.cmpb.2012.05.013 S300 S300.3 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20121201 20121231 2012 2012-06-15T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor primabst ref 0169-2607 01692607 false 108 108 3 3 Volume 108, Issue 3 13 1002 1011 1002 1011 201212 December 2012 2012-12-01 2012-12-31 2012 Section I: Methodology article fla Copyright © 2012 Elsevier Ireland Ltd. All rights reserved. CLUSTERINGDISTINCTPACSARCHIVESUSINGACOOPERATIVEPEERTOPEERNETWORK RIBEIRO L 1 Introduction 2 Background 3 Materials 3.1 DICOM standard 3.2 JavaGroups 3.3 Dicoogle Indexing System 4 Methods 4.1 Architecture 4.2 Stateless and stateful active peers 4.3 Peer-to-peer DICOM-based network 4.3.1 Distributed c-find 4.3.2 Distributed c-store 4.3.3 Distributed c-move 5 Results and discussion 5.1 Availability and load balancing 5.2 Scaling and cost saving 5.3 Unifying view 5.4 Performance 6 Conclusions Acknowledgement References PIANYKH 2008 O DIGITALIMAGINGCOMMUNICATIONSINMEDICINEAPRACTICALINTRODUCTIONSURVIVALGUIDE HUANG 2010 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS LANGER 2009 48 52 S KOMMERI 2011 40 50 J BOND 2011 1 10 R LIU 2003 165 174 B MICHAEL 2007 1 9 M IEEEINTERNATIONALPARALLELDISTRIBUTEDPROCESSINGSYMPOSIUM SCALEUPSCALEOUTACASESTUDYUSINGNUTCHLUCENE QIU 2004 367 378 D ZHANG 2011 2064 2067 B INTERNATIONALCONFERENCECOMPUTERSCIENCESERVICESYSTEMCSSS OPTIMIZATIONMODELLOADBALANCINGINPEERPEERP2PNETWORK CAO 2010 63 70 Q INTERNATIONALCONFERENCEP2PPARALLELGRIDCLOUDINTERNETCOMPUTING3PGCIC LOADBALANCINGSCHEMESFORAHIERARCHICALPEERTOPEERFILESEARCHSYSTEM HOWARD 1988 23 26 J WINTER1988USENIXCONFERENCEPROCEEDINGS OVERVIEWANDREWFILESYSTEM TREICHEL 2010 1 9 T ASSOCIATION 2007 N DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART4SERVICECLASSSPECIFICATIONS OOSTERWIJK 2005 H DICOMBASICS TREICHEL 2011 1 7 T MUSTRA 2008 21 29 M ELMAR200850THINTERNATIONALSYMPOSIUMVOL1 OVERVIEWDICOMSTANDARD WARNOCK 2007 125 129 M WARNOCK 2007 125 129 M ASSOCIATION 2003 N DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART3INFORMATIONOBJECTDEFINITIONS COSTA 2009 71 77 C INTERNATIONALJOURNALCOMPUTERASSISTEDRADIOLOGYSURGERYVOL4 INDEXINGRETRIEVINGDICOMDATAINDISPERSEUNSTRUCTUREDARCHIVES COSTA 2010 1 9 C BLANQUER 2009 16 24 I SCHANTZ 2001 1 9 R RIBEIROX2012X1002 RIBEIROX2012X1002X1011 RIBEIROX2012X1002XL RIBEIROX2012X1002X1011XL item S0169-2607(12)00152-6 S0169260712001526 10.1016/j.cmpb.2012.05.013 271322 2014-10-02T02:18:14.257946-04:00 2012-12-01 2012-12-31 true 780522 MAIN 10 49197 849 656 IMAGE-WEB-PDF 1 gr5 90958 770 1667 gr4 401888 1796 2667 gr3 210746 952 2667 gr2 237434 1174 1667 gr1 501950 1991 3000 gr5 17579 174 376 gr4 46042 405 602 gr3 30791 215 602 gr2 30503 265 376 gr1 60319 450 678 gr5 3487 101 219 gr4 5940 147 219 gr3 3075 78 219 gr2 9155 154 219 gr1 6462 145 219 COMM 3401 S0169-2607(12)00152-6 10.1016/j.cmpb.2012.05.013 Elsevier Ireland Ltd Fig. 1 System architecture. The active peers form a distributed middleware layer between the workstations and the PACS archives (passive peers) of the healthcare institution. Besides, they are responsible for coordinating the passive peers and integrating them within the new PACS archive cluster. Fig. 2 Graphical User Interface (GUI) for managing the entire distributed system as if it was a centralized system. The GUI automatically lists the current status of active peers. Fig. 3 Distributed c-move message exchange at study level from two data sources. (1) The workstation sends a c-move command specifying the study UID of the target study to be moved. (2) The called active peer (coordinator) translates the c-move message into an equivalent c-find message and queries the passive peers A and B in parallel. The coordinator waits for the c-find response and finds that both passive peers hold the wanted DICOM object. (3) The active peer creates a queue with the SOP instances to move and their respective locations on the cluster. (4) The coordinator sends a message triggering the distributed c-move to each peer holding the SOP instances. The triggering message contains the next SOP instance in the queue to be moved. When the active peer at the other end receives the message it opens an association with the calling workstation and sends the first SOP Instance. After sending, the active peer sends a message to the coordinator asking for the next SOP Instance to be sent. (5) In the same c-store association the peer injects the new SOP instance assigned to it and asks for the following one. This process is repeated until the coordinator answers the peer request with a blank SOP instance UID. When this happens, the active peer closes the association and its interaction with the distributed c-move ends. Finally, the global distributed c-move ends when the last SOP instance is transferred to the workstation. Fig. 4 Topology at the physical layer of the performed experiments. Fig. 5 Chart containing the transfer time results. Clustering of distinct PACS archives using a cooperative peer-to-peer network Luís S. Ribeiro ⁎ Carlos Costa José Luís Oliveira University of Aveiro, IEETA-DETI, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234370545; fax: +351 234370525. To face the demanding requirements of the clinical environment, PACS archives need to be resilient and reliable, supporting high availability and fault tolerance. Often, to ensure no data loss, PACS archives retain two copies of images on separate physical machines, using distributed data storage facilities. However, PACS do not take advantage of the various replicas to improve the transfer rates of medical images. This happens mostly because the DICOM standard does not comply with distributed fetching of image fragments while performing a store. Inspired by this unexplored opportunity, we designed and implemented a new solution that takes advantage of the distributed image replicas and, at the same time, respects the DICOM standard. Our strategy brought significant improvements in the exchange rates, load balancing and availability of installed PACS archives. Moreover, the adopted strategy forms a cluster of PACS archives that transparently enables horizontal scaling, facilitates the creation of backups, and gives to healthcare professionals a unified view of the distributed repositories. Keywords DICOM Peer-to-peer Load balancing PACS Scale-out 1 Introduction The Picture Archiving and Communication System (PACS) concept embraces typically dispersed sub-systems, ranging from image archiving, acquisition and distribution to visualization. With the introduction of the Digital Imaging and Communication in Medicine (DICOM) standard, PACS received a positive boost, due to the fact that PACS vendors were no longer able to lock a healthcare institution into proprietary communication protocols, forcing it to purchase only their PACS solutions. Regarding communications, DICOM relies on a set of well-defined services that binds together PACS distributed devices [1]. The PACS archive is the heart of an image center, and it is responsible for safely storing all exams produced within the healthcare institution. Due to the importance of clinical exams, these archives typically rely on data redundancy to prevent data loss and resource redundancy to guarantee continuous availability. In other words, current fault-tolerant PACS archive solutions always keep at least two copies of the same image at two different physical machines (archive server and backup server) [2,3]. The complexity of such PACS archives is high and such reliable solutions are typically associated with high purchase costs for healthcare institutions. Therefore, smaller imaging centers are not able to own a state-of-the-art PACS archive and typically rely on a centralized machine to store exams. We implemented a system that upgrades transparently any DICOM compliant PACS archive regarding availability, aggregation of resources, and scaling at low prices. Furthermore, our system also takes advantage of the image replicas already existent and improves the transfer rates between DICOM devices. This paper describes the implementation of the system that upgrades the PACS archive in several dimensions in a transparent and straightforward manner. 2 Background The PACS archive is one of the key components of the healthcare workflow, and its malfunctioning might jeopardize the delivery of high quality care services [2,4,5]. Therefore, it is desirable that PACS should be an extremely reliable system: fault-tolerant, continuously available and with an acceptable performance to satisfy the daily operations of the healthcare institution [6].The importance of the PACS archive becomes even more vital with institutions following a paperless and filmless approach in serving their patients [2]. The reliability level of a computer system can be measured by the number of single points of failure it incorporates. As the expression suggests, single points of failure increase the risk of system failure or malfunction. The risk increases when critical components to deliver a service are unique. For instance, if the power supply unit of a centralized server fails, the entire server will fail and, as a consequence, the services it provides will be unavailable until the power unit is replaced. Therefore, the conventional centralized storage server is not suitable for the healthcare environment, because if the storage server fails, all the PACS services dependent on it will also fail. To build a reliable computer system, the single points of failure must be eliminated or minimized to the extent possible. Typically, this is achieved by resource redundancy. As a result, if a system resource (physical or virtual) fails, there will be at least one backup resource to fill the gap and the requested service will not be at risk. Therefore, the storage archives serving the PACS workflow tend to be a distributed storage system (or storage network), where several machines cooperate in the storage task and ideally, if one of them fails the other machines will compensate for the failure without jeopardizing the storage service. A distributed system, besides eliminating single points of failure, also reduces bottlenecks in the workflow by distributing the workload among the several machines enabling performance improvements. Furthermore, these distributed architectures are able to scale horizontally, which brings significant economic and planning benefits compared to vertical scaling [7]. Scaling horizontally allows a system to scale progressively and according to needs in an efficient and cost friendly manner, the only drawbacks being its complexity to implement and manage compared to vertical scaling. One good example of scaling-out strategies is peer-to-peer networks (e.g. BitTorrent [8,9]) where a peer is at the same time a server and a client, similar to a DICOM device capable of performing the roles of Service Class User (SCU) and Service Class Provider (SCP). In this kind of distributed system the data is spread among the peers, each peer holds a small portion of the network's available data, and there is data redundancy since different peers may hold replicas of the same file. Peer-to-peer networks achieve good levels of availability and fault-tolerance since if a peer fails or disconnects the data it holds, this will almost certainly be available on another peer. Besides, peer-to-peer networks take advantage of this multiplicity of peers holding the wanted file and instead of downloading the entire file from one peer they download the various fragments composing the file (blocks) from multiple peers, bringing load balancing advantages and also improving the file transfer rates [10,11]. Before peer-to-peer networks, other distributed systems had used this strategy, for instance, Andrew's File System (just on read-only files) [12] and OSF's file system in a Distributed Computing Environment [13]. Of course, a conventional peer-to-peer file sharing network such as BitTorrent would not be appropriate for the medical context because the PACS archive must ensure that all exams are available 24/7 and there is no guarantee that the peers holding some piece of data will always be online. However, we believe that peer-to-peer inspired networks could suit the healthcare situation well. Peers must be more reliable and controlled by the healthcare institution (PACS archives). Moreover, they should cooperate in order to support the clinical workflow, providing reliable, widely available and high-performance services at lower prices. 3 Materials Nowadays, PACS may be seen as a mature technology. They are present in the majority of medical imaging centers around the world. This worldwide acceptance was due in part to the success of the DICOM standard which, among other things, enabled interoperability inside healthcare institutions, allowing the devices having a role in the PACS workflow to communicate transparently [14]. PACS archives are no exception. Typically, an archive is expected to implement at least the DICOM services of Storage, Query and Retrieve. As we mentioned before, our system does not intend to replace the healthcare institution's existent PACS archives. Ultimately it intends to improve the performance, availability, fault-tolerance, management and search capabilities of the PACS distributed archive. Therefore, the core of our peer-to-peer network is based on these three DICOM services which most PACS archives support, i.e. in order for a PACS archive to be extended with our system it must support the DICOM services of: Storage, Query and Retrieve. In this section we will present the technologies and software components that were used to build our system. 3.1 DICOM standard Around the world there are many manufacturers of non-invasive medical imaging equipment (modalities). In the past, manufacturers developed their own proprietary communication and data exchange standards, making the interaction between equipment from different vendors impossible [15–17]. This situation was comfortable for most manufacturers who could lock customers, but at the same time unsustainable for healthcare institutions. Therefore, in 1992 the Digital Imaging and Communication in Medicine (DICOM) standard was established and has since become the standard for exchanging medical images in digital format [1,18]. DICOM facilitates the implementation of PACS solution and enables interoperability in a multi-vendor environment [16]. This non-proprietary standard is very complete and has many dimensions. However, it is best known for the definition of objects (DICOM objects including images, waveforms, studies, etc) and for operations applied to these objects such as storing, moving, finding, creating, and printing. Operations in DICOM are called Service Classes or, simply, DICOM services. DICOM services are the tools that enable online interoperability between entities from different vendors. Typically, DICOM services rely on two entities called application entities (AE). One entity assumes the client role, called service class user (SCU), and the other entity assumes the server role, called the service class provider (SCP) [15]. The service is a two-step process: negotiation and exchange. The negotiation step (association establishment) is critical to guarantee that the two entities understand and interpret the information sent in the exchange step. After the negotiation has been completed successfully, the exchange step takes place where the actual information is exchanged. Formally, the negotiation step is known as the DICOM association establishment and the exchange information step occurs within the DICOM association [1]. The exchanged information (Service-Object-Pair SOP) is defined as a combination of a DICOM Service Element (DIMSE), which is a command, and an object. Examples of DIMSE are c-store, c-get, c-find, c-move, etc. Examples of objects are computed tomography (CT) images, magnetic resonance (MR) images, waveforms, etc. To promote transparent integration within the healthcare institution our system follows the DICOM standard. Every peer of our system implements both roles (SCP and SCU) of the DICOM services verification, storage, query and retrieve. To implement the services we used Dcm4Che toolkit. Dcm4Che toolkit is a popular open source implementation of the DICOM Standard developed in Java programming language [19,20]. Therefore, interoperability is ensured with the DICOM services and portability as well since Java is a multi-platform language. 3.2 JavaGroups Compared to server-centric approaches, distributed systems may bring several benefits, such as: improved fault-tolerance, improved availability, cost-friendly solutions and incremental growth. However, these advantages often come at the cost of more complex software requiring greater effort regarding implementation, management, configuration and security [13,7]. For instance, it is simpler to configure one central server than multiple servers in order for them to cooperate. Complex systems are typically prone to human errors, which in the clinical environment could lead to disastrous consequences. Hence, the drawbacks of complexity in distributed systems must be eliminated or diminished to a complexity level equivalent to the server-centric approach. In order to achieve a distributed system with an acceptable management and configuration effort, we used JavaGroups toolkit as a software component of our system. JavaGroups is an open source platform [21] written in Java that enables reliable group communication. Group communication uses the terms of group and member, where members are part of a group. In other words, a member is a node and a group is a cluster. Nodes can join or leave a group, send messages to all the nodes or to a single node of the group. The system keeps track of the members within the cluster and notifies the nodes when a new node joins and when an existent node leaves the cluster or crashes. This node crash awareness enables the design of robust software systems that readjust in order to compensate for eventual system malfunctions. Moreover, JavaGroups communicates through reliable multicast. Therefore, the messages exchanged between nodes are guaranteed to arrive at their destination and in the right order. At the core of JavaGroups there is low-level abstraction enabling endpoint communication with a channel entity group. When a channel is created, it is given a name and a unique address. Multiple channels with the same name form a group. When a member connects to a channel it can send and receive a message to and from all other members. The channel provides its members with a list of addresses, called a view, of the members connected to the group. A member can select an address from this list and send it a unicast message, or send a multicast message to all members of the current view. This feature enables us to create a network abstraction where the system administrator may monitor, manage and configure every node in a unified view as if it was a centralized system. JavaGroups has a decentralized architecture. However, the first member joining the group will be automatically elected as the group leader. Besides the common node functions, the group leader is responsible for sending the view of the system to the other nodes. As we mentioned before, the view consists of a list of group nodes ordered bydate of entry and the first entry is the group leader. Therefore, if the leader crashes or leaves the group, the system will automatically elect the second entry as the group leader. With JavaGroups every peer in our distributed system is immediately aware of which other peers are online/offline, and when unicast or multicast messages are sent the peer is sure that the messages were properly received by the destination peers, simplifying the coordination process of the active peers. Because of its fault tolerance and reliable communication methods, JavaGroups was chosen as one of our system's software components. 3.3 Dicoogle Indexing System Traditionally, a PACS infrastructure relies in a database engine to manage the DICOM information model [22] or, at least, holds the DICOM objects in a local directory called “dicomdir”, a structured linked list containing information related to patients, studies, series and images. When a modification is targeted to the DICOM objects, an update call is triggered at the database making the information at the dicomdir and the database coherent. Dicoogle's main idea is the replacement or extension of the traditional database by an indexing engine [23,24]. This approach makes the PACS management more flexible, because it is possible to index new text-based fields quickly without the need to create new tables or relations as would be necessary in the database supported approach. Furthermore, Dicoogle automatically refreshes the indexes by monitoring the file system events (create, delete, update) targeted to the DICOM files. As we will explain in the following sections, our system offers two operating modes: stateless and stateful. The stateless mode does not hold any extra memory regarding DICOM objects from metadata fields. On the other hand, the stateful operating mode, besides extending the number of cooperating PACS archives, is a PACS archive in itself. The stateful operating mode follows a similar strategy to Dicoogle, offering a Google-like search experience by indexing, locally, the DICOM objects that it holds. 4 Methods PACS architecture began mainly on an ad hoc basis, serving small subsets, called modules, of the radiology department. Each module functioned as an independent island, unable to communicate with other modules [2]. Later it evolved into a PACS infrastructure solution, integrating the hospital information system and the radiology information system, serving the entire hospital. Although the various PACS archives that serve the institution are typically accessible within the institution, they tend to be independent of each other. This may be useful to create federations of clinical specialties within the institution, but if a workstation needs to access the full view of one patient it has to query archive by archive [25]. Furthermore, when the central PACS archive of the healthcare institution or department depreciates (e.g. not capable of delivering an acceptable QoS) the institution typically replaces it with a more powerful machine (scales up) or adds a new and independent PACS archive. Scaling up may improve the QoS but it is normally associated with high costs and data migration problems [26]. Secondly, by simply adding independent archives to the PACS, it spreads the data repositories and as a consequence makes management and discovery of resources more complex. Our system allows the PACS archive to scale out and at the same time with a centralized abstraction of the distributed system. Besides, retrieving an aggregated view of all the distributed resources, our system goes one step further by turning the independent nodes into a cluster of cooperative nodes. It takes advantage of the various file replicas in the institution and improves the transfer performance of the DICOM objects. We now present the methods applied in our system to upgrade the healthcare institution's current PACS archives without the need to replace software and hardware resources. 4.1 Architecture Our distributed system is a symbiosis between the PACS archive(s), owned by the healthcare institution, and the nodes fully developed by us. The machines of the institution's PACS archive we call passive peers and our nodes active peers. For each passive peer there is one active peer responsible for coordinating it within the new distributed system (Fig. 1 ). The coordination is performed exclusively through standard DICOM services: storage, query, retrieve and verification. In this way the passive-active peer symbiosis, to optimize the PACS services, is achieved transparently. As result, the new cluster of peers performs the PACS archive tasks as if they were from the same vendor and planned to work together from the beginning, transforming independent and centralized PACS archives into a network-centric cooperating PACS archive. Acceptance of the network-centric paradigm is growing, and distinct machines with different quality of service (QoS) are integrated by various forms of communication services [27]. Distributed middleware systems have a major role in combining isolated and independent machines into a swarm of machines working together. Middleware approaches in general bring several benefits while developing a new system [27]: • Reducing software lifecycle costs through using the potential of previous systems, capturing development expertise and key functionalities of the existent system, rather than rebuilding them manually one by one. • Providing a higher-level abstraction oriented to the services required by the application, simplifying the development of distributed embedded systems. • Hiding the complexity of lower abstraction layers (e.g. network), focusing on the core services that generate added value. • Making management, configuration, monitoring and security (which 10 has proven to be necessary) efficient and less demanding from the development and administration point of view. Distributed middleware systems yield significant benefits when the standards (e.g. communications, formats, etc.) are mature. During the past decade there was a consolidation of the DICOM standard within the medical imaging environment and DICOM is the consensual standard when treating medical images. This fact enabled us to build a distributed middleware system for the medical imaging environment that unifies and extends the storage services of the PACS workflow. Each active peer must be installed in the same physical machine as the passive peer it must coordinate. Although the active peer and the passive peer share the same hardware resources, they still communicate through DICOM services and the two software devices are independent. The active peer only requires knowing the file directory (or directories) where the DICOM objects are stored, to map the SOP instance unique identifier (UID) of the DICOM object with its respective file path. For the active peers to interact with the passive peer, the Application Entity (AE) title, the IP address and the port of the PACS archives must be known. However, configuring the various active peers one by one requires great effort and could lead to human errors. Therefore, our system provides a graphical manager (Fig. 2 ) that automatically detects the active peers currently online giving a unified view to the network administrator. The administrator configures the required fields of all active peers, which becomes a XML configuration file of the cluster. When the start button is pressed the XML file is sent to each active peer automatically and the cluster starts operating with the desired configurations. The XML file contains: (i) the SCUs allowed to interact with the DICOM network; (ii) the AE Title, port, and IP address of the PACS archives to support; (iii) the port of each active peer where the DICOM services will be available; (iv) operating mode (stateless or stateful). 4.2 Stateless and stateful active peers The active peers may operate in two distinct modes: stateless and stateful. The difference between them is the amount of metadata that they hold. In the stateless mode, the only metadata that the active peer holds is the map between the SOP Instance UID and the respective location of the DICOM object on the local file system. In the other hand, the stateful mode indexes all the metadata of the DICOM object following a similar approach to that used by Dicoogle [23]. By default, the active peer will be operating in the stateless mode. In the stateful mode, the active peer accumulates the tasks of the PACS archive, rather than being just a relay to the cluster of PACS archives. The purpose of offering two distinct operating modes is to facilitate the integration of our system within the healthcare institution. Typically, PACS archives hold large amounts of data and substituting the software of PACS archive may take significant periods of time – since new software would have to extract and structure the metadata from all DICOM objects once again. Therefore, the stateless mode avoids this massive refactoring of metadata turning the initial integration step less demanding. However at a more advanced stage, the scaling out of the cluster will pass by the addition of new peers without any previous DICOM images. Therefore, we provide the stateful operating mode which combines the functionalities of active and passive peers in the same software device. 4.3 Peer-to-peer DICOM-based network The developed peer-to-peer network based on DICOM enables the active peers to cooperate with each other and at the same time coordinate the passive peers transparently. This peer symbiosis relies on the DIMSE commands: c-store, c-find, c-move and c-echo. Nevertheless, the purpose of the developed network is to hide the distributed nature of the system and deliver the DICOM services as if they were requested from a centralized PACS archive. The communication between active and passive peer is performed exclusively with DIMSE commands. 4.3.1 Distributed c-find The purpose of the distributed c-find is to enable queries over the distributed system rather than over a single machine. The active peer that receives the workstation c-find request forwards it to every existent passive peer. The called active peer waits at the most a timeout period for all the c-find responses, joins the search results and answers the calling workstation with the c-find response message. The c-find response is the concatenation of all distributed results without repeated entries. 4.3.2 Distributed c-store Typically, each department of the healthcare institution has its own independent PACS archive. If having a unified view of all independent archives is a positive feature while searching for a patient's exams, regarding storage it might not be the desirable scenario for the majority of healthcare institutions. For instance, the cardiology department probably prefers to continue storing exams next to the exams produced there, rather than store them next to the radiology department, justified only by a new storage system serving the institution. Therefore, each active peer represents each passive peer individually by initiating a DICOM storage service per passive peer. Furthermore, it is also possible to initiate extra DICOM storage services that store the received objects and more than one peer. In this way, when an active peer receives a c-store from a workstation, it knows which is the respective(s) passive peer(s) and where to store it. Therefore, the active peer forwards the c-store to the passive peer(s). Finally, if a PACS archive is currently offline, the active peers store the DICOM objects targeted to that archive temporarily until it is once again online. Hence, the active peers periodically try to execute the c-store command until the PACS archive successfully stores the DICOM objects. 4.3.3 Distributed c-move The traditional c-move command allows one workstation to order the movement of DICOM objects from one device to another, even if the transfer destination is not the workstation itself but a third party device. Therefore, the workstation that triggers the c-move command needs to specify the destination AE title where the DICOM objects will be moved. The Information Object Definition (IOD) commonly known as DICOM object possesses a hierarchical tree structure organized from top to bottom into: patient, study, series and image. Each level is identified by a unique identifier and the objects are moved based on that identifier. For instance, if a workstation sends a c-move command at the study level (specifying the Study UID) all the images belonging to that specific study will be retrieved. On the other hand, the c-move performed at the image level (SOP Instance UID) will just retrieve that specific image. A DICOM object is composed of several independent images that may be retrieved separately. Due to the fact that our system is aware of the DICOM object replicas and knows their location, it takes advantage of this condition and triggers a distributed c-move. The distributed c-move retrieves the images of the wanted DICOM object from several data sources. The data sources are the passive peers that contain the specific DICOM object. For instance, if the same DICOM object is located (for disaster recovery purposes) at several passive peers and the object is composed of several images, transfer of the DICOM object will be balanced among the various data sources. Fig. 3 illustrates a distributed c-move episode where the DICOM object requested by the workstation is present at two data sources. The workstation sends c-move at the study level but receives the SOP Instances that compose the study from several sources (two sources in this example). The called active peer will be the coordinator of the distributed c-move episode and is responsible for creating a queue with the location of each requested SOP instance on the distributed system. The coordinator, after creating the queue, sends a message triggering the distributed c-move. On receiving that message the peers send the respective DICOM object and request the following object to be sent. This process of requesting/sending the following object to move will balance the workload of the transfer through the cluster nodes evenly, since the idler (or faster) peers will request more images to move. Furthermore, at step four of Fig. 3 a pseudo c-move message (sent via JavaGroups communication channels) is used instead of the standard c-move message in order to make this process more efficient. The distributed c-move has the same purpose as the standard c-move message although it allows the movement of a set of independent SOP Instances (without the need for them to belong to the same branch of the DICOM object). At an early stage, we considered use of the c-move message, sent directly to the respective passive peer, but for each independent SOP instance to be moved, the standard c-move opens one association, adding a lot of overhead to the exchange and diminishing the performance results significantly. Therefore, with the distributed c-move we go a step forward, taking advantage of the various image replicas present within the distributed system to reduce transfer times between the PACS archives and the workstations or whenever a DICOM object needs to be moved. 5 Results and discussion Although the presence of the DICOM standard is a fact in the majority of healthcare institutions, the PACS workflow tends to be heterogeneous from one institution to another. PACS are specially designed to serve the workflow needs of one specific healthcare institution. As we mentioned before, our system complements or extends the PACS archives of the healthcare institution. Therefore, we built a flexible system so that it could adapt itself as far as possible to the institution's situation, and not the reverse. For the reader to understand the potentialities of our system in its several dimensions, we present the categories where it may bring benefits to healthcare institutions. 5.1 Availability and load balancing Besides joining together several PACS archives in a single integrated environment, our system may also be exploited to increase the availability and robustness of these archives. By transforming independent PACS archives of healthcare institutions in a cooperating cluster of PACS archives, it reduces the number of single points of failure and as a consequence increases the availability of the supported DICOM services. There is an improvement in the availability of the DICOM storage service: e.g., if one node crashes, the modalities may send the DICOM objects to any other active peer of the cluster so that whenever the machine is back online the object will be stored in the desired node. Regarding the DICOM query/retrieve services, our system might also improve their availability, depending on the architecture of the healthcare institution's PACS archive. In other words, if one machine holds unique DICOM objects and crashes, those objects cannot be searched or moved. However, if our system is installed in the central PACS archive and in the backup server at the same time (in the stateless mode if the backup server is DICOM compliant or stateful if not) the distributed PACS is able to compensate for it and retrieve the respective object. Therefore, peer crashes will not affect the DICOM services and retrieval is possible as long as there is at least one instance of the requested DICOM object within the cluster of PACS archives. The traditional c-move operation triggers the transfer of DICOM objects regardless if the peer is currently busy with other tasks or if it is completely idle. Instead, our distributed c-move spreads the effort of moving the DICOM objects among all peers holding the object. This load balance is not a blind division of the workload per number of peers capable of carrying the move tasks rather our system delegates the transfer tasks according to the peer's current busyness state (on real time). Hence, when a new transfer episode is triggered a peer with lower hardware capabilities (e.g. link with lower bandwidth) or currently flooded with other parallel tasks will receive less tasks of the transfer episode than an idle peer. By adapting to the current conditions of the network our load balancing algorithm accomplishes a continuous flow of DICOM objects on the receiver's link. Furthermore, our empiric tests showed that the weight of our load balancing algorithm on the network traffic is residual – while comparing with the traditional c-move – there is a slight aggravation of 0.96% in average of data exchanged however the real effort for the network is the actual transmission of the DICOM objects. Either the distributed c-move or the traditional c-move will have a similar effect on the network traffic. The main difference is that our system does not focus the movement to just one source rather it balances (in a weighted manner) to all the peers holding the DICOM object. 5.2 Scaling and cost saving Typically, upgrading the PACS archive means replacement of the old hardware and software with new and more powerful resources (vertical scaling). We do not argue that vertical scaling will improve the QoS of the PACS archive. However, vertical upgrades necessary to reach a certain QoS level come at higher financial costs, compared to horizontal scaling. Our system enables the healthcare institution's current PACS archive to scale horizontally, reusing the previous PACS archives as peers of the new cluster of PACS archives. The stateless operating mode integrates the existent PACS archives of the healthcare institution in the distributed system, while the stateful mode allows the system to grow horizontally without the need for other PACS archive software. Furthermore, the project was implemented with Java, making the system portable and enabling the use of open source operating systems (e.g. Linux). Therefore, our system is capable of upgrading the PACS archive and reducing expenses on hardware and software components. However, scaling horizontally raises the complexity of the system, making its management and configuration a more difficult task. To diminish its complexity, our system automatically scans the network for the active peers, giving a centralized view for managing and configuring the distributed system. 5.3 Unifying view Our system also performs the role of distributed middleware system between workstations and the PACS archive. It is possible to aggregate all the PACS archives dispersed over the various departments of the healthcare institution, offering a unified view of the system. Without reorganizing the location of the DICOM objects or creating new metadata (e.g. new database), our system is capable of performing distributed search queries and, at the same time, delivering the search results in a centralized manner, i.e. as if all exams in the patient's health history in the various clinical specialties were present in one central machine serving the entire healthcare institution. 5.4 Performance With implementation of the distributed c-move, there is a reduction of the server's bottleneck and, as a consequence, improved performance regarding the transfer rate of DICOM objects. To demonstrate this statement we set up an evaluation scenario (Fig. 4 ). The PACS archive(s) were based on the Conquest DICOM server [28], which supports the DICOM services of Storage, Query, Retrieve and Verification. In this test we had chosen the stateless operating mode. Nevertheless, the purpose of these tests was to analyze performance improvements while exchanging one DICOM object with several data sources holding the same DICOM object and compare results against the legacy PACS archive. The experiment workflow starts when the workstation sends a c-move message, at the patient level, and finishes when the last association binding the workstation to one active peer is closed. Seven machines (four with Windows and three with Linux) and two Gigabit switches were used for the experiment. The experiments have involved one to three PACS archives and one to four clients. The DICOM object exchange in the various experiments is composed of 9 studies, more precisely 1117 computer tomography (CT), 13 X-ray angiography (XA), 661 magnetic resonance (MR), 12 nuclear medicine, and 225 positron emission tomography (PET), performing a total of 2028 SOP instances and 1.01Gbytes. The SCU, where the measurements were taken, shares the same switch as the PACS archives and at the other switch the remaining SCUs were plugged in (Fig. 4). Fig. 5 shows the results of the evaluation. There were four main experiment sets: (1) PACS archive standing alone, (2) one peer symbiosis (PACS archive aided by its respective active peer), (3) two peer symbiosis, and (4) three peer symbiosis. Inside each experiment set four different workload scenarios were tested: one SCU, two SCUs, three SCUs and four SCUs, always performing the same c-move request in parallel. The PACS archive was tested on three machines and the final result is the average of the three outcomes. In this way we reduce the noise from the hardware distinct performance. As a reference, we consider the ideal scenario to exchange DICOM objects: one SCU requesting one PACS archive. Therefore, the following evaluations of the obtained results will be relative to this scenario. There were no major differences for all the parallel c-move cases when comparing the PACS archive standing alone with one peer symbiosis, i.e. introduction of the active peer has a very small impact on retrieval latency. The transfer time is slightly aggravated (0.6s on average) due to the fact that the peer symbiosis adds c-find messages in the middle of the c-move process (as long as the c-move request does not target the SOP instance UID directly). Continuing the analysis of Fig. 5, it is visible that the major benefit of the distributed c-move is obtained when the servers have higher workloads (i.e. performing several tasks in parallel). When the same SOP instance is in more than one PACS archive, it is exchanged efficiently taking advantage of all available resources of the cluster at that exact moment. As a result, when the joint hardware resources of the cluster are equal to or above the number of parallel c-move requests, the transfer times tend to stabilize around the times of the ideal scenario. For instance, when three parallel c-move requests (3 SCUs) are querying the cluster, the transfer times are ideal on average. Nevertheless, when the available server's resources are less than the parallel c-move requests, there is degradation of the transfer times. However, because the peers on the cluster work as a whole, the server's bottleneck occurrences are diminished and the workload is spread evenly over the cluster, making the ideal scenario always possible when there are enough available hardware resources in the cluster. 6 Conclusions Clinical data stored on the PACS archive is valuable and requires reliable storage, wide availability and a good performance, which will not impair the healthcare workflow. In this paper we describe a distributed system that may easily improve the QoS of current PACS archives. The presented solution turns existing PACS archive(s) into a network-centric PACS archive by joining available hardware resources and enabling DICOM compliant archives to scale horizontally in a transparent manner. The system grows according to the needs of the users, allowing the institution to cut costs by purchasing new hardware resources gradually and reutilizing previous ones. Its integration inside the institution is almost effortless due to communication between peers being performed through DICOM services, and due to the operating modes that ease consolidation of the cluster in two distinct stages: the stateless mode avoids the substitution of the previous PACS archives and the stateful mode eases the horizontal scaling of the cluster. Furthermore, the middleware strategy simplifies backup routines by replicating objects to previous defined nodes. Finally, we also introduced the distributed image retrieval approach which takes advantages of the existent backup replicas of the DICOM objects, bringing improvements in the movement performance of DICOM objects. Acknowledgement The research leading to these results has received funding from Fundao para a Cincia e Tecnologia (FCT) under grant agreement PTDC/EIA-EIA/104428/2008. References [1] O.S. Pianykh Digital Imaging and Communications in Medicine: A Practical Introduction and Survival Guide 2008 Springer Publishing Company, Incorporated [2] H. Huang PACS and Imaging Informatics: Basic Principles and Applications 2010 John Wiley & Sons [3] S. Langer Issues surrounding PACS archiving to external, third-party DICOM archives Journal of Digital Imaging 22 1 2009 48 52 [4] J. Kommeri M. Niinimki H. Mller Safe storage and multi-modal search for medical images Studies in Health Technology and Informatics 2011 40 50 [5] R.R. Bond D.D. Finlay C.D. Nugent G. Moore A review of ECG storage formats International Journal of Medical Informatics 2011 1 10 [6] B. Liu F. Cao M. Zhou G. Mogel L. Documet Trends in PACS image storage and archive Computerized Medical Imaging and Graphics 27 2–3 2003 165 174 [7] M. Michael J. Moreira D. Shiloach R. Wisniewski Scale-up×scale-out: a case study using nutch/lucene IEEE International Parallel and Distributed Processing Symposium 2007 1 9 [8] D. Qiu R. Srikant Modeling and performance analysis of bittorrent-like peer-to-peer networks SIGCOMM – Computer Communication Review 34 2004 367 378 [9] B. Cohen, The Bittorrent Protocol Specification, Version 11031, 2008. [10] B. Zhang S. Wang An optimization model of load balancing in peer to peer (p2p) network International Conference on Computer Science and Service System (CSSS) 2011 2064 2067 [11] Q. Cao S. Fujita Load balancing schemes for a hierarchical peer-to-peer file search system International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC) 2010 63 70 [12] J.H. Howard An overview of the Andrew file system Winter 1988 USENIX Conference Proceedings 1988 23 26 [13] W.e.a.R.P., A Distributed Computing Environment for Multidisciplinary Design, Tech. Rep., 1994. [14] T. Treichel P. Liebmann O. Burgert M. Gessat Applicability of DICOM structured reporting for the standardized exchange of implantation plans International Journal for Computer Assisted Radiology and Surgery 5 1 2010 1 9 [15] N.E.M. Association Digital Imaging and Communications in Medicine (DICOM). Part 4. Service Class Specifications 2007 [16] H. Oosterwijk P. Gihring DICOM Basics 3rd edition 2005 OTech Inc. [17] T. Treichel M. Gessat T. Prietzel O. Burgert DICOM for implantations – overview and application Journal of Digital Imaging: The Official Journal of The Society for Computer Applications in Radiology 2011 1 7 [18] M. Mustra K. Delac M. Grgic Overview of the DICOM standard ELMAR 2008, 50th International Symposium, vol. 1 2008 21 29 [19] M. Warnock C. Toland D. Evans B. Wallace P. Nagy Benefits of using the Dcm4Che DICOM archive Journal of Digital Imaging 20 2007 125 129 [20] M.J. Warnock C. Toland D. Evans B. Wallace P. Nagy Benefits of using the DCM4CHE DICOM archive Journal of Digital Imaging: The Official Journal of the Society for Computer Applications in Radiology 20 October (Suppl. 1) 2007 125 129 [21] B. Ban, Javagroups – Group Communication Patterns in Java, Tech. Rep., Cornell University, 1998. [22] N.E.M. Association Digital Imaging and Communications in Medicine (DICOM). Part 3. Information Object Definitions 2003 [23] C. Costa A. Silva J.L. Oliveira Indexing and retrieving DICOM data in disperse and unstructured archives International Journal of Computer Assisted Radiology and Surgery, vol. 4 2009 71 77 [24] C. Costa C. Ferreira L. Bastião L. Ribeiro A. Silva J. Oliveira Dicoogle – an open source peer-to-peer PACS Journal of Digital Imaging 1 2010 1 9 [25] A. Tzavaras, N. Kontodimopoulos, E. Monoyiou, I. Kalatzis, N. Piliouras, I. Trapezanidis, D. Cavouras, E. Ventouras, Upgrading Undergraduate Biomedical Engineering Laboratory Training, 27th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, vol. 1, 2005, pp. 353–356. [26] I. Blanquer V. Hernández D. Segrelles E. Torres Enhancing privacy and authorization control scalability in the grid through ontologies IEEE Transactions on Information Technology in Biomedicine 13 1 2009 16 24 [27] R.E. Schantz D.C. Schmidt Middleware for distributed systems – evolving the common structure for network-centric applications Encyclopedia of Software Engineering 1 2001 1 9 [28] M. Oskin, M. van Herk, L. Zijp, Conquest DICOM Software 1.4.16 Released, 2007. "
    },
    {
        "doc_title": "CloudMed: Promoting telemedicine processes over the cloud",
        "doc_scopus_id": "84869034852",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84869034852",
        "doc_date": "2012-11-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Asynchronous collaboration",
            "Cloud providers",
            "Collaborative activities",
            "Computer concept",
            "DICOM",
            "Evolution of technology",
            "Extensible messaging and presence protocols",
            "Instant messaging",
            "Medical images",
            "Medical institutions",
            "PACS",
            "Real time",
            "Real-time collaboration",
            "Remote assistance",
            "Remote diagnosis",
            "Service-based",
            "Software as a service",
            "Teleconsultation",
            "Telediagnosis",
            "Telemedicine services",
            "XMPP"
        ],
        "doc_abstract": "Nowadays is common to have health care institutions providing remote assistance, such as, remote diagnosis (telediagnosis) and second opinion from a specialized physicians (teleconsultation), to others medical institutions that do not have specialized human resources, or even for educational propose. The evolution of technology creates opportunities to improve telemedicine services. This article focuses on the improvement of services to support telemedicine in a paradigm \"any-time and anywhere\" adapting a new emerging computer concept, which is cloud computing. Throughout the paper is presented a software as a service that can be used by radiologist for store, share, visualize and analyse medical images, in compliant with DICOM (Digital Imaging and Communications in Medicine) format, from anywhere. Furthermore, the presented architecture allows to improve real time and asynchronous collaboration. To support collaborative activities and their coordination, we created a computer-supported collaborative work service based on Openfire real time collaboration (RTC) server, that use Extensible Messaging and Presence Protocol (XMPP) for instant messaging and presence. Finally, the platform was deployed on an Internet Cloud provider and some results was presented and discussed. © 2012 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A network sensor for medical imaging workflows",
        "doc_scopus_id": "84869009390",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84869009390",
        "doc_date": "2012-11-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Amount of information",
            "DICOM",
            "Medical institutions",
            "Multiple source",
            "Network packets",
            "Network sensors",
            "PACS",
            "Work-flows"
        ],
        "doc_abstract": "The usage of information systems has significantly augmented in the last years and healthcare is not an exception. The growing use of computer systems has increased the productivity and quality of services provided by medical institutions. These systems can generate a huge amount of information and it is possible to identify several tools to process and analyze it. However, there are few systems that allow users to aggregate data from various systems, in a centralized manner, and correlate multiple source information. Moreover, there are some sources that usually are not analyzed but their usage could help us to optimize existing resources and ensure the quality of services provided. This paper presents a tool to capture and analyse the information exchanged on a medical imaging network. This sensor colects network packets and enables the user to extract knowledge from them and associate email alerts to certain events. © 2012 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle relay - A cloud communications bridge for medical imaging",
        "doc_scopus_id": "84867301732",
        "doc_doi": "10.1109/CBMS.2012.6266402",
        "doc_eid": "2-s2.0-84867301732",
        "doc_date": "2012-10-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Imaging data",
            "Medical data",
            "Remote access"
        ],
        "doc_abstract": "Over the last decades, information systems for medical imaging sharing are imposing themselves as important tools for the diagnostic and study of pathologies. One of the most important advantages of those systems is to allow widespread sharing and remote access to medical data. Nevertheless, there is no simple solution for imaging data exchange between multiple places due to bureaucratic and technical issues. The paradigm introduced by Dicoogle project potentiates queries over a set of distributed repositories, which are logically indexed as a single federate unit. This paper describes a Cloud-based relay service that acts as a bridge of communications between the different institutions, allowing the community to access, share and discover imaging records. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Telecardiology through ubiquitous Internet services",
        "doc_scopus_id": "84865106654",
        "doc_doi": "10.1016/j.ijmedinf.2012.05.011",
        "doc_eid": "2-s2.0-84865106654",
        "doc_date": "2012-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Electronic mailboxes",
            "Internet browsers",
            "Internet services",
            "IT resources",
            "Medical images",
            "Patient safety",
            "Plug-and-play",
            "Portugal",
            "Quality of care",
            "Real environments",
            "Remote diagnosis",
            "Security model",
            "Server-based",
            "Telecardiology",
            "Work efficiency"
        ],
        "doc_abstract": "Purpose: Implementation of telemedicine in many clinical scenarios improves the quality of care and patient safety. However, its use is hindered by operational, infrastructural and financial limitations. This paper describes the design and deployment of a plug-and-play telemedicine platform for cardiologic applications. Methods: The novelty of this work is that, instead of complex middleware, it uses a common electronic mailbox and its protocols to support the core of the telemedicine information system and associated data (ECG and medical images). A security model was also developed to ensure data privacy and confidentiality. Results: The solution was validated in several real environments, in terms of performance, robustness, scalability and work efficiency. During the past three years it has been used on a daily basis by several small and medium-sized laboratories. Conclusions: The advantage of using an Internet service in opposition to a server-based infrastructure is that it does not require IT resources to set up the telemedicine centre. A doctor can configure and operate the centre with the same simplicity as any other Internet browser application. The solution is currently in use to support remote diagnosis and reports of ECG and Echocardiography in Portugal and Angola. © 2012 Elsevier Ireland Ltd.",
        "available": true,
        "clean_text": "serial JL 271161 291210 291773 291870 291901 291919 31 International Journal of Medical Informatics INTERNATIONALJOURNALMEDICALINFORMATICS 2012-06-17 2012-06-17 2012-08-11T03:36:44 S1386-5056(12)00105-0 S1386505612001050 10.1016/j.ijmedinf.2012.05.011 S300 S300.1 FULL-TEXT 2015-05-15T06:04:43.33604-04:00 0 0 20120901 20120930 2012 2012-06-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref specialabst alllist content subj ssids 1386-5056 13865056 false 81 81 9 9 Volume 81, Issue 9 5 612 621 612 621 201209 September 2012 2012-09-01 2012-09-30 2012 Regular Papers article fla Copyright © 2012 Elsevier Ireland Ltd. All rights reserved. TELECARDIOLOGYTHROUGHUBIQUITOUSINTERNETSERVICES COSTA C 1 Introduction 2 Methods 3 Framework proposal 3.1 Workflow 3.2 Security architecture 3.3 Reviewing console 3.4 Management module 3.5 Installation and setup 4 Results 5 Conclusions Authors’ contributions form Competing interests References NIKUS 2009 473 480 K WADE 2010 233 V KHASANSHINA 2006 748 775 E DAUCOURT 2006 287 293 V REINER 2005 420 426 B BRADLEY 2004 244 248 W LARSON 2005 967 970 P WILLIAM 2007 545 549 B CAFFERY 2010 20 34 L DELLAMEA 1999 84 89 V FRASER 2001 815 819 H WEISSER 2006 753 758 G CAFFERY 2008 107 L COSTA 2009 273 282 C COSTA 2002 460 462 C SMITH 2005 286 293 A EKELAND 2010 736 771 A BENJAMIN 2010 3 9 M DONNELLY 2010 2029 2038 L PIANYKH 2008 O HMSBOSTONMAUSADIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMAPRACTICALINTRODUCTIONSURVIVALGUIDE BOND 2011 681 697 R COSTA 2007 79 108 C STUDIESINCOMPUTATIONALINTELLIGENCEADVANCEDCOMPUTATIONALINTELLIGENCEPARADIGMSINHEALTHCARE CURRENTPERSPECTIVESPACSCARDIOLOGYCASESTUDY HUANG 2004 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS COSTA 2004 277 287 C COSTAX2012X612 COSTAX2012X612X621 COSTAX2012X612XC COSTAX2012X612X621XC item S1386-5056(12)00105-0 S1386505612001050 10.1016/j.ijmedinf.2012.05.011 271161 2012-08-11T00:31:33.497198-04:00 2012-09-01 2012-09-30 true 1590514 MAIN 10 48707 849 656 IMAGE-WEB-PDF 1 gr1 54461 633 263 gr1 5299 164 68 gr1 404936 2810 1167 gr2 77148 376 565 gr2 12957 146 219 gr2 738022 1662 2500 gr3 88165 380 565 gr3 13410 147 219 gr3 1388809 1681 2500 gr4 56810 355 565 gr4 12112 138 219 gr4 483432 1572 2500 gr5 54902 368 565 gr5 11110 143 219 gr5 436276 1630 2500 gr6 47482 312 744 gr6 4700 92 219 gr6 348908 1380 3293 gr7 25125 331 747 gr7 2860 97 219 gr7 187047 1464 3308 gr8 39573 362 753 gr8 3844 105 219 gr8 306271 1603 3333 IJB 2876 S1386-5056(12)00105-0 10.1016/j.ijmedinf.2012.05.011 Elsevier Ireland Ltd Fig. 1 The telecardiology central workflow. Fig. 2 Reviewing console – incoming area (messages and examinations). Fig. 3 Reviewing console – Tele-ECG module (ECG visual representation, textual report, pending examinations and reports). Fig. 4 Reviewing console – Tele-Echo module. Fig. 5 Central management – repository and accounting modules. Fig. 6 Tele-ECG centre – usage statistics. Fig. 7 Physical distance from reading centres to client sites. Fig. 8 Distribution of service requests throughout the day. Telecardiology through ubiquitous Internet services Carlos Costa ⁎ José Luís Oliveira University of Aveiro, DETI/IEETA, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234 370500. Highlights ► A secure and efficient Tele-cardiology solution. ► The solution is based upon the cloud computing paradigm. ► Can be easily installed and maintained by end-users. ► Three years of exploitation in Portugal and Angola. Purpose Implementation of telemedicine in many clinical scenarios improves the quality of care and patient safety. However, its use is hindered by operational, infrastructural and financial limitations. This paper describes the design and deployment of a plug-and-play telemedicine platform for cardiologic applications. Methods The novelty of this work is that, instead of complex middleware, it uses a common electronic mailbox and its protocols to support the core of the telemedicine information system and associated data (ECG and medical images). A security model was also developed to ensure data privacy and confidentiality. Results The solution was validated in several real environments, in terms of performance, robustness, scalability and work efficiency. During the past three years it has been used on a daily basis by several small and medium-sized laboratories. Conclusions The advantage of using an Internet service in opposition to a server-based infrastructure is that it does not require IT resources to set up the telemedicine centre. A doctor can configure and operate the centre with the same simplicity as any other Internet browser application. The solution is currently in use to support remote diagnosis and reports of ECG and Echocardiography in Portugal and Angola. Keywords Telemedicine Telecardiology ECG Echocardiography 1 Introduction Telemedicine is the provision of health care services through the use of information and communication technology, in situations where the health care professional and the patient are not in the same location [1]. Telemedicine environments have been developed as a way to improve the efficiency of health resources, to reduce costs [2–4], and to promote wider access to healthcare services [5]. However, despite many technological achievements, the requirements of patients and clinicians are also changing. In the area of cardiology, for instance, the increase in life expectancy results in a higher prevalence of chronic cardiovascular diseases and in an increasing demand for continuous health care services. Existent statistical data and productivity studies show that ECG (electrocardiogram) and medical image data can be generated in practically any healthcare institution, even one with limited human or financial resources [6]. Nevertheless, highly skilled cardiologists are usually concentrated in a reduced number of specialized medical centres. The asymmetric distribution of equipment and service providers across countries leads typically to the need to hire third party cardiology reporting services outside the institutions where the exams were made. Nighthawk radiology is already a common solution in tackling this problem, where a staff of radiologists evaluates examinations transmitted electronically to them from around the world. Using IT resources, properly trained and accredited professionals are able to provide the referring physician with a written formal opinion, based on the performed exam, on the patient's clinical history and, in some cases, other physicians’ opinions [7–9]. Communication between the technicians performing the examinations and the specialized unit responsible for the analysis can be achieved using many different available technologies, ensuring the various requirements imposed by distinct scenarios. In the last decade, several low-cost solutions have been proposed for different telemedicine scenarios [10]. Della Mea [11], for instance, was one of the first authors to suggest the use of electronic mail for store-and-forward telemedicine applications. The TeleMedMail was another experiment based on the transmission of digital camera images through email, and was used to promote telemedicine in developing countries [12]. The lack of standardization for teleradiology connections also led the German Radiology Society to propose the use of the DICOM standard in email attachments, to promote interoperability between distinct manufacturers of PACS equipment [13]. However, commercial off-the-shelf email applications were designed for peer-to-peer communication and may be inefficient in delivering telemedicine. One solution may be to develop purpose-written applications [14]. Previous telecardiology projects with national and international partners [15,16] have consolidated our awareness of the fact that, besides technology, the key factor for the success of telemedicine scenarios is the way they replicate traditional procedures and allow increased productivity. In this manuscript we present a simple, robust and low-cost telecardiology centre. 2 Methods The constant advances in information technologies are raising new challenges in the healthcare sector. Professionals are demanding new Web-based access to information, as a way to avoid local software installation and support Internet-based scenarios for remote diagnostic and cooperative work. It is possible to establish telemedicine services with a distinct technological approach to support diversified clinic requirements, including service workflow and operational modus operandi [17]. Nevertheless, the effectiveness of many reported telemedicine projects is limited and inconsistent [18]. There are two main methods of telemedicine: real-time and “store and forward” sessions. In the former, participants can send and receive information almost instantly, for instance, in Tele-consultation and Tele-monitoring practices. In the latter, data is acquired and “stored” in one physical location. When convenient, the data is transmitted (“forwarded”) to a remote peer where clinical specialists analyse it. Finally, the specialist opinion or report will be transmitted back in an uncertain time period. A very simple example of this modus operandi is the transmission of a digital X-ray image via email to a specialist for diagnostic support. The outcome of the specialist work is a final report delivered to the relevant stakeholder (referring physician, patient, administration). The report must be accurate and delivered promptly [19,20]. Those issues are especially important in the business model of outsourcing reporting, where telematics application to support information acquisition, transmission, visualization and reporting is fundamental to increase reviewer efficiency. Efficiency can be measured as speed plus accuracy in result production and delivery [19]. According to this definition, Tele-radiology based on off-the-shelf email applications is not practical for large scale, multi-centre reviewing processes. They require manual processes to send and receive examinations and reports. The outsourced medical imaging report market demands professional Tele-ECG and Tele-Imaging solutions, typically based on dedicated servers, where applications are developed to run in specific operating systems and hardware configurations. This option also requires installation and continuous maintenance processes. Moreover, the portability to other architectures is extremely restricted. Our approach is based on the idea of cloud computing where Internet resources are seen as a natural infrastructure that includes hardware, operating system, applications and services. Analysing the requirements of telematic platforms to process ECG and echocardiography examinations, three main elements were identified: central storage capacity, normalized communication channel and universal access. An electronic email box supplies all the identified requirements, through three fundamental services: • A way to send messages: an encapsulated message, in SMTP normalized format (Simple Mail Transport Protocol) [21], is passed from the sender computer to the destination message recipient box, across intermediate email servers. • A way to receive messages: there are two main email reading protocols, i.e. POP (Post Office Protocol) and IMAP (Internet Mail Access Protocol) [22]. The main difference between them is that IMAP protocol does not remove messages from the server after they are downloaded, i.e. they will remain indefinitely in the server until users explicitly delete them. • A way to store messages: messages are deposited in the server mailboxes. Each mailbox is organized in folders, sub-folder areas and messages. A message has a header, a body, and several attached files. The proposed telematics solution uses normalized email messages to upload examinations to the centre and return reports from the centre to stakeholders. The mailbox storage is used as the platform database and object repository. Upon this messaging infrastructure, a software application was developed to operate with the centre, presently supporting Tele-ECG and Tele-Echocardiography sessions. When used for this purpose, the mailbox account is blocked for regular email clients (Outlook, Eudora, Thunderbird, Entourage, etc.). The medical informatics scenarios empowered by open Internet services are promising and they can even change the pattern of traditional technology use. The proposed Tele-cardiology system is an example of using ubiquitous cloud services to support remote diagnosis on a daily basis. 3 Framework proposal To create an efficient “store and forward” telemedicine platform that supports the reporting of cardiology examinations, several components were addressed: a communication network, a messaging system and a message repository. Choosing a mail server to support our telemedicine centre revealed many advantages, including reduced operational requirements and natural integration with email communications. Moreover, to access IT resources from outside an institution and explore inter-institutional collaboration, it is necessary to adapt application communications to network firewall requirements. Due to network management policies, firewalls typically inhibit direct connections from outside the healthcare centre. Accordingly, our approach is supported by IMAP [22], a very common protocol that “behaves” well with firewalls. From a management point of view, to set up a new telemedicine community, only an email account with IMAP support is needed. Then all the system's operational information, including requests, images, reports and responses are kept in this mailbox. The email account is customized with several service folders and an overall data structure that manages the centre. Because the email server is a passive intermediary, the communication model is poll-based. The advantage of using a well-known Internet service is that it is not necessary to build new protocols and data repositories, and IT resources are not required to set up the telemedicine centre. Moreover, using this common service, it is possible to give clients transparent interaction with the Telecardiology centre. In addition, using a client software module, the central setup is accessible to everyone. 3.1 Workflow End-users, namely the technicians responsible for the acquisition of data (ECG or Echocardiograms), are provided with a simple application to upload the examinations into the email centre (Fig. 1 ). It is possible to select a directory containing exams previously received from acquisition equipment via DICOM [23,24], USB, FTP, etc. The files are introduced in a compressed container that is ciphered with a session key. The software can work in automatic mode (i.e. service mode) in which a file system monitor intercepts file creation events and, according to the kind of data, triggers specific actions. For efficient storage and reduced transmission times, individual files placed inside each container are compressed. Finally, the container is attached to an email message and sent to the central email address via SMTP protocol (Fig. 1). In the central mailbox, this encapsulated exam is stored in the Incoming folder until processed. For this, the technician can use his own email account, and despite a special client module being available, he can use any email browser (Outlook, Thunderbird, etc.) to send exams to the centre. The telecardiology reviewing software (Fig. 1) consists of a standalone application with various functions. When this application starts, it requests the user's authentication credentials. The verification process includes consultation with the central database, via IMAP protocol. After successful authentication, the software will check the Incoming folder for service requests associated with the session user. Messages will then be moved to a “pending folder” where they are separated according to its reviewer. At the same time, attachments will be downloaded and decrypted with the respective secret key. The examination data are then analysed and reported. A PDF report document will be generated with the physician's digital signature. The report is then encrypted and deposited in the central outbox area. When desired, the reviewer can use the “send all” button to send the reports to the technician's email account, according to the original SMTP “Return-path” field. 3.2 Security architecture Health information is very sensitive in nature and such an implementation can raise concerns about security and privacy. Therefore, special measures have to be taken to allay those concerns. The central security architecture was the subject of several considerations. Concerning communication, the centre uses a secure SMTP connection (with SSL – Secure Sockets Layer) to establish contact with service requesters (i.e. receive exams) and secure IMAP protocol (via SSL) between the reviewing software and the central mailbox (Fig. 1). Moreover, the transmitted data is itself encrypted. The Advanced Encryption Standard (AES) algorithm is used to encrypt each exam file inside the ZIP archive with a 256-bit key encryption key. Those containers are attached to email messages and remain ciphered in the central mailbox. The only device capable of decoding this information is the reviewing software module with the proper software service key. Because a password is easier for humans to remember, we use a key derived function to transform this human secret data into a sequence of bits suitable for cryptographic algorithms. More precisely, our symmetric key is generated using an extension of the PBKDF2 algorithm [25], which concatenates a given human password with a salt to reduce the risk of a dictionary attack. Next, the result is submitted to an iterative pseudorandom function to obtain a strong symmetric key. The central administrator needs to define the password, which will be used by client sender module to cipher the service request messages. As previously described, this software is also protected with an authentication mechanism based on username and password. The central administrator controls all users and privileges, defining labs and physicians. The administration module also includes several important security services, including users’ action monitoring, access to statistical measures and central backup. 3.3 Reviewing console Considering that we are using SMTP and IMAP as the core of our telemedicine communication infrastructure, we needed to create a special application for the reviewing process. This is made up of two main components: the repository manager and the reviewing modules. In the repository manager it is possible to control three main areas: Incoming, Reports and Processed examinations. In the Incoming (i.e. Inbox) area the cardiologist will see the service requests, including the provenance, message timestamp and the associated exams (Fig. 2 ). It is also possible to define rules to prioritize services according to request provenance or message prioritization flags. Moreover, a doctor can forward an incoming review service to another colleague, for instance, to request a second clinical opinion. Selecting other repository areas, he can also search and see previously processed studies and reports. An ECG diagnostic tool was the first reviewing module developed upon this cloud-based framework (Fig. 3 ) and it supports several popular data formats [26], namely ECG Signals (SCP) [27], HL7 aECG [28] and Mortara XML. Each new exam is accomplished with respective automatic textual interpretations (Fig. 3) and there is a translating mechanism that can improve reporting quality and speed. It is possible to convert string sequences to improve legibility or translate reports to another language, according to a pre-configured translation table. The idea is to provide a near-automatic reporting feature to minimize manual corrections or text-free inputs. Final reports are generated in PDF format, where patient information, client lab identification, ECG signals over a millimetre grid, textual report and the reviewer's signature are included. When completed, each reviewed examination is moved to the outbox area and the next request is loaded from the incoming area. The main interface also shows the list of pending reviewing requests (Fig. 3 – Inbox). This area display is optional but very useful to detect a duplicate request of examination concerning the same patient. Those situations are normal due to technical reasons associated with exam acquisition. Concerning the solution's efficiency, measured as speed plus accuracy in result production and delivery, physicians’ contributions were fundamental to tune the application and internal workflows. The software memorizes the reviewer preferences, including graphical object locations, sizes and curves display layout. A second reviewing module was developed for echocardiography (Fig. 4 ). This Tele-Echo application is similar to the Tele-ECG but with a different visualization component. It supports DICOM format [23,24] and is based on Himage [15], a cardiac PACS (Picture Archiving and Communication Systems) [29,30] used in several centres, currently supporting more than 2 million still frames and videos. The huge volumes of cardiac dynamic image modalities (i.e. the videos) are not easy to transfer in a time and cost-effective way. For instance, a typical echo-cardiogram study with 15 captures of 20 frames/each and a RGB 480*640 matrix can produce a volume of 276MB. By default, the platform compresses exams in ZIP format to reduce transmission times and storage volume data. However, the DICOM private transfer syntax that is used in Himage offers higher compression capabilities without compromising the diagnosis quality [31]. An impressive fact arising from this study was that, in a simultaneous and blind display of the original against the compressed cine-loops, 37% of trials selected the compressed sequence as the best image. This suggests that other factors related to viewing conditions are more likely to influence observer performance than the image compression itself. Several Himage components were adapted to create the Tele-Echo module over IMAP, namely the DICOM viewer and the report modules. It is possible to visualize the image sequences of still and cine-loops, and select frames to be sent to the report area. Several features are available, e.g. image manipulation algorithms (contrast/brightness), a tissue measurement tool, printing, and exporting of images into distinct formats (DICOM3.0 default transfer syntax, AVI, BMP, JPEG). It is also possible to copy a specific image to the clipboard and paste it onto some other external application. In the report area, the user can arrange (or delete) the images selected in the viewer location using a drag-and-drop function. Finally, the output images matrix (2×3 or 3×3) plus textual report and patient elements are saved in a PDF file. The user can customize the base template used to generate the report file, and even include an institution logo and report headers. 3.4 Management module Several features were also developed to support management tasks related to users, data repository, reporting and accounting (Fig. 5 ). Three user profiles are available: Administrator, Physician and Administrative. The Administrative can access only accounting reports for billing purposes and the Physician can use the centre to report examinations but does not have access to management modules. The Administrator can do everything, including user creation/deletion and repository backup/deletion. The repository data management module (Fig. 5 – front) allows the Administrator to monitor the amount of data in the centre, and choose a specific percentage to be stored and/or deleted. He can also select the type of data that will be affected by the selected action, i.e. past exams, reports or both. The backup option creates a local hierarchical repository (e.g. /dataType/ClientName/Date/Files). The report accounting module (Fig. 5 – back) allows access to statistics that are important to support billing. The Administrative user can filter the service requests by provenance and by time interval. A list of results will be extracted from the Central information system, including the institution name, email, date and number of examinations per request. For instance, Fig. 5 shows that 2682 service requests were received (12,088 exams) from a specific client. 3.5 Installation and setup The following steps can easily create a new reviewing centre: 1. Create an email account in an institutional server or in any Internet provider. 2. Open the client console in administration mode. 3. Run a setup procedure, providing the email account credentials, the central password (to encode messages) and choosing the data type supported (ECG/Medical Image). 4. Create users’ accounts. After the installation process is concluded, the user will lose access to the email account via usual clients (Outlook, Webmail, etc.) because the email password will be replaced by a service one. This option is crucial for the centre's integrity, avoiding data loss or corruption, motivated for instance by accidental deletion (rename) of the mailbox folder. The fault-tolerance of the entire system is dependent on the email provider's quality of service. For instance, Gmail offers reliability up to 99.9% [32] which is quite good for a store-and-forward service. 4 Results The framework presented was designed to solve telemedicine demand in small to middle-sized environments, without the need for expensive and complex IT solutions. The rise of the cloud paradigm was the inspiration for the conception of a new solution supported by ubiquitous services. The centre usage evaluation has been centred on the ECG data according to project development priority associated with real world needs, i.e. there are numerous and distributed ECG collecting points. In fact, there are two reasons for examination centres proliferation: electrocardiograph machines have become cheaper; and the professional skills required to perform the exam are not particularly demanding. With the Tele-ECG, the first reviewing module deployed, more than 55,000 studies were reviewed in the last three years (Fig. 6 ). During the first year, informal validation was performed and physicians’ later suggestions were incorporated in the platform. Since then, the number of examinations has been successively increasing up to 2600 examinations/month, on average, in the last 6 months (Fig. 6). In Portugal, we have currently three centres installed with different exploration scenarios and more than twenty service requesters (i.e. client). Fig. 7 presents a histogram of physical distance, in a straight line, from reading centres to client sites. The three review centres are geographically located in Oporto and, as expected, the majority of service requests come from the city itself, which has the highest population density in the North of Portugal. However, there are important clients located in rural areas and the most remote place is 85km east of Oporto, representing 1h 15m of travelling time by car (Fig. 7 – 60km of distance/4.8% of exams). Another analysed item was the service request distribution throughout the day (Fig. 8 ). As expected, the vast majority of requests arrive during office hours with two peaks, one in the morning and another in the afternoon. However, we can observe that the ubiquity of the solution also allows it to be used during the night. One of these centres receives, on average, 12.1 examinations per incoming service request and returns 10.8 reports per response. A second centre has been used more for urgent requests, with an average of 3.0 examinations per incoming request and 3.4 reports per response. In Angola, a single reading centre is installed in Luanda, providing reviewing services for two clients. The centre was installed 18 months ago and has reported about 5.000 examinations until now. User feedback has been very positive and in the future, the project will include sixteen more clients distributed throughout the country. Analysing the solution's workflow efficiency, namely the time associated with transmissions and report generation times, some measures were performed. Concerning examination analysis and report creation, we measured the physicians’ operating times and, on average, they are 2.9 ECG/min. Those values are possible because the application workflow and reviewing tools were optimized to improve productivity and in many normal examinations, the cardiologist only needs to observe the signals and validate the textual report suggested with one mouse click (i.e. sign the report). Similar operations have been replicated using a traditional email workflow. Opening the client's Outlook email, downloading the ECG exam to local hard disk, opening the file with an ECG viewer and creating a report took about 3min per exam. The reviewing process using developed Tele-ECG solution is approximately 9 times more efficient than a similar operation using a workflow based on separated components (email clients, viewer and reporting tools). We must also stress that the usage of simple store-and-forward email messages, using commercial-off-the-shelf applications, does not promote optimized reviewing procedures. Concerning the size of messages, compressed ECG files can vary between 30 and 140Kbytes, according to their original format, and the PDF report never exceeds 60Kbytes, including wave representation. With those figures, receiving exams or sending reports is almost instantaneous using current network links. The same could not be stated regarding echocardiography image modality. Nevertheless, using our DICOM private transfer syntax to upload exams to the centre, it is possible to compress the volume data significantly. The average file size is about 290Kbytes, including multi-frame captures. In a telework environment, supported by a 20Mbits ADSL (Asymmetric Digital Subscriber Line), for instance, a study with 10 cine-loops takes typically less then 15s to download, decrypt, decompress and display, including the overhead introduced with an encrypted SSL (Secure Sockets Layer) channel. From the above results and considering “store and forward” scenarios, it is possible to have efficient Tele-ECG and Tele-Echo telemedicine scenarios. 5 Conclusions With normal use of Internet services, physicians now have tools that allow them to remotely access patients’ information, fostering telemedicine, telework and collaborative work environments. However, sometimes, healthcare professionals do not adopt telemedicine or telework platforms if they need to invest in major IT infrastructure and its maintenance. This restriction is especially important for small and medium-sized labs. The proposed telecardiology platform is based on ubiquitous email services. The main idea was to decouple the typical centralized database of telemedicine platforms and replace it with a normalized mailbox. The result is a secure telemedicine platform that is easily installed by end-users due to its extremely reduced operational requirement. The solution's simplicity, associated with its usability and mobility, has been captivating new users. The three years spent using this solution in Portugal and Angola are good indicators of its robustness and of user satisfaction. Authors’ contributions form All authors equally participated in solution development, results analysis and drafting of all sections of the manuscript. All authors have read and agreed to the paper being submitted as it is. Competing interests None declared. Summary points “What was already known on the topic?” • The outsourcing of cardiology procedures reporting is a fast-growing market. • Telecardiology solutions are based on dedicated hardware and dedicated applications, requiring technical skill and IT resources to install and maintain. • The deployment of telemedicine projects, namely involving ECG and Echocardiograms, is many times hindered by financial, operational and technological constraints. • There are missing teleradiology solutions that support reviewing workflows in a simple, efficient and secure manner. “What this study added to our knowledge?” • The successful use of a telecardiology solution that provides highly optimized reviewing processes, using public available Internet services. • This work shows that it is possible to have a low-cost and secure telecardiology solution based on Internet ubiquitous services to support remote diagnosis and reporting, on a daily basis. • The proposed solution fits well in distinct usage scenarios, from small low-cost reviewing centres to more demanding environments. • Internet cloud services can be increasingly used in telemedicine system, if robustness and security aspects are guaranteed. Three years of exploitation in Portugal and Angola demonstrates the effectiveness and the robustness of this solution. References [1] K. Nikus The role of continuous monitoring in a 24/7 telecardiology consultation service – a feasibility study J. Electrocardiol. 42 6 2009 473 480 [2] V. Wade A systematic review of economic analyses of telehealth services using real time video communication BMC Health Serv. Res. 10 2010 233 10.1186/1472-6963-10-233 [3] E.V. Khasanshina M.E. Stachura Socio-economical impact of telemedicine in Russian Federation Int. J. Econ. Dev. 8 3 2006 748 775 [4] V. Daucourt Cost-minimization analysis of a wide-area teleradiology network in a French region Int. J. Qual. Health Care 18 4 2006 287 293 [5] J.L. Monteagudo, O.M. Gil, eHealth for patient empowerment in Europe, EU-Report, 2007. [6] B.I. Reiner Multi-institutional analysis of computed and direct radiography. Part II. Economic analysis Radiology 236 2 2005 420 426 [7] W.G. Bradley Offshore teleradiology J. Am. Coll. Radiol. 1 4 2004 244 248 [8] P.A. Larson M.L. Janower The nighthawk: bird of paradise or albatross? J. Am. Coll. Radiol. 2 12 2005 967 970 [9] B.M. William Nighthawks across a flat world: emergency radiology in the era of globalization Ann. Emerg. Med. 50 5 2007 545 549 [10] L.J. Caffery A.C. Smith A literature review of email-based telemedicine Stud. Health Technol. Inform. 161 2010 20 34 [11] V. Della Mea Internet electronic mail: a tool for low-cost telemedicine J. Telemed. Telecare 5 2 1999 84 89 [12] H.S. Fraser TeleMedMail: free software to facilitate telemedicine in developing countries Stud. Health Technol. Inform. 84 Pt 1 2001 815 819 [13] G. Weisser Standardization of teleradiology using Dicom e-mail: recommendations of the German Radiology Society Eur. Radiol. 16 3 2006 753 758 [14] L. Caffery A.C. Smith P.A. Scuffham An economic analysis of email-based telemedicine: a cost minimisation study of two service models BMC Health Serv. Res. 8 2008 107 [15] C. Costa Design, development, exploitation and assessment of a Cardiology Web PACS Comput. Methods Programs Biomed. 93 3 2009 273 282 [16] C. Costa A transcontinental telemedicine platform for cardiovascular ultrasound Technol. Health Care 10 6 2002 460 462 [17] A.C. Smith Telemedicine and rural health care applications J. Postgrad. Med. 51 4 2005 286 293 [18] A.G. Ekeland A. Bowes S. Flottorp Effectiveness of telemedicine: a systematic review of reviews Int. J. Med. Inform. 79 11 2010 736 771 [19] M. Benjamin Y. Aradi R. Shreiber From shared data to sharing workflow: merging PACS and teleradiology Eur. J. Radiol. 73 1 2010 3 9 [20] L.F. Donnelly Quality initiatives: department scorecard: a tool to help drive imaging care delivery performance Radiographics 30 7 2010 2029 2038 [21] J. Klensin, RFC 5321, SMTP – Simple Mail Transfer Protocol, 2008, IETF. [22] M.R. Crispin, RFC 3501: Internet Message Access Protocol – Version 4 Rev1, 2003, IETF. [23] DICOM, Digital Imaging and Communications in Medicine version 3.0, ACR (the American College of Radiology) and NEMA (the National Electrical Manufacturers Association), [24] O.S. Pianykh H.M.S., Boston, MA, USA Digital Imaging and Communications in Medicine (DICOM) – A Practical Introduction and Survival Guide 2008 Springer [25] B. Kaliski, RFC 2898: PKCS #5: Password-Based Cryptography Specification – Version 2.0, IETF Network Working Group, 2000. [26] R.R. Bond A review of ECG storage formats Int. J. Med. Inform. 80 10 2011 681 697 [27] SCP-ECG, Standard Communications Protocol for Computer-Assisted Electrocardiography, CEN European Pre-Standard ENV 1064, 1993. [28] FDA-ECG, FDA XML Data Format Design Specification – Draft – Revision C, 2002. [29] C. Costa A. Silva J.L. Oliveira Current perspectives on PACS and cardiology case study S. Vaidya L.C. Jain H. Yoshida Studies in Computational Intelligence: Advanced Computational Intelligence Paradigms in Healthcare 2007 Springer DE 79 108 (Chapter 5) [30] H.K. Huang PACS and Imaging Informatics: Basic Principles and Applications 2004 Wiley [31] C. Costa Himage PACS: a new approach to storage, integration and distribution of cardiologic images, PACS and imaging informatics Proc. SPIE 5371 2004 277 287 [32] M. Glotzbach, What we learned from 1 million businesses in the cloud, Product Management Director, Google Enterprise, 2008. "
    },
    {
        "doc_title": "Towards the development of a mobile learning model for smart phones using stakeholders' analysis",
        "doc_scopus_id": "84864874836",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84864874836",
        "doc_date": "2012-08-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            }
        ],
        "doc_keywords": [
            "Digital marketing",
            "Educational institutions",
            "Educational programmes",
            "Information sharing",
            "Learning and training",
            "M-Learning",
            "Mobile applications",
            "Mobile Learning",
            "Stakeholders' analysis",
            "Voice and video"
        ],
        "doc_abstract": "The objective of the current research is to develop a mobile learning model for smart phones which will be used by educational institutions for learning and training purposes but also for on-the-go communication for students and academics and information sharing. It would determine the important elements of mobile applications as a medium to facilitate m-Learning both in assembling responses from students and for reporting feedback to individual students. The mobile application would also integrate capabilities so the students would be able to access presentations by professionals through the 3G application in their mobile devices. They could also interact by posting their comments and sharing photos, images, voice and video though this application. The lecturer could facilitate and moderate debates while students could respond and learn without being in front of a computer. The application would also be used as a digital marketing channel for promoting Educational Programmes to potential candidates.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A secure personal health record repository",
        "doc_scopus_id": "84861985443",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861985443",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Access policies",
            "Competitive markets",
            "EHR",
            "Electronic health record systems",
            "Information exchanges",
            "Legal issues",
            "Personal health record",
            "PHR",
            "Repositories",
            "Security",
            "Storage services"
        ],
        "doc_abstract": "Due to strict regulatory, ethic and legal issues, Electronic Health Record (EHR) systems have been mainly deployed in federated health care scenarios. This situation have been hindering the wide adoption of EHRs, contributing to delaying the establishment of a competitive market where contributions from different providers could take full advantage of information exchange and regular practitioners' collaboration. Moreover, with the increasing awareness of medical subjects, patients are demanding more control over their own personal data - Personal Health Record (PHR). This paper presents a secure PHR repository which access is controlled through the joint use of a Virtual Health Card Service (VHCS) and an access Broker. This solution can be deployed in any public or private storage service since it behaves as a sandbox system which access policy is defined externally. To assure a friendly query-retrieve interaction the whole repository is indexed, and separated clinical events are kept independently to increase the efficiency of cipher and encipher algorithms.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An openehr repository based on a native XML database",
        "doc_scopus_id": "84861978859",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861978859",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "EHR",
            "Electronic health record",
            "Native xml database",
            "Open-source solutions",
            "Open-standard specification",
            "OpenEHR",
            "Service layers",
            "XML repositories"
        ],
        "doc_abstract": "openEHR is an open standard specification that describes the management, storage, retrieval and exchange of data in Electronic Health Record (EHR). Despite its growing importance in the field, the lack of open source solutions is hindering a larger visibility. In this paper we present an openEHR-based repository supported by a native XML database, which allows to store and query openEHR records through the DB service layer and a set of REST web services. The obtained results highlight the efficiency of this API and show that it can be used as a persistence component in any openEHR solution.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DICOM relay service supported on cloud resources",
        "doc_scopus_id": "84861968982",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861968982",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Different equipment",
            "Digital imaging communication in medicines",
            "Healthcare institutions",
            "Medical Devices",
            "Medical institutions",
            "PACS",
            "Picture archive and communication systems",
            "Teleradiology",
            "Work-flows"
        ],
        "doc_abstract": "Over the past decades, healthcare institutions adopted Picture Archive and Communication Systems in their workflows. The exchange and interaction between different equipment is performed with Digital Imaging Communication in Medicine (DICOM), which is a very extensive protocol covering many areas of imaging laboratories. However, the communication of a wide domain composed by several medical institutions is not well supported. This paper presents a solution to share DICOM services across healthcare institutions. The proposed implementation is supported on public cloud resources, creating the opportunity to exchange information between medical devices across several institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A RESTful image gateway for multiple medical image repositories",
        "doc_scopus_id": "84860675556",
        "doc_doi": "10.1109/TITB.2011.2176497",
        "doc_eid": "2-s2.0-84860675556",
        "doc_date": "2012-05-11",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Component based",
            "Decision support tools",
            "DICOM",
            "Image repository",
            "Imaging protocol",
            "Medical images",
            "Mobile Technology",
            "PACS",
            "Picture archiving",
            "Representational state transfer",
            "services broker",
            "Telemedicine systems",
            "Universal access"
        ],
        "doc_abstract": "Mobile technologies are increasingly important components in telemedicine systems and are becoming powerful decision support tools. Universal access to data may already be achieved by resorting to the latest generation of tablet devices and smartphones. However, the protocols employed for communicating with image repositories are not suited to exchange data with mobile devices. In this paper, we present an extensible approach to solving the problem of querying and delivering data in a format that is suitable for the bandwidth and graphic capacities of mobile devices. We describe a three-tiered component-based gateway that acts as an intermediary between medical applications and a number of Picture Archiving and Communication Systems (PACS). The interface with the gateway is accomplished using Hypertext Transfer Protocol (HTTP) requests following a Representational State Transfer (REST) methodology, which relieves developers from dealing with complex medical imaging protocols and allows the processing of data on the server side. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concepts for a Personal Health Record",
        "doc_scopus_id": "84872558617",
        "doc_doi": "10.3233/978-1-61499-101-4-636",
        "doc_eid": "2-s2.0-84872558617",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Electronic health record",
            "Longitudinal health records",
            "Medical history",
            "Personal health informations",
            "Personal health record"
        ],
        "doc_abstract": "Healthcare is about information. It is usually assumed that personal health information exists primarily for professional's use but well informed patients motivate better informed professionals. A longitudinal health record containing a patient's medical history has been the holy grail of healthcare. Personal Electronic Health Records (P-EHR) hold the potential to transform healthcare by providing a complete set of patient managed information. We present a portable P-EHR's functionalities from the patient's perspective. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancing the many-to-many relations across IHE document sharing communities",
        "doc_scopus_id": "84872536540",
        "doc_doi": "10.3233/978-1-61499-101-4-641",
        "doc_eid": "2-s2.0-84872536540",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "ATNA",
            "Clinical records",
            "Document sharing",
            "Healthcare institutions",
            "Integrating healthcare enterprise",
            "IT infrastructures",
            "Professional relationships",
            "Work in progress"
        ],
        "doc_abstract": "The Integrating Healthcare Enterprise (IHE) initiative is an ongoing project aiming to enable true inter-site interoperability in the health IT field. IHE is a work in progress and many challenges need to be overcome before the healthcare Institutions may share patient clinical records transparently and effortless. Configuring, deploying and testing an IHE document sharing community requires a significant effort to plan and maintain the supporting IT infrastructure. With the new paradigm of cloud computing is now possible to launch software devices on demand and paying accordantly to the usage. This paper presents a framework designed with purpose of expediting the creation of IHE document sharing communities. It provides semi-ready templates of sharing communities that will be customized according the community needs. The framework is a meeting point of the healthcare institutions, creating a favourable environment that might converge in new inter-institutional professional relationships and eventually the creation of new Affinity Domains. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle Mobile: A medical imaging platform for Android",
        "doc_scopus_id": "84872518920",
        "doc_doi": "10.3233/978-1-61499-101-4-502",
        "doc_eid": "2-s2.0-84872518920",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Android applications",
            "Clinical decision support",
            "DICOM",
            "Health care information system",
            "Mobile computing technology",
            "Mobile platform",
            "Quality of care",
            "Ubiquitous access"
        ],
        "doc_abstract": "Mobile computing technologies are increasingly becoming a valuable asset in healthcare information systems. The adoption of these technologies helps to assist in improving quality of care, increasing productivity and facilitating clinical decision support. They provide practitioners with ubiquitous access to patient records, being actually an important component in telemedicine and telework environments. We have developed Dicoogle Mobile, an Android application that provides remote access to distributed medical imaging data through a cloud relay service. Besides, this application has the capability to store and index local imaging data, so that they can also be searched and visualized. In this paper, we will describe Dicoogle Mobile concept as well the architecture of the whole system that makes it running. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A PACS archive architecture supported on cloud services",
        "doc_scopus_id": "84862979352",
        "doc_doi": "10.1007/s11548-011-0625-x",
        "doc_eid": "2-s2.0-84862979352",
        "doc_date": "2012-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Purpose Diagnostic imaging procedures have continuously increased over the last decade and this trend may continue in coming years, creating a great impact on storage and retrieval capabilities of current PACS. Moreover, many smaller centers do not have financial resources or requirements that justify the acquisition of a traditional infrastructure. Alternative solutions, such as cloud computing, may help address this emerging need. Methods A tremendous amount of ubiquitous computational power, such as that provided by Google and Amazon, are used every day as a normal commodity. Taking advantage of this new paradigm, an architecture for a Cloud-based PACS archive that provides data privacy, integrity, and availability is proposed. The solution is independent from the cloud provider and the core modules were successfully instantiated in examples of two cloud computing providers. Operational metrics for several medical imaging modalities were tabulated and compared for Google Storage, Amazon S3, and LAN PACS. Results A PACS-as-a-Service archive that provides storage of medical studies using the Cloud was developed. The results show that the solution is robust and that it is possible to store, query, and retrieve all desired studies in a similar way as in a local PACS approach. Conclusion Cloud computing is an emerging solution that promises high scalability of infrastructures, software, and applications, according to a \"pay-as-you- go\" business model. The presented architecture uses the cloud to setup medical data repositories and can have a significant impact on healthcare institutions by reducing IT infrastructures. © 2011 CARS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DICOM and clinical data mining in a small hospital PACS: A pilot study",
        "doc_scopus_id": "80054063236",
        "doc_doi": "10.1007/978-3-642-24352-3_27",
        "doc_eid": "2-s2.0-80054063236",
        "doc_date": "2011-10-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Clinical data",
            "Data fields",
            "Data-mining tools",
            "DICOM Data Mining",
            "PACS",
            "Picture archiving",
            "Pilot studies",
            "Technological development"
        ],
        "doc_abstract": "Technological developments in the medical imaging acquisition and storage process have triggered the use of Picture Archiving and Communication Systems (PACS) with gradually larger archives. This paper aims to exploit advantages of using a DICOM Data Mining Tool in a hospital PACS. The results showed the tool reliability and performance to obtain and index clinical data, as well as the possibility of conducting flexible research on DICOM data fields, providing means for continuous improved practices at the Radiology Departments. © 2011 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle - An open source peer-to-peer PACS",
        "doc_scopus_id": "84855567679",
        "doc_doi": "10.1007/s10278-010-9347-9",
        "doc_eid": "2-s2.0-84855567679",
        "doc_date": "2011-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computer communication networks",
            "Digital imaging and communications in medicines",
            "Information storage and retrieval",
            "Open sources",
            "PACS",
            "PACS implementation",
            "Peer to peer"
        ],
        "doc_abstract": "Picture Archiving and Communication Systems (PACS) have been widely deployed in healthcare institutions, and they now constitute a normal commodity for practitioners. However, its installation, maintenance, and utilization are still a burden due to their heavy structures, typically supported by centralized computational solutions. In this paper, we present Dicoogle, a PACS archive supported by a document-based indexing system and by peer-to-peer (P2P) protocols. Replacing the traditional database storage (RDBMS) by a documental organization permits gathering and indexing data from file-based repositories, which allows searching the archive through free text queries. As a direct result of this strategy, more information can be extracted from medical imaging repositories, which clearly increases flexibility when compared with current query and retrieval DICOM services. The inclusion of P2P features allows PACS internetworking without the need for a central management framework. Moreover, Dicoogle is easy to install, manage, and use, and it maintains full interoperability with standard DICOM services. © Society for Imaging Informatics in Medicine 2010.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A PACS gateway to the cloud",
        "doc_scopus_id": "80052507826",
        "doc_doi": null,
        "doc_eid": "2-s2.0-80052507826",
        "doc_date": "2011-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Cloud providers",
            "Concept-based",
            "Data availability",
            "Data flow",
            "DICOM",
            "High security levels",
            "Medical images",
            "Outsource",
            "PACS",
            "Pay-as-you-go",
            "Universal access",
            "Work-flows"
        ],
        "doc_abstract": "The amount of medical images has increased significantly over the last decade as result of the increase of number and quality of studies. Following some researchers, this trend will continue over the next years. Cloud computing is a new concept based on a well-know model named \"pay-as-you-go\". There is a new concept dubbed PACS Cloud, which the fundamental idea is to do PACS outsourcing taking advantages of the clouds elasticity and scalability, avoiding hardware obsolescence, providing universal access to the information anywhere, anytime and increase the data availability. This paper presents a module of PACS Cloud architecture to grant interoperability with DICOM devices. PACS Cloud Gateway is a component of PACS Cloud, which focuses mainly on the translation from DICOM commands to non-DICOM and vice-versa. While data outsource to the cloud can relieve users from the burden of local storage and maintenance, it also brings new security concerns. This paper presents a secure PACS Cloud Gateway to access PACS Cloud archive, which provides a high security level and without cloud's provider dependence. The workflows of each process was described carefully, specifying data flows since that Gateway is contacted by DICOM device, until it releases the process. Finally, the platform was instantiated in biggest Internet Cloud providers and the solution's results was analysed. © 2011 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hybrid electronic health records",
        "doc_scopus_id": "79960192049",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960192049",
        "doc_date": "2011-07-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Alternative medicine",
            "EHR",
            "Elderly care",
            "Electronic health record",
            "Health records",
            "Hybrid EHR",
            "New services",
            "Personal health record",
            "PHR",
            "Public sector",
            "Remote patient monitoring"
        ],
        "doc_abstract": "The research related with digital health records has been a hot topic since the last two decades, producing diverse results, particularly in two main types - Electronic Health Records and Personal Health Records. With the current wider citizen mobility, the liberalization of health care providing, as well as alternative medicine, elderly care and remote patient monitoring, new challenges had emerged. These brought more actors to the scene that can belong to different healthcare networks, private or public sector even from different countries. For creating a true patient-centric electronic health record, those actors need to collaborate in the creation and maintenance of the record. In this work, the Hybrid Electronic Health Record (HEHR) is presented, describing how information can be created and used, as well as focusing on how the patient defines the access control. Some new services are also discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of openEHR in a portable PHR",
        "doc_scopus_id": "79960172418",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960172418",
        "doc_date": "2011-07-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Active parts",
            "Data population",
            "Electronic health",
            "Electronic health record",
            "Information repositories",
            "Medical information",
            "OpenEHR",
            "Patient data management",
            "Personal health record",
            "Security",
            "Security and privacy"
        ],
        "doc_abstract": "Quality medical acts rely on patient medical information. With paper records, the responsibility of gathering the disparate information and making it available to the caregivers, falls exclusively upon the patient. This still is, to great extent, the case with electronic health documents. The consensus is that the advantages of patient involvement in his own health are numerous. With the advent of recent technologies and their deployment in healthcare, new ways of involving the patient and making him an active part of his own health are possible. Electronic Health Records (EHR) and specially Personal Health Records (PHR) are important tools for patient empowerment but data population and management through non-intuitive structured forms is time consuming, takes a great amount of effort, and can be deterring specially for people that are not very computer-oriented. PHRs can be simple and scalable applications that the patient uses to get started and afterwards evolve towards complexity. In any case, compliance with standards must be accomplished. In this paper we present a PHR simple to use, implemented on a USB Flash pen for mobility, and compliant with the openEHR specification. Our model builds on openEHR and adds security and privacy features, allows patient data management and can work as an information repository.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gathering and managing diagnostic tests",
        "doc_scopus_id": "77957827925",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77957827925",
        "doc_date": "2010-10-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Diagnostic tests",
            "Ease of use",
            "Health informations",
            "Medical care",
            "Personal health",
            "Portable device",
            "Privacy",
            "Security"
        ],
        "doc_abstract": "Personal health information is constituted in its greatest part by complementary diagnostic tests which are an important medical aid. This information is generated dispersedly because the patient seeks medical care in many different places over his lifetime. Access to a comprehensive set of a patient's health information is a challenge. It revolves around the patient so any managing scheme must be patient-centric. We took a pragmatic approach to this problem and developed a software standalone platform for secure personal health information storage, namely complementary diagnostic tests, on a portable device for mobility. Simplicity and ease of use were main objectives. A special attention was given to the security aspects associated with storing this kind of information.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Email-P2P gateway to distributed medical imaging repositories",
        "doc_scopus_id": "77956355091",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956355091",
        "doc_date": "2010-09-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Diagnostic tools",
            "DICOM",
            "Hardware and software",
            "Health care professionals",
            "Healthcare institutions",
            "Image communication",
            "Isolated islands",
            "Medical data",
            "Medical images",
            "P2P system",
            "PACS integration",
            "Peer interactions",
            "Peer to peer",
            "Private networks",
            "Research organization",
            "Storage devices",
            "Virtual barrier",
            "Virtual private networks"
        ],
        "doc_abstract": "For the healthcare professionals the importance of the medical imaging as a diagnostic tool is undeniable. For this reason, industry and research organizations increased significantly their interest in the medical imaging area, trying to deliver solutions for creating, storing, exchanging and displaying medical images. The raise of hardware and software solutions drove the community of vendors to gradually decrease the price of his solutions. As consequence, there was a rise of small imaging centres competing with bigger healthcare institutions. The market offers drives the patients to move across a wide range of healthcare institutions to undergo all the necessary exams. Producing a great amount of medical data dispersed over several institutions. This scenario of isolated islands of images repositories unable of interacting with each other is, in our opinion, propitious to a peer-to-peer (P2P) archive solution. Until now, medical exams (images and studies) have been exchanged through analogue films, media storage devices (CD, DVD, etc), virtual private networks or manual email procedures. This paper describes the Dicoogle P2P system, a distributed PAC system where its users may easily store, search and exchange DICOM files. However, potential peers of the Dicoogle system are usually inside private networks, behind NATs and firewalls, disabling the inter-institutional peer interaction. Therefore, we propose an Email-P2P gateway to Dicoogle that offers a way to exchange DICOM files through these virtual barriers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modelling a portable Personal Health Record",
        "doc_scopus_id": "77956310891",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956310891",
        "doc_date": "2010-09-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Direct control",
            "Electronic health record",
            "Health care professionals",
            "Health services",
            "Patient empowerment",
            "Patient health",
            "Personal health",
            "Personal health record",
            "Portable device",
            "Security and privacy issues",
            "Storage hardware"
        ],
        "doc_abstract": "Active and responsible involvement of patients in their own health is accepted as an important contribution towards an increased quality of health services in general. Management of Personal Health Information by the patient can play an important role in the improvement in quality of the information available to health care professionals and as a means of patient involvement. Electronic Health Records are a means of storing this kind of information but their management usually falls under the responsibility of an institution and not on the patient himself. A Personal Health Record under the direct control and management of the patient is the natural solution for the problem. When implemented in a storage hardware portable device, a PHR, allows for total mobility. Personal Health Information is very sensitive in nature so any implementation has to address security and privacy issues. With this in mind we propose a structure for a secure Patient Health Record stored in a USB pen device under the patient's direct management and responsibility.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards an EHR architecture for mobile citizens",
        "doc_scopus_id": "77956306890",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956306890",
        "doc_date": "2010-09-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "EHR",
            "EHR systems",
            "Electronic health record",
            "Health care professionals",
            "Health care providers",
            "Integrated access",
            "Mobile citizens",
            "Patient health",
            "Patient mobility",
            "Personal health",
            "Personal health record",
            "PHR",
            "Security",
            "USB drives"
        ],
        "doc_abstract": "Electronic Health Records are typically created and stored in different places, by different healthcare providers, using different formats and technology. This poses an obstacle to patient mobility and contributes to scatter personal health related information. Patients constantly move between healthcare providers, searching for a better service, lower prices or specialists. It is important that healthcare professionals, regardless of technology and location, have access to the complete patient health record. The access to this personal health record can be granted through a network (web-based, for example) or can be carried by the patient, in a usb drive, for example. Either approach has to enforce the patient consent to access his information, cope with different types of EHR systems and formats. This paper is an ongoing research, part of a PhD on Electronic Health Records for Mobile Citizens.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A proxy of DICOM services",
        "doc_scopus_id": "77953435038",
        "doc_doi": "10.1117/12.843572",
        "doc_eid": "2-s2.0-77953435038",
        "doc_date": "2010-06-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Diagnostic tools",
            "DICOM services",
            "Different domains",
            "Digital imaging",
            "Digital medical images",
            "Healthcare institutions",
            "Image communication",
            "interinstitutional",
            "Local networks",
            "Medical data",
            "Medical decisions",
            "Medical diagnosis",
            "Network address translations",
            "PACS integration",
            "Picture archiving",
            "Service provider",
            "Transport layer security",
            "Virtual bridges"
        ],
        "doc_abstract": "Diagnostic tools supported by digital medical images have increasingly become an essential aid to medical decisions. However, despite its growing importance, Picture Archiving and Communication Systems (PACS) are typically oriented to support a single healthcare institution, and the sharing of medical data across institutions is still a difficult process. This paper describes a proposal to publish and control Digital Imaging Communications in Medicine (DICOM) services in a wide domain composed of several healthcare institutions. The system creates virtual bridges between intranets enabling the exchange, search and store of the medical data within the wide domain. The service provider publishes the DICOM services following a token-based strategy. The token advertisements are public and known by all system users. However, access to the DICOM service is controlled through a role association between an access key and the service. Furthermore, in medical diagnoses, time is a crucial factor. Therefore, our system is a turnkey solution, capable of exchanging medical data across firewalls and Network Address Translation (NAT), avoiding bureaucratic issues with local network security. Security is also an important concern - in any transmission across different domains, data is encrypted by Transport Layer Security (TLS). © 2010 Copyright SPIE - The International Society for Optical Engineering.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design, development, exploitation and assessment of a Cardiology Web PACS",
        "doc_scopus_id": "59149083992",
        "doc_doi": "10.1016/j.cmpb.2008.10.015",
        "doc_eid": "2-s2.0-59149083992",
        "doc_date": "2009-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cardio-PACS",
            "DICOM",
            "PACS",
            "Telework",
            "Web-PACS"
        ],
        "doc_abstract": "Healthcare institutions are increasingly turning to digital medical imaging systems to promote better diagnosis and treatment of their patients. The implementation of the Picture Archiving and Communication System (PACS) clearly contributes to an increase in the productivity of health professionals. However, despite the amount of research that has been done in the past two decades, there are still several technological hurdles that hinder the wide adoption of PACS in the Web environment. In this paper, we present a Web-enabled PACS that through the inclusion of several DICOM services and compression methods promotes medical image availability and greater accessibility to users. © 2008 Elsevier Ireland Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2008-12-30 2008-12-30 2011-01-20T10:46:36 S0169-2607(08)00260-5 S0169260708002605 10.1016/j.cmpb.2008.10.015 S300 S300.1 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20090301 20090331 2009 2008-12-30T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0169-2607 01692607 93 93 3 3 Volume 93, Issue 3 7 273 282 273 282 200903 March 2009 2009-03-01 2009-03-31 2009 Section II. Systems and Programs article fla Copyright © 2008 Elsevier Ireland Ltd. All rights reserved. DESIGNDEVELOPMENTEXPLOITATIONASSESSMENTACARDIOLOGYWEBPACS COSTA C 1 Introduction 2 Methods and materials 2.1 Scenario 2.2 Image compression 2.3 DICOM private transfer syntax 3 Results 3.1 Software engineering 3.2 System workflow 3.3 Integration with a cardiovascular information system 3.4 Web-enabled interface 3.5 Exploitation 4 Discussion 5 Conclusions References HUANG 2004 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS COSTA 2007 S322 S323 C CARS2007INTERNATIONALCONGRESSEXHIBITIONCOMPUTERASSISTEDRADIOLOGYSURGERY2 ENHANCEDPACSSUPPORTDEMANDINGTELEMEDICINETELEWORKSCENARIOS POLONIA 2008 D PROCEEDINGSSPIEVOLUME6919691906MEDICALIMAGING2008 BROKERAGEMECHANISMPROPOSALFORTELERADIOLOGYSTUDIESDISTRIBUTION OTERO 2008 834 841 H DALLESSIO 2007 34 35 K KALDOUDIA 2006 117 127 E SILVA 1998 A PROCEEDINGSSPIEMEDICALIMAGING ACARDIOLOGYORIENTEDPACS COSTA 2004 C PACSIMAGINGINFORMATICSPROCEEDINGSSPIE HIMAGEPACSANEWAPPROACHSTORAGEINTEGRATIONDISTRIBUTIONCARDIOLOGICIMAGES BRENNECKE 2000 1388 1397 R KERENSKY 2000 1370 1379 R TUINENBURG 2000 1380 1387 J UMEDA 2004 1297 1303 A SEGAR 1999 714 719 D KARSON 1996 769 778 T DICOMP 2004 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART5DATASTRUCTURESENCODING ZHANGA 2003 J COMPUTERIZEDMEDICALIMAGINGGRAPHICS PACSWEBBASEDIMAGEDISTRIBUTIONDISPLAY MATSOPOULOS 2004 53 71 G MARCOS 2007 255 269 E DICOMP 2007 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART4SERVICECLASSSPECIFICATIONS BOSWORTH 2003 A XMLSOAPBINARYDATAVERSION10 LEPANTO 2006 92 97 L REINER 2002 22 26 B MACKINNON 2008 796 804 A DICOMSUPL 2004 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMSUPPLEMENT42MPEG2TRANSFERSYNTAX COSTAX2009X273 COSTAX2009X273X282 COSTAX2009X273XC COSTAX2009X273X282XC item S0169-2607(08)00260-5 S0169260708002605 10.1016/j.cmpb.2008.10.015 271322 2011-02-03T13:14:20.981783-05:00 2009-03-01 2009-03-31 true 2647107 MAIN 10 48258 849 656 IMAGE-WEB-PDF 1 gr1 104685 459 620 gr1 11739 162 219 gr2 32220 134 565 gr2 2373 52 219 gr3 67879 406 650 gr3 6824 137 219 gr4 27833 157 317 gr4 8334 109 219 gr5 95323 645 654 gr5 8754 164 166 gr6 75285 323 654 gr6 10920 108 219 gr7 107903 486 562 gr7 13899 163 189 gr8 108388 436 715 gr8 14170 134 219 gr9 103797 550 558 gr9 11491 164 166 gr10 117533 453 753 gr10 14321 132 219 COMM 2858 S0169-2607(08)00260-5 10.1016/j.cmpb.2008.10.015 Elsevier Ireland Ltd Fig. 1 A multimedia container that allows the introduction of a specific codec (MPEG4 in this case) inside the DICOM private transfer syntax. Fig. 2 An example of a DICOM dump: DICOM “default transfer syntax” vs. “private syntax”. Fig. 3 HIMAGE services and processes workflow. Fig. 4 HIMAGE Web components architecture. Fig. 5 HIMAGE main infra-structure inside the CHVNG cardiology department. Fig. 6 Himage Integration Engine (CIS+PACS). Fig. 7 Main interface of the Web HIMAGE. Fig. 8 Web HIMAGE–viewer window (Normal/Compare Mode). Fig. 9 Web HIMAGE–reporting module. Fig. 10 HIMAGE standalone DICOM viewer. Table 1 CHVNG cardiology department—US statistics. Number of procedures (exams) 36,418 Number of images (still+cine-loops) 856,215 Total size 230,984,532,254bytes (215GB) Average procedure size 6.05MB Average file size 263.5kB Average files/procedure 23.5 (majority of cine-loops) Design, development, exploitation and assessment of a Cardiology Web PACS Carlos Costa a ⁎ José L. Oliveira a Augusto Silva a Vasco Gama Ribeiro b José Ribeiro b a University of Aveiro – DETI/IEETA, 3810-193 Aveiro, Portugal b Centro Hospitalar de Vila Nova de Gaia, Portugal ⁎ Corresponding author. Tel.: +351 234370500. Healthcare institutions are increasingly turning to digital medical imaging systems to promote better diagnosis and treatment of their patients. The implementation of the Picture Archiving and Communication System (PACS) clearly contributes to an increase in the productivity of health professionals. However, despite the amount of research that has been done in the past two decades, there are still several technological hurdles that hinder the wide adoption of PACS in the Web environment. In this paper, we present a Web-enabled PACS that through the inclusion of several DICOM services and compression methods promotes medical image availability and greater accessibility to users. Keywords PACS DICOM Cardio-PACS Web-PACS Telemedicine Telework 1 Introduction A Picture Archiving and Communication System (PACS) is one of the most valuable tools supporting the medical profession both in decision-making and during treatment procedures. It encompasses several technologies that are used in the acquisition, archiving, distribution, and visualization of digital medical images [1]. The Digital Imaging and Communications in Medicine standard (DICOM) [2] was a major contribution to the facilitated exchange of structured medical imaging data, and is now a key component in PACS's success. Currently, almost all medical imaging equipment manufacturers provide DICOM digital output in their products. The deployment of PACS has enabled faster and broader access to medical image data. The movement from film-based processes to digital processes, allied to faster and more robust network infrastructure, reduced the costs associated with the storage and management of images, simplified data portability, and has paved the way for the development of new applications and working scenarios. A significant benefit of digital medical imaging is its availability, both within and between health institutions. Together with a new suite of Internet Protocol (IP)-based applications, such as IP phone and teleconference, PACS presents a tremendous opportunity for the introduction of telemedicine, telework, and collaborative teams, which will help to bridge frontiers and overcome the medical resource asymmetries found in several regions [3,4]. However, several problematic issues still remain, most of them related to the huge volume of data, and to the lack of interoperability between distinct PACS products. For instance, dynamic image modalities (films) such as cardiac ultrasound (US) and X-ray angiography (XA) typically generate hundreds of MB of data for each study. To keep these exams permanently available to practitioners it is necessary to create robust and efficient storage and communication infrastructures. There are no significant technical differences between the PACS used in cardiology and in radiology areas. However, they are often separate systems that tackle distinct user needs, namely related to visualization and reporting protocols [5]. Since almost all systems are DICOM compliant, it is possible from a radiology workstation to query and retrieve images located in a cardiology storage server, and vice-versa. The handicap is that an integrated system can lead to a more efficient workflow and improved access to images and data [6,7]. This paper presents a Web-enabled PACS framework, entitled HIMAGE, which is specially designed to support demanding cardiac imaging laboratories. HIMAGE enables the acquisition, storage, transmission, and visualization of DICOM cardiovascular sequences, providing a cost-efficient digital archive. The core of our approach is the implementation of a private DICOM transfer syntax that supports any video encoder that best suits the specificities of a particular imaging modality or working scenario. The major advantage of the proposed system stems from the high compression rate achieved by video encoding, while still maintaining diagnostic quality in the medical image sequences. This approach ensures full online availability of the studies and simplifies the transmission of medical data over the Internet 2 Methods and materials 2.1 Scenario HIMAGE was developed in the CHVNG cardiology department (Centro Hospitalar de Vila Nova de Gaia – Hospital of Gaia), which is supported by two imaging laboratories. The first implementation of HIMAGE was based on JPEG Baseline (Joint Photographic Experts Group) [8]. With this system, an echocardiography (US) study typically generates data between 20 and 30MB and an angiographic (XA) study between 40 and 60MB, depending on the technical characteristics of the equipment, on the operator expertise, and on the procedure type [9]. Although this volume can be easily transferred inside an institution Intranet or through the Internet, the latency, especially in the latter case, may be a drawback for some scenarios. One possible solution can be the exploitation of lossy compression techniques that still preserve the diagnostic quality of the existent JPEG solutions. Over the years, several studies have investigated the impact of various compression ratios on image quality, and on the observer performance for several diagnostic tasks. The general trend shown by the results of these studies is that optimal compression ratios for a modality are rarely achieved, as the observer's performance is tightly coupled with each particular diagnostic task. Currently, guidelines are published by international scientific and professional organizations, recommending suitable algorithms and compression ratios for a broad range of diagnostic tasks (see [10–12] for XA and [13–15] for US). 2.2 Image compression In cardiology, the majority of the data produced in the XA and US procedures is related to cine-loop sequences, which have a significant time-space redundancy, especially in ultrasound. Because the JPEG-based DICOM compression algorithms only explore the intra-frame image redundancy (space) [16], we decided to investigate the utilization of a more powerful video encoder that could also contemplate the inter-frame redundancy (time). In general, distinct digital video codecs produce significantly dissimilar results and medical image sequence types (modalities, cine/still, color/grayscale, etc.). This occurs because distinct sequences contain distinct kinds of information (motion, noise, etc.). As a result, it is not possible to select the very best among several codecs, to use for compression of all types of image/video. In the particular case of US cardiovascular images, after several trials and result evaluations with dynamic encoders [9], we realized that Moving Picture Experts Group (MPEG4) was the coding standard that provided the best tradeoff between image quality and storage requirements. The emergence of MPEG4 [17] as a coding standard for multimedia data with object-based and other enhanced encoding facilities appears to be a good alternative for the cost-effective storage and transmission of cardiac digital cine-loop sequences. With this new codec, a typical US file rarely exceeds 200–300kB [9]. At this order of magnitude, it is already feasible to envisage a pure online archive solution capable of handling all the registered procedures whatever their clinical or epidemiological life-cycle may be. Another immediate consequence of our encoding approach is that the reduced transmission times, either in Intranet mode or in Internet mode, may now be considered, in the worst cases, minor drawbacks in the overall imaging workflow. 2.3 DICOM private transfer syntax Since MPEG4 is not a DICOM native coding schema, subsequent image transmission, decoding and reviewing is accomplished through a specifically designed DICOM private transfer syntax. Moreover, to ensure that HIMAGE has the flexibility to support other modalities/encoders, it was decided not to insert the MPEG4 directly in the Tag Length Value (TLV) of the DICOM data structure [18]. Instead, we developed a multimedia container that dynamically supports several encoders (Fig. 1 ). The container includes a field that stores the encoder ID code, which is similar to the field used by the Audio Video Interleave (AVI) RIFF headers [19]. This approach represents the best modular software solution, as we simply need to change a single parameter in the HIMAGE conversion engine if a more efficient codec is developed. In Fig. 2 it is possible to compare parts of the DICOM representation of the “default transfer syntax” against our “private syntax,” in the specific case of a US 25-frame image sequence, with a RGB 576*768 matrix. Two important aspects are noticeable. First, the DICOM transfer syntax identifier is changed (from DefaultTransferSyntaxUID to PrivateTransferSyntaxUID). Second, the “PixelData” field size is reduced 120 times (from 33177600 to 275968). 3 Results 3.1 Software engineering In the initial versions of HIMAGE, the main objective was to reduce data volume through the implementation of a private transfer syntax that was capable of supporting multiple video encoders. In the current version herein described, another attained goal is the provisioning of a Web-based PACS that can be accessed easily and securely through a common Web browser with no further need for complex local installation of software. Web solutions can be created using one of the current available architectures [20–22]. Our approach is based on the .NET framework, which allows for smooth integration with existing code (in C, C++ and Visual Basic). All of the core functions used to manipulate the images and the DICOM structures were written in C++ and packed as Dynamic-link Library (DLL) components. We have developed a DICOM C++ SDK (Software Development Kit) that allows manipulation of the DICOM persistent object (i.e. structured files) and implements several HIMAGE (DICOM) network services such as Storage, Query/Retrieve and Modality Worklist (Fig. 3 ) [23]. The graphical components supporting all image-related tasks (visualization, reporting, etc.) were developed as Visual Basic plugins (ActiveX) and are directly embedded in the ASPX .NET pages (Fig. 4 ). The ActiveX viewer allows the direct integration of private DICOM files with the Web content. The communication between the dynamic HTML contents and the ActiveX binary is performed by JavaScript code. To be visualized or processed, the study images must be downloaded from the server to the client platform. All DICOM persistent object transfers (retrieves) are supported by a Web Service developed in C#. Though the Web Services are actually using XML-SOAP (Extensible Markup Language-Simple Object Access Protocol) [24] to transfer data, the XML [25] does not easily handle embedded binary data, which can create problems for DICOM file transfer. There are several issues, such as memory size and computation costs, that are associated with the conversion of binary data into base64 format (which is treated as a string). This encoding schema (base64) typically increases the original object size by 33% and also increases the processing time relative to binary-text-binary conversion [26]. To bypass this XML-SOAP drawback, it was necessary to develop a parallel gateway channel (HTTP encapsulated) to transfer DICOM files in a more efficient way. 3.2 System workflow Our clinical facility (CHVNG) is equipped with nine echocardiography machines distributed over three geographically dispersed hospital units. They have standard DICOM3.0 output interfaces and a configuration that ensures an automatic DICOM SCU (client) storage service at the end of each medical procedure. Daily output to the network typically reaches approximately 1200 DICOM files (both still and cine-loops). The ultrasound image data is sent to the Acquisition Processing Unit (APU) in DICOM default transfer syntax, i.e. uncompressed format (Fig. 5 ). The received procedures are analyzed (the alphanumeric data is extracted from DICOM headers) to detect eventual exam/patient ID errors and, if format conditions are verified, the exam is added to HIMAGE with the image data compressed and embedded in a new DICOM private syntax file. The result is then passed into the “Storage Server” that makes it permanently available to the PACS users. The original raw images are also saved, but they are only kept online for 6 months. The developed client application consists of a Web DICOM viewer (Fig. 7) that handles medical images and films available in the HIMAGE database, formatted in standard DICOM or in DICOM extended with our private syntax. Since the HIMAGE client solution is completely developed in Web technology, the access to the exams is controlled by appropriate security rules. Authentication is performed through a username/password pair and communication security is assured by an HTTPS connection. 3.3 Integration with a cardiovascular information system Implementation of an integrated healthcare access interface to patient clinical data represents the core element to accomplish new healthcare services with improved quality and efficiency. After the emergence of PACS as a fundamental infrastructure of any digital imaging department, the focus turned to the possibilities of integrating the medical image with other sources of information. Cardiologists need structured reports including images and related patient information. In our CHVNG scenario, a commercial Cardiovascular Information System (CIS) handles this additional information. Integration of the PACS with the existing CIS is an important feature for the healthcare professional. Besides the interface conformance, on network and application protocols, it is crucial to ensure data consistency among several systems. To implement this requirement, HIMAGE provides a DICOM Modality Worklist SCP service that is connected to the CIS database. It enables modalities (using C-FIND command) to query the server for patient and study-related information that will be thereafter inserted in the DICOM header. This process ensures consistency between both information systems and also that all PACS studies can be correctly referenced from the CIS. Concerning integrated access to information, the HIMAGE provides smooth integration of patient image data in the CIS environment. The inclusion of PACS images in other environments is easy due to DICOM private transfer syntax container portability. A “Himage Integration Engine” module (Fig. 6 ) was developed with two integration options: a. Multimedia AVI container: the engine can extract the MPEG4 encoded image data from the DICOM and encapsulate it in an AVI file to be, for instance, inserted in the CIS Web document; b. Plug-in module: an ActiveX application viewer is available, allowing the direct integration of the HIMAGE DICOM files in both Winforms and Web environments. This plug-in downloads the image data from HIMAGE archive and displays it. 3.4 Web-enabled interface During the last 5 years, the ubiquity of Web interfaces has pushed practically all PACS suppliers to develop client applications in which clinical practitioners can receive and analyze medical images using conventional personal computers and Web browsers [20,21]. Because of security and performance issues, use of these software packages has mostly been restricted to Intranets. Paradoxically, one of the most important advantages of digital imaging systems was to enable the widespread sharing and remote access to medical data between healthcare institutions. The HIMAGE Web version is fully operational, provides all the necessary functionality, and has the same performance and flexibility as the previous desktop version [9]. The application setup is very simple. It is downloaded from the Web server and is automatically installed when given explicit authorization by the user. Graphically, the HIMAGE main window includes a grid box with a list of patients and a movie preview of three sequences for the current selection (Fig. 7 ). The user can search all procedures using criteria such as patient name, patient ID, procedure ID/type/date, source institutions and equipment. A second graphical application layer provides interfaces to the system modules: communications to telecardiology, DICOM viewer (Fig. 8 ), report (Fig. 9 ) and export. In the DICOM viewer window (Fig. 8), it is possible to visualize the image sequences (still and cine-loops) and to select frames from various sequences that are then sent to the report area. Other traditional functions have been included such as image manipulation tools (contrast/brightness), printing capability, and the export of images in distinct formats (DICOM3.0 default transfer syntax, AVI, BMP, JPEG). It is also possible to copy a specific image to the clipboard and paste it onto some other external application. In the compare mode, the viewer window can work in either of two ways: automatic or manual. The automatic mode is used for a specific type of procedure—the “Stress eco” (Fig. 8). In this case, the visualization technique is based on the information stored in the DICOM file headers. HIMAGE provides a simultaneous and synchronized display of the several “Stages” of a heart “View.” The term “Stage” is defined as a phase in the stress echo exam protocol. The “View” is the result of a particular combination of the transducer position and orientation at the time of the image acquisition. Manual mode allows the user to select among distinct images of the same and/or different procedures that can then be displayed in a defined window matrix. In the report module (Fig. 9), the user can arrange the location of the image(s) or delete some frames using a drag-and-drop functionality. Finally, the output image matrix (2×3 or 3×3) is bundled with the clinical report to generate an Rich Text Format (RTF) file that is compatible with common text editors. The user can customize the base template used to generate the report file. For example, one could include the institution logo and report headers. The export module allows the procedure to be saved in a CDROM or DVD-ROM, using the uncompressed DICOM default transfer syntax format or the AVI format. A standalone fully compliant DICOM viewer application is also stored (Fig. 10 ), which starts automatically whenever the disk is used. 3.5 Exploitation The HIMAGE is installed in two central hospitals and three small diagnostic centers. Our main installation and research laboratory is the CHVNG cardiology department with approximately 65,000 patient records. In Table 1 it is possible to observe some general statistics relative to the CHVNG US data. Concerning transmission times, the Web HIMAGE images are downloaded in packs of 3 sequences (due to preview mode) based on the user selection. In a telework environment, supported by a 4Mbits ADSL (Asymmetric Digital Subscriber Line), a complete pack of 3 cine-loops takes typically less then 10s to download, decompress and display, including the overhead introduced with an encrypted SSL (Secure Sockets Layer) channel. 4 Discussion We have described a Web-enabled PACS software solution that is specially oriented to support demanding cardiac imaging laboratories. It provides a cost-efficient digital archive, ensures full online availability of studies and simplifies the transmission of medical data over the Internet. The benefits obtained from the availability of all historical image data in a simple, fast and integrated Web interface are unquestionable to practitioners and to patients, and are likely to induce a significant improvement in the overall quality of healthcare services. In our main installation, two years after the introduction of HIMAGE in the echocardiography laboratory, and maintaining the human resources, one realize an increase of procedures/year from 4000 to 7000, which is a good indication considering other reported results of PACS productivity improvements [27–29]. Although this result can be explained by several factors, to an increase on patient demand and on optimized workflow process, an informal and continuous assessment performed near the physicians suggest that HIMAGE was a major driving to this change: (a) the dead times and the information handling mistakes were considerably reduced; (b) since HIMAGE is ubiquitously available, the remote diagnostic can be easily performed without moving the patient or his records between several buildings; (c) the patient history (CIS reports and PACS images) is permanently available, which avoids the need for CD or other media storage manual handling. One main HIMAGE advantage is associated with its transfer rate efficiency. Healthcare professionals do not adopt telemedicine or telework platforms if they need to wait, for instance, 2–3h to receive/download a clinical image study with diagnostic quality. The advantage of the proposed system stems from the implementation of a private DICOM transfer syntax that supports any video encoder. In the cardiac US we are using a MPEG4 codec that provides high compression and maintains diagnostic quality. Qualitative assessments have been made of the previous HIMAGE Desktop version (no Web interface) [9]. In a simultaneous and blind display of the original against the compressed cine-loops, 37% of trials have selected the compressed sequence as the best image. This suggests that other factors related to visualization conditions are more likely to influence observer accuracy than the image compression itself. The interoperability of HIMAGE with other DICOM PACS implies the decompression of private MPEG4 images to an uncompressed normalized format like, for instance, the DICOM Default Transfer Syntax. However, we believe that, in the future, MPEG4 will be adopted by the DICOM standard that currently only supports MPEG-2 [30]. Finally, a major constraint of the presented solution is its dependency on Windows and on Internet Explorer browser. This has to do with the richness of the interface that requires a powerful platform to work (like ActiveX or Flash). The result will not be possible using pure web2.0 technologies. This decision provides a Web interface supporting all the requirements of a DICOM viewer. In the future, it is planned to update the HIMAGE platform to Microsoft .NET Silverlight technology, which is multi operating system compliant. 5 Conclusions In this paper, we have presented the Web HIMAGE software, a DICOM conformant Web PACS. Reducing the size of exam images, while preserving the diagnostic quality, is a major novel feature of this HIMAGE application. This is especially important for ensuring time-effective transmission through Internet connections. We successfully demonstrated the utility of HIMAGE (started with the HIMAGE desktop version) in a telemedicine project established between CHVNG (Portugal) and the Central Hospital of Maputo (Mozambique). The Web version of HIMAGE is currently being used in clinical environments with different requirements, from small private laboratories to public central hospitals. In the main installation, (CHVNG), we have made more than 36,000 US procedures permanently available through this system (850,000 cardiovascular digital sequences). References [1] H.K. Huang PACS and Imaging Informatics: Basic Principles and Applications 2004 Wiley [2] DICOM, Digital Imaging and Communications in Medicine version 3.0, ACR (the American College of Radiology) and NEMA (the National Electrical Manufacturers Association), [3] C. Costa J.L. Oliveira A. Silva V. Gama J. Ribeiro Enhanced PACS to support demanding telemedicine and telework scenarios CARS 2007-International Congress and Exhibition: Computer Assisted Radiology and Surgery 2 2007 S322 S323 [4] D. Polónia A. Silva C. Costa J.L. Oliveira Brokerage mechanism proposal for teleradiology studies distribution P.A. Katherine M.S. Khan Proceedings of SPIE-Volume 6919, 691906. Medical Imaging 2008 2008 PACS and Imaging Informatics San Diego, USA [5] H.J. Otero L. Nallamshetty F.J. Rybicki Interdepartmental conflict management and negotiation in cardiovascular imaging Journal of the American College of Radiology 5 2008 834 841 [6] K.M. Dallessio Integrating cardiology and radiology PACS Applied Radiology 36 2007 34 35 [7] E. Kaldoudia D. Karaiskakisb A service based approach for medical image distribution in healthcare Intranets Computer Methods and Programs in Biomedicine 81 2006 117 127 [8] A. Silva C. Costa P. Abrantes V. Gama A. Boer A cardiology oriented PACS Proceedings of SPIE: Medical Imaging San Diego, USA 1998 [9] C. Costa A. Silva J.L. Oliveira V. Ribeiro J. Ribeiro Himage PACS: a new approach to storage, integration and distribution of cardiologic images PACS and Imaging Informatics-Proceedings of SPIE San Diego, CA, USA 2004 [10] R.U. Brennecke U. Burgel R.U. Simon G. Rippin H.P. Fritsch T. Becker S.E. Nissen American College of Cardiology/European Society of Cardiology international study of angiographic data compression phase III: measurement of image quality differences at varying levels of data compression Journal of the American College of Cardiology 35 2000 1388 1397 [11] R.A. Kerensky J.T. Cusma P. Kubilis R.U. Simon T.M. Bashore J.W. Hirshfeld Jr. D.R. Holmes Jr. C.J. Pepine S.E. Nissen American College of Cardiology/European Society of Cardiology international study of angiographic data compression phase I: the effects of lossy data compression on recognition of diagnostic features in digital coronary angiography Journal of the American College of Cardiology 35 2000 1370 1379 [12] J.C. Tuinenburg G. Koning E. Hekking A.H. Zwinderman T. Becker R.U. Simon J.H.C. Reiber American College of Cardiology/European Society of Cardiology international study of angiographic data compression phase II: the effects of varying JPEG data compression levels on the quantitative assessment of the degree of stenosis in digital coronary angiography Journal of the American College of Cardiology 35 2000 1380 1387 [13] A. Umeda Y. Iwata Y. Okada M. Shimada A. Baba Y. Minatogawa T. Yamada M. Chino T. Watanabe M. Akaishi A low-cost digital filing system for echocardiography data with MPEG4 compression and its application to remote diagnosis Journal of the American Society of Echocardiography 17 2004 1297 1303 [14] D.S. Segar D. Skolnick S.G. Sawada G. Fitch D. Wagner D. Adams H. Feigenbaum A comparison of the interpretation of digitized and videotape recorded echocardiogram Journal of the American Society of Echocardiography 12 1999 714 719 [15] T.H. Karson R.C. Zepp S. Chandra A. Morchead J.D. Thomas Digital storage of echocardiograms offers superior image quality to analog storage, even with 20:1 digital compression: results of the digital echo record access study Journal of the American Society of Echocardiography 9 1996 769 778 [16] Joint Photographic Experts Group, JPEG standard (ITU-T T.81 | ISO/IEC 10918-1), 1994. [17] ISO/IEC Moving Picture Experts Group, MPEG-4 standard (ISO/IEC 14496), 1999. [18] DICOM-P5 Digital Imaging and Communications in Medicine (DICOM), Part 5: Data Structures and Encoding 2004 National Electrical Manufacturers Association [19] IBM Corporation and Microsoft Corporation, Multimedia Programming Interface and Data Specifications 1.0, 2001 (cited, available from: [20] J. Zhanga J. Suna J.N. Stahl PACS and Web-based image distribution and display Computerized Medical Imaging and Graphics vol. 27 2003 Elsevier pp. 197–206 [21] G.K. Matsopoulos V. Kouloulias P. Asvestas N. Mouravliansky K. Delibasis D. Demetriades MITIS: a WWW-based medical system for managing and processing gynecological–obstetrical–radiological data Computer Methods and Programs in Biomedicine 76 2004 53 71 [22] E. Marcos C.J. Acuña B. Vela J.M. Cavero J.A. Hernández A database for medical image management Computer Methods and Programs in Biomedicine 86 2007 255 269 [23] DICOM-P4 Digital Imaging and Communications in Medicine (DICOM), Part 4: Service Class Specifications 2007 National Electrical Manufacturers Association [24] World Wide Web Consortium (W3C), Web Services Architecture. 2004, W3C Working Group Note 11, 2004 (available from [25] World Wide Web Consortium (W3C), Extensible Markup Language (XML) 1.0, 4th ed., 2006, W3C Recommendation (available from [26] A. Bosworth D. Box M. Gudgin M. Nottingham D. Orchard J. Schlimmer XML, SOAP and Binary Data—Version 1.0 2003 BEA Systems, Microsoft Corporation [27] L. Lepanto G. Pare D. Aubry P. Robillard J. Lesage Impact of PACS on dictation turnaround time and productivity Journal of Digital Imaging 19 2006 92 97 [28] B. Reiner E. Siegel M. Scanlon Changes in technologist productivity with implementation of an enterprisewide PACS Journal of Digital Imaging 15 2002 22 26 [29] A.D. Mackinnon R.A. Billington E.J. Adam D.D. Dundas U. Patel Picture archiving and communication systems lead to sustained improvements in reporting times and productivity: results of a 5-year audit Clinical Radiology 63 2008 796 804 [30] DICOM-SUPL42 Digital Imaging and Communications in Medicine (DICOM), Supplement 42: MPEG2 Transfer Syntax 2004 National Electrical Manufacturers Association "
    },
    {
        "doc_title": "Indexing and retrieving DICOM data in disperse and unstructured archives",
        "doc_scopus_id": "63149146090",
        "doc_doi": "10.1007/s11548-008-0269-7",
        "doc_eid": "2-s2.0-63149146090",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objective: This paper proposes an indexing and retrieval solution to gather information from distributed DICOM documents by allowing searches and access to the virtual data repository using a Google-like process. Methods and materials: The medical imaging modalities are becoming more powerful and less expensive. The result is the proliferation of equipment acquisition by imaging centers, including the small ones. With this dispersion of data, it is not easy to take advantage of all the information that can be retrieved from these studies. Furthermore, many of these small centers do not have large enough requirements to justify the acquisition of a traditional PACS. Results: A peer-to-peer PACS platform to index and query DICOM files over a set of distributed repositories that are logically viewed as a single federated unit. The solution is based on a public domain document-indexing engine and extends traditional PACS query and retrieval mechanisms. Conclusion: This proposal deals well with complex searching requirements, from a single desktop environment to distributed scenarios. The solution performance and robustness were demonstrated in trials. The characteristics of presented PACS platform make it particularly important for small institutions, including educational and research groups. © CARS 2008.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Alternative lossless compression algorithms in X-ray cardiac images",
        "doc_scopus_id": "60749130789",
        "doc_doi": null,
        "doc_eid": "2-s2.0-60749130789",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Cardiac images",
            "Digital imaging and communications in medicines",
            "Digital medical images",
            "Digital medical imaging systems",
            "Healthcare institutions",
            "Image compression algorithms",
            "Image datum",
            "Lossless",
            "Lossless compression algorithms",
            "Lossless compressions",
            "Medical datum",
            "Medical professions",
            "Medical studies",
            "Picture archiving",
            "Remote access",
            "Storage costs",
            "Storage efficiencies",
            "Time redundancies",
            "Transmission performance",
            "Transmission speed",
            "Workgroups"
        ],
        "doc_abstract": "Over the last decade, the use of digital medical imaging systems has increased greatly in healthcare institutions. Today, Picture Archiving and Communication System (PACS) is one of the most valuable tools supporting medical profession in both decision making and treatment procedures. It reduced the costs associated with the storage and management of image data and also increased both the intra and inter-institutional portability of data. One of the most important benefits of the digital medical image is that it allows the widespread sharing and remote access to medical data by outside institutions. PACS presents an opportunity to improve cooperative workgroups taking place either within or with other healthcare institutions. Storage and transmissions costs are continuously decreasing, but, as individual digital medical studies become significantly larger, further improvements on transmission performance and on storage efficiency are critical. Image compression algorithms offer the means to reduce storage cost and to increase transmission speed. Following previous methodologies [1], this paper provides a comparison about the application of DICOM (Digital Imaging and Communications in Medicine) lossless compression standards on Angiography cine acquisition images. A new lossless compression approach that exploits time redundancy between successive frames is also presented. Finally, the standards codecs values are compared with results obtained with the proposed method and with video lossless codecs. © 2008 Taylor & Francis Group,.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Brokerage mechanism proposal for teleradiology studies distribution",
        "doc_scopus_id": "42949109165",
        "doc_doi": "10.1117/12.770753",
        "doc_eid": "2-s2.0-42949109165",
        "doc_date": "2008-05-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Teleradiology",
            "Workflow optimization"
        ],
        "doc_abstract": "The asymmetric distribution of PACS equipment and service providers across countries leads typically to the need to hire third party service professionals outside the institutions where the exams were made. In this paper we present a brokerage mechanism that puts customers and remote providers together in a seamless way. The proposed solution, asserted with a case study for the Portuguese national health system, addresses the problems that now impair the optimal provision of those services, enabling a more agile relationship between buyers and sellers, optimizing administrative work and complying with clinical and legal requirements under discussion in the European Union for the free movement of patients and professional health workers. In this document, the detailed process and technical description of the broker functioning is made, and the main benefits for the participants are also evaluated from a technical and economical point of view. Finally, in the discussion chapter, an assessment of the creation of a spot market for imaging studies is made and the integration with other similar markets is discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web platform to support the share and remote access to medical images",
        "doc_scopus_id": "67650261376",
        "doc_doi": null,
        "doc_eid": "2-s2.0-67650261376",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Image network",
            "Medical images",
            "PACS",
            "WADO"
        ],
        "doc_abstract": "The production of digital medical images has been growing in every healthcare institution, representing nowadays one the most valuable tools supporting the medical decision process and treatment procedures. One of the most important advantages of these digital systems is to simplifr the widespread sharing and remote access of medical data between healthcare institutions. However, due to security and performance issues, the usage of these software packages has been restricted to Intranets. In general, the storage and transmission of digital medical image is based on the international DICOM standard and PACS systems. This paper analyses the traditional PACS communication limitations that contribute to their reduced usage in the Internet. It is also proposed an architecture, based on Webservices and encapsulation of DICOM objects in HTTP, to enable trans-institutional medical data transfers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A model to optimize the use of imaging equipment and human skills scattered in very large geographical areas",
        "doc_scopus_id": "67650240159",
        "doc_doi": null,
        "doc_eid": "2-s2.0-67650240159",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "CSCW",
            "Health information systems",
            "Radiology information systems",
            "Teleradiology",
            "Ubiquitous systems"
        ],
        "doc_abstract": "Recent studies have shown that the good geographical coverage of Imagiologic Information Systems and equipment such as Picture Archiving and Communication Systems (PACS) is not matched by similar coverage levels of radiologists, especially in rural and academic health institutions. In this paper, we address this problem proposing a solution that is twofold, with the first one being process based, through the optimization of work assignment within pools of human resources according to Service Providers availability, and the second part being technology based, through the interconnection of all the health institutions PACS equipment and radiologists geographically dispersed. After describing the high level solution, we present some of the results of the implementation of this concept and some of the technical challenges still to overcome. Finally, the conclusion chapter presents the impact of the system in the involved institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Current perspectives on PACS and a cardiology case study",
        "doc_scopus_id": "34250214449",
        "doc_doi": "10.1007/978-3-540-72375-2_5",
        "doc_eid": "2-s2.0-34250214449",
        "doc_date": "2007-06-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Since the first experiments on digital medical imaging, Pictures Archiving and Communication Systems (PACS) have been gaining acceptance along healthcare practitioners. PACS based infrastructures are currently being driven by powerful medical applications that rely completely on the seamless access to images' databases and related metadata. New and demanding applications such as study co-registration and content based retrieval are already driving PACS into new prominent roles. In this chapter we will revise the major key factors that have promoted this technology. We will then present our own solution for a Web-based PACS and the results achieved by its use on a Cardiology Department. We will finally consider future applications that are pushing developmental research in this field. © 2007 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A demanding Web-based PACS supported by Web Services technology",
        "doc_scopus_id": "33745410012",
        "doc_doi": "10.1117/12.653935",
        "doc_eid": "2-s2.0-33745410012",
        "doc_date": "2006-06-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Encoding syntax",
            "PACS",
            "Web Services technology"
        ],
        "doc_abstract": "During the last years, the ubiquity of web interfaces have pushed practically all PACS suppliers to develop client applications in which clinical practitioners can receive and analyze medical images, using conventional personal computers and Web browsers. However, due to security and performance issues, the utilization of these software packages has been restricted to Intranets. Paradigmatically, one of the most important advantages of digital image systems is to simplify the widespread sharing and remote access of medical data between healthcare institutions. This paper analyses the traditional PACS drawbacks that contribute to their reduced usage in the Internet and describes a PACS based on Web Services technology that supports a customized DICOM encoding syntax and a specific compression scheme providing all historical patient data in a unique Web interface.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Architecture evaluation for the implementation of a regional integrated electronic health record",
        "doc_scopus_id": "84886915633",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84886915633",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Architecture evaluation",
            "Communication infrastructure",
            "Health care information system",
            "Health information systems",
            "Health planning",
            "Health systems",
            "Medical record",
            "Organizational perspectives",
            "Communication",
            "Electronic Health Records",
            "Health Information Systems",
            "Medical Records Systems, Computerized",
            "Systems Integration"
        ],
        "doc_abstract": "The interconnection between different healthcare information systems is not yet a trivial task. Solid communication infrastructures do exist, solid solutions for Health Information Systems (HIS) are installed, but, unfortunately, too many different solutions hinder the integration of HIS at a regional, national or European level. In this paper we propose a solution and an implementation of an Integrated Electronic Health Record that is a composition of two distinct integration models - centralized and distributed. We exploit this solution against a set of predefined users and institutional requirements, at a regional level in two Portuguese regions. As a conclusion, an evaluation of the cost and inherent benefits is made involving the clinical, economical and organizational perspectives.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data management and visualization issues in a fully digital echocardiography laboratory",
        "doc_scopus_id": "33745324967",
        "doc_doi": "10.1007/11573067_2",
        "doc_eid": "2-s2.0-33745324967",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Digital echocardiography laboratory",
            "Imaging modality",
            "Ultrasound sequences",
            "Visualization issues"
        ],
        "doc_abstract": "This paper presents a PACS solution for echocardiography laboratories, denominated as Himage, that provides a cost-efficient digital archive, and enables the acquisition, storage, transmission and visualization of DICOM cardiovascular ultrasound sequences. The core of our approach is the implementation of a DICOM private transfer syntax designed to support any video encoder installed on the operating system. This structure provides great flexibility concerning the selection of an encoder that best suits the specifics of a particular imaging modality or working scenario. The major advantage of the proposed system stems from the high compression rate achieved by video encoding the ultrasound sequences at a proven diagnostic quality. This highly efficient encoding process ensures full online availability of the ultrasound studies and, at the same time, enables medical data transmission over low-bandwidth channels that are often encountered in long range telemedicine sessions. We herein propose an imaging solution that embeds a Web framework with a set of DICOM services for image visualization and manipulation, which, so far, have been traditionally restricted to intranet environments. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Himage PACS: A new approach to storage, integration and distribution of cardiologie images",
        "doc_scopus_id": "12144281711",
        "doc_doi": "10.1117/12.534135",
        "doc_eid": "2-s2.0-12144281711",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Cardiology images",
            "DICOM",
            "HIS",
            "PACS",
            "Patient data integration"
        ],
        "doc_abstract": "This paper presents a Cardiology oriented information system that provides permanent availability of all clinical history, including alphanumeric and image data, with time and cost-effective transmission (reduced download time), without loss of image diagnosis quality and based on a Web Multimedia Integrated Access Interface. This implies the integration of HIS and PACS in a unique access interface, providing on-line and fast access to authorized healthcare professionals. The benefits obtained from the HIS-PACS integration and from the availability of all historical patient data are unquestionable to practitioners but also to the patients. Moreover, the system includes a telematic platform capable of establishing cooperative telemedicine sessions where our most impressive utilization is a transcontinental work platform for cardiovascular ultrasound. The key point of our approach starts with the construction of a DICOM private transfer syntax that is prepared to support any video encoder installed on a Windows-based station. With this structure it is possible to select the best encoder to a specific modality and work scenario. Good trade-off between compression ratio and diagnostic quality, low network traffic load, backup facilities and data portability are other achievements of this system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "E-services in mission-critical organizations: Identification enforcement",
        "doc_scopus_id": "8444230475",
        "doc_doi": null,
        "doc_eid": "2-s2.0-8444230475",
        "doc_date": "2004-11-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Authentication and identification",
            "Digital credentials",
            "E-sevices security",
            "Intranet scenarios"
        ],
        "doc_abstract": "The increasing dependency of enterprise on IT has rise up major concerns on security technology and procedures. Access control mechanisms, which are the core of most security policies, are mostly based on PIN and, some times, in Public Key Cryptography (PKC). Despite these techniques can be already broadly disseminated, the storage and retrieval of security secrets is yet a sensitive and open issue for organization and users. One possible solution can be provided by the utilization of smart cards to store digital certificates and private keys. However, there are special organizations where even this solution does not solve the security problems. When users deal with sensible data and it is mandatory to prevent the delegation of access privileges to third persons new solutions must be provided. In this case the access to the secrets can be enforced by a three-factor scheme: the possession of the token, the knowledge of a PIN code and the fingerprint validation. This paper presents a Professional Information Card system that dynamically combines biometrics with PKC technology to assure a stronger authentication that can be used indistinctly in Internet and Intranet scenarios. The system was designed to fulfill current mission-critical enterprises access control requirements, and was deployed, as a proof of concept, in a Healthcare Information System of a major Portuguese Hospital.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An integrated access interface to multimedia EPR",
        "doc_scopus_id": "79952568682",
        "doc_doi": "10.1016/S0531-5131(03)00258-9",
        "doc_eid": "2-s2.0-79952568682",
        "doc_date": "2003-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper describes a multi-platform interface system that provides integrated access to a distributed Electronic Patient Record (EPR). Information associated with this kind of EPR may be stored either at the wide level of the Healthcare Information Systems (HIS) or at a more specific instance such as a departmental PACS. The system is supported by strong authentication and access control mechanisms, ensuring privacy, confidentiality and no-repudiation of clinical acts, providing the adequate privileges in the patient data handling. © 2003, Elsevier Science B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272802 291210 291682 291735 291857 291889 291929 31 International Congress Series INTERNATIONALCONGRESSSERIES 2003-05-28 2003-05-28 2010-11-13T17:30:03 S0531-5131(03)00258-9 S0531513103002589 10.1016/S0531-5131(03)00258-9 S300 S300.2 FULL-TEXT 2015-05-14T08:23:58.562302-04:00 0 0 20030601 20030630 2003 2003-05-28T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confdate confeditor confloc contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl vol volfirst volissue figure body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0531-5131 05315131 1256 1256 C Volume 1256 145 880 886 880 886 200306 June 2003 2003-06-01 2003-06-30 2003 CARS 2003. Computer Assisted Radiology and Surgery. Proceedings of the 17th International Congress and Exhibition London, UK 25–28 June 2003 20030625 20030628 H. U. Lemke, M.W. Vannier, K. Inamura, A.G. Farman, K. Doi and J. H.C. Reiber converted-article fla Copyright © 2003 Published by Elsevier B.V. INTEGRATEDACCESSINTERFACEMULTIMEDIAEPR COSTA C 1 Introduction 2 Material and methods 2.1 Scenario 2.2 Departmental PACS 2.3 HIS and PACS integration 2.4 Security enforcement 3 Results 4 Discussion References TSIKNAKIS 2002 3 26 M COSTA 2003 C MIE2003PROCEEDING ANEWCONCEPTFORINTEGRATEDHEALTHCAREACCESSMODEL SILVA 1998 A SOBLE 1997 192 197 J DIGITALCARDIACIMAGINGIN21STCENTURYAPRIMER MPEGDIGITALVIDEOCOMPRESSION MENEZES 1996 A HANDBOOKAPPLIEDCRYPTOGRAPHY COSTA 2002 460 462 C TECHNOLOGYHEALTHCARE ATRANSCONTINENTALTELEMEDICINEPLATFORMFORCARDIOVASCULARULTRASOUND COSTA 2002 462 464 C TECHNOLOGYHEALTHCARE AMULTISERVICEPATIENTDATACARD CIMINO 2002 113 127 J COSTAX2003X880 COSTAX2003X880X886 COSTAX2003X880XC COSTAX2003X880X886XC item S0531-5131(03)00258-9 S0531513103002589 10.1016/S0531-5131(03)00258-9 272802 2010-12-19T22:39:48.374939-05:00 2003-06-01 2003-06-30 true 175018 MAIN 7 47550 849 656 IMAGE-WEB-PDF 1 gr1 83408 532 831 gr1 7044 140 219 ICS 2036 S0531-5131(03)00258-9 10.1016/S0531-5131(03)00258-9 Fig. 1 HIS and PACS integration to provide a multimedia EPR. An integrated access interface to multimedia EPR Carlos Costa * a José L Oliveira a Augusto Silva a Vasco Gama b a DET/IEETA, Aveiro University, 3810-193 Aveiro, Portugal b Centro Hospitalar de Vila Nova de Gaia, 4434-502 V.N. Gaia, Portugal * Corresponding author. Tel.: +351-234370500; fax: +351-234370545. This paper describes a multi-platform interface system that provides integrated access to a distributed Electronic Patient Record (EPR). Information associated with this kind of EPR may be stored either at the wide level of the Healthcare Information Systems (HIS) or at a more specific instance such as a departmental PACS. The system is supported by strong authentication and access control mechanisms, ensuring privacy, confidentiality and no-repudiation of clinical acts, providing the adequate privileges in the patient data handling. Keywords EPR HIS PACS Security Access control 1 Introduction A macroscopic analysis of a clinical process reveals that patient data is often generated, manipulated and stored throughout several institutions where the patients are treated or followed. These health institutions can be public or private, national or international, small or wide scope (hospitals, health centers, doctor's offices). In this heterogeneous and complex scenario, the sharing and the remote access to patient information is of capital importance for nowadays-healthcare best practices. The first step towards this sharing was already done by the worldwide trend for the adoption of digital processes and of the Electronic Patient Record (EPR) concept. EPR is a means to manage and integrate all types of clinical data. The information is collected, archived and distributed, introducing automated methods for traditional medical evidence recording. Above all, EPR represents the core element to accomplish new healthcare services with improved quality and efficiency. One of the major problems related with this concept is that we are often dealing with highly heterogeneous, autonomous and distributed medical systems, making very difficult the interoperability task. In this scenario we believe that on a near future healthcare will rapidly take full advantage of Web technology and Internet to allow controlled widespread sharing of medical data between healthcare intervenient. 2 Material and methods World globalization is increasingly promoting peoples' mobility. This fact, associated to the actual healthcare services trend to the creation of specialized diagnostic and therapeutic centers are creating a higher dispersion of patient clinical records. The above reality is forcing even more the healthcare providers to implement telematic infrastructures in order to promote the sharing and remote access to patient clinical data. Moreover, it is not only the dispersion of data that is growing up, but also the number of diagnostic and routine appointments (with production of information) between citizen and practitioners. The result is that one citizen, during his lifetime, is likely to produce a tremendous quantity of clinical information, stored in distinct institutions often supported by heterogeneous information systems. Within such scenario, the need for a unique and integrated access to all distributed patient data is demanded and fundamental for supporting continuous care and improved clinical services [1]. The herein described Integrated Multimedia EPR (IM-EPR) combines different media provided from specialized sub-systems and infrastructures. This integration inside institutions' domains will drive more easily a second level of integration at a national or international scale. Besides, we defend that the integration within a wide scope scenario must contemplate other integration elements as well as a new conceptual model [2]. The political and administrative awareness of the necessity of infrastructures that can provide all the types of clinical data in an integrated way are growing up and are identified as a fundamental key in care services for the upcoming years. We claim that a greater objective, like a truly uniform interface that could provide a regional/national access to all the historical patient data in an integrated way, must start with small and solid steps over the institution islands domains and progressively implement and aggregate new items over the time. Our first step, concerning this objective, was the integration of Healthcare Information System (HIS) and PACS in a unique Web interface, providing on-line and real time access to the authorized healthcare professional and enabling the delivery of time-efficient services with enhanced quality. To address the described motivations and objectives, several functional requirements were identified: – General usage of Web-based interfaces. – Quality of Service (QoS) handling for real time retrieval of remote information without compromising service reliability. – Transparent access to all the historic clinical data of a patient. – Select and retrieve facilities. – Reducing or elimination, of possible, redundant exams and procedures, with a direct impact on service costs and some times life risk. – Implementation of strong and efficient authentication mechanisms and the enforcement of identification procedures by practitioners. – Promoting collaborative research, tele-work and telemedicine. 2.1 Scenario The functional model that is described in this paper is being developed upon the CHVNG Cardiology Department information systems (HIS and PACS). This unity gathers 45,000 clinical patient records and two digital laboratories: a Cardiac Catherization Laboratory (cathlab) with 2500 procedures/year, and a Cardiovascular Laboratory (echolab) with 5000 procedures/year, both supported by a mature PACS system. Inside the departmental network it is possible to access, in a modular but integrated way, the conventional HIS alphanumeric information and also the patients images (still/cine-loops) available in DICOM format. The first PACS system [3] was implemented in 1997 initially covering the cathlab unity. In the beginning of 2001, it was extended to support the echolab through the creation and implementation of a DICOM private syntax (MPEG4 based). This approach enables the storage and online visualization of all the released exams. 2.2 Departmental PACS The production of medical images has been blowing up in the healthcare institutions, representing at this moment an important element to support the medical decision and imposing new difficulties concerning the storage volume, their management and the network infrastructure to support their distribution. The two heavily loaded departmental digital laboratories are producing medical image modalities (XA and US) that demand a huge storage capacity as well communications channel bandwidth. The data rate and volume associated with typical studies pose several problems in the design and deployment of systems contemplating the acquisition, archiving, processing, visualization and transmission functionalities. Our echolab work-reality tell us that, for example, an optimised time-acquisition and uncompressed echocardiogram study can typically produce a volume of 110 Mbytes of information when considering the storage of 10 colour cine-loop (RGB-24 bits/pixel) with 15 frame (≈1 cardiac cycle), and a sampling matrix (480×512). Concerning the cathlab the average study size, for greyscale images (8 bits/pixel) with a 512×512 matrix, is about 280 Mbytes in their uncompressed form. In the above scenario, the digital video compression is a crucial technology when considering key issues like storage and transmission times. Angiographies studies are directly sent to the DICOM storage server by the cathlab equipment. Here these raw data images are compressed and stored in a JPEG (quality factor 95) DICOM format and the PACS system database is updated with this new exam. The echolab is mainly based on multi-vendor echocardiography machines with standard DICOM output interface, storage-processing server unities and review stations. Given the significant time–space redundancy that characterizes this type of cardiovascular video signal there are important gains by choosing a compression methodology that copes well with both intra-frame and inter-frame redundancies [4]. The novelty of our approach starts by embedding each storage server with highly efficient MPEG4 encoding software. Since MPEG4 is not a native DICOM coding standard, subsequent image transmission, decoding and reviewing is accomplished through a DICOM private storage and transfer syntax mechanisms enabled between storage servers and review clients, making use of in house developed software (EcoHimage). In order to achieve maximum compliance with the standard, all the other DICOM information elements were kept unchanged. The original raw data ultrasounds are saved and kept online during 6 months and the DICOM private syntax sequences are made available permanently. 2.3 HIS and PACS integration The implementation of the proposed multi-platform IM-EPR (Fig. 1) starts with the development of a Web-based interface module to the department HIS. It is based on XML/XSL technology for dynamic content creation and formatting, according to the user terminal and to the access privileges of different user profiles. The implemented model was designed to collaborate with the existing HIS system (no Web-based) and to be easily adjustable to different types of applications and client platforms. The interface usability was a major functionality in the analysis and implementation phases, it was necessary to conjugate the high accuracy of clinical information representation and the capacities of visualization and interaction of the different types of terminals (mobile phones, PDA, desktops) since we aim to create a flexible interface to distinct access proveniences (indoor/outdoor) and client interfaces. The developed multi-platform interface integrates in run time the patient information retrieved from the HIS system with PACS system images, making these alphanumeric and multimedia data available in an Internet browser. More concretely when, for example, clinical information is retrieved from the HIS database, the eventual existence of associated DICOM images is checked in the department PACS. Since we have a substantial part of PACS images in DICOM-MPEG4 private syntax format (the ultrasounds), a particular software engine extracts the MPEG4 encoded image data from the DICOM and encapsulates it in an AVI file format. If the PACS image modality is not on this format the engine processes the standard DICOM file (RAW, JPEG) and return the AVI encoded file. The option by the Audio/Video Interleaved (AVI) video format was based in the following aspects: – It is the most used multimedia format in the Internet space. – It supports distinct information encoders, since uncompressed information until optimised MPEG4 encoded information. In this way, it is possible to make distinct compression adjustments to specific medical images modalities. – Our echolab is actually supporting a specially constructed DICOM private transfer syntax, based on MPEG4, easily exportable to an encapsulated AVI file format. – The small image file-size reduces the waiting time to download and display of images. – It is simple to integrate with other information to be displayed in the Web environment. – It does not need any specific client viewer, including mobile equipments browsers' case. 2.4 Security enforcement Security and access control mechanisms are fundamental aspects for the development and wide use of healthcare information services. In most of the current commercial transactions, the security assurance is typically provider-oriented, i.e. the service provider just imposes its security police to the user that have to assure that its own secrets are preserved. Although for most of the scenarios this solution might by acceptable, since the disruption of secrets only affects the owner–provider relation, there are situations where the loss of privacy and data integrity involves third persons—the patient in our scenario. The communication privacy can be resolved with the adoption of protocols like the HTTPS to encrypt the data transferred between server and clients (we are using SSL, 128 bits, in our server). However, concerning access control the problem is a bit more complex. To cope with this we have developed an innovative Healthcare Professional Card (HPC) that integrates strong user authentication based in smart cards and Public Key Cryptography (PKC) technology. The confidence on security issues depends strongly on the trust we have on digital certificates, on private key storage and how it is verified that the correct person is the owner of the private key [5]. Contemplating these demands, the HPC is hosted by a PKC smart card that includes card owner verification procedures. The first identity proof is related with the user card possession. However, the local and remote authentication is made through the professional private key that signs a host-side challenge and proof the user identity. The user private key is unique and securely stored on the card, protected by a PIN or biometric device. The implemented access policy comprises the type of source (indoor/outdoor) as well the client interface and the type of authentication and identification provided, often limited by the technological client platform constraints. The HPC model, oriented to the authentication, was delineated to work in a dual mode scenario. Inside the institution, the user must provide the token password to gain access to the private key (Fig. 1). In outside access, it is additionally demanded the presence of a biometric recognition element. This decision was based on the observation that inside the institution domain the identity control of users is associated to their physical presence that simplifies the authentication procedure. In outdoor access, we assume that the physical control factor should be replaced or at least attenuated by the inclusion of biometric mechanisms. This approach imposes the idea of not allowing delegation of access permission to third persons and it results on a strongest identification method, making possible not only the users indoor access to HIS and PACS but also the department professionals' remote access. 3 Results The use of MPEG4 encoding represents a cost-effective solution to store and transmit a patient study; in special it provides excellent results with ultrasounds image modality. With more than 3000 studies performed in 9 months in the echolab, we observe that file sizes of 15 to 30 frames cine-loops with Doppler colour coded information, rarely exceed 200 kbytes. Moreover, an excellent diagnostic quality was achieved as a clinical validation has been successfully carried out. The usage of MPEG4 as coding standard for multimedia data appears to be a good alternative for cost-effective storage and transmission of digital sequences in the intranet PACS system but also permits the presented integration with HIS information in a Web-based interface. On other hand it reduces the transfer time over the network, which is a very critical issue when we are dealing with costly or low bandwidth connections [6]. Moreover, the presented model is compliant with a previous proposed Multi-Service Patient Data Card [7] developed to provide a set of functionalities that allow the management of structured links to distributed EPR stored in distinct healthcare providers. 4 Discussion The benefits obtained with the availability of all historical patient data in a simple, fast and integrated interface are unquestionable to practitioners and to patients, and is likely to induce a significant improvement in the overall healthcare services quality. The development of this integrating interface also creates conditions to, in a near future, citizens gain access to their clinical data aiming to improve the knowledge of their own health conditions [8]. The presented multi-platform system represents a cost-efficient solution that provides a unique access portal to healthcare clinical information and implements a flexible access model over open and heterogeneous environments as the Internet. Strong securities enforcements and user transparent utilization are other achievements of the proposal. References [1] M. Tsiknakis D.G. Katehakis S.C. Orphanoudakis An open, component-based information infrastructure for integrated health information networks International Journal of Medical Informatics 68 1–3 2002 3 26 [2] C. Costa A new concept for an integrated healthcare access model MIE 2003 Proceeding 2003 IOS Press St. Malo, France [3] A. Silva A cardiology oriented PACS SPIE Proceedings 1998 (San Diego, USA) [4] J.S. Soble MPEG digital video compression Digital Cardiac Imaging in the 21st Century: A Primer 1997 192 197 [5] A.J. Menezes P.C. Oorschot S.A. Vanstone Handbook of Applied Cryptography 1996 CRC Press [6] C. Costa A transcontinental telemedicine platform for cardiovascular ultrasound Technology and Health Care vol. 10(6) 2002 IOS Press 460 462 [7] C. Costa A multi-service patient data card Technology and Health Care vol. 10(6) 2002 IOS Press 462 464 [8] J.J. Cimino V.L. Patel A.W. Kushniruk The patient clinical information system (PatCIS): technical solutions for and experience with giving patients access to their electronic medical records International Journal of Medical Informatics 68 2–3 2002 113 127 "
    },
    {
        "doc_title": "A new user-oriented model to manage multiple digital credentials",
        "doc_scopus_id": "84908869217",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84908869217",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Digital certificates",
            "Digital credentials",
            "Electronic credentials",
            "New approaches",
            "Secure transactions",
            "Security",
            "User oriented",
            "Web-based service"
        ],
        "doc_abstract": "E-Commerce and Services are become a major commodity reality. Aspects like electronic identification, authentication and trust are core elements in referred web market areas. The use of electronic credentials and the adoption of a unique worldwide-accepted digital certificate stored in a smart card will provide a higher level of security while allowing total mobility with secure transactions over the web. While this adoption does not take place, the widespread use of digital credentials will inevitably lead to each service client having to be in possession of different electronic credentials needed for all the services he uses. We present a new approach that provides a user-oriented model to manage multiple electronic credential, based in utilization of only one smart card per user as a basis for secure management of web-based services, thus contributing for a more generalized use of the technology.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A New Concept for an IntegratedHealthcare access model",
        "doc_scopus_id": "8444232153",
        "doc_doi": "10.3233/978-1-60750-939-4-101",
        "doc_eid": "2-s2.0-8444232153",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Distributed HIS",
            "Electronic patient record",
            "Health care information system",
            "Healthcare information systems (HIS)",
            "Identification of persons",
            "Patient data",
            "Patient information",
            "Web-based interface"
        ],
        "doc_abstract": "The increase of population mobility has been promoting a crescent dispersion of patient clinical records in Healthcare Information Systems. In this scenario, it is mandatory that new services will be available for healthcare practitioners, namely web-based interfaces with strong control access mechanisms providing effective authentication and identification of persons, and the establishment of new access models to the disperse patient information. This paper proposes and describes a Healthcare Access Model that integrates a new set of functionalities coping with patient mobility and implements an innovative concept of a virtual unique Electronic Patient Record - EPR.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Electronic patient record virtually unique based on a crypto Smart Card",
        "doc_scopus_id": "35248872485",
        "doc_doi": "10.1007/3-540-45068-8_102",
        "doc_eid": "2-s2.0-35248872485",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Electronic patient record",
            "Multi-services",
            "Patient data",
            "Patient mobility",
            "Web technologies"
        ],
        "doc_abstract": "This paper presents a Multi-Service Patient Data Card (MS-PDC) based on a crypto smart card and Web technology that integrates a new set of functionalities that allow handling well patient mobility. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Critical information systems authentication based on PKC and biometrics",
        "doc_scopus_id": "35248824578",
        "doc_doi": "10.1007/3-540-45068-8_101",
        "doc_eid": "2-s2.0-35248824578",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Access control models",
            "Authentication mechanisms",
            "Health care professionals"
        ],
        "doc_abstract": "This paper presents an access control model that dynamically combines biometrics with PKC technology to assure a stronger authentication mechanism to healthcare professional that can be used indistinctly in Internet and Intranets access scenario. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A trusted brokering service for PKI interoperability and thin-clients integration",
        "doc_scopus_id": "84973866184",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84973866184",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Current problems",
            "Essential features",
            "Information protection",
            "Network infrastructure",
            "Security",
            "Security issues",
            "Security services",
            "WEB security"
        ],
        "doc_abstract": "E-commerce seems to be one of most promising business areas for the upcoming years. However, for its plain fulfillment, security issues have to be judiciously managed. Information protection and digital signatures are essential features for documents that represent commitments. While several solutions already exist on the market, current problems are mainly related with the lack of interoperability. On this paper we present a security broker that uses XML to provide the end-user with a set of security services and tools that are independent of the client hardware, operating system, PKI solutions and network infrastructure.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A cardiology oriented PACS",
        "doc_scopus_id": "58749107317",
        "doc_doi": "10.1117/12.319775",
        "doc_eid": "2-s2.0-58749107317",
        "doc_date": "1998-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Asynchronous transfer mode (ATM)",
            "ATM",
            "Cardiac imaging",
            "DICOM",
            "PACS",
            "Short term",
            "Switched LANs",
            "Video sequencing"
        ],
        "doc_abstract": "This paper describes an integrated system designed to provide efficient means for DICOM compliant cardiac imaging archival, transmission and visualization based on a communications backbone matching recent enabling telematic technologies like Asynchronous Transfer Mode (ATM) and switched Local Area Networks (LANs). Within a distributed client-server framework, the system was conceived on a modality based bottom-up approach, aiming ultrafast access to short term archives and seamless retrieval of cardiac video sequences throughout review stations located at the outpatient referral rooms, intensive and intermediate care units and operating theaters. ©2003 Copyright SPIE - The International Society for Optical Engineering.",
        "available": false,
        "clean_text": ""
    }
]