[
    {
        "doc_title": "On the impact of the data acquisition protocol on ecg biometric identification",
        "doc_scopus_id": "85109073528",
        "doc_doi": "10.3390/s21144645",
        "doc_eid": "2-s2.0-85109073528",
        "doc_date": "2021-07-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Biometric identification systems",
            "Biometric identifications",
            "Data acquisition protocol",
            "Data collection",
            "Dry electrode",
            "Measure of similarities",
            "Segment duration",
            "Several variables",
            "Algorithms",
            "Biometric Identification",
            "Data Compression",
            "Electrocardiography",
            "Fingers",
            "Humans",
            "Signal Processing, Computer-Assisted"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.Electrocardiographic (ECG) signals have been used for clinical purposes for a long time. Notwithstanding, they may also be used as the input for a biometric identification system. Several studies, as well as some prototypes, are already based on this principle. One of the methods already used for biometric identification relies on a measure of similarity based on the Kolmogorov Com-plexity, called the Normalized Relative Compression (NRC)—this approach evaluates the similarity between two ECG segments without the need to delineate the signal wave. This methodology is the basis of the present work. We have collected a dataset of ECG signals from twenty participants on two different sessions, making use of three different kits simultaneously—one of them using dry electrodes, placed on their fingers; the other two using wet sensors placed on their wrists and chests. The aim of this work was to study the influence of the ECG protocol collection, regarding the biometric identification system’s performance. Several variables in the data acquisition are not controllable, so some of them will be inspected to understand their influence in the system. Move-ment, data collection point, time interval between train and test datasets and ECG segment duration are examples of variables that may affect the system, and they are studied in this paper. Through this study, it was concluded that this biometric identification system needs at least 10 s of data to guarantee that the system learns the essential information. It was also observed that “off-the-person” data acquisition led to a better performance over time, when compared to “on-the-person” places.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic analysis of artistic paintings using information-based measures",
        "doc_scopus_id": "85100446723",
        "doc_doi": "10.1016/j.patcog.2021.107864",
        "doc_eid": "2-s2.0-85100446723",
        "doc_date": "2021-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Artistic paintings",
            "Automatic analysis",
            "Block decomposition",
            "Community IS",
            "Computational analysis",
            "Correlation function",
            "Hidden patterns",
            "Local information"
        ],
        "doc_abstract": "© 2021 Elsevier LtdThe artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website (http://panther.web.ua.pt) for fast author characterization and authentication.",
        "available": true,
        "clean_text": "serial JL 272206 291210 291718 291872 291874 31 Pattern Recognition PATTERNRECOGNITION 2021-02-01 2021-02-01 2021-02-08 2021-02-08 2021-03-02T12:42:57 S0031-3203(21)00051-0 S0031320321000510 10.1016/j.patcog.2021.107864 S300 S300.1 FULL-TEXT 2022-06-12T13:56:47.120829Z 0 0 20210601 20210630 2021 2021-02-01T16:28:18.995615Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref vitae 0031-3203 00313203 true 114 114 C Volume 114 18 107864 107864 107864 202106 June 2021 2021-06-01 2021-06-30 2021 Regular papers Objects and image analysis article fla © 2021 Elsevier Ltd. All rights reserved. AUTOMATICANALYSISARTISTICPAINTINGSUSINGINFORMATIONBASEDMEASURES SILVA J 1 Introduction 2 Related work 3 Methods 3.1 Information-based measures 3.1.1 Normalized compression (NC) 3.1.2 Normalized block decomposition method (NBDM) 3.1.3 Local complexity analysis using the normalized compression 3.2 Two-point height difference correlation function 3.2.1 Assessment pipeline 3.2.2 Finding an effective data compressor 4 Results 4.1 Comparison of NC and BDM 4.2 Information-based measures in images of artistic paintings 4.2.1 Global measures analysis 4.2.2 Combining the NC with the roughness exponent of HDC function 4.2.3 Local complexity of paintings 4.2.4 Evaluation of measures for classification purposes 5 Discussion 6 Conclusions Website CRediT authorship contribution statement Acknowledgements Appendix A Supplementary materials References WEISBERG 2006 R CREATIVITYUNDERSTANDINGINNOVATIONINPROBLEMSOLVINGSCIENCEINVENTIONARTS HERTZMANN 2018 18 A ARTS CANCOMPUTERSCREATEART KHAN 2014 1385 1397 F LYU 2004 17006 17010 S KIM 2014 7370 D ZHANG 2017 34 H ZENIL 2018 605 H DELAHAYE 2012 63 77 J SOLERTOSCANO 2014 F SMIERS 2003 J ARTSUNDERPRESSUREPROTECTINGCULTURALDIVERSITYINAGEGLOBALISATION FERREIRA 2014 12 19 P INTERNATIONALCONFERENCEIMAGEANALYSISRECOGNITION AMETHODDETECTREPEATEDUNKNOWNPATTERNSINIMAGE PINHO 2011 584 588 A 201119THEUROPEANSIGNALPROCESSINGCONFERENCE FINDINGUNKNOWNREPEATEDPATTERNSINIMAGES PRATAS 2012 158 165 D INTERNATIONALCONFERENCEIMAGEANALYSISRECOGNITION DETECTIONUNKNOWNLOCALLYREPEATINGPATTERNSINIMAGES ROMASHCHENKO 2002 111 123 A NIVEN 2009 49 63 R MANTACI 2008 411 429 S SHANNON 1948 379 423 C KOLMOGOROV 1965 1 7 A SOLOMONOFF 1964 1 22 R SOLOMONOFF 1964 224 254 R CHAITIN 1966 547 569 G SOLERTOSCANO 2017 F GAUVRIT 2017 N LI 2001 149 154 M CILIBRASI 2005 1523 1545 R CILIBRASI 2006 2309 2313 R 2006IEEEINTERNATIONALSYMPOSIUMINFORMATIONTHEORY AUTOMATICEXTRACTIONMEANINGWEB CEBRIAN 2007 1895 1900 M COHEN 2014 1602 1614 A PRATAS 2017 259 266 D IBERIANCONFERENCEPATTERNRECOGNITIONIMAGEANALYSIS APPROXIMATIONKOLMOGOROVCOMPLEXITYFORDNASEQUENCES MANICCAM 2004 475 486 S LU 2016 Z LOSSLESSINFORMATIONHIDINGINIMAGES PRATAS 2016 231 240 D 2016DATACOMPRESSIONCONFERENCEDCC EFFICIENTCOMPRESSIONGENOMICSEQUENCES PRATAS 2017 265 272 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS SUBSTITUTIONALTOLERANTMARKOVMODELSFORRELATIVECOMPRESSIONDNASEQUENCES PINHO 2008 1 5 A 200816THEUROPEANSIGNALPROCESSINGCONFERENCE INVERTEDREPEATSAWAREFINITECONTEXTMODELSFORDNACODING RISSANEN 1978 465 471 J LI 2004 3250 3264 M LI 2008 M INTRODUCTIONKOLMOGOROVCOMPLEXITYAPPLICATIONS TAYLOR 1999 R JOHNSON 2008 37 48 C LI 2004 340 353 J BRESSAN 2008 113 116 M 200815THIEEEINTERNATIONALCONFERENCEIMAGEPROCESSING ANALYSISRELATIONSHIPBETWEENPAINTERSBASEDWORK OLSHAUSEN 2010 1027 B HUGHES 2010 1279 1283 J STORK 2008 68100J D COMPUTERIMAGEANALYSISINSTUDYART IMAGEANALYSISPAINTINGSBYCOMPUTERGRAPHICSSYNTHESISINVESTIGATIONILLUMINATIONINGEORGESDELATOURSCHRISTINCARPENTERSSTUDIO LETTNER 2008 68100C M COMPUTERIMAGEANALYSISINSTUDYART ESTIMATINGORIGINALDRAWINGTRACEPAINTEDSTROKES SHAHRAM 2008 68100D M COMPUTERIMAGEANALYSISINSTUDYART RECOVERINGLAYERSBRUSHSTROKESTHROUGHSTATISTICALANALYSISCOLORSHAPEAPPLICATIONVANGOGHSSELFPORTRAITGREYFELTHAT HEDGES 2008 681009 S COMPUTERIMAGEANALYSISINSTUDYART IMAGEANALYSISRENAISSANCECOPPERPLATEPRINTS PETROV 2002 197 202 V MACHADO 2019 614 626 J PENG 2015 3057 3061 K 2015IEEEINTERNATIONALCONFERENCEIMAGEPROCESSINGICIP CROSSLAYERFEATURESINCONVOLUTIONALNEURALNETWORKSFORGENERICCLASSIFICATIONTASKS MAO 2017 1183 1191 H PROCEEDINGS25THACMINTERNATIONALCONFERENCEMULTIMEDIA DEEPARTLEARNINGJOINTREPRESENTATIONSVISUALARTS CHU 2018 2491 2502 W HAMMER 2000 442 464 D HENRIQUES 2013 1101 1106 T TERWIJN 2009 S RYBALOV 2007 268 270 A BLOEM 2014 336 350 P INTERNATIONALCONFERENCEALGORITHMICLEARNINGTHEORY ASAFEAPPROXIMATIONFORKOLMOGOROVCOMPLEXITY SOKAL 1958 1409 1438 R KRUSKAL 1956 48 50 J LLOYD 1982 129 137 S TAUBMAN 2002 D HOSSEINI 2019 68 76 M CLEARY 1984 396 402 J MAHONEY 2005 M TECHNICALREPORT ADAPTIVEWEIGHINGCONTEXTMODELSFORLOSSLESSDATACOMPRESSION RISSANEN 1979 149 162 J MOFFAT 1998 256 294 A KNOLL 2012 377 386 B 2012DATACOMPRESSIONCONFERENCE AMACHINELEARNINGPERSPECTIVEPREDICTIVECODINGPAQ8 WANG 2017 3462 3471 X IEEECVPR CHESTXRAY8HOSPITALSCALECHESTXRAYDATABASEBENCHMARKSWEAKLYSUPERVISEDCLASSIFICATIONLOCALIZATIONCOMMONTHORAXDISEASES SHAPIRO 1978 175 214 C ROSENBERG 1994 H TRADITIONNEW GARRARD 2007 L COLOURFIELDPAINTINGMINIMALCOOLHARDEDGESERIALPOSTPAINTERLYABSTRACTARTSIXTIESPRESENT HOCKNEY 2006 D SECRETKNOWLEDGEREDISCOVERINGLOSTTECHNIQUESOLDMASTERS YANG 2016 1 6 S 2016EIGHTHINTERNATIONALCONFERENCEQUALITYMULTIMEDIAEXPERIENCEQOMEX COMPUTATIONALMODELINGARTISTICINTENTIONQUANTIFYLIGHTINGSURPRISEFORPAINTINGANALYSIS FICHNERRATHUS 2011 L FOUNDATIONSARTDESIGNENHANCEDMEDIAEDITION CHEN 2016 785 794 T PROCEEDINGS22NDACMSIGKDDINTERNATIONALCONFERENCEKNOWLEDGEDISCOVERYDATAMINING XGBOOSTASCALABLETREEBOOSTINGSYSTEM NANNI 2017 158 172 L SILVAX2021X107864 SILVAX2021X107864XJ 2023-02-08T00:00:00.000Z 2023-02-08T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 2022-06-07T13:51:03.718Z Scientific Employment Stimulus CI-CTTI-94-ARH/2019 Foundation for Science and Technology SFRH/BD/137000/2018 SFRH/BD/141851/2018 UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia Fundalo para a Ciłncia e a Tecnologia This work was funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019 and the research grants SFRH/BD/141851/2018 and SFRH/BD/137000/2018 for J.M.S and R.A, respectively. D.P. is funded by national funds through FCT - Fundalo para a Ciłncia e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019. item S0031-3203(21)00051-0 S0031320321000510 10.1016/j.patcog.2021.107864 272206 2021-04-24T03:25:50.185202Z 2021-06-01 2021-06-30 true 3739076 MAIN 13 55894 849 656 IMAGE-WEB-PDF 1 gr1 15979 221 382 gr2 78329 280 760 gr3 193217 586 756 gr4 252946 579 760 gr5 56425 375 756 gr6 230839 574 760 gr7 204219 761 761 gr1 4886 127 219 gr2 7699 81 219 gr3 24575 164 211 gr4 27479 164 215 gr5 7038 109 219 gr6 28301 164 217 gr7 12502 164 164 gr1 124625 1174 2032 gr2 845953 1489 4039 gr3 2593723 3113 4017 gr4 3278848 3076 4039 gr5 538446 1994 4016 gr6 2253011 3050 4039 gr7 2190689 4042 4042 mmc1 false 6905460 APPLICATION si7 3360 si17 2830 si31 4158 si1 1201 si40 2942 si50 5831 si8 3227 si9 4679 si10 1707 si11 1524 si12 9785 si13 2878 si14 1151 si15 5646 si16 13543 si18 3320 si19 13222 si2 687 si20 4753 si30 4648 si21 4784 si22 5274 si23 3195 si24 1272 si25 1568 si26 891 si27 11153 si28 4355 si29 3865 si3 875 si32 1743 si33 1573 si34 2100 si35 10893 si36 1389 si37 930 si38 1800 si39 3102 si4 1910 si41 1085 si42 1241 si43 13196 si44 1004 si45 1734 si46 1486 si47 3197 si48 17639 si49 6487 si5 1429 si51 3688 si52 1170 si53 871 si54 2793 si55 2746 si6 3709 am 10718106 PR 107864 107864 S0031-3203(21)00051-0 10.1016/j.patcog.2021.107864 Elsevier Ltd Fig. 1 Benchmark of lossless data compression tools specifically for the processed dataset of artistic paintings. The y -axis depicts the sum of the number of bytes to compress the dataset, where each image was compressed individually using each tool. Fig. 1 Fig. 2 Information-based measures evaluation. (A) Impact of increasing pseudo-random substitution on information-based measures: NC (approximated using the PAQ8 algorithm) and two BDM normalizations (NBDM 1 and NBDM 2 ). (B) Values of the NC and NBDM 1 for different types of images. (C) Image transformation pipeline leading to BDM underestimation of the amount of information contained in the transformed object. Fig. 2 Fig. 3 Examples of artistic paintings with different levels of complexity where painting images were quantized to 8 bits. The NC and NBDM 1 values of each painting are displayed in its lower right corner. Fig. 3 Fig. 4 Average Normalized Block Decomposition Method using NBDM 1 (A), and Average Normalized Compression (B) for each author where images of paintings where quantized for 4, 6, and 8 bits. The authors are sort given the value of NBDM 1 and NC, respectively. To see this result in more detail, please visit the website associated with the article. Fig. 4 Fig. 5 Combining the HDC with NC. (A) Average and standard deviation for each style in NC and α , respectively. (B) Results grouped by styles using average NC and average α of HDC for each artist labeled on the dataset. Fig. 5 Fig. 6 Heat maps of the local complexity matrix (fingerprint) of some authors, computed with the NC. This fingerprint shows the author’s range of complexity and the locations in the canvas painted with more detail (or complexity). To see all matrices, please visit the website associated with this article. Fig. 6 Fig. 7 Artists’ phylogenetic tree computed recurring to the UPGMA algorithm. Each artist has a sample painting and a colour associated with one of his styles (the colour was chosen based on nearest leaves) assigned to him, as well as a description of some styles usually associated with the author. To obtain an improved view of the tree, please visit the website related to this article. Fig. 7 Table 1 Accuracy results obtained for the test set in style and author classification task using state-of-the-art (SoA), state-of-the-art with regional complexity (RC) and ensemble with our measures (RC and HDC). Table 1 Classification Task Number of Classes Number of Images SoA Baseline SoA Baseline + RC SoA Baseline + RC + HDC Style 13 2338 0.622 0.644 0.650 Author 91 4266 0.480 0.490 0.500 Automatic analysis of artistic paintings using information-based measures Automatic analysis of artistic paintings using information-based measures Jorge Miguel Silva Conceptualization Validation Writing - original draft Writing - review & editing ⁎ a b Diogo Pratas Conceptualization Validation Writing - original draft Writing - review & editing a b c Rui Antunes Conceptualization Validation Writing - original draft Writing - review & editing a b Sérgio Matos Conceptualization Validation Writing - original draft Writing - review & editing a b Armando J. Pinho Conceptualization Validation Writing - original draft Writing - review & editing a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal b Department of Electronics, Telecomunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecomunications and Informatics University of Aveiro Portugal Department of Electronics, Telecomunications and Informatics, University of Aveiro, Portugal c Department of Virology, University of Helsinki, Finland Department of Virology University of Helsinki Finland Department of Virology, University of Helsinki, Finland ⁎ Corresponding author at: Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal. Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal The artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website for fast author characterization and authentication. Keywords Image analysis Data compression BDM Artistic paintings Algorithmic information theory 1 Introduction Artistic paintings are concrete visual expressions of human evolution and creativity to share emotions, values, visions, beliefs, and trends of history and culture. The creation, interpretation, and analysis of artistic paintings are social, contextual, subjective, passive, and, beyond superficial characteristics, complex to compute and automatize [1]. In particular, it is theorized that art is an output of social agents, particularly a human experience, that can only be imitated by machines [2]. One of the non-trivial characteristic analysis of artistic paintings is related to the process of measuring the information contained in those paintings. Artistic paintings contain information related to schools, periods, and artists [3]. The artistic community widely uses automatic computational analysis of artistic paintings for authentication of artistic paintings [4,5]. Currently, this process does not substitute human experts completely; however, it is an essential additional control for fraud and mislead detections [6]. Furthermore, applying new techniques and pre-existing ones that are new to the field, can be useful not only for authorship attribution and fraud detection but also for art style categorization and organization, and even for art content explanation. In this paper, we introduce novel solutions for automatic computational analysis of artistic paintings and for the problem of artist authentication. When addressing artist authentication, several questions arise: What defines a painter’s style? How does the author expose information? How does the author differs and relates to other artists? Furthermore, taking inspiration from information theory: How do we best quantify information in a painting? How is the information utilized across the canvas? Moreover, what can information quantification tell us about the author’s style, way of painting, and relationships with other authors? These complex questions are at the core of this paper’s development, where we describe and compare solutions for unsupervised measures of probabilistic and algorithmic information in images (2D) of artistic paintings. Our contributions are as follows: • We perform a direct comparison between state-of-the-art unsupervised probabilistic and algorithmic information measures to specify each measure’s strengths and weaknesses. • We show that hidden patterns and relationships present in artistic paintings can be identified by analysing their complexity. • We show an efficient stylistic descriptor by combining the Normalized Compression and a measure of the paintings’ roughness. • We propose a new descriptor of the artists’ style, artistic influences, and shared techniques. • We show that average local complexity describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. • We demonstrate that these measures can serve as useful auxiliary features capable of improving current methodologies in the classification of artistic paintings. To explain how we achieve this, we first compare the Normalized Compression (NC), employing a data compression tool chosen after a competitive benchmark, with the Block Decomposition Method (BDM) [7], and the inherent Coding Theorem Method (CTM) measures [8,9]. The BDM is an information-based measure that uses small Turing machines to approximate the algorithmic information, approximating to the Shannon entropy as a fallback mechanism. After this comparison, we make use of the average NC of each artist together with the roughness exponent α of the two-point height difference correlation function (HDC), to group artists by style. Furthermore, we provide a local complexity matrix that characterizes each artist using the NC and use it to construct a phylogenetic tree that portraits the relationship between artists in terms of exposing information to the observer. Finally, we use the regional complexity fingerprints and the roughness exponent α as useful auxiliary features that, combined with state-of-the-art approaches, improve the results of style and artist classification tasks. The remaining of this paper is organized as follows. In the next section, we describe related work, followed by a description of the methods. We present the major results in the next section, with further results presented in Supplementary Material. Finally, we discuss the results obtained, draw final conclusions, and point out possible future lines of work. 2 Related work Measuring the information contained in paintings requires fast, efficient, and automatic computation due to the diversity and large quantity of the existing artistic paintings [10]. To measure the information (or complexity) contained in paintings, we first need to define what is the quantity of information of an image. We define the quantity of information of an image as the smallest number of bits required by a model to represent an image losslessly. To perform this task, the model searches for unknown patterns of similarity between sub-regions of the image [11–13] and uses this information to create this compressed representation of the image, relying exclusively in the patterns of the two-dimensional pixels without using exogenous information. There are several approaches to quantify the amount of information. Kolmogorov described three, namely combinatorial [14–16], probabilistic [17], and algorithmic [18]. Independently, the works of Solomonoff [19,20] and Chaitin [21] addressed the same lines. While the Kolmogorov complexity is non-computable, it can be approximated with programs for such purpose, such as data compressors, using probabilistic and algorithmic schemes. Practical applications to approximate the Kolmogorov complexity for multiple dimensional digital objects have been developed using Turing machines [7,9,22,23] and data compressors [24–29]. Recently, Zenil et al. have shown that this methodology has a closer connection to algorithmic information than other measures based on statistical regularities [7], namely fast lossless compression methods, for sources that follow algorithmic schemes. The majority of the lossless compression algorithms are limited to finding simple statistical regularities as they have been designed for fast storage reduction [30,31]; accordingly, they provide slight improvements over the Shannon entropy [17]. However, there are several which are designed for efficient compression at the expense of more computational resources. For example, lossless compression algorithms, such as GeCo [32], are hybrids between probabilistic and algorithmic schemes. Besides having several context models of different orders, GeCo uses sub-programs that allow substitution [33] and reverse complement modeling [34]. These last two are sub-programs of probabilistic and algorithmic information nature. Another example is PAQ8 [35], a general-purpose compressor that combines multiple context models using a neural network, transform functions, secondary structure predictors, and other simple sub-programs. Usually, the problem is how to find fast and efficient algorithmic models for data compression. Lossless data compressors are tightly related to the concept of minimal description length [36] and algorithmic probability [25,37,38]. Therefore, representative algorithms can be efficiently embedded in these data compressors, including small Turing machines. The idea of automatic computational analysis of artistic paintings is mature [4,5], and the artistic community has widely relied on it for authentication of artistic paintings. Specifically, the characteristics of artistic paintings have been analysed through several statistical techniques and properties, namely fractal [39], wavelet-based [4], hidden Markov models [40,41], Fisher kernel based [42], sparse coding model [43,44], color and brightness [5], illumination [45], stroke [46,47], Print Index [48], and entropy-based analysis [49,50]. Recently, the work of Machado and Lopes [50], using fractional calculus, showed the potentiality of measures based on entropy to describe hierarchical clustering of paintings and their correlation with artistic movements. Regarding style and author classification, several recent works have proposed the usage of Convolutional Neural Networks (CNNs). A straightforward approach is to combine features extracted from multiple CNN layers, such as proposed by Peng et al. [51]. Another more effective approach is based on representing images by the principal components of a Gram matrix that captures correlations across the different feature maps obtained from a convolutional layer of a pretrained deep CNN, such as VGG16 or VGG19. Mao et al. [52] combine this representation with the features from all the five convolutional blocks of the VGG16, learning a joint representation that can simultaneously capture content and style of visual arts. On the other hand, Chu et al. [53] apply a support vector machine (SVM) to the Gram representation to perform author and style classification. Then, they improve the results by automatically learning correlations between feature maps. 3 Methods In this section, we describe the measures used, their normalizations, the methodology, and the compression benchmark performed. 3.1 Information-based measures Algorithmic information [18–21] differs from a perspective of pure probabilistic information [17] because it considers that the source, rather than generating symbols from a probabilistic ergodic function, creates structures that represent algorithmic schemes [54,55]. Therefore, to reverse the problem, there is the need to identify the program(s) and parameter(s) that generate the outcome(s) [18,21,38]. However, the algorithmic information, K ( x ) , is non-computable [56], mostly because of the halting problem [57]. Therefore, we have to rely on approximations. Namely, in this subsection, we describe the Normalized Compression and two BDM normalizations. Then, we establish the local application of the Normalized Compression to create a complexity matrix for each author and the methods used to create a distance matrix and the phylogenetic tree. Finally, we describe a non-information-based measure, the two-point height difference correlation function. 3.1.1 Normalized compression (NC) An efficient compressor, C ( x ) , gives a possible approximation for the Kolmogorov complexity ( K ( x ) ), where K ( x ) < C ( x ) ≤ | x | ( | x | is the length of string x in the appropriate scale). Usually, an efficient data compressor is a program that approximates both probabilistic and algorithmic sources using affordable computational resources (Time and RAM). Although the algorithmic nature may be more complex to model, data compressors may have embedded sub-programs to handle this nature. For a definition of safe approximation, see [58]. The normalized version, known as the Normalized Compression (NC), is defined by (1) NC ( x ) = C ( x ) | x | log 2 | A | = C ( x ) | x | , where x is a string, C ( x ) is the compressed size of x in bits, | A | the number of different elements in x (size of the alphabet) and | x | the length of x . Since we consider a binary matrix of each image, | A | = 2 , log 2 2 = 1 . Given the normalization, the NC enables to compare the information contained in the strings independently from their sizes [29]. If the compressor is efficient, then the compressor can approximate the quantity of probabilistic-algorithmic information in data using affordable computational resources. 3.1.2 Normalized block decomposition method (NBDM) Another possible approximation to the Kolmogorov complexity is given by the use of small Turing machines, where these small computer programs approximate the components of a broader representation. The Coding Theorem Method uses the algorithmic probability between a string’s production frequency from a random program and its algorithmic complexity. As such, the more frequent a string is, the lower Kolmogorov complexity it has; and strings of lower frequency have higher Kolmogorov complexity. The Block Decomposition Method (BDM) extends the power of a CTM, approximating local estimations of algorithmic information based on Solomonoff-Levin’s algorithmic probability theory. In practice, it approximates the algorithmic information and, when it loses accuracy, it approximates the Shannon entropy. Since in this article we intend to perform a direct comparison of both measures, we first considered the normalization of the BDM (NBDM 1 ), given by the number of elements (length) of the digital object as (2) NBDM 1 ( x ) = B D M ( x ) | x | log 2 | A | = B D M ( x ) | x | . However, the normalization of the BDM is usually performed using a minimum complexity object (BDM M i n ) and a maximum complexity object (BDM M a x ). A minimum complexity object is filled with only one symbol, like a binary string of only zeros. In contrast, a maximum complexity object is an object that, when decomposed (by a given decomposition algorithm), yields slices that cover the highest CTM values and are repeated only after all possible slices of a given shape have been used once. Using these two objects, the NBDM 2 for a given string can be computed as (3) NBDM 2 ( x ) = B D M ( x ) − B D M M i n B D M M a x − B D M M i n , where B D M ( x ) is the BDM value of that string, B D M M i n is the minimum complexity object, and B D M M a x is the maximum complexity object. Kolmogorov complexity is invariant only up to a constant factor, which depends on the choice of a description language K = K ′ + L , where K is the total complexity, K ′ is the description of the object and L is the description of the language. As such, by performing the normalization according to Eq. (3), the normalization is aiming to remove the constant factor as (4) K − K M i n K M a x − K M i n = K ′ + L − K M i n ′ − L K M a x ′ + L − K M i n ′ − L = K ′ − K M i n ′ K M a x ′ − K M i n ′ , where K M a x and K M i n are the maximum and minimum Kolmogorov complexity objects and K M a x ′ and K M i n ′ are the maximum and minimum Kolmogorov complexity description of the objects. In this article, we perform a direct comparison between the NC and the NBDM 1 . Furthermore, we compare the two types of BDM normalization and their impact on the results. 3.1.3 Local complexity analysis using the normalized compression The Normalized Compression (NC) was used to approximate the local (or regional) complexity of images of artistic paintings. To that end, all of the dataset images were divided into 16 × 16 blocks (256 equal regions) and the NC was computed for each block, generating a complexity matrix. Other patch sizes were also tested, specifically patch sizes of 8 × 8 and 32 × 32 blocks. Following this operation, the average complexity matrix was generated for each author, using the complexity matrices of their paintings. The average complexity matrices were then used to obtain a similarity matrix, in which the distance between matrices was determined as (5) d ( A , B ) = ∑ i = 0 n ∑ j = 0 n ∣ a i j − b i j ∣ , where d is the distance between the complexity matrix A and B , and a i j and b i j are the complexity values at the index i and j of matrices A and B , respectively. Subsequently, using the similarity matrix, a phylogenetic tree was computed recurring to two methods, namely UPGMA (unweighted pair group method with arithmetic mean) [59] and the Kruskal minimum spanning tree algorithm [60], in order to portrait complexity relationships among different authors. 3.2 Two-point height difference correlation function The two-point height difference correlation (HDC) function was computed to quantify brightness contrast as (6) HDC ( r ) = [ h ( x → + r → ) − h ( x → ) ] 2 ¯ = 1 N r ∑ x → , | r → | = r [ h ( x → + r → ) − h ( x → ) ] 2 , where the r is the distance between two-pixel points, over-bar represents the spatial average at a fixed distance r for all possible points; N r is the number of possible pairs at a distance r , h ( x ) is pixel intensity at the position x . Using the HDC function, its roughness exponent α was determined as (7) α = log 10 ( H D C ( r f i n a l ) ) − log 10 ( H D C ( r i n i t i a l ) ) log 10 ( r f i n a l ) − log 10 ( r i n i t i a l ) , where ( α ) is the slope of the HDC curve in a double logarithmic plot of the surface growth model. The slope was calculated from r i n i t i a l = 10 to r f i n a l , which matches the point where the HDC function saturates, approximately 30% of the image’s width. 3.2.1 Assessment pipeline In order to fairly evaluate the information-based measures, we designed a pipeline for processing images. It respects the following steps: Obtaining the dataset images; converting the images to PGM format; quantization of the images to 8 bits (256 levels) using the Lloyd-Max algorithm; binarization of the images (conversion to 01 format in ASCII) and finally, applying the information-based measurements (NC, NBDM 1 and NBDM 2 ). Quantization was performed using the Lloyd-Max algorithm [61,62] since reducing the precision of the pixels (alphabet) in images enables the filtering of small variations that might occur during the digitalization process. Binarization to 01 format in ASCII was performed since the BDM currently only supports a small alphabet. 3.2.2 Finding an effective data compressor To compute the NC, we have to find an effective data compressor, meaning, a compressor that best represents each image, while using reasonable resources. Since our aim is later to apply this measure to a dataset of artistic painting, we compared seven compression tools, namely GZIP [63], BZIP2 [64], XZ [65], LZMA [66], AC [67], PPMD [68], and PAQ8 [35]. As depicted in Fig. 1 , the PAQ8 tool shows the best compression ratio for this dataset. In fact, it shows an improvement of ≈ 26 % to the second best tool (XZ). The disadvantage is the use of higher RAM and substantially more computational time. Nevertheless, since our purpose is to find the number of bits of a shortest program to reproduce the image, it is affordable to spend these computational resources. Therefore, we used the PAQ8 tool to compress each of the quantized images. The code was compiled using the package provided from Buchner [69]. The PAQ8 version used was kx v7. PAQ8kx v7 is an archiver that achieves the highest compression rates at the expense of speed and memory (approximately 1,6 GB of RAM for this dataset). We used the mode that usually provides the highest compression ratio (command parameter: “-8”). The PAQ8 compressor uses a context mixing algorithm between a large number of models independently predicting each quantized pixel’s next bit [70]. The predictions are combined using a neural network and arithmetic coding [71,72]. For automatic installation, use the script Install.sh, while for more information of PAQ, see the work of Knoll and Freitas [73]. The computations ran in a single core Ubuntu Linux computer running at 2.13 GHz with 1.6 GB of RAM. Using this machine, the compression of the whole dataset with PAQ8 required approximately 270 h of real-time, without parallelization. 4 Results 4.1 Comparison of NC and BDM In order to compare NC with BDM, we performed three types of tests. Namely, we compared the robustness of both measures according to increasing rates of random pixel changes in paintings, tested their application on different types of images, and made an assessment of the minimal information bounds. In the first test, we assessed the impact of an increasing rate of pixel editions using a pseudo-random uniform distribution and compared both information-based measures. This approach is not identical to image noise, but rather a pure edition of pixels. For the purpose, for each of the three authors (Theodore Gericault, Marc Chagall, and Rene Magritte) we select a painting, making 50 adulterated copies of each painting with increasing edition rate (from 1 to 50%). Finally, we measured the NC (Eq. (1)), the NBDM 1 (Eq. (2)), and NBDM 2 (Eq. (3)) in all the paintings. Fig. 2 (A) depicts the values obtained for the NC and BDM. The results show that, when using the same type of normalization, NC is more robust to the increment of pixel edition than NBDM (NBDM 1 ). On the other hand, whereas NBDM 1 considers the normalization by the length of the input object, NBDM 2 performs a normalization that aims to mimic the removal of the constant factor related to Kolmogorov complexity (see Eq. (4)). Since the NBDM 2 normalization does not take into account the constant of the description language, it shows a more robust behavior than NBDM 1 , which increases rapidly with the increase of pixel edition. Since NC and NBDM 1 have the same type of normalization, we will focus on comparing these normalizations from now on. In the second test, we applied both measures to six datasets with distinct nature (9 images each) to understand how NBDM 1 and NC behave with different types of images. The six datasets were: artistic images from 2 different datasets [3,74]; cellular automata images; diabetic retinopathy images [75]; chest computed radiography (CR) images [76] and photographic images [77]. The results are depicted in Fig. 2(B). Overall, the majority of the datasets show similar behavior regarding the NC and NBDM 1 . The exceptions to this are the CR and cellular automata datasets, which exhibit a more algorithmic behavior. The latter dataset is constituted by images created with small programs with simple rules. Whereas the compressor has difficulty compressing this type of images, the BDM can point to their algorithmic nature, and, thus attribute them with minimal value. This outcome shows the importance of the BDM in the detection of simple algorithmic outputs embedded into data. In the last test, we selected one of the most complex images identified by the NBDM in the last subsection to test if the BDM could accommodate specific data alterations. This test is depicted in Fig. 2 (C). After the binarization process, we performed a super-sample image transformation where each char was amplified to a 4 × 4 representation. This value was selected since the BDM has the default block size value of 4 × 4 in 2D structures. After this operation, the BDM was computed for the original and the super-sampled image. While the original image was measured with 370981 bits, the super-sampled image had only 79 bits. This abrupt decrease in the complexity value indicates that the BDM underestimates the amount of information contained in the object. The BDM analyses object information in blocks instead of looking at the whole object. Specifically, blocks analysed by the BDM (default block size value of 4 × 4 in 2D structures) have the same size as the super-sample image transformation (each char was amplified to a 4 × 4 representation); therefore, the complexity attributed to each block is approximately zero (since each block is composed of all zeros or ones), and hence the overall value attributed to the complexity of the object will drop dramatically. This analysis shows that BDM is not prepared to deal with the information associated with the choice of the model, unlike the NC. The NC relies on the use of a lossless data compressor, bounded by a maximum information channel capacity. From these three tests, we are able to notice some advantages and limitations of both measures. Ranking these measures is not a fair task because they have different characteristics and nature. Therefore, in the remainder of the article, we use the NC and NBDM in a combined mode to recover insights and characteristics from the images of the artistic paintings. 4.2 Information-based measures in images of artistic paintings Herein, we investigate the use of information measures to analyse a dataset of artistic paintings. This dataset [3] contains 4,266 images of artistic paintings from 91 authors, with approximate geometric sizes. The 91 authors are well-known painters, such as Claude Monet, Frida Kahlo, Henri Matisse, Jackson Pollock, Picasso, Rembrandt, and Salvador Dali. In the following subsections, we present the results of applying the measures, combining the NC with the HDC function, measuring local complexity for different authors and constructing a phylogenetic tree, as well as using these features to improve style and artist classification. We also measure the impact of normalizing these images by performing image normalization and then applying the measures mentioned above in the dataset. Afterwards, we compared the average variation difference and the percentage difference between the results obtained for each author. The results are shown in the Supplementary Material in Section A.1. 4.2.1 Global measures analysis In this subsection, we measure an approximation to the Kolmogorov complexity for the dataset of artistic paintings. The same pipeline, described in the methods section, was used, with the difference that the Lloyd-Max algorithm quantization was set to 16, 64, and 256 levels (4, 6, and 8 bits respectively). Important to note that Lloyd-Max algorithm forced normalization of the images for the 16 and 64 levels, while the 256 level was the original level of the images, and, as such, these images were not normalized. This process was performed to evaluate the impact of the quantization on the measures used to approximate the Kolmogorov complexity in artistic painting images. From the results obtained from the measures, we show unknown characteristics and insights into temporal traits. In general, the complexity of each painting follows the example of Fig. 3 . Paintings with low complexity are classified as abstract and minimalist, following simple patterns. As the complexity increases, we start to recognize paintings with different local complexities, meaning, there are regions with high complexity and detail (generally on the center/bottom of the paintings) surrounded by low complexity regions (same color background) namely known as chiaroscuro. This pattern begins fading, as the complexity increases since the highest complexity paintings are also the most irregular, detailed, and convoluted. Regarding the average complexity values for each artist, Fig. 4 shows the average of NBDM 1 and NC, respectively. Each artist has an associated color, and lines of the same color illustrate its relative positional deviation in different quantizations. The same results for NBDM 2 are exposed and discussed in the Supplementary Material. Noticeably, quantization impacts the NBDM 1 more than the NC, since the relative positioning between authors varies more in the former. On average, the variation is 13.4 ± 11.37 relative positions of each author in NBDM 1 , while in NC, the variation is 4.9 ± 4.3 positions. Despite the higher variation present in the NBDM 1 , both measures are capable of detecting styles with low and high complexity. Artists such as Mark Rothko, Lucio Fontana, Piet Mondrian, El Lissitzky can be easily identified on the low side of the complexity spectrum. Minimalism, Abstract Expressionism, and Constructivism movements are associated with these styles. On the other hand, artists from Abstract Expressionism, such as Willem de Kooning, Jackson Pollock, and Jasper Johns, characterize the highest complexity side of the spectrum, as well as other artists with a more detailed and convoluted style, like Gustav Klimt and Vincent van Gogh. Abstract Expressionism is characterized by aggressive features combined with random and geometric features and spontaneity [78]. The reason for Abstract Expressionism artists being present at both extremes of the complexity spectrum is because this style itself divided into two opposites, Action Painting and Color Field. In Action Painting, the paint was thrown directly on the canvas, through instinctive gestures, where chance and randomness determined the evolution of painting [79]. This style is characteristic of artists like Jackson Pollock (known for the technique of ǣdrippingǥ) and Willem de Kooning. On the other hand, Color Field is more mystical and meditative. This style of painting has few elements in the frames, indefinite limits, and explores the sensory effects of color, as well as the subtlety of chromatic relations [80]. A specific example of an artist that followed this trend was Mark Rothko. In all cases, Jackson Pollock had complexity values utterly different from other artists, the average complexity of his paintings being approximate to random (normalized value close to 1). Although he denied his paintings were random, similar results were also found in previous work, which defined Jackson Pollock’s dripping paintings as not typical artworks [5]. 4.2.2 Combining the NC with the roughness exponent of HDC function We used the average NC together with the roughness exponent ( α ) of the two-point height difference correlation (HDC) function, which measures the roughness exponents of brightness surfaces, to assess the ability of these measures to distinguish different styles. Accordingly, we made usage of style labeled paintings available in the dataset. From these labeled images, we computed for its author the average NC and the value of α . The roughness exponent was used as an additional measure since it has proven to be capable of some differentiation between styles [5]. We discarded the usage of BDM due to quantization impacting it more than the NC. Using the average NC and α of each labeled painter, we created a scatter plot (Fig. 5 ) and represented each artistic movement as an ellipse, with the center in the points’ center of mass and with a width corresponding to the standard deviation. As shown in Fig. 5(A), both measures alone are not capable of efficiently separating styles, However, when combined, the styles are well confined into different regions (except for Abstract Expressionism), showing that together these measures are representatives of artistic movements. The roughness exponent α captures the level of brightness and relative spatial position and is correlated to variations in painting techniques and genres [5]. The NC adds to the level of brightness and relative spatial position provided by the HDC, the notion of average information present in each artist’s painting. This amount of information differs depending on the artistic movement and historical circumstances. Interestingly, similar to NC, the roughness exponent of the HDC varies greatly in Abstract Expressionism, being that in this artistic movement, there is an inverse correlation between the NC and α . Namely, artists like Jackson Pollock and Willem de Kooning (Action Painting) presented a high average NC and a low α , whereas, Mark Rothko (Color Field) had polar results. This atypical behavior corroborates the big difference between the two currents of Abstract Expressionism. The Action Painting usage of instinctive gestures and randomness creates high NC values and spatial correlation approaching a random image. In contrast, in Color Field, we get more minimalist images with high spatial contrast between regions but low complexity. 4.2.3 Local complexity of paintings In this section, we divided the images into identical quadrilateral sizes and measure the algorithmic information for each one ( 16 × 16 blocks). Then, we computed the average of each quadrilateral for all the paintings for each painter. The results are shown in Fig. 6 , illustrating the same authors as those in Fig. 3. Note however that matrices of Fig. 6 were computed using all the authors’ paintings present in the dataset. The complete results are available on the website associated with this article. The same computation was repeated for blocks of sizes 8 × 8 and 32 × 32 . Analysis of these results, included in the Supplementary Material, show that 16 × 16 is the minimum patch size for which the differences in the compression rate are noticeable and can therefore be used as a measure between paintings. All artists have a unique complexity matrix (fingerprint). This fingerprint shows, on average, where artists paint with more detail and give more emphasis as well as the average range of complexity the artist operates. For instance, Jackson Pollock and Jasper Johns show high complexity values dispersed over the canvas. At the same time, artists like Francis Bacon and George de la Tour focus more on the center of the canvas, and Mark Rothko and Piet Mondrian give their highest complexities around the borders of paintings. Since the 16 × 16 fingerprints conveyed the best results regarding detail and differentiation (see Supplementary Material in Section A.2), the phylogenetic trees were constructed utilizing the distance computed from the fingerprints with block size. Concretely, two phylogenetic trees were constructed to portray the relations between different artists. One tree was constructed using the UPGMA algorithm, which is illustrated in Fig. 7 , and another tree was build using the Kruskal minimum spanning tree algorithm [60], which is depicted in the Figure S3 of the Supplementary Material in Section A.4. The tree shows the fingerprint’s capacity of grouping artists from the same artistic movements mutually. Broad groupings of artists from styles are present in the tree, namely, Renaissance, Baroque, Romanticists, Impressionists, Surrealism, Cubism, and Abstract Expressionism. Also, the tree shows smaller groupings of sister leaf-nodes with the same style. On the other hand, the tree depicts relationships of influence between authors of different artistic movements. This relation is seen in the case of Titian, who influenced Diego Velazquez; Caravaggio, who influenced Francisco de Zurbarán; Frida Kahlo, who influenced Amedeo Modigliani; Sandro Botticelli who influenced William Blake; Claude Lorrain who influenced Joseph Mallord William Turner; and Peter Paul Rubens who influenced Jean-Antoine Watteau. On the other hand, some authors seem unrelated in style and influence, for instance, Francis Bacon and Georges de la Tour, George Braque and Hieronymus Bosch, Peter Paul Rubens and Frida Kahlo, Max Ernst and Giorgione, and Rembrandt van Rijn and Roy Lichtenstein. There can be many reasons for this to occur, for instance, the number of regions the images were divided can be sub-optimal for some images of artistic paintings, decreasing the sensitivity of the measure and jeopardizing the tree’s construction. On the other hand, the algorithm used to measure the similarity between matrices or the algorithm used to construct the tree (UPGMA) may not be the most appropriate for all cases, however, we have tested the Kruskal minimum spanning tree algorithm which yielded similar results (see Supplementary Material in section ). Additionally, these seemingly unrelated connections could reveal undiscovered elements and relationships. For instance, one of Roy Lichtenstein’s early artistic idols was Rembrandt van Rijn. Moreover, if artists are not related regarding the artistic movement or influence, the vicinity among them could be representing another property. This aspect is not necessarily related to the period or movement the artists were inserted in, but rather, the way authors projected their compositions, ideas, and impressions onto the canvas. Complexity can be approximated by the total number of properties transmitted by an object and detected by an observer. By dividing images into blocks of equal size and evaluating its local complexity, we are quantifying the local information being transmitted. On the other hand, by averaging the canvas results per artist, we obtain a matrix that describes how the author exposes information to the observer. This information intertwines various notions critical to how the work is perceived, such as composition which describes where the artist places the subject and how the background elements support it, as well as the unity, balance, movement, rhythm, focus, contrast, pattern, and proportion of the painting. For instance, the proximity between Hans Holbein and Vermeer could be due to both of them having used optics to achieve precise positioning in their compositions, namely by performing a combination of curved mirrors, camera obscura, and camera lucida [81]. Another example that this information can convey is space by depicting where positive (subject itself, which is usually more detailed) and negative (the area of painting around it) spaces are on the canvas. Artists can play with a balance between these two spaces to further influence how viewers interpret their work. Therefore, the similarity between different artists concerning the regional (local) complexity can reflect the similarity in thought regarding their approaches to painting. For instance, the proximity between Francis Bacon and Georges de la Tour could be due to the former being heavily influenced by the Baroque style and having made dramatic use of contrasts of light and shadow. These methods are characteristic of the chiaroscuro principle and its radicalization in the Tenebrista school (signature style of Georges de la Tour) [82,83]. The intense contrasts of light and shadow highlight the characters, and although exaggerated, it is lighting that increases the feeling of realism, making the muscles and facial expressions more evident. Simultaneously, the presence of large blackened areas highlights the chromatic research and the illuminated space, which acquire their value as elements of the composition. We conclude that this novel technique is a unique descriptor of the authors’ paintings since it not only aggregates authors of the same style close to each other and demonstrates the influences that authors had on others. It also serves as an insight into the way the artist projects its art. 4.2.4 Evaluation of measures for classification purposes To quantitatively evaluate the use of these measures for classification purposes, we assessed their impact when used as additional features to improve state-of-the-art classification methods. For this purpose, we recreated a recently published state-of-the-art method as a baseline and improved the results by combining our proposed measures. Based on current methods [52,53], we extracted a Gram representation using the first convolutional layer from the fifth convolutional block of the VGG16 network which was pre-trained with the imagenet dataset (no significant result difference was found between the use of VGG16 or VGG19). Principal component analysis (PCA) was applied to the Gram matrix to reduce the dimensionality and, finally, this vector was provided to an SVM to perform classification. Afterwards, the features obtained from computing the HDC and the regional complexity were used for author and style classification using the XGBoost classifier [84] and combined with the baseline classifier via a Voting Classifier ensemble. The results of the baseline and ensemble classifiers, applied to the Paintings91 dataset in the author and style classification task using the labels provided in the dataset, are shown in Table 1 . The results show that the inclusion of the Regional Complexity, increased the accuracy of the results 2.2 p.p. and 1.0 p.p in the style and author classification tasks respectively. Moreover, the overall inclusion of the proposed measures (HDC + RC) increased the accuracy in both classification tasks by 2.8 p.p. and, 2.0 p.p. in the style and author classification tasks, respectively. These results indicate that these predictors are useful auxiliary features capable of improving current methodologies in the classification of artistic paintings. This is congruent with the results obtained with Nanny et al. [85], since handcrafted features and non-handcrafted features seem to extract different information from the input images and, as a result, the fusion of the two types of features improves the results obtained when using non-handcrafted features only. Furthermore, regional complexity (RC) has a higher impact on the improvement of the accuracy than the HDC features, demonstrating the importance and distinction of Regional Complexity as a feature. 5 Discussion In this work, we develop, use, and compare unsupervised pattern recognition techniques to quantify information in images of artistic paintings. We rely on two approaches, namely data compression using the Normalized Compression (NC), and the Block Decomposition Method (BDM), to estimate information of both probabilistic and algorithmic sources. To approximate the NC, we benchmark a set of data compressors, where we show that the most effective for this dataset is PAQ8. Subsequently, this article is organized into two broad sections. The first is the evaluation and comparison of information-based measures; the second is applying these information-based measures to a dataset of artistic paintings. On the measure evaluation section, we assessed the NC and BDM using three tests. In the first test, we evaluated the NC and two normalizations of the BDM, regarding their robustness when images undergo uniform pixel editing and their behavior when applied to different types of datasets. We found that in terms of uniform pixel editing, the NC is more robust than BDM with the same kind of normalization. The NC is a measure of compression (in this case, using the PAQ compressor) that makes use of the digital object in its entirety to create the shortest possible representation without loss of information. In contrast, BDM divides the digital object into blocks and, based on the complexity of the blocks, estimates the image complexity in its entirety. This means that BDM cannot determine the information shared between the blocks, which causes it to increase, when compared to the NC, with the increase in uniform pixel editing. In the second test, we compared both measures using different image natures. We found that the results of the NC and NBDM are similar, except for the computed radiography and the cellular automata dataset, which exhibited a more algorithmic behavior. The cellular automata data was created with small programs with simple rules. While the compressor had difficulty compressing this data, BDM could approximate their algorithmic nature and thus assign them a value close to a minimal complexity value. The ability to identify an algorithmic nature incorporated in the data demonstrates the relevance of BDM as a measure. In the third test, we found that a super sample image transformation causes an underestimation of the amount of information contained in the object by BDM. Again, this is due to BDM analysing the object in blocks, instead of using the object in its entirety. Since the ampliation size was the same as the blocks analysed by BDM, the complexity attributed to each block was approximately zero. Consequently, the overall value attributed to the image complexity decreased dramatically. This aspect demonstrated that BDM cannot handle information contained between each block and can easily underestimate the amount of information present in a digital object. In the second phase, we applied these measures to estimate the complexity of a dataset of paintings. We calculated the NC and NBDM in this dataset with different quantizations and assessed the results in terms of average complexity per author. Afterward, we combined the NC with the exponent of the roughness of the HDC function in the labeled paintings of the dataset. Finally, we computed the average regional complexity of each author regarding their paintings and built a phylogenetic tree. We found that paintings with low complexity are abstract, minimalist, and follow simple patterns. Paintings with a slightly higher average complexity possess different regional complexities, specifically, a region with high complexity and detail surrounded by a background of low complexity. With more complexity, this noticeable pattern begins to fade, and the most complex paintings are globally irregular, detailed, and convoluted. Regarding the average complexity values for each artist, we found that NC and NBDM behave similarly, where quantization impacted more the NBDM. We also found that the low side of the complexity spectrum was characterized by Abstract Expressionism, Minimalism, Constructivism movements, with authors such as Mark Rothko, Lucio Fontana, Piet Mondrian, and El Lissitzky. Also, artists from Abstract Expressionism characterized the high complexity side of the spectrum, such as Willem de Kooning, Jackson Pollock, and Jasper Johns, as well as other artists with a more detailed and convoluted style, like Gustav Klimt and Vincent van Gogh. Due to two different currents (Color Field with authors with low average complexity and Action Painting with authors with high complexity), Abstract Expressionism was present at the polar ends of the spectrum. In all cases, Jackson Pollock had average complexity values that were utterly different from other artists, being the average complexity of his paintings close to random. Although he denied being a creator of random paintings, this result and others [5] seem to indicate that Jackson Pollock’s dripping paintings are not typical artworks, and this is possibly related to the inclusion of many symbolic layers and dispersion intentions over the canvas by the author. When evaluating the artists’ average NC together with the roughness exponent ( α ) of the HDC function in the labels images of the dataset, we found that styles are well confined into different regions, showing that the combination of these measures gives a robust representation of artistic movements. The NC adds to the level of brightness and relative spatial position evidenced by the roughness exponent, the notion of average information present in each artist’s painting, which is consistent within the same style and historical circumstances. We also find that in Abstract Expressionism, the NC is inversely correlated to α . Concretely, artists related to Colour Field painting presented a high α and low NC, whereas artists related to Action Painting presented the exact polar results (low α and high NC). Finally, we divided the image into equal quadrilateral parts and estimated the local complexity of each painting on the dataset and used it to ascertain each artists average regional matrix (fingerprint). Complexity can be thought of as a measure of the total number of properties transmitted by an object and detected by an observer. By dividing images into blocks of equal size and evaluating its local complexity, we quantified the local information being transmitted. Furthermore, by averaging the canvas results per artist, we obtain a unique fingerprint that describes how the author exposes information to the observer. Among other things, these fingerprints give specific insights regarding each artist’s way of painting, showing where, on average, artists paint with more detail and give more emphasis, while also providing insights into each artist’s range of complexity. Using these matrices, we computed a distance matrix and utilized it to construct a phylogenetic tree. We discovered that these phylogenetic trees aggregated authors of the same style close to each other, as well as artists’ influence relationships, like Francis Bacon and Georges de la Tour, and George Braque and Hieronymus Bosch. Furthermore, we observed proximity between artists due to shared methods and techniques which are not correlated with the temporal era or artistic movement. An example of this occurrence is the proximity between Hans Holbein and Vermeer which don’t share styles, but both used optics to achieve precise positioning in their compositions. This evidence shows that artists’ fingerprints contain critical information into how the work is perceived, such as composition, unity, balance, movement, rhythm, focus, contrast, pattern, and proportion of the painting and space. Finally, we show that these measures improve current methodologies in the classification of artistic paintings and thus extract information which differs from non-handcrafted features. Furthermore, regional complexity provided the largest increase in accuracy on the classification tasks, showing its relevance as a descriptor of images of artistic paintings. 6 Conclusions In this paper, we introduce novel solutions to the field of computer analysis of artistic paintings and the problem of artist classification and authentication. Specifically, we assessed the viability of unsupervised measures that approximate the quantity of probabilistic and algorithmic information for performing these tasks. Our direct comparison between NC and BDM allowed us to understand the strengths and weaknesses of both measures. Although BDM has difficulty dealing with uniform pixel edition and full information quantification given the block representability, it serves as a useful tool for measure and indentification of data content having similarity to simple algorithms. On the other hand, the NC is more robust to data alterations (pixel edition and quantization) and is able to measure the quantity of information without underestimation. Regarding the application of information-based measures in artistic paintings, we studied and developed techniques that can be valuable for art authorship attribution and validation, art style categorization and organization, and art content explanation. Namely, the NC proved to be a robust measure that as a whole gives us some insight regarding the complexity of different styles showing hidden patterns and relationships present in artistic paintings that share the same range in complexity. Furthermore, it could be a stylistic descriptor when coupled with the roughness exponent α . On the other hand, fingerprints depict how each author perform typical content distribution on canvas. Thus, they can provide a suitable means of art content explanation, as well as being valuable for art authorship attribution and validation. Moreover, since they provided insights regarding the artists’ way of painting, they can be used as a means of relating authors, being therefore useful for depicting artists’ stylistic influences, and shared techniques. Additionally, using the distance between the artists’ regional complexity, we also find some interesting links between authors regarding the usage of space, technique, composition, rhythm, and proportion. Finally, we demonstrated that the regional complexity and the HDC function of the paintings could serve as useful auxiliary features capable of improving current methodologies in author and style classification of images of artistic paintings. Regarding future continuations to this study, there are many possible future lines of work that can be considered. For example, in this work we analysed the images of paintings by converting them to monochrome, and it would be interesting to separate the colour channels and to analyse them separately, therefore studying the influence of colour in the paintings in terms of complexity. Additionally, it would be interesting to explore how to separate different characteristics of the fingerprint, as well as detecting unknown repeated patterns that appear multiple times in a painting by creating and analysing their complexity surfaces [13]. Lastly, another interesting study would be to replicate the developed work in this article using a competitive compressor that would select the best compressor model for each painting or region. Website A support website to this site can be accessed at This site showcases among other things, the pipeline of this study, the author’s average NC and NBDM variation for different quantization levels, the results of combining the NC with the roughness exponent of HDC function ( α ), a complete catalogue of each author’s fingerprints as well as several examples of each author’s paintings, and the computed phylogenetic trees with a magnifier to allow a better observation of the results. CRediT authorship contribution statement Jorge Miguel Silva: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Diogo Pratas: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Rui Antunes: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Sérgio Matos: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Armando J. Pinho: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Declaration of Competing Interest The authors declare no competing interests. Acknowledgements This work was funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019 and the research grants SFRH/BD/141851/2018 and SFRH/BD/137000/2018 for J.M.S and R.A, respectively. D.P. is funded by national funds through FCT - Fundalo para a Ciłncia e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019. Supplementary material Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.patcog.2021.107864. Appendix A Supplementary materials Supplementary Data S1 Supplementary Raw Research Data. This is open data under the CC BY license Supplementary Data S1 References [1] R.W. Weisberg Creativity: Understanding Innovation in Problem Solving, Science, Invention, and the Arts 2006 John Wiley & Sons R. W. Weisberg, Creativity: Understanding innovation in problem solving, science, invention, and the arts, John Wiley & Sons, 2006. [2] A. Hertzmann Can computers create art? Arts vol. 7 2018 Multidisciplinary Digital Publishing Institute 18 A. Hertzmann, Can computers create art?, in: Arts, volume 7, Multidisciplinary Digital Publishing Institute, 2018, p. 18. [3] F.S. Khan S. Beigpour J. Van de Weijer M. Felsberg Painting-91: a large scale database for computational painting categorization Mach. Vis. Appl. 25 6 2014 1385 1397 F. S. Khan, S. Beigpour, J. Van de Weijer, M. Felsberg, Painting-91: a large scale database for computational painting categorization, Machine vision and applications 25(6) (2014) 1385–1397. [4] S. Lyu D. Rockmore H. Farid A digital technique for art authentication Proc. Natl. Acad. Sci. 101 49 2004 17006 17010 S. Lyu, D. Rockmore, H. Farid, A digital technique for art authentication, Proceedings of the National Academy of Sciences 101(49) (2004) 17006–17010. [5] D. Kim S.-W. Son H. Jeong Large-scale quantitative analysis of painting arts Sci. Rep. 4 2014 7370 D. Kim, S.-W. Son, H. Jeong, Large-scale quantitative analysis of painting arts, Scientific reports 4 (2014) 7370. [6] H. Zhang S. Sfarra K. Saluja J. Peeters J. Fleuret Y. Duan H. Fernandes N. Avdelidis C. Ibarra-Castanedo X. Maldague Non-destructive investigation of paintings on canvas by continuous wave terahertz imaging and flash thermography J. Nondestruct. Eval. 36 2 2017 34 H. Zhang, S. Sfarra, K. Saluja, J. Peeters, J. Fleuret, Y. Duan, H. Fernandes, N. Avdelidis, C. Ibarra-Castanedo, X. Maldague, Non-destructive investigation of paintings on canvas by continuous wave terahertz imaging and flash thermography, Journal of Nondestructive Evaluation 36(2) (2017) 34. [7] H. Zenil S. Hernández-Orozco N.A. Kiani F. Soler-Toscano A. Rueda-Toicen J. Tegnér A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity Entropy 20 8 2018 605 H. Zenil, S. Hernández-Orozco, N. A. Kiani, F. Soler-Toscano, A. Rueda-Toicen, J. Tegnér, A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity, Entropy 20(8) (2018) 605. [8] J.-P. Delahaye H. Zenil Numerical evaluation of algorithmic complexity for short strings: a glance into the innermost structure of randomness Applied Mathematics and Computation 219 1 2012 63 77 10.1016/j.amc.2011.10.006 J.-P. Delahaye, H. Zenil, Numerical evaluation of algorithmic complexity for short strings: A glance into the innermost structure of randomness, Applied Mathematics and Computation 219(1) (2012) 63–77. Towards a Computational Interpretation of Physical Theories. 10.1016/j.amc.2011.10.006 Towards a Computational Interpretation of Physical Theories. [9] F. Soler-Toscano H. Zenil J.-P. Delahaye N. Gauvrit Calculating Kolmogorov complexity from the output frequency distributions of small Turing machines PloS one 9 5 2014 F. Soler-Toscano, H. Zenil, J.-P. Delahaye, N. Gauvrit, Calculating Kolmogorov complexity from the output frequency distributions of small Turing machines, PloS one 9(5) (2014). [10] J. Smiers Arts Under Pressure: Protecting Cultural Diversity in the Age of Globalisation 2003 Zed Books J. Smiers, Arts under pressure: Protecting cultural diversity in the age of globalisation, Zed Books, 2003. [11] P.J. Ferreira A.J. Pinho A method to detect repeated unknown patterns in an image International Conference Image Analysis and Recognition 2014 Springer 12 19 P. J. Ferreira, A. J. Pinho, A method to detect repeated unknown patterns in an image, in: International Conference Image Analysis and Recognition, Springer, 2014, pp. 12–19. [12] A.J. Pinho P.J. Ferreira Finding unknown repeated patterns in images 2011 19th European Signal Processing Conference 2011 IEEE 584 588 A. J. Pinho, P. J. Ferreira, Finding unknown repeated patterns in images, in: 2011 19th European Signal Processing Conference, IEEE, 2011, pp. 584–588. [13] D. Pratas A.J. Pinho On the detection of unknown locally repeating patterns in images International Conference Image Analysis and Recognition 2012 Springer 158 165 D. Pratas, A. J. Pinho, On the detection of unknown locally repeating patterns in images, in: International Conference Image Analysis and Recognition, Springer, 2012, pp. 158–165. [14] A. Romashchenko A. Shen N. Vereshchagin Combinatorial interpretation of Kolmogorov complexity Theor. Comput. Sci. 271 1–2 2002 111 123 A. Romashchenko, A. Shen, N. Vereshchagin, Combinatorial interpretation of Kolmogorov complexity, Theoretical Computer Science 271(1-2) (2002) 111–123. [15] R.K. Niven Combinatorial entropies and statistics Eur. Phys. J. B 70 1 2009 49 63 R. K. Niven, Combinatorial entropies and statistics, The European Physical Journal B 70(1) (2009) 49–63. [16] S. Mantaci A. Restivo G. Rosone M. Sciortino A new combinatorial approach to sequence comparison Theory Comput. Syst. 42 3 2008 411 429 S. Mantaci, A. Restivo, G. Rosone, M. Sciortino, A new combinatorial approach to sequence comparison, Theory of Computing Systems 42(3) (2008) 411–429. [17] C.E. Shannon A mathematical theory of communication Bell Syst. Tech. J. 27 3 1948 379 423 C. E. Shannon, A mathematical theory of communication, Bell system technical journal 27(3) (1948) 379–423. [18] A.N. Kolmogorov Three approaches to the quantitative definition of information Probl. Inf. Transm. 1 1 1965 1 7 A. N. Kolmogorov, Three approaches to the quantitative definition of information’, Problems of information transmission 1(1) (1965) 1–7. [19] R.J. Solomonoff A formal theory of inductive inference. Part I Inf. Control 7 1 1964 1 22 R. J. Solomonoff, A formal theory of inductive inference. Part I, Information and control 7(1) (1964a) 1–22. [20] R.J. Solomonoff A formal theory of inductive inference. Part II Inf. Control 7 2 1964 224 254 R. J. Solomonoff, A formal theory of inductive inference. Part II, Information and control 7(2) (1964b) 224–254. [21] G.J. Chaitin On the length of programs for computing finite binary sequences J. ACM (JACM) 13 4 1966 547 569 G. J. Chaitin, On the length of programs for computing finite binary sequences, Journal of the ACM (JACM) 13(4) (1966) 547–569. [22] F. Soler-Toscano H. Zenil A computable measure of algorithmic probability by finite approximations with an application to integer sequences Complexity 2017 2017 F. Soler-Toscano, H. Zenil, A computable measure of algorithmic probability by finite approximations with an application to integer sequences, Complexity 2017 (2017). [23] N. Gauvrit H. Zenil F. Soler-Toscano J.-P. Delahaye P. Brugger Human behavioral complexity peaks at age 25 PLoS Comput. Biol. 13 4 2017 N. Gauvrit, H. Zenil, F. Soler-Toscano, J.-P. Delahaye, P. Brugger, Human behavioral complexity peaks at age 25, PLoS computational biology 13(4) (2017). [24] M. Li J.H. Badger X. Chen S. Kwong P. Kearney H. Zhang An information-based sequence distance and its application to whole mitochondrial genome phylogeny Bioinformatics 17 2 2001 149 154 M. Li, J. H. Badger, X. Chen, S. Kwong, P. Kearney, H. Zhang, An information-based sequence distance and its application to whole mitochondrial genome phylogeny, Bioinformatics 17(2) (2001) 149–154. [25] R. Cilibrasi P.M.B. Vitányi Clustering by compression IEEE Trans. Inf. Theory 51 4 2005 1523 1545 R. Cilibrasi, P. M. B. Vitányi, Clustering by compression, IEEE Transactions on Information theory 51(4) (2005) 1523–1545. [26] R. Cilibrasi P. Vitanyi Automatic extraction of meaning from the web 2006 IEEE International Symposium on Information Theory 2006 2309 2313 R. Cilibrasi, P. Vitanyi, Automatic extraction of meaning from the web, in: 2006 IEEE International Symposium on Information Theory, 2006, pp. 2309–2313. [27] M. Cebrián M. Alfonseca A. Ortega The normalized compression distance is resistant to noise IEEE Trans. Inf. Theory 53 5 2007 1895 1900 M. Cebrián, M. Alfonseca, A. Ortega, The normalized compression distance is resistant to noise, IEEE Transactions on Information Theory 53(5) (2007) 1895–1900. [28] A.R. Cohen P.M.B. Vitányi Normalized compression distance of multisets with applications IEEE Trans. Pattern Anal. Mach.Intell. 37 8 2014 1602 1614 A. R. Cohen, P. M. B. Vitányi, Normalized compression distance of multisets with applications, IEEE transactions on pattern analysis and machine intelligence 37(8) (2014) 1602–1614. [29] D. Pratas A.J. Pinho On the approximation of the Kolmogorov complexity for DNA sequences Iberian Conference on Pattern Recognition and Image Analysis 2017 Springer 259 266 D. Pratas, A. J. Pinho, On the approximation of the Kolmogorov complexity for DNA sequences, in: Iberian Conference on Pattern Recognition and Image Analysis, Springer, 2017, pp. 259–266. [30] S.S. Maniccam N. Bourbakis Lossless compression and information hiding in images Pattern Recognit. 37 3 2004 475 486 S. S. Maniccam, N. Bourbakis, Lossless compression and information hiding in images, Pattern Recognition 37(3) (2004) 475–486. [31] Z.-M. Lu S.-Z. Guo Lossless Information Hiding in Images 2016 Syngress Z.-M. Lu, S.-Z. Guo, Lossless information hiding in images, Syngress, 2016. [32] D. Pratas A.J. Pinho P.J. Ferreira Efficient compression of genomic sequences 2016 Data Compression Conference (DCC) 2016 IEEE 231 240 D. Pratas, A. J. Pinho, P. J. Ferreira, Efficient compression of genomic sequences, in: 2016 Data Compression Conference (DCC), IEEE, 2016, pp. 231–240. [33] D. Pratas M. Hosseini A.J. Pinho Substitutional tolerant Markov models for relative compression of DNA sequences International Conference on Practical Applications of Computational Biology & Bioinformatics 2017 Springer 265 272 D. Pratas, M. Hosseini, A. J. Pinho, Substitutional tolerant markov models for relative compression of DNA sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2017, pp. 265–272. [34] A.J. Pinho A.J.R. Neves P.J. Ferreira Inverted-repeats-aware finite-context models for DNA coding 2008 16th European Signal Processing Conference 2008 IEEE 1 5 A. J. Pinho, A. J. R. Neves, P. J. Ferreira, Inverted-repeats-aware finite-context models for DNA coding, in: 2008 16th European Signal Processing Conference, IEEE, 2008, pp. 1–5. [35] M. Mahoney, Data Compression Programs, 2020. accessed May 16, [36] J. Rissanen Modeling by shortest data description Automatica 14 5 1978 465 471 J. Rissanen, Modeling by shortest data description, Automatica 14(5) (1978) 465–471. [37] M. Li X. Chen X. Li B. Ma P.M.B. Vitányi The similarity metric IEEE Trans. Inf. Theory 50 12 2004 3250 3264 M. Li, X. Chen, X. Li, B. Ma, P. M. B. Vitányi, The similarity metric, IEEE transactions on Information Theory 50(12) (2004) 3250–3264. [38] M. Li P. Vitányi An Introduction to Kolmogorov Complexity and its Applications vol. 3 2008 Springer M. Li, P. Vitányi, et al., An introduction to Kolmogorov complexity and its applications, volume 3, Springer, 2008. [39] R.P. Taylor A.P. Micolich D. Jonas Fractal analysis of Pollock’s drip paintings Nature 399 6735 1999 R. P. Taylor, A. P. Micolich, D. Jonas, Fractal analysis of Pollock’s drip paintings, Nature 399(6735) (1999) 422–422. 422–422 [40] C.R. Johnson E. Hendriks I.J. Berezhnoy E. Brevdo S.M. Hughes I. Daubechies J. Li E. Postma J.Z. Wang Image processing for artist identification IEEE Signal Process. Mag. 25 4 2008 37 48 C. R. Johnson, E. Hendriks, I. J. Berezhnoy, E. Brevdo, S. M. Hughes, I. Daubechies, J. Li, E. Postma, J. Z. Wang, Image processing for artist identification, IEEE Signal Processing Magazine 25(4) (2008) 37–48. [41] J. Li J.Z. Wang Studying digital imagery of ancient paintings by mixtures of stochastic models IEEE Trans. Image Process. 13 3 2004 340 353 J. Li, J. Z. Wang, Studying digital imagery of ancient paintings by mixtures of stochastic models, IEEE Transactions on Image Processing 13(3) (2004) 340–353. [42] M. Bressan C. Cifarelli F. Perronnin An analysis of the relationship between painters based on their work 2008 15th IEEE International Conference on Image Processing 2008 IEEE 113 116 M. Bressan, C. Cifarelli, F. Perronnin, An analysis of the relationship between painters based on their work, in: 2008 15th IEEE International Conference on Image Processing, IEEE, 2008, pp. 113–116. [43] B.A. Olshausen M.R. DeWeese Applied mathematics: the statistics of style Nature 463 7284 2010 1027 B. A. Olshausen, M. R. DeWeese, Applied mathematics: The statistics of style, Nature 463(7284) (2010) 1027. [44] J.M. Hughes D.J. Graham D.N. Rockmore Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder Proc. Natl. Acad. Sci. 107 4 2010 1279 1283 J. M. Hughes, D. J. Graham, D. N. Rockmore, Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder, Proceedings of the National Academy of Sciences 107(4) (2010) 1279–1283. [45] D.G. Stork Y. Furuichi Image analysis of paintings by computer graphics synthesis: an investigation of the illumination in Georges de la Tour’s Christ in the carpenter’s studio Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100J D. G. Stork, Y. Furuichi, Image analysis of paintings by computer graphics synthesis: an investigation of the illumination in Georges de la Tour’s Christ in the carpenter’s studio, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100J. [46] M. Lettner R. Sablatnig Estimating the original drawing trace of painted strokes Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100C M. Lettner, R. Sablatnig, Estimating the original drawing trace of painted strokes, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100C. [47] M. Shahram D.G. Stork D. Donoho Recovering layers of brush strokes through statistical analysis of color and shape: an application to van Gogh’s self portrait with grey felt hat Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100D M. Shahram, D. G. Stork, D. Donoho, Recovering layers of brush strokes through statistical analysis of color and shape: an application to van Gogh’s\" self portrait with grey felt hat\", in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100D. [48] S.B. Hedges Image analysis of renaissance copperplate prints Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 681009 S. B. Hedges, Image analysis of renaissance copperplate prints, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 681009. [49] V.M. Petrov Entropy and stability in painting: an information approach to the mechanisms of artistic creativity Leonardo 35 2 2002 197 202 V. M. Petrov, Entropy and stability in painting: An information approach to the mechanisms of artistic creativity, Leonardo 35(2) (2002) 197–202. [50] J.T. Machado A.M. Lopes Artistic painting: a fractional calculus perspective Appl. Math. Modell. 65 2019 614 626 J. T. Machado, A. M. Lopes, Artistic painting: A fractional calculus perspective, Applied Mathematical Modelling 65 (2019) 614–626. [51] K.-C. Peng T. Chen Cross-layer features in convolutional neural networks for generic classification tasks 2015 IEEE International Conference on Image Processing (ICIP) 2015 IEEE 3057 3061 K.-C. Peng, T. Chen, Cross-layer features in convolutional neural networks for generic classification tasks, in: 2015 IEEE International Conference on Image Processing (ICIP), IEEE, 2015, pp. 3057–3061. [52] H. Mao M. Cheung J. She DeepArt: learning joint representations of visual arts Proceedings of the 25th ACM International conference on Multimedia 2017 1183 1191 H. Mao, M. Cheung, J. She, Deepart: Learning joint representations of visual arts, in: Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1183–1191. [53] W. Chu Y. Wu Image style classification based on learnt deep correlation features IEEE Trans. Multimed. 20 9 2018 2491 2502 W. Chu, Y. Wu, Image style classification based on learnt deep correlation features, IEEE Transactions on Multimedia 20(9) (2018) 2491–2502. [54] D. Hammer A. Romashchenko A. Shen N. Vereshchagin Inequalities for Shannon entropy and Kolmogorov complexity J. Comput. Syst. Sci. 60 2 2000 442 464 D. Hammer, A. Romashchenko, A. Shen, N. Vereshchagin, Inequalities for Shannon entropy and Kolmogorov complexity, Journal of Computer and System Sciences 60(2) (2000) 442–464. [55] T. Henriques H. Gonçalves L. Antunes M. Matias J.a. Bernardes C. Costa-Santos Entropy and compression: two measures of complexity J. Eval. Clin. Pract. 19 6 2013 1101 1106 T. Henriques, H. Gonçalves, L. Antunes, M. Matias, J. a. Bernardes, C. Costa-Santos, Entropy and compression: two measures of complexity, Journal of Evaluation in Clinical Practice 19(6) (2013) 1101–1106. [56] S. Terwijn L. Torenvliet P.M.B. Vitányi Nonapproximablity of the normalized information distance CoRR 2009 abs/0910.4353 S. Terwijn, L. Torenvliet, P. M. B. Vitányi, Nonapproximablity of the normalized information distance, CoRR abs/0910.4353 (2009). 0910.4353 [57] A. Rybalov On the strongly generic undecidability of the halting problem Theor. Comput. Sci. 377 1–3 2007 268 270 A. Rybalov, On the strongly generic undecidability of the halting problem, Theoretical Computer Science 377(1-3) (2007) 268–270. [58] P. Bloem F. Mota S. de Rooij L. Antunes P. Adriaans A safe approximation for Kolmogorov complexity International Conference on Algorithmic Learning Theory 2014 Springer 336 350 P. Bloem, F. Mota, S. de Rooij, L. Antunes, P. Adriaans, A safe approximation for Kolmogorov complexity, in: International Conference on Algorithmic Learning Theory, Springer, 2014, pp. 336–350. [59] R.R. Sokal A statistical method for evaluating systematic relationships Univ. Kansas, Sci. Bull. 38 1958 1409 1438 R. R. Sokal, A statistical method for evaluating systematic relationships, Univ. Kansas, Sci. Bull. 38 (1958) 1409–1438. [60] J.B. Kruskal On the shortest spanning subtree of a graph and the traveling salesman problem Proc. Am. Math. Soc. 7 1 1956 48 50 J. B. Kruskal, On the shortest spanning subtree of a graph and the traveling salesman problem, Proceedings of the American Mathematical society 7(1) (1956) 48–50. [61] S. Lloyd Least squares quantization in PCM IEEE Trans. Inf. Theory 28 2 1982 129 137 S. Lloyd, Least squares quantization in PCM, IEEE transactions on information theory 28(2) (1982) 129–137. [62] D.S. Taubman M.W. Marcellin JPEG2000: image compression fundamentals Stand. Pract. 11 2 2002 D. S. Taubman, M. W. Marcellin, JPEG2000: Image compression fundamentals, Standards and Practice 11(2) (2002). [63] J. Gailly, M. Adler, The gzip home page, 2020. accessed May 16, [64] bzip2, 2020. accessed May 16, [65] L. Collin, XZ Utils, 2020. accessed May 16, [66] I. Pavlov, 7-Zip, 2020. accessed May 16, [67] M. Hosseini D. Pratas A.J. Pinho AC: a compression tool for amino acid sequences Interdiscip. Sci. Comput. Life Sci. 11 1 2019 68 76 M. Hosseini, D. Pratas, A. J. Pinho, AC: A compression tool for amino acid sequences, Interdisciplinary Sciences: Computational Life Sciences 11(1) (2019) 68–76. [68] J. Cleary I. Witten Data compression using adaptive coding and partial string matching IEEE Trans. Commun. 32 4 1984 396 402 J. Cleary, I. Witten, Data compression using adaptive coding and partial string matching, IEEE transactions on Communications 32(4) (1984) 396–402. [69] A.J. Buchner, PAQ, 2020. accessed May 16, [70] M.V. Mahoney Adaptive weighing of context models for lossless data compression Technical Report 2005 Florida Institute of Technology CS Department of the W University Blvd M. V. Mahoney, Adaptive weighing of context models for lossless data compression, Technical Report, Florida Institute of Technology CS Department of the W University Blvd, 2005. [71] J. Rissanen G.G. Langdon Arithmetic coding IBM J. Res. Dev. 23 2 1979 149 162 J. Rissanen, G. G. Langdon, Arithmetic coding, IBM Journal of research and development 23(2) (1979) 149–162. [72] A. Moffat R.M. Neal I.H. Witten Arithmetic coding revisited ACM Trans. Inf. Syst. (TOIS) 16 3 1998 256 294 A. Moffat, R. M. Neal, I. H. Witten, Arithmetic coding revisited, ACM Transactions on Information Systems (TOIS) 16(3) (1998) 256–294. [73] B. Knoll N. de Freitas A machine learning perspective on predictive coding with PAQ8 2012 Data Compression Conference 2012 IEEE 377 386 B. Knoll, N. de Freitas, A machine learning perspective on predictive coding with PAQ8, in: 2012 Data Compression Conference, IEEE, 2012, pp. 377–386. [74] Best Artworks of All Time, 2020. accessed May 18, [75] Diabetic Retinopathy Detection, 2020. accessed May 18, [76] X. Wang Y. Peng L. Lu Z. Lu M. Bagheri R.M. Summers ChestX-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases IEEE CVPR 2017 3462 3471 X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, in: IEEE CVPR, 2017, pp. 3462–3471. [77] COCO - Common Objects in Context, 2020. accessed May 18, [78] C. Shapiro Abstract expressionism: the politics of apolitical painting Prospects 3 1978 175 214 C. Shapiro, et al., Abstract Expressionism: The politics of apolitical painting, Prospects 3 (1978) 175–214. [79] H. Rosenberg The Tradition of The New 1994 Hachette Books H. Rosenberg, The Tradition Of The New, Hachette Books, 1994. [80] L. Garrard Colourfield Painting: Minimal, Cool, Hard Edge, Serial and Post-Painterly Abstract Art from the Sixties to the Present 2007 Crescent Moon Publishing L. Garrard, Colourfield painting: Minimal, Cool, Hard Edge, Serial and Post-painterly Abstract Art from the Sixties to the present, Crescent Moon Publishing, 2007. [81] D. Hockney Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters 2006 Viking Studio D. Hockney, Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters, Viking Studio, 2006. [82] S. Yang G. Cheung P. Le Callet J. Liu Z. Guo Computational modeling of artistic intention: quantify lighting surprise for painting analysis 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX) 2016 IEEE 1 6 S. Yang, G. Cheung, P. Le Callet, J. Liu, Z. Guo, Computational modeling of artistic intention: Quantify lighting surprise for painting analysis, in: 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), IEEE, 2016, pp. 1–6. [83] L. Fichner-Rathus Foundations of Art and Design: An Enhanced Media Edition 2011 Cengage Learning L. Fichner-Rathus, Foundations of art and design: An enhanced media edition, Cengage Learning, 2011. [84] T. Chen C. Guestrin XGBoost: a scalable tree boosting system Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD ’16 2016 ACM New York, NY, USA 785 794 10.1145/2939672.2939785 T. Chen, C. Guestrin, XGBoost: A scalable tree boosting system, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, ACM, New York, NY, USA, 2016, pp. 785–794. 10.1145/2939672.2939785 [85] L. Nanni S. Ghidoni S. Brahnam Handcrafted vs. non-handcrafted features for computer vision classification Pattern Recognit. 71 2017 158 172 10.1016/j.patcog.2017.05.025 L. Nanni, S. Ghidoni, S. Brahnam, Handcrafted vs. non-handcrafted features for computer vision classification, Pattern Recognition 71 (2017) 158–172. 10.1016/j.patcog.2017.05.025 Jorge Miguel Silva is a Ph.D. Student and Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), a Computer Science and Engineering research unit. He holds a Master’s degree in Bioengineering from the Faculty of Engineering at the University of Porto. His research interests are Information Theory, Artificial Intelligence and Compression. Diogo Pratas is a computer scientist at IEETA/DETI at the University of Aveiro. Diogo’s research interests include computational biology, data compression, and information theory. Diogo earned his Ph.D. in computer science from the University of Aveiro and holds a B.S. degree in information and communications technologies also from the University of Aveiro, with a segment carried at the Pontifical University of Salamanca. He joined IEETA/DETI in 2009, after being working some years in the private sector. Diogo actively collaborates with the Department of Virology at the University of Helsinki, namely in the identification and reconstruction of viral genomes, and the respective human-hosts, at multi-organ level. Diogo is a member of the Super Dimension Fortress, the International Society for Computational Biology, and the Portuguese Association for Pattern Recognition. Rui Antunes is a Ph.D. Student and Research Fellow at IEETA, a Computer Science and Engineering research unit. He holds a Master’s degree in Electronic and Telecommunications Engineering from University of Aveiro. His research interests include artificial intelligence, machine learning, natural language processing, and information extraction. Sergio Matos graduated in Systems Engineering and Computing by the University of Algarve (Portugal) in 1999 and received his Ph.D. in computer engineering in 2007 by the University of Leicester (UK), where he developed a system, based on speech processing and recognition technologies, for monitoring patients with acute or chronic cough. Sergio Matos is an integrated member of the Biomedical Informatics and Technologies group at IEETA, University of Aveiro, since 2008 and Assistant Professor at the Department of Electronics, Telecommunications and Informatics since 2018. His research interests are information retrieval, text mining, NLP and machine-learning. Armando J. Pinho received the Electronics and Telecommunications Engineering degree from the University of Aveiro, Portugal, in 1988, the Master’s degree in electrical and computers engineering from IST, Technical University of Lisbon, Portugal, in 1991 and the Ph.D. in electrical engineering from the University of Aveiro, in 1996. Currently, Armando J. Pinho is an Associate Professor with habilitation at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, and a researcher at the Signal Processing Laboratory of the Institute of Electronics and Telematics Engineering of Aveiro - IEETA. His main research activity is in the area of image coding, data compression, and computational biology. He also has interests in other areas of research, such as bioinformatics and image and video analysis. "
    },
    {
        "doc_title": "Statistical n-Best AFD-Based Sparse Representation for ECG Biometric Identification",
        "doc_scopus_id": "85117096492",
        "doc_doi": "10.1109/TIM.2021.3119138",
        "doc_eid": "2-s2.0-85117096492",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adaptive fourier decompositions",
            "Biometric identifications",
            "Biometric recognition",
            "Electrocardiogram",
            "Identification method",
            "Personal identification",
            "Real- time",
            "Sparse representation",
            "Statistical n-best AFD",
            "Time-frequency representations"
        ],
        "doc_abstract": "© 1963-2012 IEEE.Electrocardiogram (ECG) biometric recognition as a personal identification method is receiving more and more attention because it can support live verification results. Compared with other biometric-based methods, it can provide higher security performance. The difficulty of the problem lies in how to stably extract ECG signal features and achieve real-time verification. In this study, a new type of sparse representation learning framework called statistical $n$ -best adaptive Fourier decomposition (SAFD) originated by Qian is adopted in ECG biometric identification. Adaptive Fourier decomposition (AFD) is a recently developed combination of transform-based signal decomposition and sparse representation method, which can adaptively select the atoms from a redundant dictionary through orthogonal processing. The advantage of the AFD-type methods is that each atom in the dictionary has a precise mathematical formula with good analytic properties. This characteristic is significantly distinguished it from other existing sparse representations, where the atoms learned are usually matrix data and cannot be described mathematically. The proposed SAFD extends the existing $n$ -best AFD from processing single signal to multi-signals and implements the $n$ -best AFD in the stochastic Hardy space. Therefore, the small number of learned atoms by SAFD is sufficient to capture internal structure and robustness of the signal and generate a discriminative representation that reflects the time-frequency characteristics of signals. It is very suitable for non-stationary signals like ECG. The proof of convergence of the algorithm is presented. Extensive experiments are conducted on five public databases collected in different realistic conditions, and an average identification accuracy of 98.0% is achieved. In addition, less than 1 ms for one matching process makes it possible to be implemented in real time. Experimental results demonstrate that the proposed method can achieve superior performance compared to other state-of-the-art ECG biometric identification methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ac2: An efficient protein sequence compression tool using artificial neural networks and cache-hash models",
        "doc_scopus_id": "85105434500",
        "doc_doi": "10.3390/e23050530",
        "doc_eid": "2-s2.0-85105434500",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.Recently, the scientific community has witnessed a substantial increase in the generation of protein sequence data, triggering emergent challenges of increasing importance, namely efficient storage and improved data analysis. For both applications, data compression is a straightforward solution. However, in the literature, the number of specific protein sequence compressors is relatively low. Moreover, these specialized compressors marginally improve the compression ratio over the best general-purpose compressors. In this paper, we present AC2, a new lossless data compressor for protein (or amino acid) sequences. AC2 uses a neural network to mix experts with a stacked generalization approach and individual cache-hash memory models to the highest-context orders. Compared to the previous compressor (AC), we show gains of 2–9% and 6–7% in reference-free and reference-based modes, respectively. These gains come at the cost of three times slower computations. AC2 also improves memory usage against AC, with requirements about seven times lower, without being affected by the sequences’ input size. As an analysis application, we use AC2 to measure the similarity between each SARS-CoV-2 protein sequence with each viral protein sequence from the whole UniProt database. The results consistently show higher similarity to the pangolin coronavirus, followed by the bat and human coronaviruses, contributing with critical results to a current controversial subject. AC2 is available for free download under GPLv3 license.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Efficient DNA sequence compression with neural networks",
        "doc_scopus_id": "85096082682",
        "doc_doi": "10.1093/gigascience/giaa119",
        "doc_eid": "2-s2.0-85096082682",
        "doc_date": "2020-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Base Sequence",
            "High-Throughput Nucleotide Sequencing",
            "Neural Networks, Computer",
            "Sequence Analysis, DNA",
            "Software"
        ],
        "doc_abstract": "© 2020 The Author(s). Published by Oxford University Press GigaScience.Background: The increasing production of genomic data has led to an intensified need for models that can cope efficiently with the lossless compression of DNA sequences. Important applications include long-term storage and compression-based data analysis. In the literature, only a few recent articles propose the use of neural networks for DNA sequence compression. However, they fall short when compared with specific DNA compression tools, such as GeCo2. This limitation is due to the absence of models specifically designed for DNA sequences. In this work, we combine the power of neural networks with specific DNA models. For this purpose, we created GeCo3, a new genomic sequence compressor that uses neural networks for mixing multiple context and substitution-tolerant context models. Findings: We benchmark GeCo3 as a reference-free DNA compressor in 5 datasets, including a balanced and comprehensive dataset of DNA sequences, the Y-chromosome and human mitogenome, 2 compilations of archaeal and virus genomes, 4 whole genomes, and 2 collections of FASTQ data of a human virome and ancient DNA. GeCo3 achieves a solid improvement in compression over the previous version (GeCo2) of 2.4%, 7.1%, 6.1%, 5.8%, and 6.0%, respectively. To test its performance as a reference-based DNA compressor, we benchmark GeCo3 in 4 datasets constituted by the pairwise compression of the chromosomes of the genomes of several primates. GeCo3 improves the compression in 12.4%, 11.7%, 10.8%, and 10.1% over the state of the art. The cost of this compression improvement is some additional computational time (1.7-3 times slower than GeCo2). The RAM use is constant, and the tool scales efficiently, independently of the sequence size. Overall, these values outperform the state of the art. Conclusions: GeCo3 is a genomic sequence compressor with a neural network mixing approach that provides additional gains over top specific genomic compressors. The proposed mixing method is portable, requiring only the probabilities of the models as inputs, providing easy adaptation to other data compressors or compression-based data analysis tools. GeCo3 is released under GPLv3 and is available for free download at https://github.com/cobilab/geco3.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GTO: A toolkit to unify pipelines in genomic and proteomic research",
        "doc_scopus_id": "85086630979",
        "doc_doi": "10.1016/j.softx.2020.100535",
        "doc_eid": "2-s2.0-85086630979",
        "doc_date": "2020-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "C language",
            "Life-sciences",
            "Modular architectures",
            "Next-generation sequencing",
            "Proteomic"
        ],
        "doc_abstract": "© 2020 The AuthorsNext-generation sequencing triggered the production of a massive volume of publicly available data and the development of new specialised tools. These tools are dispersed over different frameworks, making the management and analyses of the data a challenging task. Additionally, new targeted tools are needed, given the dynamics and specificities of the field. We present GTO, a comprehensive toolkit designed to unify pipelines in genomic and proteomic research, which combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of the data. This toolkit combines novel tools with a modular architecture, being an excellent platform for experimental scientists, as well as a useful resource for teaching bioinformatics enquiry to students in life sciences. GTO is implemented in C language and is available, under the MIT license, at https://bioinformatics.ua.pt/gto.",
        "available": true,
        "clean_text": "serial JL 312019 291210 291690 291735 291791 291848 31 90 SoftwareX SOFTWAREX 2020-06-20 2020-06-20 2020-06-20 2020-06-20 2021-06-24T16:39:01 S2352-7110(20)30147-3 S2352711020301473 10.1016/j.softx.2020.100535 S300 S300.3 FULL-TEXT 2021-06-24T16:23:59.139745Z 0 0 20200701 20201231 2020 2020-06-20T23:10:09.350353Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid primabst pubtype ref 2352-7110 23527110 UNLIMITED NONE true 12 12 C Volume 12 21 100535 100535 100535 202007 202012 July–December 2020 2020-07-01 2020-12-31 2020 simple-article osp © 2020 The Authors. Published by Elsevier B.V. GTOATOOLKITUNIFYPIPELINESINGENOMICPROTEOMICRESEARCH ALMEIDA J 1 Motivation and significance 2 Software description 2.1 Software architecture 2.2 Software functionalities 2.2.1 Genomics 2.2.2 Proteomics 2.2.3 General purpose 2.2.4 External tools 3 Illustrative examples 3.1 Bi-directional complexity profiles 3.2 Rearrangements map generation 3.3 Viral metagenomic identification 4 Impact 5 Conclusions Acknowledgements References MARDIS 2017 213 E VANDERAUWERA 2013 G KESSNER 2008 2534 2536 D CHEN 2018 i884 i890 S LIU 2019 4560 4567 Y GRABOWSKI 2019 677 678 S MALYSA 2015 3122 3129 G LIU 2019 2066 2074 Y CHANDAK 2019 2674 2676 S PINHO 2013 e79922 A PINHO 2011 2024 2028 A 201119THEUROPEANSIGNALPROCESSINGCONFERENCE SYMBOLICNUMERICALCONVERSIONDNASEQUENCESUSINGFINITECONTEXTMODELS PRATAS 2019 137 145 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS GECO2OPTIMIZEDTOOLFORLOSSLESSCOMPRESSIONANALYSISDNASEQUENCES HOSSEINI 2019 68 76 M CARVALHO 2018 49 55 J HOSSEINI 2016 56 M AGUEROCHAPIN 2006 723 730 G PRATAS 2015 10203 D ZIELEZINSKI 2019 A BENCHMARKINGALIGNMENTFREESEQUENCECOMPARISONMETHODS FORSLUND 2019 469 504 S EVOLUTIONARYGENOMICS EVOLUTIONPROTEINDOMAINARCHITECTURES ROST 1999 85 94 B PRATAS 2018 D FALCONAMETHODINFERMETAGENOMICCOMPOSITIONANCIENTDNA PRATAS 2018 1177 1181 D 201826THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO METAGENOMICCOMPOSITIONANALYSISSEDIMENTARYANCIENTDNAISLEWIGHT HUANG 2011 593 594 W DROOP 2016 1883 1884 A AFGAN 2018 W537 W544 E SHEN 2016 e0163962 W DEPRISTO 2011 491 M GOECKS 2010 R86 J BLANKENBERG 2010 1783 1785 D LIU 2017 3364 3372 Y OCHOA 2014 626 633 I DEOROWICZ 2015 11565 S PRATAS 2018 105 113 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS COMPRESSIONAMINOACIDSEQUENCES PRATAS 2014 40 D ESCALONA 2016 459 M ALMEIDAX2020X100535 ALMEIDAX2020X100535XJ Full 2020-06-03T16:10:38Z Author This is an open access article under the CC BY license. © 2020 The Authors. Published by Elsevier B.V. 2021-06-03T16:43:35.966Z Foundation for Science and Technology SFRH/BD/147837/2019 UIDB/00127/2020 FCT Fundação para a Ciência e a Tecnologia Centro 2020 program NETDIAMOND POCI-01-0145-FEDER-016385 This work has received support from the NETDIAMOND, Portugal project ( POCI-01-0145-FEDER-016385 ), co-funded by Centro 2020 program, Portugal 2020, European Union , and from the FCT - Foundation for Science and Technology, Portugal , in the context of the project UIDB/00127/2020. João Almeida is supported by FCT - Foundation for Science and Technology, Portugal (national funds), grant SFRH/BD/147837/2019 . item S2352-7110(20)30147-3 S2352711020301473 10.1016/j.softx.2020.100535 312019 2021-06-24T16:23:59.139745Z 2020-07-01 2020-12-31 UNLIMITED NONE true 1061501 MAIN 6 51343 849 656 IMAGE-WEB-PDF 1 gr3 21082 116 376 gr1 51221 263 470 gr4 24493 141 376 fx1001 4519 27 817 gr2 16382 83 470 gr3 6347 68 219 gr1 10351 123 219 gr4 7605 82 219 fx1001 777 7 219 gr2 3191 39 219 gr3 155334 516 1668 gr1 436164 1166 2083 gr4 160054 624 1667 fx1001 16238 73 2171 gr2 128419 370 2083 si1 667 SOFTX 100535 100535 S2352-7110(20)30147-3 10.1016/j.softx.2020.100535 The Authors Fig. 1 Bi-directional complexity profiles of four types of human Herpesvirus (HHV2, HHV3, HHV4 and HHV7) generated with GTO using the pipeline: gto_complexity_profile_regions.sh. Complexity values below one are highlighted with blue colour while the others with green. Bps stands for bits per symbol where lower values represent redundancy. The length is in Kb (Kilobases) and all profiles use the same scale. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Bi-directional complexity profiles of human titin protein generated with GTO using the pipeline: gto_proteins_complexity_profile_regions.sh. Complexity values below three are highlighted with a red colour while the others with blue. Bps stands for bits per symbol where lower values represent redundancy. The length is in Ks (Kilosymbols). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Rearrangements map generated with GTO using the pipeline: gto_map_rearrangements.sh. The length of both sequences (A and B) is 5 MB. Wave pattern stands for inverted repeated regions. Fig. 4 Rearrangements map generated with GTO using the pipeline: gto_map_rearrangements_proteins.sh. Table 1 The eight most representative reference sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. ID Length RS (%) Reference GID Virus name 1 124884 97.767 X04370.1 HHV3 2 5596 96.603 AY386330.1 B19V 3 172764 94.143 DQ279927.1 HHV4 4 154675 81.400 JN561323.2 HHV2 5 154746 80.153 Z86099.2 HHV2 6 2785 78.300 AB041963.1 TTV 7 7372 71.445 MG921180.1 HPV 8 549 47.591 AY034056.1 PHV3-BALF1-gene Original software publication GTO: A toolkit to unify pipelines in genomic and proteomic research João R. Almeida a b ⁎ Armando J. Pinho a José L. Oliveira a Olga Fajarda a Diogo Pratas a c a Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal b Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain c Department of Virology, University of Helsinki, Helsinki, Finland Department of Virology, University of Helsinki Helsinki Finland Department of Virology, University of Helsinki, Helsinki, Finland ⁎ Corresponding author at: Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal. Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Next-generation sequencing triggered the production of a massive volume of publicly available data and the development of new specialised tools. These tools are dispersed over different frameworks, making the management and analyses of the data a challenging task. Additionally, new targeted tools are needed, given the dynamics and specificities of the field. We present GTO, a comprehensive toolkit designed to unify pipelines in genomic and proteomic research, which combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of the data. This toolkit combines novel tools with a modular architecture, being an excellent platform for experimental scientists, as well as a useful resource for teaching bioinformatics enquiry to students in life sciences. GTO is implemented in C language and is available, under the MIT license, at Keywords Genomic Toolkit Proteomic Toolkit Next-generation sequencing Code metadata Current code version v1.5 Permanent link to code/repository used for this code version Legal Code License MIT Code versioning system used GIT Software code languages, tools, and services used C Compilation requirements, operating environments & dependencies GCC and Make If available Link to developer documentation/manual Support email for questions pratas@ua.pt Software metadata Current software version v1.5 Permanent link to executables of this version Legal Software License MIT Computing platforms/Operating Systems Linux and Unix-like Installation requirements & dependencies GCC and Make If available, link to user manual - if formally published include a reference to the publication in the reference list Support email for questions pratas@ua.pt 1 Motivation and significance Next-generation sequencing (NGS) has become an essential tool in genetic and genomic analysis with a substantial impact in the fields of biomedicine and anthropology. The advantages of NGS over traditional methods include its multiplex capability and analytical resolution, making it a time and cost-efficient approach for fast clinical and forensic screening [1]. The development of efficient bioinformatics tools is essential to assess and analyse the large volumes of sequencing data produced by next-generation sequencers. However, more important than that are the computational methods that unify the existing tools, given the notable pace at which these tools become available. Toolkits are sets of tools that combine multiple features in a custom-based manner as some examples show, both in genomics [2] and proteomics [3]. Developing a toolkit requires a specific architecture, namely, taking into account the purpose and technologies, accessibility, compatibility, portability, interoperability, and usability. Moreover, implementation needs to consider efficiency, while maintaining affordable computational resources and the absence of dependencies (standalone use). We contribute with GTO (Genomic Toolkit), a set of tools to unify pipelines operating both at genomic and proteomic levels, with an open licence and free of any dependency. This toolkit includes information theory-based tools for reference-free and reference-based data compression applied to data analysis. Among many applications, this toolkit supports the creation of workflows for identification of metagenomic composition in FASTQ reads, detection and visualisation of genomic rearrangements, mapping and visualisation of variation, localisation of low complexity regions, or simulation of sequences with specific SNP and structural variant rates. The toolkit was designed for Unix/Linux-based systems, built for ultra-fast computations. It supports pipes for easy integration with the sub-programmes as well as external tools. GTO works as LEGOsTM , since it allows the construction of multiple pipelines with many combinations. We support the toolkit with a detailed manual and a website with several examples, including an online manual for fast learning. Due to the variety and distribution of the given tools and their tight interconnection using the command line with pipes, the toolkit is an excellent platform for scientists as well as for empowering students to progress to the scientific aspects of bioinformatics analysis efficiently. Therefore, without the need to install multiple programmes, dependencies, and read different manuals or licences, it is possible to maintain an easy-to-follow connection with all the phases of each pipeline application. 2 Software description GTO is a powerful toolkit composed of more than 75 tools with particular focus on genomics and proteomics, following an integrative and flexible design between the tools. GTO includes tools for information display, randomisation, edition, conversion, extraction, search, calculation, compression, simulation and visualisation. The toolkit can be used in common Linux distributions. We have been using GTO in common personal computers (e.g. a laptop with 8 GB RAM, 128 GB of SSD and an intel-i3 CPU from the 5th generation), but these characteristics can vary according to the data size and the execution requirements. 2.1 Software architecture The tools composing this toolkit aim for key features such as being easy to use, compile and improve and specially designed for work in Unix/Linux command line. These tools can be used in isolation, or combined as one, forming execution workflows. This is technically possible due to the two streams used for the computation, namely the standard input and output. Furthermore, the tools’ aggregation is possible with mechanisms for inter-process communication using message passing, provided by the Unix operating system. This creates a chain of processes in which the output of each process is passed directly as input to the subsequent one, as shown in the following example: In addition to the input/output standard streams, some of the tools accept parameterisation through the definition of arguments when executed. There is also a small set of tools in which the input or output does not make sense to be the standard streams and for those the argument definition is considered. 2.2 Software functionalities The toolkit contains three main groups of tools according to its characteristic: Genomics, Proteomics, and General purpose. The genomics group is subdivided in: FASTQ, FASTA, SEQ (genomic sequences); while the proteomics contains AA (amino acid); the general-purpose tools can be applied to any format sequence. 2.2.1 Genomics The toolkit allows data conversion between different formats namely FASTQ, FASTA and SEQ. It also provides features for filtering and randomising DNA sequences, as well as for analytic purposes followed by simulations of a generation and alteration nature. The SEQ cluster works directly with the DNA sequences without any standard format. These tools allow data extraction, summary, classification and mathematical operations in the field of information theory. Among many examples, which are better described in the supporting website and manual, the toolkit allows preparations of the reads, namely filtering and trimming, the automatic construction of nucleotide reference databases, and comparative genomics. 2.2.2 Proteomics The toolkit has a specific cluster of tools designed to group, compress, and analyse amino acid sequences. These tools allow proteomic analysis based on the amino acids properties, such as electric charge (positive and negative), uncharged side chains, hydrophobic side chains and special cases. The toolkit allows translation of codons into amino acids, permits finding approximate amino acid sequences and performing comparative proteomics analysis. 2.2.3 General purpose This set of tools is complementary to the genomics and proteomics tools, not being designed to work in a specific field, but to assist the pipelines composed of the previously described subsets. These tools provide operations in the symbolical domain, including reversion, segmentation, and permutation; while in the numerical domain they contain tools with low-pass filters (with multiple window types), sum, min and max operations over streams. 2.2.4 External tools External top-performing tools have been integrated in order to increase the variety of functionalities available. The tools integrated are the following: • fastp [4]: enables ultra-fast preprocessing and quality control of FASTQ files. • bfMEM [5]: detects maximal exact matches between a pair of genomes based on bloom filters and rolling hashs. • copMEM [6]: another tool for computing maximal exact matches in a pair of genomes. • qvz [7]: implements a lossy compression algorithm for storing quality scores associated with DNA sequencing. • minicom [8]: a compressor for short reads in FASTQ files that uses large k-minimisers to index the reads. • SPRING [9]: which is reference-free compression tool for FASTQ files. 3 Illustrative examples All the tools in the toolkit were tested with synthetic sequences aiming for individual validation. Therefore, the documented examples are easily replicable with the written tests. Besides applying these tools in controlled environments, the toolkit was also used in several research workflows both as a primary and auxiliary tool. Several complete workflows are available in the repository, under the pipelines folder while an extensive description of the tool can be found in the manual. Next, we include some pipeline examples. 3.1 Bi-directional complexity profiles A workflow example is the computation of bi-directional complexity profiles in any genomic or proteomic sequence [10]. These profiles can localise specific features in the sequences, namely low and high complexity sequences, inverted repeats regions, tandem duplications, among others. The construction of these profiles follows a pipeline formed of many transformations (e.g. reversing, segmenting, inverting) as well as the use of specific low-pass filters after data compression applications [11]. Fig. 1 depicts the complexity profiles of four human Herpesvirus whole genomes using the same scale, where redundant regions are highlighted in blue (below a Bps of one). GTO uses GeCo2 [12] and AC [13] compressors to estimate the local complexity of DNA and amino acid sequences, respectively. However, GTO is not limited to using these data compressors. For example, new models can be tested under this framework, namely with extended alphabets [14]. In general, any data compressor able to output local estimations can be used in the pipeline as an alternative [15]. Analogous to the complexity profiles for DNA sequences, an example using amino acid sequences is given in Fig. 2. This example depicts a bi-directional complexity profile for the largest human protein sequence, titin. Several regions with low complexity are usually associated with specific characteristics, namely loops [16]. 3.2 Rearrangements map generation Another example workflow is in the domain of comparative genomics, namely to map and visualise rearrangements. This workflow is completely automatic from the input of the sequences to the generation of an SVG image, with the associated and transformed regions corresponding to the rearrangements. The pipeline applies smash technology [17,18] for mapping the rearrangements using an alignment-free methodology [19]. To prove the efficiency of the mapping pipeline, we use another pipeline to generate two identical FASTA files with simulated rearrangements between them (gto_simulate_rearragements.sh). After, loading the two FASTA files into the mapping pipeline (gto_map_rearrangements.sh), the output is two files, one with the mapping positions and the other is an SVG image depicting the mapped positions as can be seen in Fig. 3. All the rearrangements have been efficiently mapped with GTO according to the ground truth ( < 1 s of computational time). Analogous to the rearrangements map pipeline, for mapping at proteomic level, we consider the NAV2 HUMAN Neuron navigator 2 and the neuron navigator 2 isoform X15 of Macaca mulatta proteins. Although there are many examples under the proteome evolution [20], these are protein sequences considering identical scale [21]. Additionally, we shuffled the Macaca mulatta proteins using a block size of 300 amino acids. Fig. 4 depicts the proteins map after running the pipeline (gto_map_rearrangements_proteins.sh). Despite a low level of dissimilarity of the sequences with an additional pseudo-random permutation of blocks of 300 symbols, all the regions have been efficiently mapped with GTO ( < 1 s of computational time). 3.3 Viral metagenomic identification A final workflow example is the full automatic metagenomic identification of viral (or any other) content in FASTQ reads. This includes the filtering and trimming of the reads, mapping, and sensitive identification of the most representative genomes, under a ranking of abundance. In this particular example, we generate a semi-synthetic viral dataset containing several real viruses with applied degrees of substitutions and block permutations shuffled with synthetic noisy DNA. This dataset is generated using the gto_create_viral_dataset.sh pipeline. The intention is to perform a metagenomic analysis on this dataset without informing the programme what organisms are contained in the sample since the programme needs to infer the results. Then, we compare the results with the ground truth. If the results are similar to the ground truth, then the methodology is validated. For the purpose, GTO uses falcon-meta technology [22,23] that relies on assembly-free and alignment-free comparison of each reference according to the whole reads. The dataset contains synthetic reads (uniform distribution) merged with the following viruses with the respective modifications: • B19V: two Parvovirus, one with 1% of editions and the other with permuted blocks of 500 bases (GID: AY386330.1); • HHV2: one human Herpesvirus 2 with permuted blocks of size 100 bases (GID: JN561323.2); • HHV3: one human Herpesvirus 3 (GID: X04370.1); • HHV4: two human Herpesvirus 4, one with permuted blocks of 300 bases (GID: DQ279927.1); • TTV: one human Torque teno virus with 5% of editions (GID: AB041963.1); • HPV: one human Papillomavirus with 5% of editions and permuted blocks of 300 bases (GID: MG921180.1). After merging all FASTA sequences, ART [24] was used to generate the paired end FASTQ reads. Meanwhile, another workflow example was used to create the viral database (gto_build_dbs.sh). Then, the pipeline (gto_metagenomics.sh) ran, obtaining the top output presented in Table 1. We can conclude that despite the noise, editions, and permutations applied to real data, all the viruses have been efficiently identified with GTO, including the exact genotype ( < 1.5 min of computational time). 4 Impact Many software application exist to analyse and manipulate sequencing data, namely fqtools [25], GALAXY [26], FASTX-Toolkit [27], SeqKit [28], GATK [29], among others. The fqtools is a suite of tools to view, manipulate and summarise FASTQ data [25]. This software was designed to work specifically with FASTQ files and can be easily integrated into our toolkit. However, the features existent in this software are similar to some of the ones that GTO has in the gto_fastq_* section. Both were written in C which in terms of performance could be similar. GALAXY, is an open and web-based scientific platform for analysing genomic data [30]. This platform integrates several specialised sets of tools, e.g. for manipulating FASTQ files [31]. In this web application, the FASTX-Toolkit was integrated, which is a collection of command-line tools to process FASTA and FASTQ files [27]. The available features in the FASTX-Toolkit are also similar to some of the GTO tools designed to preprocess the FASTA/FASTQ files, which are available in the gto_fastq_* and gto_fasta_* sections. As our goal always was to have an easy to use toolkit written in low-level programming languages and not a web interface, we cannot compare it with GALAXY. However, regarding the FASTX-Toolkit which was also written in C, it is possible to compare and combine it with some of the GTO’s features. The SeqKit is another toolkit used to process FASTA and FASTQ files and it is available for all major operating systems [28]. Comparing the performance and limitations of this toolkit with the fqtools and FASTX-Toolkit is easier than comparing them with GTO, mainly because these three toolkits were designed specifically to manipulate FASTA/FASTQ files. On the other hand, these functionalities are only a fraction of the features that we provide in GTO. The idea never was to create more tools to compete with the ones existing, but instead, aggregate them in order to obtain a more complete toolkit for genomics analysis. This idea of simplifying the development and aggregation of analysis tools for genomic manipulation and analysis is not new. Initially designed as a structured programming framework, the Genome Analysis Toolkit (GATK) is a set of bioinformatics tools for analysing high-throughput sequencing focused on variant discovering and genotyping [2]. The high performance of this toolkit is due to the required infrastructures that a personal computer cannot offer. This is an excellent toolkit that integrates Apache Spark for optimisation, but it is only possible to take advantage of this potential in cloud computing. The efficient performance from some of the presented tools as well as GTO’s tools is due to the use of low-level programming languages (e.g. C language). However, one limitation of this strategy, in which the performance is prioritised, is the lack of a graphical user interface. Moreover, to take full advantage from those tools, the end-users need to have basic shell script knowledge. Nevertheless, GTO combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of data. Therefore, we would like to highlight some important details that characterise this toolkit: • The toolkit aggregates different tools in order to build research pipelines to deal with very large data sets without losing performance due to its modular architecture. Adoption of standard streams to interconnect the tools improved data processing. Throughout this procedure, the disk read/write operations between tools have been removed by sending the output directly to the input of the next tool. • The toolkit can integrate external tools, besides the ones already available. As such, some specific tools that have already been evaluated and used outside this context were aggregated: – For compression purposes, the toolkit integrates GeCo2 [12], which along with HiRGC [32], iDoComp [33] and GDC2 [34] are considered to have some of the best performance for reference-free DNA compression [35]. Regarding the amino acid sequences, the toolkit uses the AC tool for lossless sequence compression. The performance of AC was compared in [36] to several general-purpose lossless compressors and several protein compressors using different proteomes and AC provides on average the best bit-rates. – Concernings simulation, GTO integrates XS [37] which is a FASTQ read simulation tool. Escalona et al. [38] reviewed 23 NGS simulation tools and XS stands out in relation to the others because it is the only one that does not need a reference sequence. – Additionally, we added a section in the toolkit specially designed for tools from other authors. This way, we simplify their integration and installation using GTO. Those were described in Section 2.2.4. • Finally, as briefly presented in Section 3, the toolkit can answer new genomics questions without the need to create new software. 5 Conclusions We contribute with GTO, a toolkit to unify research pipelines, composed of distinct tools aiming at efficient combinations of them towards specific workflows. GTO’s efficient performance is due to the use of low-level programming languages, which increases the processing speed and decreases the RAM of addressing genomics and proteomics data. The flexibility of this toolkit allows the end-user to quickly create new processing pipelines in the genomic and proteomic field as it was described in the examples provided in this manuscript. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work has received support from the NETDIAMOND, Portugal project (POCI-01-0145-FEDER-016385), co-funded by Centro 2020 program, Portugal 2020, European Union, and from the FCT - Foundation for Science and Technology, Portugal, in the context of the project UIDB/00127/2020. João Almeida is supported by FCT - Foundation for Science and Technology, Portugal (national funds), grant SFRH/BD/147837/2019. References [1] Mardis E.R. DNA sequencing technologies: 2006–2016 Nat Protoc 12 2 2017 213 E. R. Mardis, DNA sequencing technologies: 2006–2016, Nature Protocols 12 (2) (2017) 213. [2] Van der Auwera G.A. Carneiro M.O. Hartl C. Poplin R. Del Angel G. Levy-Moonshine A. From FASTQ data to high-confidence variant calls: the genome analysis toolkit best practices pipeline Curr Protoc Bioinform 43 1 2013 11.10.1-11.10.33 G. A. Van der Auwera, M. O. Carneiro, C. Hartl, R. Poplin, G. Del Angel, A. Levy-Moonshine, T. Jordan, K. Shakir, D. Roazen, J. Thibault, others,From FASTQ data to high-confidence variant calls: the genome analysis toolkit best practices pipeline, Current Protocols in Bioinformatics 43 (1) (2013) 11–10. [3] Kessner D. Chambers M. Burke R. Agus D. Mallick P. ProteoWizard: open source software for rapid proteomics tools development Bioinformatics 24 21 2008 2534 2536 D. Kessner, M. Chambers, R. Burke, D. Agus, P. Mallick, ProteoWizard: open source software for rapid proteomics tools development, Bioinformatics 24 (21) (2008) 2534–2536. [4] Chen S. Zhou Y. Chen Y. Gu J. Fastp: an ultra-fast all-in-one FASTQ preprocessor Bioinformatics 34 17 2018 i884 i890 S. Chen, Y. Zhou, Y. Chen, J. Gu, fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics 34 (17) (2018) i884–i890. [5] Liu Y. Zhang L.Y. Li J. Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers Bioinformatics 35 22 2019 4560 4567 Y. Liu, L. Y. Zhang, J. Li, Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers, Bioinformatics 35 (22) (2019) 4560–4567. [6] Grabowski S. Bieniecki W. CopMEM: finding maximal exact matches via sampling both genomes Bioinformatics 35 4 2019 677 678 S. Grabowski, W. Bieniecki, copMEM: finding maximal exact matches via sampling both genomes, Bioinformatics 35 (4) (2019) 677–678. [7] Malysa G. Hernaez M. Ochoa I. Rao M. Ganesan K. Weissman T. QVZ: lossy compression of quality values Bioinformatics 31 19 2015 3122 3129 G. Malysa, M. Hernaez, I. Ochoa, M. Rao, K. Ganesan, T. Weissman, QVZ: lossy compression of quality values, Bioinformatics 31 (19) (2015) 3122–3129. [8] Liu Y. Yu Z. Dinger M.E. Li J. Index suffix–prefix overlaps by (w, k)-minimizer to generate long contigs for reads compression Bioinformatics 35 12 2019 2066 2074 Y. Liu, Z. Yu, M. E. Dinger, J. Li, Index suffix–prefix overlaps by (w, k)-minimizer to generate long contigs for reads compression, Bioinformatics 35 (12) (2019) 2066–2074. [9] Chandak S. Tatwawadi K. Ochoa I. Hernaez M. Weissman T. SPRING: a next-generation compressor for FASTQ data Bioinformatics 35 15 2019 2674 2676 S. Chandak, K. Tatwawadi, I. Ochoa, M. Hernaez, T. Weissman, SPRING: a next-generation compressor for FASTQ data, Bioinformatics 35 (15) (2019) 2674–2676. [10] Pinho A.J. Garcia S.P. Pratas D. Ferreira P.J. DNA sequences at a glance PLoS One 8 11 2013 e79922 A. J. Pinho, S. P. Garcia, D. Pratas, P. J. Ferreira, DNA sequences at a glance, PloS one 8 (11) (2013) e79922. [11] Pinho A.J. Pratas D. Ferreira P.J. Garcia S.P. Symbolic to numerical conversion of DNA sequences using finite-context models 2011 19th European signal processing conference 2011 IEEE 2024 2028 A. J. Pinho, D. Pratas, P. J. Ferreira, S. P. Garcia, Symbolic to numerical conversion of DNA sequences using finite-context models, in: 2011 19th European Signal Processing Conference, IEEE, 2011, 2024–2028. [12] Pratas D. Hosseini M. Pinho A.J. GeCo2: an optimized tool for lossless compression and analysis of DNA sequences International conference on practical applications of computational biology & bioinformatics 2019 Springer 137 145 D. Pratas, M. Hosseini, A. J. Pinho, GeCo2: an optimized tool for lossless compression and analysis of DNA sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2019, 137–145. [13] Hosseini M. Pratas D. Pinho A.J. AC: a compression tool for amino acid sequences Interdiscip Sci: Comput Life Sci 11 1 2019 68 76 M. Hosseini, D. Pratas, A. J. Pinho, AC: a compression tool for amino acid sequences, Interdisciplinary Sciences: Computational Life Sciences 11 (1) (2019) 68–76. [14] Carvalho J.M. Brás S. Pratas D. Ferreira J. Soares S.C. Pinho A.J. Extended-alphabet finite-context models Pattern Recognit Lett 112 2018 49 55 J. M. Carvalho, S. Brás, D. Pratas, J. Ferreira, S. C. Soares, A. J. Pinho, Extended-alphabet finite-context models, Pattern Recognition Letters 112 (2018) 49–55. [15] Hosseini M. Pratas D. Pinho A. A survey on data compression methods for biological sequences Information 7 4 2016 56 M. Hosseini, D. Pratas, A. Pinho, A survey on data compression methods for biological sequences, Information 7 (4) (2016) 56. [16] Agüero-Chapin G. González-Díaz H. Molina R. Varona-Santos J. Uriarte E. González-Díaz Y. Novel 2D maps and coupling numbers for protein sequences. The first QSAR study of polygalacturonases; isolation and prediction of a novel sequence from Psidium guajava L. FEBS Lett 580 3 2006 723 730 G. Agüero-Chapin, H. González-Díaz, R. Molina, J. Varona-Santos, E. Uriarte, Y. González-Díaz, Novel 2D maps and coupling numbers for protein sequences. The first QSAR study of polygalacturonases isolation and prediction of a novel sequence from Psidium guajava L., FEBS Letters 580 (3) (2006) 723–730. [17] Pratas D. Silva R.M. Pinho A.J. Ferreira P.J. An alignment-free method to find and visualise rearrangements between pairs of DNA sequences Sci Rep 5 2015 10203 D. Pratas, R. M. Silva, A. J. Pinho, P. J. Ferreira, An alignment-free method to find and visualise rearrangements between pairs of DNA sequences, Scientific Reports 5 (2015) 10203. [18] Hosseini M, Pratas D, Morgenstern B, Pinho AJ. Smash++: an alignment-free and memory-efficient tool to find genomic rearrangements. GigaScience 9(5). [19] Zielezinski A. Girgis H.Z. Bernard G. Leimeister C.-A. Tang K. Dencker T. Benchmarking of alignment-free sequence comparison methods 2019 BioRxiv A. Zielezinski, H. Z. Girgis, G. Bernard, C.-A. Leimeister, K. Tang, T. Dencker, A. K. Lau, S. Röhling, J. Choi, M. S. Waterman, others,Benchmarking of alignment-free sequence comparison methods, BioRxiv (2019) 611137. [20] Forslund S.K. Kaduk M. Sonnhammer E.L. Evolution of protein domain architectures Evolutionary genomics 2019 Springer 469 504 S. K. Forslund, M. Kaduk, E. L. Sonnhammer, Evolution of protein domain architectures, in: Evolutionary Genomics, Springer, 2019, 469–504. [21] Rost B. Twilight zone of protein sequence alignments Protein Eng 12 2 1999 85 94 B. Rost, Twilight zone of protein sequence alignments, Protein engineering 12 (2) (1999) 85–94. [22] Pratas D. Pinho A.J. Silva R.M. Rodrigues J.M. Hosseini M. Caetano T. FALCON: a method to infer metagenomic composition of ancient DNA 2018 BioRxiv D. Pratas, A. J. Pinho, R. M. Silva, J. M. Rodrigues, M. Hosseini, T. Caetano, P. J. Ferreira, FALCON: a method to infer metagenomic composition of ancient DNA, BioRxiv (2018) 267179. [23] Pratas D. Pinho A.J. Metagenomic composition analysis of sedimentary ancient DNA from the isle of wight 2018 26th european signal processing conference (EUSIPCO) 2018 IEEE 1177 1181 D. Pratas, A. J. Pinho, Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight, in: 2018 26th European Signal Processing Conference (EUSIPCO), IEEE, 2018, 1177–1181. [24] Huang W. Li L. Myers J.R. Marth G.T. ART: a next-generation sequencing read simulator Bioinformatics 28 4 2011 593 594 W. Huang, L. Li, J. R. Myers, G. T. Marth, ART: a next-generation sequencing read simulator, Bioinformatics 28 (4) (2011) 593–594. [25] Droop A.P. Fqtools: an efficient software suite for modern FASTQ file manipulation Bioinformatics 32 12 2016 1883 1884 A. P. Droop, fqtools: an efficient software suite for modern FASTQ file manipulation, Bioinformatics 32 (12) (2016) 1883–1884. [26] Afgan E. Baker D. Batut B. Van Den Beek M. Bouvier D. Čech M. The galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update Nucleic Acids Res 46 W1 2018 W537 W544 E. Afgan, D. Baker, B. Batut, M. Van Den Beek, D. Bouvier, M. Čech, J. Chilton, D. Clements, N. Coraor, B. A. Grüning, others,The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update, Nucleic acids research 46 (W1) (2018) W537–W544. [27] Gordon A, Hannon G et al. Fastx-toolkit, FASTQ/A short-reads preprocessing tools Accessed: 2020-06-17. [28] Shen W. Le S. Li Y. Hu F. SeqKit: a cross-platform and ultrafast toolkit for FASTA/Q file manipulation PLoS One 11 10 2016 e0163962 W. Shen, S. Le, Y. Li, F. Hu, SeqKit: a cross-platform and ultrafast toolkit for FASTA/Q file manipulation, PLoS One 11 (10) (2016) e0163962. [29] DePristo M.A. Banks E. Poplin R. Garimella K.V. Maguire J.R. Hartl C. A framework for variation discovery and genotyping using next-generation DNA sequencing data Nature Genet 43 5 2011 491 M. A. DePristo, E. Banks, R. Poplin, K. V. Garimella, J. R. Maguire, C. Hartl, A. A. Philippakis, G. Del Angel, M. A. Rivas, M. Hanna, others,A framework for variation discovery and genotyping using next-generation DNA sequencing data, Nature Genetics 43 (5) (2011) 491. [30] Goecks J. Nekrutenko A. Taylor J. Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences Genome Biol 11 8 2010 R86 J. Goecks, A. Nekrutenko, J. Taylor, Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences, Genome Biology 11 (8) (2010) R86. [31] Blankenberg D. Gordon A. Von Kuster G. Coraor N. Taylor J. Nekrutenko A. Manipulation of FASTQ data with galaxy Bioinformatics 26 14 2010 1783 1785 D. Blankenberg, A. Gordon, G. Von Kuster, N. Coraor, J. Taylor, A. Nekrutenko, G. Team, Manipulation of FASTQ data with Galaxy, Bioinformatics 26 (14) (2010) 1783–1785. [32] Liu Y. Peng H. Wong L. Li J. High-speed and high-ratio referential genome compression Bioinformatics 33 21 2017 3364 3372 Y. Liu, H. Peng, L. Wong, J. Li, High-speed and high-ratio referential genome compression, Bioinformatics 33 (21) (2017) 3364–3372. [33] Ochoa I. Hernaez M. Weissman T. iDoComp: a compression scheme for assembled genomes Bioinformatics 31 5 2014 626 633 I. Ochoa, M. Hernaez, T. Weissman, iDoComp: a compression scheme for assembled genomes, Bioinformatics 31 (5) (2014) 626–633. [34] Deorowicz S. Danek A. Niemiec M. GDC 2: Compression of large collections of genomes Sci Rep 5 2015 11565 S. Deorowicz, A. Danek, M. Niemiec, GDC 2: Compression of large collections of genomes, Scientific Reports 5 (2015) 11565. [35] Hernaez M, Pavlichin D, Weissman T, Ochoa I. Genomic data compression. Annu Rev Biomed Data Sci 2. [36] Pratas D. Hosseini M. Pinho A.J. Compression of amino acid sequences International conference on practical applications of computational biology & bioinformatics 2018 Springer 105 113 D. Pratas, M. Hosseini, A. J. Pinho, Compression of amino acid sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2018, 105–113. [37] Pratas D. Pinho A.J. Rodrigues J.M. XS: a FASTQ read simulator BMC Res Notes 7 1 2014 40 D. Pratas, A. J. Pinho, J. M. Rodrigues, XS: a FASTQ read simulator, BMC Research Notes 7 (1) (2014) 40. [38] Escalona M. Rocha S. Posada D. A comparison of tools for the simulation of genomic next-generation sequencing data Nature Rev Genet 17 8 2016 459 M. Escalona, S. Rocha, D. Posada, A comparison of tools for the simulation of genomic next-generation sequencing data, Nature Reviews Genetics 17 (8) (2016) 459. "
    },
    {
        "doc_title": "Multimodal emotion evaluation: A physiological model for cost-effective emotion classification",
        "doc_scopus_id": "85086756182",
        "doc_doi": "10.3390/s20123510",
        "doc_eid": "2-s2.0-85086756182",
        "doc_date": "2020-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Electrocardiogram signal",
            "Electrodermal activity",
            "Emotion classification",
            "Emotion classification systems",
            "Emotion identifications",
            "Emotional classification",
            "Physiological model of emotions",
            "Physiological signals",
            "Cost-Benefit Analysis",
            "Electrocardiography",
            "Electromyography",
            "Emotions",
            "Humans",
            "Models, Biological",
            "Neural Networks, Computer"
        ],
        "doc_abstract": "© 2020 by the authors.Emotional responses are associated with distinct body alterations and are crucial to foster adaptive responses, well-being, and survival. Emotion identification may improve peoples’ emotion regulation strategies and interaction with multiple life contexts. Several studies have investigated emotion classification systems, but most of them are based on the analysis of only one, a few, or isolated physiological signals. Understanding how informative the individual signals are and how their combination works would allow to develop more cost-effective, informative, and objective systems for emotion detection, processing, and interpretation. In the present work, electrocardiogram, electromyogram, and electrodermal activity were processed in order to find a physiological model of emotions. Both a unimodal and a multimodal approach were used to analyze what signal, or combination of signals, may better describe an emotional response, using a sample of 55 healthy subjects. The method was divided in: (1) signal preprocessing; (2) feature extraction; (3) classification using random forest and neural networks. Results suggest that the electrocardiogram (ECG) signal is the most effective for emotion classification. Yet, the combination of all signals provides the best emotion identification performance, with all signals providing crucial information for the system. This physiological model of emotions has important research and clinical implications, by providing valuable information about the value and weight of physiological signals for emotional classification, which can critically drive effective evaluation, monitoring and intervention, regarding emotional processing and regulation, considering multiple contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Smash++: An alignment-free and memory-efficient tool to find genomic rearrangements",
        "doc_scopus_id": "85084965113",
        "doc_doi": "10.1093/gigascience/giaa048",
        "doc_eid": "2-s2.0-85084965113",
        "doc_date": "2020-05-20",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Computational Biology",
            "Gene Rearrangement",
            "Genome",
            "Genomics",
            "High-Throughput Nucleotide Sequencing",
            "Sequence Analysis, DNA",
            "Software"
        ],
        "doc_abstract": "© 2020 The Author(s) 2020. Published by Oxford University Press.Background: The development of high-throughput sequencing technologies and, as its result, the production of huge volumes of genomic data, has accelerated biological and medical research and discovery. Study on genomic rearrangements is crucial owing to their role in chromosomal evolution, genetic disorders, and cancer. Results: We present Smash++, an alignment-free and memory-efficient tool to find and visualize small- and large-scale genomic rearrangements between 2 DNA sequences. This computational solution extracts information contents of the 2 sequences, exploiting a data compression technique to find rearrangements. We also present Smash++ visualizer, a tool that allows the visualization of the detected rearrangements along with their self- and relative complexity, by generating an SVG (Scalable Vector Graphics) image. Conclusions: Tested on several synthetic and real DNA sequences from bacteria, fungi, Aves, and Mammalia, the proposed tool was able to accurately find genomic rearrangements. The detected regions were in accordance with previous studies, which took alignment-based approaches or performed FISH (fluorescence in situ hybridization) analysis. The maximum peak memory usage among all experiments was ∼1 GB, which makes Smash++ feasible to run on present-day standard computers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Competitive Segmentation Performance on Near-Lossless and Lossy Compressed Remote Sensing Images",
        "doc_scopus_id": "85084144859",
        "doc_doi": "10.1109/LGRS.2019.2934997",
        "doc_eid": "2-s2.0-85084144859",
        "doc_date": "2020-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Geotechnical Engineering and Engineering Geology",
                "area_abbreviation": "EART",
                "area_code": "1909"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression standards",
            "Image segmentation algorithm",
            "Lossy compressions",
            "Near lossless compression",
            "Perfect reconstruction",
            "Remote sensing data",
            "Remote sensing images",
            "Segmentation performance"
        ],
        "doc_abstract": "© 2020 IEEE.Image segmentation lies at the heart of multiple image processing chains, and achieving accurate segmentation is of utmost importance as it affects later processing. Image segmentation has recently gained interest in the field of remote sensing, mostly due to the widespread availability of remote sensing data. This increased availability poses the problem of transmitting and storing large volumes of data. Compression is a common strategy to alleviate this problem. However, lossy or near-lossless compression prevents a perfect reconstruction of the recovered data. This letter investigates the image segmentation performance in data reconstructed after a near-lossless or a lossy compression. Two image segmentation algorithms and two compression standards are evaluated on data from several instruments. Experimental results reveal that segmentation performance over previously near-lossless and lossy compressed images is not markedly reduced at low and moderate compression ratios (CRs). In some scenarios, accurate segmentation performance can be achieved even for high CRs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessing job market dynamics using elk stack",
        "doc_scopus_id": "85099605396",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85099605396",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Digital technologies",
            "European union",
            "Job announcement",
            "Job description",
            "NAtural language processing",
            "Revenue models",
            "Visual interpretation",
            "Work activities"
        ],
        "doc_abstract": "© Ibero-American WWW / Internet Conference 2020.The adoption of digital technologies promises to accelerate the transformation and the agility of processes, work activities and revenue models. Yet, the promised gains come together with dramatic needs for qualified professionals who can effectively leverage the technology potential. Job contexts are being reshaped as new models for the interaction and integration of humans and technologies take shape. To increase the readiness of the job market in this fast-changing context it is important that all stakeholders - companies, professionals, policy makers - are aware of the job market dynamics and needs. This can be observed from the collection of job announcements, but its high volume requires effective tools for analyzing and simplifying it in order to draw timely and correct conclusions. ELK stack was used for dealing with the high volume of job announcements. ELK is a stable platform that can manage large quantities of data and the Kibana layer enables to rapidly explore data and create visualization dashboards. As job announcements have distinct formulations for similar roles, depending on the hiring company, this raises the necessity of establishing a common ground for comparing the job descriptions. In this work were mapped job descriptions to ESCO occupations. ESCO is an ontology published by the European Union and its occupations are job positions. Results show that the ELK stack is a suitable tool for providing a visual interpretation on the job market dynamics. Moreover, the first experiments using natural language processing techniques and machine learning algorithms revealed an accuracy over 0.9 in mapping job descriptions to ESCO occupations. This result is very promising and shows that ESCO a good candidate as common ground to enable comparison of job market dynamics for distinct environments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Information-Theoretical Method for Emotion Classification",
        "doc_scopus_id": "85075882231",
        "doc_doi": "10.1007/978-3-030-31635-8_30",
        "doc_eid": "2-s2.0-85075882231",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Affective Computing",
            "Dissimilarity measures",
            "Emotion",
            "Emotion classification",
            "Exploratory data mining",
            "Kolmogorov complexity",
            "Symbolic representation",
            "Theoretical methods"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Identifying the emotion that someone is feeling will allow to improve the experience of the person interaction with environments, devices, and contents. Our body responds to events around us, by emotional responses, reflected in cognitive, behavioral and physiological dimensions. In the present work, we target the electrocardiogram (ECG) response as a mean to express emotions. Its processing is performed using information-theoretical measures, allowing true exploratory data mining. Participants recruited for the experiment watched three video sets in three different days, with a different emotion being induced in each day: fear, happiness, and neutral condition. The method is divided in: (1) conversion of the real-valued ECG record into a symbolic time-series; (2) relative compression of the symbolic representation of the ECG, using the symbolic ECG records stored in the database as a reference; (3) identification of the ECG record class, using a 1-NN (nearest neighbor) classifier. An accuracy of 90% was obtained. A posteriori analysis of the false negative results indicated that there was a relation between the relative dissimilarity measure and the self-reported emotions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection and Characterization of Local Inverted Repeats Regularities",
        "doc_scopus_id": "85068623167",
        "doc_doi": "10.1007/978-3-030-23873-5_14",
        "doc_eid": "2-s2.0-85068623167",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cruciform",
            "Distance distributions",
            "Genome sequences",
            "Human genomes",
            "Inverted repeat",
            "Periodic regularities",
            "Sliding window methods",
            "Total mass"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.To explore the inverted repeats regularities along the genome sequences, we propose a sliding window method to extract the concentration scores of inverted repeats periodic regularities and the total mass of possible inverted repeats pairs. We apply the method to the human genome and locate the regions with the potential for the formation of large number of hairpin/cruciform structures. The number of found windows with periodic regularities is small and the patterns of occurrence are chromosome specific.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeCo2: An Optimized Tool for Lossless Compression and Analysis of DNA Sequences",
        "doc_scopus_id": "85068615307",
        "doc_doi": "10.1007/978-3-030-23873-5_17",
        "doc_eid": "2-s2.0-85068615307",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical applications",
            "Command line interface",
            "Computational resources",
            "DNA data compressions",
            "Genomic sequence",
            "Lossless compression",
            "Lossless data compression",
            "Mixture model"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.The development of efficient DNA data compression tools is fundamental for reducing the storage, given the increasing availability of DNA sequences. The importance is also reflected for analysis purposes, given the search for optimized and new tools for anthropological and biomedical applications. In this paper, we describe the characteristics and impact of the GeCo2 tool, an improved version of the GeCo tool. In the proposed tool, we enhanced the mixture of models, where each context model or tolerant context model has now a specific decay factor. Additionally, specific cache-hash sizes and the ability to run only a context model with inverted repeats was developed. A new command line interface, twelve new pre-computed levels, and several optimizations in the code were also included. The results show a compression improvement using less computational resources (RAM and processing time). This new version permits more flexibility for compression and analysis purposes, namely a higher ability of addressing different characteristics of the DNA sequences. The decompression is performed using symmetric computational resources (RAM and time). The GeCo2 is freely available, under GPLv3 license, at https://github.com/pratas/geco2.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visualization of Similar Primer and Adapter Sequences in Assembled Archaeal Genomes",
        "doc_scopus_id": "85068589420",
        "doc_doi": "10.1007/978-3-030-23873-5_16",
        "doc_eid": "2-s2.0-85068589420",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Approximate search",
            "Archaeal",
            "Genomics",
            "Primer-adapter sequences",
            "Similarity threshold",
            "Synthetic DNA"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Primer and adapter sequences are synthetic DNA or RNA oligonucleotides used in the process of amplification and sequencing. In theory, while similar primer sequences can be present on assembled genomes, adapter sequences should be trimmed (filtered) and, hence, absent from assembled genomes. However, given ambiguity problems, inefficient parameterization of trimming tools, and others, uncommonly they can be found in assembled genomes, on an exact or approximate state. In this paper, we investigate the occurrence of exact and approximate primer-adapter subsequences in assembled and, specifically, in the whole archaeal genomes of the NCBI database. We present a new method that combines data compression with custom signal processing operations, namely filtering and segmentation, to localize and visualize these regions given a defined similarity threshold. The program is freely available, under GPLv3 license, at https://github.com/pratas/maple.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning the Scope of Applicability for Task Planning Knowledge in Experience-Based Planning Domains",
        "doc_scopus_id": "85081160340",
        "doc_doi": "10.1109/IROS40897.2019.8968013",
        "doc_eid": "2-s2.0-85081160340",
        "doc_date": "2019-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Abstract representation",
            "Classical planning",
            "Learning from experiences",
            "Logical structure",
            "Planning domains",
            "Problem instances",
            "Task knowledge",
            "Three-valued logic"
        ],
        "doc_abstract": "© 2019 IEEE.Experience-based planning domains (EBPDs) have been proposed to improve problem solving by learning from experience. They rely on acquiring and using task knowledge, i.e., activity schemata, for generating solutions to problem instances in a class of tasks. Using Three-Valued Logic Analysis (TVLA), we extend our previous work to generate a set of conditions that determine the scope of applicability of an activity schema. The inferred scope is an abstract representation of a potentially unbounded set of problems, in the form of a 3-valued logical structure, which is used to test the applicability of the respective activity schema for solving different task problems. We validate this work on two classical planning domains and a simulated PR2 in Gazebo.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A reference-free lossless compression algorithm for DNA sequences using a competitive prediction of two classes of weighted models",
        "doc_scopus_id": "85075423993",
        "doc_doi": "10.3390/e21111074",
        "doc_eid": "2-s2.0-85075423993",
        "doc_date": "2019-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2019 by the authors.The development of efficient data compressors for DNA sequences is crucial not only for reducing the storage and the bandwidth for transmission, but also for analysis purposes. In particular, the development of improved compression models directly influences the outcome of anthropological and biomedical compression-based methods. In this paper, we describe a new lossless compressor with improved compression capabilities for DNA sequences representing different domains and kingdoms. The reference-free method uses a competitive prediction model to estimate, for each symbol, the best class of models to be used before applying arithmetic encoding. There are two classes of models: weighted context models (including substitutional tolerant context models) and weighted stochastic repeat models. Both classes of models use specific sub-programs to handle inverted repeats efficiently. The results show that the proposed method attains a higher compression ratio than state-of-the-art approaches, on a balanced and diverse benchmark, using a competitive level of computational resources. An efficient implementation of the method is publicly available, under the GPLv3 license.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A probabilistic method to find and visualize distinct regions in protein sequences",
        "doc_scopus_id": "85075624401",
        "doc_doi": "10.23919/EUSIPCO.2019.8902695",
        "doc_eid": "2-s2.0-85075624401",
        "doc_date": "2019-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Alignment-free",
            "Bloom filters",
            "Neanderthals",
            "Palaeoproteomics",
            "Relative uniqueness"
        ],
        "doc_abstract": "© 2019 IEEEStudies on identification of species-specific protein regions, i.e., unique or highly dissimilar regions with respect to close species, will lead us to understanding of evolutionary traits, which can be related to novel functionalities or diseases. In this paper, we propose an alignment-free method to find and visualize distinct regions between two collections of proteins. We applied the proposed method, FRUIT, on multiple synthetic and real datasets to analyze its behavior when different rates of substitutional mutation occur. Testing with different k-mer sizes showed that the higher the mutation rate, the higher the relative uniqueness. We also employed FRUIT to find and visualize distinct regions in modern human proteins relatively to the proteins of Altai, Sidron and Vindija Neanderthals. The results show that four of the most distinct proteins, named ataxin-8, 60S ribosomal protein L26, NADH-ubiquinone oxidoreductase chain 3 and cytochrome c oxidase subunit 2 are involved in SCA8, DBA11, LS and MT-C1D, and MT-C4D diseases, respectively. There is also Interferon-induced transmembrane protein 3, among others, which is part of the immune system. Besides, we report the most similar primate exomes to the found modern human one, in terms of identity, query cover and length of sequences. The reported results can give us insight to the evolution of proteomes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Distribution of Distances Between Symmetric Words in the Human Genome: Analysis of Regular Peaks",
        "doc_scopus_id": "85068597987",
        "doc_doi": "10.1007/s12539-019-00326-x",
        "doc_eid": "2-s2.0-85068597987",
        "doc_date": "2019-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Chromosomes, Human",
            "Databases, Genetic",
            "DNA",
            "Genome, Human",
            "Genomics",
            "Humans",
            "Models, Genetic",
            "Nucleic Acid Conformation",
            "Sequence Analysis, DNA",
            "Software"
        ],
        "doc_abstract": "© 2019, International Association of Scientists in the Interdisciplinary Areas.Finding DNA sites with high potential for the formation of hairpin/cruciform structures is an important task. Previous works studied the distances between adjacent reversed complement words (symmetric word pairs) and also for non-adjacent words. It was observed that for some words a few distances were favoured (peaks) and that in some distributions there was strong peak regularity. The present work extends previous studies, by improving the detection and characterization of peak regularities in the symmetric word pairs distance distributions of the human genome. This work also analyzes the location of the sequences that originate the observed strong peak periodicity in the distance distribution. The results obtained in this work may indicate genomic sites with potential for the formation of hairpin/cruciform structures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AC: A Compression Tool for Amino Acid Sequences",
        "doc_scopus_id": "85062690507",
        "doc_doi": "10.1007/s12539-019-00322-1",
        "doc_eid": "2-s2.0-85062690507",
        "doc_date": "2019-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Amino Acid Sequence",
            "Data Compression",
            "High-Throughput Nucleotide Sequencing",
            "Markov Chains",
            "Software"
        ],
        "doc_abstract": "© 2019, International Association of Scientists in the Interdisciplinary Areas.Advancement of protein sequencing technologies has led to the production of a huge volume of data that needs to be stored and transmitted. This challenge can be tackled by compression. In this paper, we propose AC, a state-of-the-art method for lossless compression of amino acid sequences. The proposed method works based on the cooperation between finite-context models and substitutional tolerant Markov models. Compared to several general-purpose and specific-purpose protein compressors, AC provides the best bit-rates. This method can also compress the sequences nine times faster than its competitor, paq8l. In addition, employing AC, we analyze the compressibility of a large number of sequences from different domains. The results show that viruses are the most difficult sequences to be compressed. Archaea and bacteria are the second most difficult ones, and eukaryota are the easiest sequences to be compressed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression-Based Classification of ECG Using First-Order Derivatives",
        "doc_scopus_id": "85065031461",
        "doc_doi": "10.1007/978-3-030-16447-8_3",
        "doc_eid": "2-s2.0-85065031461",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Biometric identifications",
            "Compression model",
            "Entertainment systems",
            "First order derivatives",
            "Kolmogorov complexity",
            "Measure of similarities",
            "Personal identification",
            "Point of interest"
        ],
        "doc_abstract": "© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2019.Due to its characteristics, there is a trend in biometrics to use the ECG signal for personal identification. There are different applications for this, namely, adapting entertainment systems to personal settings automatically. Recent works based on compression models have shown that these approaches are suitable to ECG biometric identification. However, the best results are usually achieved by the methods that, at least, rely on one point of interest of the ECG – called fiducial methods. In this work, we propose a compression-based non-fiducial method, that uses a measure of similarity, called the Normalized Relative Compression—a measure related to the Kolmogorov complexity of strings. Our method uses extended-alphabet finite-context models (xaFCMs) on the quantized first-order derivative of the signal, instead of using directly the original signal, as other methods do. We were able to achieve state-of-the-art results on a database collected at the University of Aveiro, which was used on previous works, making it a good preliminary benchmark for the method.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cryfa: A secure encryption tool for genomic data",
        "doc_scopus_id": "85058744156",
        "doc_doi": "10.1093/bioinformatics/bty645",
        "doc_eid": "2-s2.0-85058744156",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Computational Biology",
            "Data Compression",
            "Genomics",
            "High-Throughput Nucleotide Sequencing",
            "Software"
        ],
        "doc_abstract": "© 2018 The Author(s). Published by Oxford University Press.The ever-increasing growth of high-throughput sequencing technologies has led to a great acceleration of medical and biological research and discovery. As these platforms advance, the amount of information for diverse genomes increases at unprecedented rates. Confidentiality, integrity and authenticity of such genomic information should be ensured due to its extremely sensitive nature. In this paper, we propose Cryfa, a fast secure encryption tool for genomic data, namely in Fasta, Fastq, VCF, SAM and BAM formats, which is also capable of reducing the storage size of Fasta and Fastq files. Cryfa uses advanced encryption standard (AES) encryption combined with a shuffling mechanism, which leads to a substantial enhancement of the security against low data complexity attacks. Compared to AES Crypt, a general-purpose encryption tool, Cryfa is an industry-oriented tool, which is able to provide confidentiality, integrity and authenticity of data at four times more speed; in addition, it can reduce the file sizes to 1/3. Due to the absence of a method similar to Cryfa, we have simulated its behavior with a combination of encryption and compression tools, for comparison purpose. For instance, our tool is nine times faster than its fastest competitor in Fasta files. Also, Cryfa has a very low memory usage (only a few megabytes), which makes it feasible to run on any computer. Availability and implementation Source codes and binaries are available, under GPLv3, at https://github.com/pratas/cryfa. Supplementary informationSupplementary dataare available at Bioinformatics online.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "NET-ASAR: A tool for DNA sequence search based on data compression",
        "doc_scopus_id": "85052975949",
        "doc_doi": "10.1007/978-3-319-98702-6_14",
        "doc_eid": "2-s2.0-85052975949",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context models",
            "Data compression techniques",
            "Genetic information",
            "Measure of similarities",
            "Reference modeling",
            "Research and development",
            "Similarity",
            "Statistical modeling"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.The great increase in the amount of sequenced DNA has created a problem: the storage of the sequences. As such, data compression techniques, designed specifically to compress genetic information, is an important area of research and development. Likewise, the ability to search similar DNA sequences in relation to a larger sequence, such as a chromosome, has a really important role in the study of organisms and the possible connection between different species. This paper proposes NET-ASAR, a tool for DNA sequence search, based on data compression, or, specifically, finite-context models, by obtaining a measure of similarity between a reference and a target. The method uses an approach based on finite-context models for the creation of a statistical model of the reference sequence and obtaining the estimated number of bits necessary for the encoding of the target sequence, using the reference model. NET-ASAR is freely available, under license GPLv3, at https://github.com/manuelgaspar/NET-ASAR.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Analysis of Symmetric Words in Human DNA: Adjacent vs Non-adjacent Word Distances",
        "doc_scopus_id": "85052960907",
        "doc_doi": "10.1007/978-3-319-98702-6_10",
        "doc_eid": "2-s2.0-85052960907",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cruciform",
            "Distance distributions",
            "Genomic word",
            "High potential",
            "Reversed complement"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.It is important to develop methods for finding DNA sites with high potential for the formation of hairpin/cruciform structures. In a previous work, we studied the distances between adjacent reversed complement words (symmetric words), and we observed that for some words some distances were favored. In the work presented here, we extended the study to the distance between non-adjacent reversed complement words and we observed strong periodicity in the distance distribution of some words. This may be an indication of potential for the formation of hairpin/cruciform structures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DNA sequence corpus for compression benchmark",
        "doc_scopus_id": "85052935792",
        "doc_doi": "10.1007/978-3-319-98702-6_25",
        "doc_eid": "2-s2.0-85052935792",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Compression algorithms",
            "Computational model",
            "Computational resources",
            "Large volumes",
            "Sequence compression"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.The progress in sequencing technologies and the increasing availability of DNA sequences from extant and extinct organisms is shaping our knowledge about species origin and development, as well as originating an improvement of the computational methods for storage and analysis purposes. Given the large volume of DNA sequences, computational models that efficiently represent diverse DNA sequences using low computational resources are very welcome. Currently, for benchmarking compression algorithms there is absence of a standard corpus that enables a wide and fair comparison. This should be a corpus that reflects the main domains and kingdoms, without being exaggerated in size and number of sequences. In this paper, we provide such DNA sequence corpus, overviewing its elements and furnishing a comparison of some of the algorithms for DNA sequence compression. The corpus is available at https://tinyurl.com/DNAcorpus.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression of amino acid sequences",
        "doc_scopus_id": "85052918694",
        "doc_doi": "10.1007/978-3-319-98702-6_13",
        "doc_eid": "2-s2.0-85052918694",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Amino acid sequence",
            "Computational resources",
            "Context models",
            "Lossless",
            "Lossless compression",
            "Multiple contexts"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.Amino acid sequences are known to be very hard to compress. In this paper, we propose a lossless compressor for efficient compression of amino acid sequences (AC). The compressor uses a cooperation between multiple context and substitutional tolerant context models. The cooperation between models is balanced with weights that benefit the models with better performance, according to a forgetting function specific for each model. We have shown consistently better compression results than other approaches, using low computational resources. The compressor implementation is freely available, under license GPLv3, at https://github.com/pratas/ac.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight",
        "doc_scopus_id": "85059813385",
        "doc_doi": "10.23919/EUSIPCO.2018.8553297",
        "doc_eid": "2-s2.0-85059813385",
        "doc_date": "2018-11-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Ancient dnas",
            "Co-evolution",
            "Composition analysis",
            "Isle of Wight",
            "Metagenomics",
            "New zealand",
            "United kingdom",
            "Whole genome sequences"
        ],
        "doc_abstract": "© EURASIP 2018.The DNA from several organisms is sequenced conjointly in metagenomics. This allows searching for exogenous microorganisms contained in the samples, with the goal of studying the evolution and co-evolution of host-pathogen, namely for building better diagnostics and therapeutics. However, the quantity and quality of the DNA present in the samples is very poor, pushing the responsibility of analysis improvements into the development of better computational methods. Here, we develop a new processing paradigm to infer the metagenomic composition analysis based on the relative compression of whole genome sequences. Using this method, we present the metagenomic composition analysis of a sedimentary ancient DNA sample, dated to 8,000 years before the present, from the Isle of Wight, United Kingdom. The results show several viruses and bacteria expressing high levels of similarity relative to the samples, namely a circular virus similar to the Avon-Heathcote estuary virus 14 sequenced in New Zealand.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Metagenomic composition analysis of an ancient sequenced polar bear jawbone from Svalbard",
        "doc_scopus_id": "85053281877",
        "doc_doi": "10.3390/genes9090445",
        "doc_eid": "2-s2.0-85053281877",
        "doc_date": "2018-09-06",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 by the authors. Licensee MDPI, Basel, Switzerland.The sequencing of ancient DNA samples provides a novel way to find, characterize, and distinguish exogenous genomes of endogenous targets. After sequencing, computational composition analysis enables filtering of undesired sources in the focal organism, with the purpose of improving the quality of assemblies and subsequent data analysis. More importantly, such analysis allows extinct and extant species to be identified without requiring a specific or new sequencing run. However, the identification of exogenous organisms is a complex task, given the nature and degradation of the samples, and the evident necessity of using efficient computational tools, which rely on algorithms that are both fast and highly sensitive. In this work, we relied on a fast and highly sensitive tool, FALCON-meta, which measures similarity against whole-genome reference databases, to analyse the metagenomic composition of an ancient polar bear (Ursus maritimus) jawbone fossil. The fossil was collected in Svalbard, Norway, and has an estimated age of 110,000 to 130,000 years. The FASTQ samples contained 349 GB of nonamplified shotgun sequencing data. We identified and localized, relative to the FASTQ samples, the genomes with significant similarities to reference microbial genomes, including those of viruses, bacteria, and archaea, and to fungal, mitochondrial, and plastidial sequences. Among other striking features, we found significant similarities between modern-human, some bacterial and viral sequences (contamination) and the organelle sequences of wild carrot and tomato relative to the whole samples. For each exogenous candidate, we ran a damage pattern analysis, which in addition to revealing shallow levels of damage in the plant candidates, identified the source as contamination.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extended-alphabet finite-context models",
        "doc_scopus_id": "85048212515",
        "doc_doi": "10.1016/j.patrec.2018.05.026",
        "doc_eid": "2-s2.0-85048212515",
        "doc_date": "2018-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Biometric identifications",
            "Computational time",
            "Context modeling",
            "Context models",
            "Dissimilarity measures",
            "Kolmogorov complexity",
            "Probability of occurrence",
            "Sample applications"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.The normalized relative compression (NRC) is a recent dissimilarity measure, related to the Kolmogorov complexity. It has been successfully used in different applications, like DNA sequences, images or even ECG (electrocardiographic) signal. It uses a compressor that compresses a target string using exclusively the information contained in a reference string. One possible approach is to use finite-context models (FCMs) to represent the strings. A finite-context model calculates the probability distribution of the next symbol, given the previous k symbols. In this paper, we introduce a generalization of the FCMs, called extended-alphabet finite-context models (xaFCM), that calculates the probability of occurrence of the next d symbols, given the previous k symbols. We perform experiments on two different sample applications using the xaFCMs and the NRC measure: ECG biometric identification, using a publicly available database; estimation of the similarity between DNA sequences of two different, but related, species – chromosome by chromosome. In both applications, we compare the results against those obtained by the FCMs. The results show that the xaFCMs use less memory and computational time to achieve the same or, in some cases, even more accurate results.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2018-06-01 2018-06-01 2018-06-14 2018-06-14 2020-11-09T08:44:07 S0167-8655(18)30209-5 S0167865518302095 10.1016/j.patrec.2018.05.026 S300 S300.2 FULL-TEXT 2020-11-09T09:14:08.408487Z 0 0 20180901 2018 2018-06-01T09:39:47.171082Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0167-8655 01678655 true 112 112 C Volume 112 8 49 55 49 55 20180901 1 September 2018 2018-09-01 2018 article sco © 2018 Elsevier B.V. All rights reserved. EXTENDEDALPHABETFINITECONTEXTMODELS CARVALHO J 1 Introduction 1.1 Compression-based measures 2 Extended-alphabet finite-context models 2.1 Compressing using extended-alphabet finite-context models 2.1.1 Example 2.2 Parameter selection 2.2.1 Selection of α 2.2.2 Selection of d 3 Application 1 – ECG biometric identification 3.1 R-peak detection 3.2 Quantization 3.3 Experimental results 4 Application 2 – DNA sequence relative similarity 5 Conclusions and future work Acknowledgments References PINHO 2011 A 19THEURSIGNALPROCESSCONFEUSIPCO2011 FINDINGUNKNOWNREPEATEDPATTERNSINIMAGES PRATAS 2014 421 422 D 2014DATACOMPRESSIONCONFERENCE ACONDITIONALCOMPRESSIONDISTANCEUNVEILSINSIGHTSGENOMICEVOLUTION COUTINHO 2015 D PINHO 2016 A DATACOMPRESSIONCONFERENCE AUTHORSHIPATTRIBUTIONUSINGCOMPRESSIONDISTANCES PRATAS 2018 D LI 2004 3250 3264 M LI 1997 M INTRODUCTIONKOLMOGOROVCOMPLEXITYAPPLICATIONS BENNETT 1998 1407 1423 C BRAS 2015 5838 5841 S 201537THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBC ECGBIOMETRICIDENTIFICATIONACOMPRESSIONBASEDAPPROACH CARVALHO 2017 169 176 J PROCEEDINGSPATTERNRECOGNITIONTIMEANALYSIS8THIBERIANCONFERENCEIBPRIA IMPACTACQUISITIONTIMEECGCOMPRESSIONBASEDBIOMETRICIDENTIFICATIONSYSTEMS PINHO 2011 A 18THIEEEINTCONFERIMAGEPROCESS IMAGESIMILARITYUSINGNORMALIZEDCOMPRESSIONDISTANCEBASEDFINITECONTEXTMODELS COUTINHO 2013 D COUTINHO 2010 3858 3861 D PATTERNRECOGNITICPR20THINTCONF ONELEADECGBASEDPERSONALIDENTIFICATIONUSINGZIVMERHAVCROSSPARSING CARVALHO 2016 75 76 J PROCEEDINGS22NDRECPAD IRREGULARITYDETECTIONINECGSIGNALUSINGASEMIFIDUCIALMETHOD GROSSI 2016 12 R KATHIRVEL 2011 408 425 P LIN 2003 2 11 J DMKD03PROCEEDINGS8THACMSIGMODWORKSHOPRESEARCHISSUESINDATAMININGKNOWLEDGEDISCOVERY ASYMBOLICREPRESENTATIONTIMESERIESIMPLICATIONSFORSTREAMINGALGORITHMS RAFAEL 2007 G DIGITALIMAGEPROCESSING SAMPLINGQUANTIZATION FERREIRA 2016 J PINHO 2011 125 128 A IEEEWORKSHOPSTATISTICALSIGNALPROCESSINGPROCEEDINGS BACTERIADNASEQUENCECOMPRESSIONUSINGAMIXTUREFINITECONTEXTMODELS PRATAS 2014 D EUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO2014 EXPLORINGDEEPMARKOVMODELSINGENOMICDATACOMPRESSIONUSINGSEQUENCEPREANALYSIS PRATAS 2016 D DATACOMPRESSIONCONFERENCE EFFICIENTCOMPRESSIONGENOMICSEQUENCES HOBOLTH 2011 349 356 A PINHO 2014 30 37 A CARVALHOX2018X49 CARVALHOX2018X49X55 CARVALHOX2018X49XJ CARVALHOX2018X49X55XJ 2020-06-14T00:00:00.000Z © 2018 Elsevier B.V. All rights reserved. 2020-02-24T09:05:16.874Z S0167865518302095 Foundation for Science and Technology FCT FundaÃ§Ã£o para a CiÃªncia e a Tecnologia COMPETE 2020 PTDC/EEI-SII/6608/2014 UID/CEC/00127/2013 FEDER FEDER European Regional Development Fund FCT FCT FundaÃ§Ã£o para a CiÃªncia e a Tecnologia This work was partially supported by national funds through the FCT - Foundation for Science and Technology , and by European funds through FEDER, under the COMPETE 2020 and Portugal 2020 programs, in the context of the projects UID/CEC/00127/2013 and PTDC/EEI-SII/6608/2014. S. Brás acknowledges the Postdoc Grant from FCT, ref. SFRH/BPD/92342/2013 . item S0167-8655(18)30209-5 S0167865518302095 10.1016/j.patrec.2018.05.026 271524 2020-11-09T09:14:08.408487Z 2018-09-01 true 1617585 MAIN 7 53166 849 656 IMAGE-WEB-PDF 1 gr1 12685 237 244 gr2 53064 434 814 gr3 6149 222 244 gr4 11385 223 244 gr5 67365 321 702 gr6 34775 183 792 gr1 4777 163 168 gr2 5447 117 219 gr3 3263 164 180 gr4 4840 163 179 gr5 15146 100 219 gr6 7193 50 219 gr1 89749 1052 1082 gr2 416247 1921 3604 gr3 45514 986 1082 gr4 78226 987 1081 gr5 466359 1423 3108 gr6 351210 808 3505 si1 243 13 55 si10 1188 44 203 si11 540 18 156 si12 541 19 176 si13 626 24 119 si14 1547 48 270 si15 833 22 253 si16 162 15 17 si17 479 19 92 si18 296 17 60 si19 281 19 50 si2 260 16 59 si20 1069 43 225 si21 975 24 214 si22 614 15 197 si23 1070 56 168 si24 577 16 106 si25 221 16 51 si26 174 17 20 si27 216 13 45 si28 330 16 76 si29 141 15 24 si3 236 14 46 si30 2360 45 430 si31 2418 66 357 si32 1866 42 418 si33 570 16 136 si34 245 16 51 si35 1487 16 373 si36 1940 42 449 si37 1629 42 339 si38 545 16 126 si39 1101 27 233 si4 645 21 155 si40 1072 27 232 si41 422 19 121 si42 239 17 40 si43 616 18 144 si44 599 14 193 si45 641 20 157 si46 1112 43 193 si47 515 16 144 si48 1304 43 318 si49 204 17 24 si5 202 14 46 si50 745 43 145 si51 234 14 55 si52 127 13 23 si53 242 16 56 si6 229 14 47 si7 423 16 98 si8 951 43 176 si9 552 18 160 PATREC 7192 S0167-8655(18)30209-5 10.1016/j.patrec.2018.05.026 Elsevier B.V. Fig. 1 FCM biometry process: context k changing; the blue line represents the biometry accuracy (in %) and the red line represents the time of execution (seconds). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 1 Fig. 2 xaFCM biometry process using high contexts k (fixed); depth d is changing from 1 to 11; the blue line represents the biometry accuracy (in %); the red line represents the time (seconds); the green line represents the accuracy that a FCM with the same context would obtain. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Fig. 3 FCM average model complexity per participant - context k changing. Fig. 3 Fig. 4 xaFCM average model complexity per participant - context d is changing; k is fixed. Fig. 4 Fig. 5 Normalized compression of the chimpanzee (C) chromosomes relatively to the human (H), using: (left) FCM ( k = 12 ); (right) xaFCM ( k = 12 , d = 8 ). Fig. 5 Fig. 6 Profiles of information content of the chimpanzee chromosome 22 relatively to the human chromosome 22 using different models (FCM and xaFCM). Fig. 6 Table 1 FCM representation of the sequence AAABCC. Table 1 Context c v(A|c) v(B|c) v(C|c) v ( c ) = ∑ a ∈ A v ( a | c ) BC 0 0 1 1 CA 1 0 0 1 AB 0 0 1 1 CC 1 0 0 1 AA 1 1 0 2 Table 2 Proposed xaFCM representation of the sequence AAABCC (with d = 1 ). Notice that this model has exactly the same information as the one in Table 1. Table 2 Context c BC C: 1 Total: 1 CA A: 1 Total: 1 AB C: 1 Total: 1 CC A: 1 Total: 1 AA A: 1 B: 1 Total: 2 Table 3 Proposed xaFCM representation of the sequence AAABCC (with d = 2 ). Table 3 Context c BC CA: 1 Total: 1 CA AA: 1 Total: 1 AB CC: 1 Total: 1 CC AA: 1 Total: 1 AA AB: 1 BC: 1 Total: 2 Table 4 CPU time and memory usage (RAM) of the experiments with DNA sequences. Table 4 Parameters Average time to Average time to Average memory Total time to (context k and depth d) learn the Model compress per model run the experiment k = 12 , d = 1 1649.6 s 1580.5 s 5043.2MB 274.4 h k = 12 , d = 8 2181.2 s 269.5 s 14350.3MB 59.5 h Extended-alphabet finite-context models João M. Carvalho ⁎ a Susana Brás a b Diogo Pratas a Jacqueline Ferreira c d Sandra C. Soares c e f Armando J. Pinho a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal b Department of Electronics Telecommunications and Informatics of Aveiro, University of Aveiro, Portugal Department of Electronics Telecommunications and Informatics of Aveiro University of Aveiro Portugal c Department of Education and Psychology, University of Aveiro, Portugal Department of Education and Psychology University of Aveiro Portugal d IBILI, Faculty of Medicine, University of Coimbra, Portugal IBILI, Faculty of Medicine University of Coimbra Portugal e CINTESIS-UA, University of Aveiro, Portugal CINTESIS-UA University of Aveiro Portugal f Department of Clinical Neurosciences, Karolinska Institute, Stockholm, Sweden Department of Clinical Neurosciences Karolinska Institute Stockholm Sweden ⁎ Corresponding author. The normalized relative compression (NRC) is a recent dissimilarity measure, related to the Kolmogorov complexity. It has been successfully used in different applications, like DNA sequences, images or even ECG (electrocardiographic) signal. It uses a compressor that compresses a target string using exclusively the information contained in a reference string. One possible approach is to use finite-context models (FCMs) to represent the strings. A finite-context model calculates the probability distribution of the next symbol, given the previous k symbols. In this paper, we introduce a generalization of the FCMs, called extended-alphabet finite-context models (xaFCM), that calculates the probability of occurrence of the next d symbols, given the previous k symbols. We perform experiments on two different sample applications using the xaFCMs and the NRC measure: ECG biometric identification, using a publicly available database; estimation of the similarity between DNA sequences of two different, but related, species – chromosome by chromosome. In both applications, we compare the results against those obtained by the FCMs. The results show that the xaFCMs use less memory and computational time to achieve the same or, in some cases, even more accurate results. 1 Introduction Data compression models have been used to address several data mining and machine learning problems, usually by means of a formalization in terms of the information content of a string or of the information distance between strings [1–5]. This approach relies on solid foundations of the concept of algorithmic entropy and, because of its non-computability, approximations provided by data compression algorithms [6]. A finite-context model (FCM) calculates the probability distribution of the next symbol, given the previous k symbols. In this work, we propose an extension of the FCMs, which we call extended-alphabet finite-context models (xaFCM). Usually, these models provide better compression ratios, leading to better results for some applications, especially when using small alphabet sizes – and also by performing much less computations. We show this in practice for the ECG biometric identification and DNA sequence similarity. The source code for the compressor was implemented using Python 3.5 and is publicly available under the GPL v3 license. 1 1 1.1 Compression-based measures Compression-based distances are tightly related to the Kolmogorov notion of complexity, also known as algorithmic entropy. Let x denote a binary string of finite length. Its Kolmogorov complexity, K(x), is the length of the shortest binary program x* that computes x in a universal Turing machine and halts. Therefore, K ( x ) = | x * | , the length of x*, represents the minimum number of bits from which x can be computationally retrieved [7]. The Information Distance (ID) and its normalized version, the Normalized Information Distance (NID), were proposed by Bennett et al. almost two decades ago [8] and are defined in terms of the Kolmogorov complexity of the strings involved, as well as the complexity of one when the other is provided. However, since the Kolmogorov complexity of a string is not computable, an approximation (upper bound) for it can be used by means of a compressor. Let C(x) be the number of bits used by a compressor to represent the string x. We will use a measure based on the notion of relative compression [4], denoted by C(x||y), which represents the compression of x relatively to y. This measure obeys the following rules: • C(x||y) ≈ 0 iff string x can be built efficiently from y; • C(x||y) ≈ |x| iff K(x|y) ≈ K(x). Based on these rules, the Normalized Relative Compression (NRC) of the binary string x given the binary string y, is defined as (1) NRC ( x ∥ y ) = C ( x ∥ y ) | x | , where |x| denotes the length of x. A more general formula for the NRC of string x, given string y, where the strings x and y are sequences from an alphabet A = { s 1 , s 2 , ⋯ s | A | } , is given by (2) NRC ( x ∥ y ) = C ( x ∥ y ) | x | log 2 | A | . 2 Extended-alphabet finite-context models 2.1 Compressing using extended-alphabet finite-context models Let A = { s 1 , s 2 , ⋯ s | A | } be the alphabet that describes the objects of interest. An extended-alphabet finite-context model (xaFCM) complies to the Markov property, i.e., it estimates the probability of the next sequence of d > 0 symbols of the information source (depth-d) using the k > 0 immediate past symbols (order-k context). Therefore, assuming that the k past outcomes are given by x n − k + 1 n = x n − k + 1 … x n , the probability estimates, P ( x n + 1 n + d | x n − k + 1 n ) are calculated using sequence counts that are accumulated, while the information source is processed, (3) P ( w | x n − k + 1 n ) = v ( w | x n − k + 1 n ) + α v ( x n − k + 1 n ) + α | A | d where A d = { w 1 , w 2 , ⋯ w | A | , … w | A | d } is an extension of alphabet A to d dimensions, v ( w | x n − k + 1 n ) represents the number of times that, in the past, sequence w ∈ A d was found having x n − k + 1 n as the conditioning context and where (4) v ( x n − k + 1 n ) = ∑ a ∈ A d v ( a | x n − k + 1 n ) denotes the total number of events that has occurred within context x n − k + 1 n . In order to avoid problems with “shifting” of the data, the sequence counts are performed symbol by symbol, when learning a model from a string. Parameter α allows controlling the transition from an estimator initially assuming a uniform distribution to a one progressively closer to the relative frequency estimator. The theoretical information content average provided by the ith sequence of d symbols from the original sequence x, is given by (5) − log 2 P ( X i = t i | x i d − k i d − 1 ) bits, where t i = x i d , x i d + 1 … x ( i + 1 ) d − 1 . As testbed applications, we perform ECG biometric identification and compute a similarity measure between DNA sequences;After processing the first n symbols of x, the total number of bits generated by an order-k with depth-d xaFCM is equal to (6) − ∑ i = 1 n / d log 2 P ( t i | x d i − k d i − 1 ) , where, for simplicity, we assume that n ( m o d d ) = 0 . If we consider a xaFCM with depth d = 1 , then it becomes a regular FCM with the same order k. In that sense, we can consider that a FCM is a particular case of a xaFCM. An intuitive way of understanding how a xaFCM works is to think of it as a FCM which, for each context of length k, instead of counting the number of occurrences of symbols of A , counts the occurrences of sequences w ∈ A d . In other words, for each sequence of length k found, it counts the number of times each sequence of d symbols appeared right after it. Even though, when implemented, this might use more memory to represent the model, an advantage is that it is possible to compress a new sequence of length m, relatively to some previously constructed model, making only m/d accesses to the model. This significantly reduces the time of computation, as we will show in the experimental results presented in Sections 3 and 4. Since, for compressing the first k symbols of a sequence, we do not have enough symbols to represent a context of length k, we always assume that the sequence is “circular”. For long sequences, specially using small contexts/depths, this should not make much difference in terms of compression, but as the contexts/depths increase, this might not be always the case. Since the purpose for which we use these models is to provide an approximation for the number of bits that would be produced by a compressor based on them, whenever we use the word “compression”, in fact we are not performing the compression itself. For that, we would need to use an encoder, which would take more time to compute. It would also be needed to add some side information for the compressor to deal with the circular sequences – but that goes out of scope for our goal. 2.1.1 Example Let x be the circular sequence AAABCC. Using a regular FCM with k = 2 and α = 0.01 , we would build the model from Table 1 to represent x. It is easy to notice that this representation can be implemented using an hash-table of strings to arrays of integers with fixed size (alphabet size + 1 ). However, we propose a different alternative, which consists of building a hash-table of hash-tables. The reason for doing so is that often the number of counts of symbols for each context is very sparse, which would be a waste of memory. To represent exactly the same model, we would build the structure presented in Table 2 . For compressing the sequence x, relatively to itself, we would need C(x||x) bits, where (7) C ( x ∥ x ) = C ( A | C C ) + C ( A | C A ) + C ( A | A A ) + C ( B | A A ) + C ( C | A B ) + C ( C | B C ) and, (8) C ( A | C C ) = C ( A | C A ) = C ( C | A B ) = C ( C | B C ) = − log 2 1 + 0.01 1 + 3 × 0.01 = 0.0283 and (9) C ( A | A A ) = C ( B | A A ) = − log 2 2 + 0.01 1 + 3 × 0.01 = 1.007 , which means C ( x ∥ x ) = 2.1272 or, in other words, it is possible to compress x relatively to itself using just 2.1272 bits. Using a xaFCM, also with k = 2 and α = 0.01 , but with d = 2 , we would build the model presented in Table 3 to represent x. Therefore, (10) C ( x ∥ x ) = C ( A A | C C ) + C ( A B | A A ) + C ( C C | A B ) where, (11) C ( A A | C C ) = C ( C C | A B ) = − log 2 1 + 0.01 1 + 3 2 × 0.01 = 0.110 and (12) C ( A B | A A ) = − log 2 1 + 0.01 2 + 3 2 × 0.01 = 1.049 which means C ( x ∥ x ) = 1.269 or, in other words, using a xaFCM to represent the sequence x it is possible to compress it relatively to itself using just 1.269 bits. Calculating the NRC for both compressors we obtain: • Using FCM – NRC ( x ∥ x ) = 2.1272 6 × log 2 3 = 0.224 ; • Using xaFCM – NRC ( x ∥ x ) = 1.049 6 × log 2 3 = 0.110 . Based on this example, we can infer that, at least for some cases, it is possible to obtain better compression ratios, using xaFCMs instead of traditional FCMs to represent a sequence. 2.2 Parameter selection 2.2.1 Selection of α Since adjusting the α parameter might not be trivial, as it depends on the choice of d as well as on the alphabet size. It is, however, possible to choose α based on a certain desired probability p for a specific outcome. In our experiments, in order to avoid having one more parameter to “tweak”, we are defining α automatically, in a way such that, if sequence w ∈ A d was only found once after a certain context c = x n − k + 1 n ∈ x , and no other sequence ∈ A d was found after that context c (in other words, the total of that line, in the model, is 1), we want to be 90% sure that the same situation happens when compressing a sequence relatively to the learned model. In other words, when we calculate the number of bits, (13) − log 2 P ( X i = t i | c ) needed to compress sequence t i = x i d , x i d + 1 … x i d + d − 1 , we want to choose an α such that (14) P ( X i = t i | c ) = 0 . 9 d . But, since (15) P ( w | c ) = v ( w | c ) + α v ( c ) + α | A | d , where, we have chosen c and w such that v ( c ) = v ( w | c ) = 1 . Therefore (16a) P ( w | c ) = 0 . 9 d ⇔ 1 + α 1 + α | A | d = 0 . 9 d . Since A d is an extension of A to d dimensions, (16b) 1 + α 1 + α | A | d = 0 . 9 d . Also, since both the alphabet size A and the depth d are static parameters, it is easy to solve the equation and choose α in this way. It is also worth mentioning that there is always a possible solution for the equation, since the denominator of the fraction on the left is never equal to zero. 2.2.2 Selection of d The parameter d is an integer greater or equal to one. As mentioned in Section 2.1, when d = 1 , we are using a xaFCM which is equivalent to a FCM of the same order k. Therefore, they both produce exactly the same number of bits. As d increases, so does the RAM needed to store the xaFCM model – but there is not much of an impact (for d = 11 the increase in memory usage is about 10%). The reason for the model complexity to only increase this is that the number of different “leaves” in the hash-tables does not change with the choice of d – only the size of each string stored does. Something to take into account when choosing d is that, the greater the value of d, the harder it would be for an arithmetic encoder to complete its process. Since we only want to compute the NRC, we do not use an encoder. However, to avoid unrealistic results, we want to choose a d that produces an alphabet size of, at most, the MaximumValue(integer) − 1 (e.g. 2 31 − 1 ) symbols. For that reason, using an alphabet of size 6, we can say that 1 ≤ d ≤ 11. Often, we are mostly interested in the time it takes to compress a new target sequence, given an already built model representing the reference sequence. With this application in mind, we can say for sure that the d should be as big as possible, since, as mentioned before, less computations need to be done to compress a new target sequence and, therefore, much less time is needed. Results from real experiments can be seen in the next section. 3 Application 1 – ECG biometric identification In previous works, we have addressed the topic of ECG based biometric identification using a measure of similarity related to the Kolmogorov complexity, called the normalized relative compression (NRC). To attain the goal, we built finite-context models (FCM) to represent each individual [9,10] – a compression-based approach that has been shown successful for different pattern recognition applications [2,4,11]. Other recent works, also based on a compression approach, use the Ziv–Merhav cross parsing algorithm for attaining the same goal [12,13]. Compression-based approaches found in the literature for ECG biometric identification does not seem to take advantage of the fact that the ECG is a quasi-periodical time-series. Since our method uses a semi-fiducial approach (it only detects the R-peak), it is trivial to know where the repetition should happen and take advantage of that fact. From previous results [14], we concluded that, when consecutive heartbeats 2 2 For readability, by “heartbeat” we mean the interval between two consecutive R-peaks. present low levels of noise, their quantization is almost identical. As a consequence of this, we consider that any sequence we analyze is a circular sequence [15]. From this result, it is possible to infer that, compressing the beginning of an heartbeat using the end of the same heartbeat, may be identical to compress it using the end of the previous heartbeat. This may not sound as an advantage, however, this fact allows us to use heartbeats that are not consecutive, when performing the identification of a participant. Since the purpose of this paper is to introduce the method, and not to focus too much in the ECG signal, we do not explore this fact. However, it is already being taken it into account when building the algorithm (one of the arguments that the algorithm accepts as input is the length of the expected repetition – i.e. for this application, how many symbols has one heartbeat), because it will be important for building a real system, as we expect more noise to be present and, therefore, some segments need to be discarded when performing the compression [14]. 3.1 R-peak detection The development of a robust automatic R-peak detector is essential, but it is still a challenging task, due to irregular heart rates, various amplitude levels and QRS morphologies, as well as all kinds of noise and artifacts [16]. We decided to use a semi-fiducial method for segmenting the ECG signal and, since this was not the major focus of the work, we used a preexisting implementation to detect R-peaks, based on the method proposed in [16]. The reason for using a semi-fiducial approach is that fiducial methods have a higher error of detection, while detecting the R-peaks is, nowadays, an almost trivial process [16]. The method used detects the R-peaks by calculating the average points between the Q and S points (from the QRS-complexes) – this may not give the real local maxima corresponding to the R-peaks, but it produces a very close point. For more information regarding the process used for detecting the R-peaks check [16]. The process was already validated by its authors using the standard MIT-BIH arrhythmia database, achieving an average sensitivity of 99.94% and a positive predictivity of 99.96%. It uses bandpass filtering and differentiation operations, aiming to enhance the QRS complexes and to reduce out-of-band noise. A nonlinear transformation is used to obtain a positive-valued feature signal, which includes large candidate peaks corresponding to the QRS complex regions. 3.2 Quantization Data compression algorithms are symbolic in nature. Text and DNA sequences are well-known examples of symbolic sequences, with well-defined associated alphabets. Contrarily the ECG signal needs first to be transformed into symbols before data compression can be applied. In this work, we have relied on the SAX (Symbolic Aggregate ApproXimation) representation [17] to transform the ECG into a symbolic time-series. We consider that the signal is already discrete in the time domain, i.e., that it is already sampled. However, we perform re-sampling using the previously detected R-peaks. There is a fundamental trade-off to take into account while performing the choice of the alphabet size: the quality produced versus the amount of data necessary to represent the sequence [18]. We tested the experiments using alphabet sizes from 3 up to 20 symbols and using different numbers of symbols each R-R segment (per heartbeat), and found that a combination of using an alphabet size of 6 and 200 symbols per heartbeat produced a good balance between the complexity of the strings/models and the accuracies obtained for biometric identification, allowing us to proceed with the improvement of the compressors, while these parameters remained static. However, this result does not guarantee that the same will hold true for a different dataset or application, nor does it guarantee that these are the optimal parameters. Future work is needed to perform this choice in a more robust and automatic way. 3.3 Experimental results The database used in our experiments was collected in house [10,19], where 25 participants were exposed to different external stimuli – disgust, fear and neutral. Data were collected on three different days (once per week), at the University of Aveiro, using a different stimulus per day. The data signals were collected during 25 min on each day, giving a total of around 75 minof ECG signal per participant. Before being exposed to the stimuli, during the first 4 min of each data acquisition, the participants watched a movie with a beach sunset and an acoustic guitar soundtrack, and were instructed to try to relax as much as possible. By using a database where the participants were exposed to different stimuli, we can check if the emotional state of participants affects the biometric identification process. The database is publicly available for download in. 3 3 After all the already explained preprocessing steps are complete, the process in which we perform the biometric identification is the following: 1. Use the complete ECG signals from two days, in order to build a xaFCM model that describes each of the participants; 2. For the remaining day, split the signal, such that each segment has 10 consecutive heartbeats inside it; 3. “Compress” (compute the NRC) each of the segments obtained in the previous step using each of the models obtained in the first step; 4. The model which produces a lowest result is chosen as the candidate for biometric identification. The justification for the first step is that we do not want to use any information from the ECG of the day where we are trying to perform the ECG biometric identification, since, if we used that information, our results would not match a real situation. The number of heartbeats needed for ECG biometric identification is undoubtedly useful when building a biometric identification system – any system should ask participants to provide data for identification, using the smallest time interval that is possible, for practical reasons. Based on the results from a previous study [10], we concluded that 10 heartbeats is a good trade-off between collection time (which should be as low as possible) and statistical relevance of the data. All the experiments were implemented and ran using Python 3.5 (Linux 64 bits) on an Intel(R) Core(TM) i7-6700 CPU @ 3.40 GHz, with 32GB of RAM. For simplicity of code, we have not parallelized the process yet – therefore, only one logical core was used for each experiment. In Fig. 1 , it is possible to see a plot with the accuracy obtained for the process described, by using FCM models, with all possible values of k from 1 up to 20. In the red line, it is also possible to see how much time does this process take in total. An important fact is that the time taken to perform the biometry is approximately directly proportional to the size of the context, k, used. Since the purpose of this paper is to show the appropriateness of xaFCM models, in Fig. 2 are shown six examples of the same experiment, but instead of changing the context k, we have chosen a fixed value of k and tested all possible values of d, the depth of the xaFCMs. From these plots, it is possible to see that the time taken to perform the biometry process for the whole database is up to 3–4 times shorter when using high values of d, having, usually, accuracy ratios comparable with the FCMs of the same order k. On the experiments using “lower” values for the context k (in this case, k ≤ 14), it is possible to notice a minor improvement in terms of accuracy as the d increases, at least for the first values of d (d ≤ 7, more or less). This makes us think that increasing the depth d behaves in a similar way to increasing the depth k of the xaFCM, without the additional cost in terms of testing speed (quite the opposite, actually) and the memory needed does not increase so much as it would by increasing k (Fig. 3 ). In higher contexts k we get the same advantages in terms of computing time and memory requirements, however, after a certain point, there is just no real benefit from increasing neither the context k, nor the depth d, since we are looking for “too specific” patterns, that may not appear again on the segments being tested – which, making an analogy to machine learning, we would be overfitting to the training data. Another aspect we wanted to show, regarding the advantages of using xaFCMs, is the model complexity. In order for the biometric identification to be executed fast, in practice, it is needed to have all the participant models previously loaded into memory. This usually does not pose a problem, if there are not many participants, but it may be useful for building a real biometric identification system. In Fig. 3, we can see that by increasing the context k of FCM models, the complexity of each model increases exponentially. From our interpretation, a way to avoid this exponential increase is to use an xaFCM with an order slightly lower and increase its depth d. In order to show this, we display the complexity of such models in Fig. 4 . 4 Application 2 – DNA sequence relative similarity An approach for computing the similarity of a sequence relatively to other is to calculate the NRC using one of them as reference and the other as the target. In previous works, this has been done using FCM compressors [2,20–22]. In order to show that the xaFCMs are also suitable for this application, we ran some simulations using the human and chimpanzee DNA sequences, removing the unknown symbols (N). The idea was to use each chromosome of the human species as reference and then compress each chromosome of chimpanzee as the target, using exclusively the model from the reference. Since we know from evolution theory that these two species are closely related [23], it is expected that, when we are compressing homologous pairs of chromosomes, the NRC should be lower than on the other cases. To perform the experiment, we used the assembled human chromosomes 1–22, X and Y (3.1GB of data in total) and assembled chimpanzee chromosomes 1, 2a, 2b, 3 to 22, X and Y (3.2GB of data in total). 4 4 All the assembled genome data were downloaded from We ran two different simulations: the first one, with a FCM of context k = 12 ; the other with a xaFCM with k = 12 and d = 8 . All the experiments ran on a server with 16-cores 2.13 GHz Intel Xeon CPU E7320 and 256GB of RAM, but the implementation used a single core. Table 4 shows the average times taken by each experiment, as well as the average memory needed to store the xaFCM model to represent the human chromosomes. It is clear from these results that the xaFCMs are almost d times faster than an FCM of the same order k. Another advantage is that the memory needed for the xaFCMs does not increase exponentially with d. The NRC results for the two simulations, with k = 12 , can be seen in Fig. 5 . It is possible to notice that the heatmap corresponding to the FCM shows better compressions on average. However, using the “perfect” relative compressor, we would expect the NRCs to be as low as possible on the diagonal of the matrix, 5 5 Not exactly the diagonal, because of the second chromosome of the chimpanzee is split into 2a and 2b, making the matrix not a square one. since they represent related chromosomes. The other squares should have higher NRCs, as they have more variation. This is exactly what happens on the xaFCM test (bottom one in Fig. 5). This becomes even more clear when we are comparing the compression along the same sequence, as can be seen in Fig. 6 . 5 Conclusions and future work We have shown that xaFCMs are good candidates to represent models for ECG biometric identification. When compared with FCMs, with the same memory usage, better accuracy ratios are usually obtained, using up to around 3–4 times less time to compute the NRCs (depending on the choice of d). The gains in computational speed increase in the DNA sequence given the higher order of data length. Our experiments show that it is possible to use them for DNA sequence pattern recognition, making them a suitable alternative to the traditional FCMs. These are promising results, and it seems appropriate to infer that the xaFCMs can be suitable to some other applications, specially when the problem of memory usage or testing speed are crucial. For that reason, in the near future, we plan to test them in different applications, where FCMs have proven suitable, like image pattern recognition [1,11,24] and authorship attribution [4]. Acknowledgments This work was partially supported by national funds through the FCT - Foundation for Science and Technology, and by European funds through FEDER, under the COMPETE 2020 and Portugal 2020 programs, in the context of the projects UID/CEC/00127/2013 and PTDC/EEI-SII/6608/2014. S. Brás acknowledges the Postdoc Grant from FCT, ref. SFRH/BPD/92342/2013. References [1] A.J. Pinho P. Ferreira Finding unknown repeated patterns in images 19th Eur. Signal Process. Conf. (EUSIPCO 2011) 2011 [2] D. Pratas A.J. Pinho A conditional compression distance that unveils insights of the genomic evolution 2014 Data Compression Conference 2014 IEEE 421 422 10.1109/DCC.2014.58 [3] D.P. Coutinho M.A.T. Figueiredo Text classification using compression-based dissimilarity measures Int. J. Pattern Recognit. Artif. Intell. 29 05 2015 10.1142/S0218001415530043 [4] A.J. Pinho D. Pratas P.J.S.G. Ferreira Authorship attribution using compression distances Data Compression Conference 2016 10.1109/DCC.2016.53 [5] D. Pratas A.J. Pinho R.M. Silva J.a. Rodrigues M. Hosseini T. Caetano P. Ferreira FALCON: a method to infer metagenomic composition of ancient DNA BioRxiv 2018 [6] M. Li X. Chen X. Li The similarity metric IEEE Trans. Inf. Theory 50 12 2004 3250 3264 [7] M. Li P. Vitányi An Introduction to Kolmogorov Complexity and its Applications 3rd Ed. 1997 Springer 10.1016/S0898-1221(97)90213-3 [8] C.H. Bennett P. Gács M. Li Information distance IEEE Trans. Inf. Theory 44 4 1998 1407 1423 [9] S. Brás A.J. Pinho ECG biometric identification: a compression based approach 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2015 5838 5841 10.1109/EMBC.2015.7319719 [10] J.a.M. Carvalho S. Brás J. Ferreira S.C. Soares A.J. Pinho Impact of the Acquisition Time on ECG Compression-based Biometric Identification Systems Proceedings of Pattern Recognition and Time Analysis - 8th Iberian Conference (IbPRIA) 2017 169 176 [11] A.J. Pinho P. Ferreira Image similarity using the normalized compression distance based on finite context models 18th IEEE Int. Confer. Image Process. 2011 [12] D.P. Coutinho H. Silva H. Gamboa A. Fred M. Figueiredo Novel fiducial and non-fiducial approaches to electrocardiogram-based biometric systems IET Biom. 2 2 2013 [13] D.P. Coutinho A. Fred M. Figueiredo One-lead ECG-based personal identification using Ziv–Merhav cross parsing Pattern Recognit. (ICPR), 20th Int. Conf. 2010 3858 3861 [14] J.M. Carvalho A.J. Pinho S. Brás Irregularity detection in ECG signal using a semi-fiducial method Proceedings of the 22nd RecPad 2016 75 76 [15] R. Grossi C.S. Iliopoulos R. Mercas N. Pisanti S.P. Pissis A. Retha F. Vayani Circular sequence comparison: algorithms and applications. Algorithms Mol. Biol. 11 2016 12 10.1186/s13015-016-0076-6 [16] P. Kathirvel M. Sabarimalai S.R.M. Prasanna K.P. Soman An efficient R-peak detection based on new nonlinear transformation and first-order Gaussian differentiator Cardiovasc. Eng. Technol. 2 4 2011 408 425 10.1007/s13239-011-0065-3 [17] J. Lin E. Keogh S. Lonardi B. Chiu A symbolic representation of time series, with implications for streaming algorithms DMKD ’03 Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2003 2 11 [18] G.C. Rafael R.E. Woods Sampling and quantization Digital Image Processing 2007 Prentice Hall PTR 10.1002/0470870109.ch3 [19] J. Ferreira S. Brás C.F. Silva S.C. Soares An automatic classifier of emotions built from entropy of noise Psychophysiology 2016 10.1111/psyp.12808 [20] A.J. Pinho D. Pratas P.J.S.G. Ferreira Bacteria DNA sequence compression using a mixture of finite-context models IEEE Workshop on Statistical Signal Processing Proceedings 2011 125 128 10.1109/SSP.2011.5967637 [21] D. Pratas A.J. Pinho Exploring deep Markov models in genomic data compression using sequence pre-analysis European Signal Processing Conference, EUSIPCO 2014, 2014 [22] D. Pratas A.J. Pinho P.J.S.G. Ferreira Efficient compression of genomic sequences Data Compression Conference 2016 10.1109/DCC.2016.60 [23] A. Hobolth J.Y. Dutheil J. Hawks M.H. Schierup T. Mailund Incomplete lineage sorting patterns among human, chimpanzee, and orangutan suggest recent orangutan speciation and widespread selection Genome Res. 21 3 2011 349 356 10.1101/gr.114751.110 [24] A.J. Pinho D. Pratas P. Ferreira A new compressor for measuring distances among images Image Anal. Recognit. 1 2014 30 37 "
    },
    {
        "doc_title": "Comparison of compression-based measures with application to the evolution of primate genomes",
        "doc_scopus_id": "85048723392",
        "doc_doi": "10.3390/e20060393",
        "doc_eid": "2-s2.0-85048723392",
        "doc_date": "2018-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 by the authors.An efficient DNA compressor furnishes an approximation to measure and compare information quantities present in, between and acrossDNAsequences, regardless of the characteristics of the sources. In this paper, we compare directly two information measures, the Normalized Compression Distance (NCD) and the Normalized Relative Compression (NRC). These measures answer different questions; the NCD measures how similar both strings are (in terms of information content) and the NRC (which, in general, is nonsymmetric) indicates the fraction of one of them that cannot be constructed using information from the other one. This leads to the problem of finding out which measure (or question) is more suitable for the answer we need. For computing both, we use a state of the art DNA sequence compressor that we benchmark with some top compressors in different compression modes. Then, we apply the compressor on DNA sequences with different scales and natures, first using synthetic sequences and then on real DNA sequences. The last include mitochondrial DNA (mtDNA), messenger RNA (mRNA) and genomic DNA (gDNA) of seven primates. We provide several insights into evolutionary acceleration rates at different scales, namely, the observation and confirmation across the whole genomes of a higher variation rate of the mtDNA relative to the gDNA. We also show the importance of relative compression for localizing similar information regions using mtDNA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Biometric and emotion identification: An ECG compression based method",
        "doc_scopus_id": "85045075792",
        "doc_doi": "10.3389/fpsyg.2018.00467",
        "doc_eid": "2-s2.0-85045075792",
        "doc_date": "2018-04-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Psychology (all)",
                "area_abbreviation": "PSYC",
                "area_code": "3200"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 Brás, Ferreira, Soares and Pinho.We present an innovative and robust solution to both biometric and emotion identification using the electrocardiogram (ECG). The ECG represents the electrical signal that comes from the contraction of the heart muscles, indirectly representing the flow of blood inside the heart, it is known to convey a key that allows biometric identification. Moreover, due to its relationship with the nervous system, it also varies as a function of the emotional state. The use of information-theoretic data models, associated with data compression algorithms, allowed to effectively compare ECG records and infer the person identity, as well as emotional state at the time of data collection. The proposed method does not require ECG wave delineation or alignment, which reduces preprocessing error. The method is divided into three steps: (1) conversion of the real-valued ECG record into a symbolic time-series, using a quantization process; (2) conditional compression of the symbolic representation of the ECG, using the symbolic ECG records stored in the database as reference; (3) identification of the ECG record class, using a 1-NN (nearest neighbor) classifier. We obtained over 98% of accuracy in biometric identification, whereas in emotion recognition we attained over 90%. Therefore, the method adequately identify the person, and his/her emotion. Also, the proposed method is flexible and may be adapted to different problems, by the alteration of the templates for training the model.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing Reverse Complementary Genomic Words Based on Their Distance Distributions and Frequencies",
        "doc_scopus_id": "85042850854",
        "doc_doi": "10.1007/s12539-017-0273-0",
        "doc_eid": "2-s2.0-85042850854",
        "doc_date": "2018-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "DNA, Complementary",
            "Genome, Human",
            "Genomics",
            "Humans",
            "Molecular Sequence Annotation"
        ],
        "doc_abstract": "© 2017, Springer-Verlag GmbH Germany, part of Springer Nature.In this work, we study reverse complementary genomic word pairs in the human DNA, by comparing both the distance distribution and the frequency of a word to those of its reverse complement. Several measures of dissimilarity between distance distributions are considered, and it is found that the peak dissimilarity works best in this setting. We report the existence of reverse complementary word pairs with very dissimilar distance distributions, as well as word pairs with very similar distance distributions even when both distributions are irregular and contain strong peaks. The association between distribution dissimilarity and frequency discrepancy is also explored, and it is speculated that symmetric pairs combining low and high values of each measure may uncover features of interest. Taken together, our results suggest that some asymmetries in the human genome go far beyond Chargaff’s rules. This study uses both the complete human genome and its repeat-masked version.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Application of Data Compression Models to Handwritten Digit Classification",
        "doc_scopus_id": "85054849437",
        "doc_doi": "10.1007/978-3-030-01449-0_41",
        "doc_eid": "2-s2.0-85054849437",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Algorithmic information theory",
            "Context models",
            "Handwritten digit",
            "Kolmogorov complexity",
            "Probabilistic algorithm"
        ],
        "doc_abstract": "© 2018, Springer Nature Switzerland AG.In this paper, we address handwritten digit classification as a special problem of data compression modeling. The creation of the models—usually known as training—is just a process of counting. Moreover, the model associated to each class can be trained independently of all the other class models. Also, they can be updated later with new examples, even if the old ones are not available anymore. Under this framework, we show that it is possible to attain a classification accuracy consistently above 99.3% on the MNIST dataset, using classifiers trained in less than one hour on a common laptop.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An approach to robot task learning and planning with loops",
        "doc_scopus_id": "85041943595",
        "doc_doi": "10.1109/IROS.2017.8206501",
        "doc_eid": "2-s2.0-85041943595",
        "doc_date": "2017-12-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Action sequences",
            "Planning domains",
            "Problem instances",
            "Robot tasks",
            "Simulated environment"
        ],
        "doc_abstract": "© 2017 IEEE.This paper addresses robot task model learning and planning with loops. By detecting and modeling loops in solved tasks it is possible to learn and solve wider classes of problems. We extend our previous work on experience-based planning domains in robotics to detect, represent and generate loops in action sequences. This approach provides methods for, (i) conceptualizing robot experiences possibly containing loops and learning high-level robot activity schemata with loops; and (ii) instantiating schemata with loops for solving problem instances of the same task with varying sets of objects. Demonstrations of this system in both real and simulated environments prove its potentialities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA word analysis based on the distribution of the distances between symmetric words",
        "doc_scopus_id": "85018995715",
        "doc_doi": "10.1038/s41598-017-00646-2",
        "doc_eid": "2-s2.0-85018995715",
        "doc_date": "2017-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Chromosomes, Human",
            "Databases, Genetic",
            "DNA",
            "Genome, Human",
            "Genomics",
            "Humans",
            "Markov Chains",
            "Models, Genetic",
            "Nucleic Acid Conformation",
            "Sequence Analysis, DNA",
            "Structure-Activity Relationship"
        ],
        "doc_abstract": "© 2017 The Author(s).We address the problem of discovering pairs of symmetric genomic words (i.e., words and the corresponding reversed complements) occurring at distances that are overrepresented. For this purpose, we developed new procedures to identify symmetric word pairs with uncommon empirical distance distribution and with clusters of overrepresented short distances. We speculate that patterns of overrepresentation of short distances between symmetric word pairs may allow the occurrence of non-standard DNA conformations, such as hairpin/cruciform structures. We focused on the human genome, and analysed both the complete genome as well as a version with known repetitive sequences masked out. We reported several well-defined features in the distributions of distances, which can be classified into three different profiles, showing enrichment in distinct distance ranges. We analysed in greater detail certain pairs of symmetric words of length seven, found by our procedure, characterised by the surprising fact that they occur at single distances more frequently than expected.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning robot tasks with loops from experiences to enhance robot adaptability",
        "doc_scopus_id": "85020823138",
        "doc_doi": "10.1016/j.patrec.2017.06.003",
        "doc_eid": "2-s2.0-85020823138",
        "doc_date": "2017-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Conceptualization",
            "Longest common prefixes",
            "Longest common subsequences",
            "Loop detection",
            "Planning domains",
            "Robot tasks",
            "Simulated environment",
            "Task knowledge"
        ],
        "doc_abstract": "© 2017 Elsevier B.V.Learning robot task models with loops helps to increase both the applicability and the compactness of task knowledge. In the framework of Experience-Based Planning Domains (EBPDs), previously formalized by the authors, an approach was developed for learning and exploiting high-level robot task models (the so-called activity schemata) with loops. The paper focuses on the development of: (i) a method—Contiguous Non-overlapping Longest Common Subsequence (CNLCS)—based on the Longest Common Prefix (LCP) array for detecting loops of actions in a robot experience; and (ii) an abstract planner to instanciate a learned task model with loops for solving particular instances of the same task with varying numbers of objects. Demonstrations of this system in both real and simulated environments prove its potentialities.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2017-06-09 2017-06-09 2017-10-23 2017-10-23 2018-07-21T17:18:19 S0167-8655(17)30199-X S016786551730199X 10.1016/j.patrec.2017.06.003 S300 S300.2 FULL-TEXT 2018-07-21T17:19:08.830666Z 0 0 20171101 2017 2017-06-09T16:23:35.30795Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb vol volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref 0167-8655 01678655 true 99 99 C Volume 99 9 57 66 57 66 20171101 1 November 2017 2017-11-01 2017 User Profiling and Behavior Adaptation for Human-Robot Interaction Silvia Rossi Dongheui Lee article sco © 2017 Elsevier B.V. All rights reserved. LEARNINGROBOTTASKSLOOPSEXPERIENCESENHANCEROBOTADAPTABILITY MOKHTARI V 1 Introduction 2 Related work 3 Representation 4 Learning task models 4.1 Generalization and abstraction 4.2 Feature extraction 4.3 Loop detection 5 Using task knowledge to solve new problems 5.1 Loop expansion 5.2 Planning 6 Experimental results 6.1 Real robot demonstration 6.2 Simulated domain 7 Conclusion and future work Acknowledgment References GHALLAB 2004 M AUTOMATEDPLANNINGTHEORYPRACTICE MOKHTARI 2016 463 483 V MOKHTARI 2016 509 517 V 26STINTERNATIONALCONFERENCEAUTOMATEDPLANNINGSCHEDULINGICAPS EXPERIENCEBASEDROBOTTASKLEARNINGPLANNINGGOALINFERENCE CHAUHAN 2013 A 16THPORTUGUESECONFERENCEARTIFICIALINTELLIGENCEEPIA2013 TOWARDSSUPERVISEDACQUISITIONROBOTACTIVITYEXPERIENCESONTOLOGYBASEDAPPROACH MOKHTARI 2016 993 1005 V INTELLIGENTAUTONOMOUSSYSTEMS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES HAMMOND 1986 267 271 K PROCEEDINGSFIFTHNATIONALCONFERENCEARTIFICIALINTELLIGENCE CHEFAMODELCASEBASEDPLANNING BORRAJO 2015 35:1 35:39 D FIKES 1972 251 288 R CHRPA 2010 281 297 L MITCHELL 1986 47 80 T SHAVLIK 1990 39 70 J SRIVASTAVA 2008 991 997 S PROCEEDINGSTWENTYTHIRDCONFERENCEARTIFICIALINTELLIGENCE LEARNINGGENERALIZEDPLANSUSINGABSTRACTCOUNTING SRIVASTAVA 2011 615 647 S LEVESQUE 2005 509 515 H PROCEEDINGSNINETEENTHINTERNATIONALJOINTCONFERENCEARTIFICIALINTELLIGENCEIJCAI PLANNINGLOOPS WINNER 2007 E WORKSHOPAIPLANNINGLEARNINGICAPS LOOPDISTILLLEARNINGDOMAINSPECIFICPLANNERSEXAMPLEPLANS MANBER 1993 935 948 U ZHUO 2009 1804 1810 H PROCEEDINGSTWENTYFIRSTINTERNATIONALJOINTCONFERENCEARTIFICIALINTELLIGENCEIJCAI LEARNINGHTNMETHODPRECONDITIONSACTIONMODELSPARTIALOBSERVATIONS ILGHAMI 2002 131 142 O PROCEEDINGSSIXTHINTERNATIONALCONFERENCEARTIFICIALINTELLIGENCEPLANNINGSYSTEMSAIPS CAMELLEARNINGMETHODPRECONDITIONSFORHTNPLANNING HOGG 2008 950 956 C PROCEEDINGSTWENTYTHIRDAAAICONFERENCEARTIFICIALINTELLIGENCE HTNMAKERLEARNINGHTNSMINIMALADDITIONALKNOWLEDGEENGINEERINGREQUIRED DSACERDOTI 1974 115 135 E HASLUM 2007 1007 1012 P PROCEEDINGSTWENTYSECONDAAAICONFERENCEARTIFICIALINTELLIGENCE DOMAININDEPENDENTCONSTRUCTIONPATTERNDATABASEHEURISTICSFORCOSTOPTIMALPLANNING KNOBLOCK 1991 541 546 C PROCEEDINGSNINTHAAAICONFERENCEARTIFICIALINTELLIGENCE INTEGRATINGABSTRACTIONEXPLANATIONBASEDLEARNINGINPRODIGY BERGMANN 1995 53 118 R SEABRALOPES 1999 294 300 L ASSEMBLYTASKPLANNING1999ISATP99PROCEEDINGS1999IEEEINTERNATIONALSYMPOSIUM FAILURERECOVERYPLANNINGINASSEMBLYBASEDACQUIREDEXPERIENCELEARNINGBYANALOGY SEABRALOPES 2007 65 70 L IFACWORKSHOPINTELLIGENTASSEMBLYDISASSEMBLYIAD2007 FAILURERECOVERYPLANNINGFORROBOTIZEDASSEMBLYBASEDLEARNEDSEMANTICSTRUCTURES TENORTH 2013 643 651 M TENORTH 2015 M HERTZBERG 2014 297 304 J SHAFII 2016 2895 2900 N 2016IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS LEARNINGGRASPFAMILIAROBJECTSUSINGOBJECTVIEWRECOGNITIONTEMPLATEMATCHING RINTANEN 2012 45 86 J HOFFMANN 2001 253 302 J MOKHTARIX2017X57 MOKHTARIX2017X57X66 MOKHTARIX2017X57XV MOKHTARIX2017X57X66XV 2019-10-23T00:00:00.000Z UnderEmbargo © 2017 Elsevier B.V. All rights reserved. item S0167-8655(17)30199-X S016786551730199X 10.1016/j.patrec.2017.06.003 271524 2018-07-21T17:19:08.830666Z 2017-11-01 true 1920868 MAIN 10 56781 849 656 IMAGE-WEB-PDF 1 gr1 6346 65 219 gr10 10340 163 132 gr11 11045 164 200 gr12 10655 164 146 gr13 11610 164 186 gr14 10290 132 219 gr15 7412 128 219 gr16 7575 128 219 gr17 7155 110 219 gr2 4260 27 219 gr3 20243 96 219 gr4 7969 164 176 gr5 5957 163 100 gr6 12590 129 219 gr7 11629 129 219 gr8 4966 26 219 gr9 7118 95 219 gr1 16453 112 376 gr10 44177 347 280 gr11 27146 235 287 gr12 36321 297 265 gr13 32624 260 295 gr14 20792 172 287 gr15 17900 200 341 gr16 17905 200 343 gr17 20758 200 397 gr2 9075 48 386 gr3 24049 169 387 gr4 30347 359 386 gr5 50585 631 386 gr6 41723 228 386 gr7 39961 228 386 gr8 10726 45 386 gr9 20107 167 386 gr1 116627 496 1667 gr10 325844 1539 1243 gr11 195371 1041 1273 gr12 269581 1318 1176 gr13 239069 1152 1309 gr14 146170 765 1273 gr15 160304 886 1511 gr16 169082 886 1521 gr17 182489 886 1761 gr2 56558 214 1708 gr3 214074 750 1713 gr4 242564 1592 1710 gr5 431905 2794 1710 gr6 474511 1008 1708 gr7 441196 1008 1708 gr8 64047 201 1708 gr9 134882 740 1708 si10 144 13 12 si23 481 17 111 si6 151 13 14 si21 432 16 95 si30 218 16 50 si40 413 18 91 si7 235 14 45 si8 162 15 17 si9 172 13 16 si11 194 14 23 si15 267 11 77 si12 247 15 49 si13 468 16 108 si14 487 17 124 si16 251 13 47 si17 583 16 143 si18 220 13 43 si24 274 15 52 si19 428 16 106 si2 471 17 72 si22 390 16 92 si25 280 18 52 si3 248 16 53 si26 278 12 61 si20 300 14 60 si27 2223 51 455 si28 572 17 116 si29 162 13 38 si31 380 16 77 si32 187 16 32 si33 272 16 63 si34 174 17 20 si35 479 17 110 si36 462 16 103 si37 205 13 41 si38 774 17 247 si39 273 15 67 si4 869 17 222 si41 190 18 26 si42 173 15 21 si43 865 17 221 si44 275 13 64 si45 698 17 145 si46 215 13 48 si5 149 13 14 si1 553 15 97 PATREC 6842 S0167-8655(17)30199-X 10.1016/j.patrec.2017.06.003 Elsevier B.V. Fig. 1 An overview of the learning and planning system. Fig. 1 Fig. 2 Suppose (ab)*cd is an abstract plan, in which each letter represents an abstract operator and (ab)* represents a loop. ASBP generates two classes of successors when gets to a loop in an abstract plan: (i) ASBP copies the loop and appends it to the beginning of the abstract plan and generates the successors for this abstract plan; and (ii) ASBP skips the loop and generates the successors for the rest of the abstract plan. ASBP then picks a successor with the lowest cost to develop. This procedure either expands or skips a loop. Fig. 2 Fig. 3 From left to right, robot moves to the cup, picks up the cup from the table, carries the cup, and place it on the tray. Fig. 3 Fig. 4 The learned knowledge for the task (Stack ?table ?pile) in the stripped tower domain. Each action node represents an enriched abstract operator, i.e., an abstract operator on the left side of colon, and a set of features associated with the abstract operator on the right side of colon. The decisions, i.e., diamonds, appear before the loops and specify the cost of two parallel branches to either expand or skip the loops. Depending on the minimum cost of abstract actions, the ASBP planner moves toward the loops or skips the loops, i.e., moves to the next abstract operator, in an abstract plan (see Algorithm 2). Fig. 4 Fig. 5 The learned knowledge for the task (UnstackStack ?pile1 ?pile2) in the stripped tower domain. Fig. 5 Fig. 6 Performance of the SBP, Mp and FF in the task (Stack ?table ?pile), in the stripped tower domain. Fig. 6 Fig. 7 Performance of the SBP and Mp in the task (UnstackStack ?pile1 ?pile2), in the stripped tower domain. FF failed to solve problems in this task. Fig. 7 Listing 1 An abstract operator in EBPD. Listing 1 Listing 2 Representation of a planning operator in EBPD. With respect to the standard PDDL, parent and static are new properties. Listing 2 Listing 3 An experience for ‘clear table’ task problem, in robotic_arm domain. The plan solution to this problem contains 8 primitive actions. Listing 3 Listing 4 A task planning problem for ‘clear table’ task in EBPD. Listing 4 Listing 5 An experience after generalization and abstraction. Compared to Listing 3, the constants in the experience are replaced with variables, and the actions in the plan are replaced with their abstract operator parents. Listing 5 Listing 6 A learned activity schema for ‘clear table’ task after generalization, abstraction and feature extraction. Each abstract operator is associated with a set of features. Listing 6 Listing 7 A learned activity schema for the ‘clear table’ task with a loop. Listing 7 Algorithm 1 Contiguous Non-overlapping Longest Common Prefixes (CNLCP). Algorithm 1 Algorithm 2 Abstract Schema-Based Planner (ASBP). Algorithm 2 Algorithm 3 Schema-Based Planner (SBP). Algorithm 3 Table 1 Abstract and planning operators developed for the r o b o t i c _ a r m domain. Table 1 Abstract operators Planning operators pick(object, table) pickup(arm, object, table) ; pick up an object from a table put(object, tray) putdown(arm, object, tray) ; put down an object on a tray nil grip(arm, old_posture, new_posture) ; change the gripper posture to a new posture move(arm, from, to) ; move arm to a new position Table 2 The arrays SA, LCP and NLCP for the sequence ababa (i.e., pick put pick put pick). Table 2 i SA[i] LCP[i] NLCP[i]* suffix SA [ i ] 0 4 0 0 a 1 2 1 1 aba 2 0 3 2 ababa 3 3 0 0 ba 4 1 2 2 baba * Each number in ith row specifies the length of the non-overlapping longest common prefix between two suffixes in rows i and ( i − 1 ) for i ≥ 1. Table 3 Performance of the planners in task (Stack ?table ?pile), in the stripped tower domain. Table 3 Problem* Total time (s) Memory (MB) Evaluated states Plan length Mp FF SBP Mp FF SBP Mp FF SBP Mp FF SBP p10 0.4 0.0 0.2 27.6 5.4 6.9 95 106 71 39 39 39 p12 0.8 0.0 0.3 37.8 6.2 9.3 196 140 88 48 49 48 p14 0.1 0.0 0.5 47.1 6.7 12.6 407 177 103 56 57 55 p16 0.2 0.1 0.8 61.8 7.7 17.0 766 217 120 63 63 64 p18 0.4 0.1 1.2 79.6 8.5 23.4 1107 263 136 72 73 72 p20 0.7 0.1 1.4 102.1 9.6 22.9 1418 310 152 80 80 80 p22 1.0 0.2 1.9 120.4 10.6 27.5 1160 364 168 87 87 88 p24 1.5 0.4 2.6 151.8 11.7 30.5 1428 422 183 96 97 95 p26 2.2 0.3 3.2 196.2 13.0 36.7 2026 483 199 104 105 103 p28 3.2 0.7 4.2 255.5 14.6 40.5 2832 548 215 112 113 111 p30 4.2 0.6 5.5 278.6 15.9 48.7 2355 616 232 119 119 120 p32 6.2 0.8 6.9 351.6 17.5 54.7 4009 690 248 128 129 128 p34 8.2 0.1 8.9 433.5 19.1 65.3 4575 766 263 135 135 135 p36 11.1 0.1 11.8 534.1 21.0 76.6 5884 847 280 143 143 144 p38 14.9 0.1 15.1 653.7 22.8 85.8 7288 932 295 151 151 151 p40 19.6 0.1 20.0 785.8 24.7 98.6 9276 1020 311 160 160 159 p42 23.8 0.2 26.5 867.3 26.8 117.4 8054 1113 328 168 168 168 p44 30.8 0.2 34.2 1.0k 28.9 137.1 9922 1210 344 176 176 176 p46 39.8 0.3 45.1 1.2k 31.0 152.4 12,124 1311 360 184 184 184 p48 48.5 0.3 60.1 1.4k 33.4 177.6 12,589 1416 376 192 192 192 p50 59.9 0.4 79.2 1.5k 35.8 198.2 13,106 1526 392 199 199 200 * The name of each problem describes the number of blocks used in that problem. Table 4 Performance of the planners in task (UnstackStack ?pile1 ?pile2), in the stripped tower domain. Table 4 Problem Total time (s) Memory (MB) Evaluated states Plan length Mp SBP Mp SBP Mp SBP Mp SBP p10 0.30 0.23 83.4 8.4 3455 152 76 76 p12 0.31 0.31 91.9 11.0 2304 198 91 92 p14 1.25 0.44 211.8 14.1 5093 248 108 107 p16 1.32 0.61 233.0 17.9 3675 305 123 124 p18 1.62 0.83 269.4 22.9 4889 365 140 139 p20 1.56 0.98 263.7 23.6 2498 356 156 156 p22 4.21 1.14 571.5 27.1 6244 387 171 172 p24 4.12 1.37 563.5 30.0 4462 423 188 188 p26 4.78 1.68 586.1 34.4 4133 464 204 204 p28 11.0 2.05 1.2k 39.3 12,485 510 220 220 p30 13.5 2.57 1.4k 43.6 13,106 561 235 236 p32 20.6 3.16 2.1k 50.0 15,367 617 251 252 p34 21.0 3.97 2.0k 55.8 10,645 678 267 268 p36 37.9 5.15 2.7k 63.8 19,213 744 284 284 p38 33.9 6.55 2.7k 72.4 12,353 815 299 300 p40 42.7 8.84 2.8k 82.1 14,503 914 316 316 p42 58.5 11.3 3.0k 93.7 18,362 1000 332 332 p44 – 15.3 – 105.0 – 1091 – 348 p46 – 19.1 – 119.2 – 1187 – 364 p48 – 24.7 – 137.4 – 1288 – 380 p50 – 32.5 – 149.6 – 1394 – 396 Learning robot tasks with loops from experiences to enhance robot adaptability Vahid Mokhtari ⁎ Luís Seabra Lopes Armando J. Pinho Institute of Electronics and Informatics Engineering of Aveiro (IEETA), University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro (IEETA) University of Aveiro Portugal ⁎ Corresponding author. Learning robot task models with loops helps to increase both the applicability and the compactness of task knowledge. In the framework of Experience-Based Planning Domains (EBPDs), previously formalized by the authors, an approach was developed for learning and exploiting high-level robot task models (the so-called activity schemata) with loops. The paper focuses on the development of: (i) a method—Contiguous Non-overlapping Longest Common Subsequence (CNLCS)—based on the Longest Common Prefix (LCP) array for detecting loops of actions in a robot experience; and (ii) an abstract planner to instanciate a learned task model with loops for solving particular instances of the same task with varying numbers of objects. Demonstrations of this system in both real and simulated environments prove its potentialities. Keywords Robot task learning with loops Learning and planning Conceptualization Loop detection Experience-based planning domains 1 Introduction Automated planning is a field of Artificial Intelligence (AI) that studies the computational synthesis of sequences of actions to perform the given tasks [1]. AI planning techniques typically require hand-coding of domain knowledge, e.g., in the form of a set of planning action classes, the so-called planning operators. However, it is not always easy to specify all knowledge that may be necessary. For instance, among different plans to achieve a given task or goal, some plans may be preferable due to factors that may be hard to specify in the planning domain description, such as social conventions. Moreover, despite significant advances in general purpose planning algorithms, i.e., algorithms for solving any problem in any planning domain, AI planners often fail to solve large-scale problems. In this context, the authors have been developing conceptualization techniques that derive generic task models from concrete plans appropriate for solving the given tasks. The acquired and stored task models can then be used to derive solutions for other tasks in the same class. Plans used to solve concrete problems are derived from human demonstrations and/or verbal instructions. Solutions to large-scale problems often include repetitive structures, i.e., loops in which a sequence of actions is successively applied to different objects. Since the set of target objects can vary from task to task, the number of iterations of the loop will also vary. Therefore, the appropriate plan solutions can vary significantly within a class of tasks that involves loops. The inclusion of loops in a task model allows to improve its compactness and helps to increase its applicability by repeating sequences of actions for different sets of objects. Conceptualizing tasks containing loops is therefore particularly useful for solving large problems. This paper contributes towards, (i) detecting and representing loops of actions in a task demonstration; (ii) and adapting and exploiting loops to different instances of the same class of tasks. We assume the basic skills, e.g., pick up or put down, are already implemented in the robot system. We provide the robot with step-by-step operating instructions, i.e., a solution plan possibly containing loops to achieve a task, and allow the robot to conceptualize the task. The obtained task models can later be used for solving different instances of the same task with different number of objects. In previous works, the authors proposed and formalized the notion of Experience-Based Planning Domain [2,3], and presented approaches for collecting experiences through human-robot interaction [4,5], conceptualizing experiences in the form of activity schemata and exploiting the acquired activity schemata to plan solutions to new tasks [2,3]. In this paper, previous work is extended to detect loops in experiences and represent them in the learned activity schemata. An abstract planner was developed and integrated into the system to expand the loops in activity schemata adapting them for given tasks. Fig. 1 provides an overview of the adopted learning and planning framework. The user interface allows a human to instruct a robot to carry out sequences of actions and teach the robot which tasks were achieved. A standard planner is also integrated into the system to generate solutions for tasks, when a human user is not present or when providing a solution is too complex for the human user. When a task is successfully carried out, the experience extractor collects the world information as well as the applied actions as an experience. The conceptualizer builds activity schemata, i.e., abstract task models, from single experiences. The planning module picks an activity schema for a given task problem and generates a plan solution. The robot platform includes perception and execution modules. This paper addresses how the conceptualizer generates a task model with loops and how a symbolic planner 1 1 Throughout this paper the word ‘planner’ means ‘symbolic planner’ as used in the AI planning literature. finds solutions to similar tasks with varying numbers of objects using the learned task model. A ‘clear table’ scenario is used to illustrate the proposed algorithms. A robotic application EBPD, r o b o t i c _ a r m (Table 1 ), was developed and used in a real environment, where a robotic arm learns and performs tasks. To show the potential of our approach we also demonstrate the system in two challenging classes of problems in a simulated domain and compare the performance of the system with two state-of-the-art planners. The rest of this paper is organized as follows. Related works are discussed in Section 2. The formalization of Experience-Based Planning Domains (EBPDs) is recapitulated in Section 3. In Section 4, the approach to learn task models from robot experiences is presented. In addition to summarizing general aspects of the approach, special attention is given in this section to the loop detection algorithm. In Section 5, the planning approach, including an abstract planner, to extend and adapt loops of actions in an activity schema, and a planner, which generates a concrete solution to a given task problem, is described in detail. Finally, Section 6 shows the experimental results and the performance of our system. 2 Related work Several works have focused on acquiring planning knowledge to reduce planning search. The earliest and best known approaches are case based planning [6,7] and macro operators [8,9]. These methods tend to suffer from the utility problem, in which learning more information can be counterproductive due to the difficulty with storage and management of the information and with determining which information should be used to solve a particular problem. Some work has focused on analyzing example plans to generalize them from concrete objects to variables. A well known approach is Explanation-Based Learning (EBL), which generalizes a proof or an explanation of a solution to be applicable to different problem instances [8,10]. One example is BAGGER2 [11], which uses example solutions and domain knowledge to learn an algorithm for problem solving in the form of recursive structures. Other work to find plans with loops includes generalized planning [12–14]. Given a goal condition and a class of initial states with varying number of objects, generalized planning finds a plan that is algorithmic in nature and works for a set of problems. Much of the work on generalized planning leans heavily on theorem-proving. These techniques were mostly applied in simulated domains, and the problems are not expressible in widespread planning languages such as PDDL. LoopDistill [15] addresses the problem of learning loops from examples applied to simulated domains. It identifies the largest matching sub-plan in a given example, and converts the repeating occurrences of these sub-plans into a loop. The output is a domain-specific planning program (dsPlanner), actually a plan with with if statements and while-loops. Our approach builds a task model with loop structures that are used by a domain-specific planner. Our loop detection algorithm uses the standard Longest Common Prefix (LCP) array [16]. LoopDistill does not address how to terminate loops in its learned dsPlanners. A body of work focuses on learning Hierarchical Task Networks (HTNs) [17]. In HTN planning, a plan is generated by decomposing a method for a given task into simpler tasks until primitive tasks are reached, i.e., tasks that can be directly achieved by planning operators. Recursive HTN methods are an alternative approach for representing loops. Research into HTN method learning has been much more application oriented than other AI-planning research. However, identifying a good hierarchical structure is an issue, and most of the techniques in HTN learning rely on the hierarchical structure of the HTN methods specified by a human expert [18,19]. Abstraction has also played a significant role in reducing planning complexity [20,21]. A few works also integrated abstraction with EBL to learn more generic concepts called abstract plans or plan skeletons [22–25]. Similarly, we integrate an abstraction hierarchy with EBL to reduce problem space and learn general concepts. However, we do not propose an automatic procedure to construct the abstraction hierarchy. The Web Ontology Language (OWL) is also becoming increasingly prevalent in cognitive robotics for knowledge representation and for supporting reusability and shareability [26,27]. These techniques combine information of many types from different sources. Our approach was initially integrated into a OWL-based cognitive system [28]. Overall, most of the approaches in AI planning concentrate on the theory and simulated domains, and have seldom been used in real robots. On the other hand, in the LfD approaches, the expressive representation and identification of loops in a task demonstration, as well as the exploitation of the learned knowledge by a planner, are given little attention. 3 Representation A formal definition of Experience-Based Planning Domains (EBPDs), i.e., planning domains that evolve through learning from experience, is proposed in this section. Definition 1 An EBPD is a tuple, D = ( L , Σ , S , A , O , E , M ) , where L is a first-order logic language that has finitely many predicate and constant symbols, Σ is a set of ground atoms of L that are always true, i.e., static world information, S is a set of states in which every state s ∈ S is a set of ground atoms of L which may become false, i.e., transient world information, A is a set of abstract operators, O is a set of planning operators, E is a set of experiences, and M is a set of methods in the form of activity schemata. Definition 2 An abstract operator a ∈ A is a triple, a = ( h , P , E ) , where h is the abstract operator’s head, P is the precondition, and E is the effect of a. A head takes a form n ( x 1 , … , x k ≥ 0 ) , in which n is the name, and x 1 , … , x k are the arguments, e.g., (pick ?object ?table) 2 2 The notation in the Planning Domain Definition Language (PDDL) is used to represent EBPDs. All terms starting with a question mark (?) are variables, and the rest are constants or function symbols. . A ground instance of an abstract operator is called an abstract action. In the proposed EBPDs, an abstract operator is a class of planning operators. Listing 1 shows an abstract operator represented in EBPD. Definition 3 A planning operator o ∈ O is a tuple, o = ( h , a , S , P , E ) , where h is the operator’s head, a is an abstract operator head which specifies the superclass or parent of o, S is the static world information, and P and E are respectively the precondition and effect of o. A ground instance of an operator is called an action. Listing 2 shows a planning operator represented in EBPD, and Table 1 shows the implemented abstract and planning operators in this work. Experiences are episodic descriptions of plan-based robot activities including environment perception, sequences of applied actions and achieved tasks. Definition 4 An experience e ∈ E is a triple of ground structures, e = ( t , K , π ) , where t is the head of a task, taught by a human user to a robot, e.g., (clear table1), K is a set of key propositions, and π is a plan solution to achieve t. K is a subset of the world description captured during an experience. Every key proposition in K is a predicate with a temporal symbol specifying the temporal extent of the predicate in an experience. Three types of temporal symbols are used to represent key propositions, static (always true during an experience), e.g., (static(reach arm1 table1)), init (true at the initial state), e.g., (init(on cup table1)), and end (true at the goal state), e.g., (end(on cup tray1)). Listing 3 shows an experience for the ‘clear table’ task. Extracted experiences are the inputs to acquire task knowledge. An activity schema is a task model obtained from an experience: Definition 5 An activity schema m ∈ M is a pair, m = ( h , Ω ) , where h is the head of a target task (e.g., (clear ?table)), and Ω is an abstract plan to achieve the h, i.e., a sequence of abstract operators or loops of abstract operators enriched with features. Definition 6 An enriched abstract operator ω is a pair, ω = ( a , F ) , where a ∈ A is an abstract operator head (Definition 2), and F is a set of features, i.e., a set of unground key propositions (Definition 4), describing the arguments of a. Listing 7 shows later a learned activity schema in this paper. Definition 7 A task planning problem is a tuple of ground structures, P = ( t , s 0 , g ) , where t is the head of a target task to be planned, s 0 ∈ S is the initial state, and g is the goal, i.e., a set of propositions to be satisfied in a goal state s g ∈ S . Listing 4 shows a task planning problem in EBPD. 4 Learning task models Experiences are the inputs for learning task knowledge. Experiences are generated through human-robot interaction. We previously presented an infrastructure to instruct and teach a robot how to carry out a task using human-robot interaction, and approaches to gathering world information and recording experiences [2]. Obtaining a plan solution from human instructions may have some advantages over the using a standard planner. For instance when there are different alternatives to achieve a goal, some alternatives may be preferable to correctly achieve the goal based on different factors that have not been encoded in the domain specification, such as social norms, physical constraints, etc. Nevertheless, a standard planner is alternatively integrated in the system to generate a plan solution for a given task demonstration when a human user is not present or when providing a plan solution is too complex for the human user. In this section we present methods for conceptualizing experiences. By conceptualization it is meant the process of generating an activity schema from an experience. The conceptualization approach is a combination of different techniques including deductive generalization, abstraction, feature extraction and loop detection. 4.1 Generalization and abstraction Generalization is the first step in conceptualizing an experience. Through deductive generalization, general concepts are formulated from single training examples and domain knowledge. We employed a goal regression algorithm, as in explanation-based generalization (EBG) [10,24,25], to: (i) build an explanation of how an experience is solved with domain operators; and (ii) generalize the obtained explanation. The generalization is carried out by substitution of variables for the observed constants in an experience and propagating this substitution in the whole experience. Abstraction is an important technique for improving problem solving performance. To reduce the level of detail in a generalized experience, we use an operator abstraction hierarchy, which results in more widely applicable task models. This hierarchy is specified in an EBPD using the parent property of planning operators which links to an abstract operator (see Definitions 2 and 3, and Table 1). Abstraction creates an abstract plan by replacing operators with their parents in the operator abstraction hierarchy. In this abstraction, some operators are mapped onto nil, meaning they are excluded from the learned activity schema, and some arguments of operators are excluded in the respective abstract operators. The nil class operators are auxiliary operators that are reconstructed again during instantiation of a learned activity schema for a given task problem. Listing 5 shows an experience after generalization and abstraction. 4.2 Feature extraction A feature is a key proposition that reveals a connection between an abstract operator and the task achieved in an experience. A feature is defined as a key proposition, τ(Pn ), for n ≥ 1, where Pn is an n − a r y predicate and τ is a temporal symbol, i.e., static, init, or end. Each argument of P either appears in an abstract operator’s head or in the task head, or in both. For example in Listing 5, the key proposition, (init(on ?cup ?table)), on line 9, is a feature that connects the ?cup, an argument of the abstract operator pick, on line 21, to the ?table, an argument of the clear task, on line 2. Relevant features are discovered in the generalized key propositions. For each abstract operator in the abstract plan of a generalized and abstracted experience, all possible relationships between the arguments of the abstract operator and the task arguments are extracted and associated to the abstract operator. The set of features, F, for an abstract operator with a set of arguments, A, and a task with a set of arguments, T, is extracted from a set of key propositions, K, as follows: (1) F ( A , T , K ) = { τ ( P ( t 1 , … , t n ) ) ∈ K ∣ { t 1 , … , t n } ∩ A ≠ ϕ , { t 1 , … , t n } ⊆ ( A ∪ T ) , n ≥ 1 } . Listing 6 shows an example of an activity schema after generalization, abstraction and feature extraction. During problem solving, features lead a planner toward a goal state, greatly reducing the probability of backtracking. 4.3 Loop detection Solutions to a class of problems with varying number of objects may differ in the repetition of some actions. The final step in conceptualization is to detect possible loops of actions. A loop is a contiguous repetition of a subsequence of enriched abstract operators for different objects. Two or more contiguous subsequences of enriched abstract operators belong to a loop if, (i) the names and the order of the corresponding abstract operators in each subsequence are the same; (ii) the sets of features describing the corresponding abstract operators in each subsequence, are the same; and (iii) the variables appearing in the corresponding abstract operators and in their corresponding features, in each subsequence, play the same role. For example in Listing 6, two contiguous subsequences of pick and put with the same corresponding features, and the corresponding variables with the same role belong to a loop. We developed a loop detection approach based on the Suffix Array (SA)—an array of integers providing the starting positions of all suffixes of a string, sorted in lexicographical order—and the Longest Common Prefix array (LCP array)—an integer array storing the lengths of the longest common prefixes between all pairs of consecutive suffixes in a suffix array—for an abstract plan [16]. We compute a loop as the Contiguous Non-overlapping Longest Common Prefix (CNLCP) between two consecutive suffixes in a suffix array (Algorithm 1 ). We modified the standard LCP algorithm to find the Non-overlapping Longest Common Prefix (NLCP) between a pair of consecutive suffixes in a suffix array by controlling the size of the common prefix to be at most equal to the difference between the lengths of the two given suffixes (line 13). Algorithm 1 first creates a suffix array, SA, for a given abstract plan, Ω, in line 1, and then builds an NLCP array, in line 3. This is an integer array of size n = l e n g t h ( Ω ) such that NLCP[0] is undefined and NLCP[i], for 1 ≤ i < n, is the length of the non-overlapping longest common prefix between suffixes i − 1 and i in a suffix array, SA (i.e., SA [ i − 1 ] and SA[i]). Table 2 shows the computed SA, LCP and NLCP arrays for an abstract plan example. In this example the NLCP gives the subsequence ab (i.e., pick put) as the non-overlapping longest common prefix between two consecutive suffixes aba and ababa (in rows 1 and 2 in Table 2), in contrast to LCP, which gives the overlapping longest common prefix aba. In Algorithm 1 on lines 5–10, the main function constructs a contiguous non-overlapping longest common prefixes array of strings, CNLCP array, such that each CNLCP[i], for i ≥ 0, is an iteration of a loop that consecutively occurs in a given string, i.e., in an abstract plan. A non-overlapping longest common prefix at NLCP[i] is consecutive if NLCP [ i ] = a b s ( SA [ i ] − SA [ i − 1 ] ) , for 1 ≤ i < n (line 9). Finally, we select a contiguous non-overlapping longest common prefix in the CNLCP array with a largest length, i.e., an iteration of a loop with the largest length, and check that all the features associated with each occurrence of this iteration in the abstract plan are the same. In the current approach, we do not address the possibility of nested loops (i.e., loops inside loops). When a loop is detected, new variables are substituted for the different variables playing the same role in the corresponding abstract operators and in their corresponding features in each subsequence. For example, in Listing 6 the variables ?cup and ?spoon, playing the same role in the respective loop iterations, are replaced with a new variable ?X. Finally, the subsequences of abstract operators are merged and an intersection of their corresponding features is computed. Listing 7 shows the learned activity schema of ‘clear table’ task with a loop of actions. 5 Using task knowledge to solve new problems If there is a learned activity schema for a task, the planning system attempts to generate a plan to achieve any particular instance of that task. If there are several learned activity schemata for a task, the most recent one is used by the planner. In case of failure, the next recent one is used, and so on. Generating a plan for a given task may involve loop expansion, i.e., determining concrete iterations of a loop. 5.1 Loop expansion We developed an abstract planner, Abstract Schema-Based Planner (ASBP), to expand the loops in an activity schema (Algorithm 2 ). The general idea is to search forward while following the activity schema. ASBP takes as input a domain of abstract operators, A , a task problem, P = ( t , s 0 , g ) , and a learned activity schema, m = ( h , Ω 0 ) , for task t such that h = t (i.e., h should be syntactically unifiable with t), and returns a ground abstract plan without loops, Π. Each node in the search tree retains a state, s, the ground abstract plan without loops built so far, Π, the remaining part of the abstract plan, Ω, the total cost, f, and the cost, c, of the path from the start node to the current node. In each planning iteration, a leaf node with the lowest f value is retrieved (line 3). If the front enriched abstract operator, head(Ω), is a loop (line 5), the planner generates successors for two alternatives: it adds an iteration of the loop to the front of Ω (line 6); and skips the loop and moves on to the next abstract operator in Ω (line 7). This way, successors are generated by applying, not only abstract actions instantiated from the front abstract operator inside the loop, but also abstract actions instantiated from the following abstract operator after the loop. Fig. 2 illustrates this idea. On line 9, if head(Ω) is not a loop, successors are generated by applying abstract actions instantiated from head(Ω). Notice that in the current work we assume there are no consecutive loops in an activity schema. The core of ASBP is the procedure step, which generates all successors for the class of the front abstract operator in a given abstract plan. On line 12, step selects the front enriched abstract operator, (a, F), and generates applicable instances of a (line 13). On lines 14–18, the features of each abstract actions, F′, are extracted and verified with the features in F, and a cost is computed as, (2) c n = c + c r e a l · ( k + 1 ) / ( v + 1 ) , where k is the total number of features in F, v is the number of features in F′ that are verified in F, and creal is the real cost of o (e.g., c r e a l = 1 ). It means the applicable abstract actions that verify all features in F, gain the lower cost, and abstract actions with the lower percentage of verified features, gain the higher cost. On line 20, the planner appends o to the current ground abstract plan, Π · o; moves forward in the abstract plan, tail(Ω); and computes the total cost as, c n + h + ( s n ) . The heuristic function, h + , estimates a cost from the current state to the goal g (i.e., we used an additive heuristic). Notice the h + is extremely efficient since the ASBP works with abstract domain. Finally, ASBP stops when the goal g is achieved on line 4. 5.2 Planning A ground abstract plan produced by ASBP forms the main skeleton of the final solution, so the general idea is to generate a plan solution, by substituting base operators for abstract operators and inserting the auxiliary actions into the abstract plan, i.e., actions from the nil class (see Table 1). Schema-Based Planner (SBP), in Algorithm 3 , takes as input a domain, D = ( L , Σ , S , A , O , E , M ) , and a task problem, P = ( t , s 0 , g ) , and generates a plan, π. SBP selects an activity schema m ∈ M for a given task t (line 1). On line 2, a ground abstract plan without loops is built, Π (see Algorithm 2). Line 3 creates the root node. In SBP, each node of the search tree retains a state, s, the plan built so far, π, the remaining part of the ground abstract plan, Π, the total cost, f, and the cost, c, of the path from the root node to the current node. On each planning iteration, a node with the lowest f value is retrieved (line 5). The planner selects the front abstract action, a (line 7), and develops the search tree (lines 8–17). SBP on each iteration either generates all actions that have the parent exactly as a (line 8), and moves forward on Π (line 10); or generates all auxiliary actions from the nil class (line 12). In all cases, the action cost is computed as, c + c r e a l (line 14), and the planner appends o to the current plan, π · o; and computes the total cost as, c n + α · l e n g t h ( Π ) (line 17). We estimate a heuristic (i.e., α · length(Π)) as the length of the remaining abstract plan, Π, multiplied by a factor, α. The factor α is used to calibrate the heuristic value to achieve the admissibility. Empirically, α, can be estimated as an average number of actions in a real plan per abstract operator in the abstract plan, for example, in the r o b o t i c _ a r m domain we estimated α = 2 (see number of actions in Listing 3 compared to number of abstract operators in Listing 5). The lowest value of α may cause to underestimate the cost of reaching the goal, and the highest value may cause to overestimate the actual cost. Determining an accurate value of α may need a little tweaking depending on the used domain. Finally, a plan solution is successfully generated when the goal g is achieved (line 6). 6 Experimental results The performance of the system in three classes of problems in both real and simulated environments is presented. 6.1 Real robot demonstration An EBPD was developed for a real robotic arm application domain. The planning and abstract operators were listed in Table 1 . The system is demonstrated in three different scenarios, one for teaching a task to a robot, and two for evaluating the performance of the learned task knowledge. We designed a ‘clear table’ training scenario with two objects on a table, and instructed and taught a robotic arm how to clear the table using a command-line interface. The main objective in this scenario is to clear the table by removing the two objects from the table and placing them on a tray. Fig. 3 shows snapshots of teaching the ‘clear table’ task to a robot. The plan solution provided by the human user, and the initial and goal states of the problem create an experience for this task. This experience was shown in Listing 3. Conceptualization derived the activity schema, shown in Listing 7, immediately after extracting the experience. To evaluate the performance of the system in solving problems, we demonstrated the system on two test scenarios with three and four objects on thetable respectively, and observed that the robot successfully solved and performed these two tasks. The system was integrated with the work in [29], where a robotic arm learns to grasp different objects through kinesthetic teaching. Videos of this performance are available in goo.gl/RxDCa3. 6.2 Simulated domain In addition to the real robot demonstration, a modified version of the stripped tower domain, originally introduced in [12], was developed. The following planning operators are considered: stack/5, unstack/5, pickup/4, putdown/4, move/4. Additionally, the following abstract operators were defined: stack/4, unstack/4, pick/3, put/3 (i.e., the numbers indicate arities). Two classes of tasks were considered in this domain, Stack and UnstackStack. In the Stack class, a number of red and blue blocks are initially on top of a table. In the UnstackStack class, a pile of red and blue blocks, with red blocks at the bottom and blue blocks on top, is initially given. In both classes, the goal is to construct a pile of alternating red and blue blocks, with a red base block at the bottom and a blue block on top. In each class, a problem with a certain number of objects was randomly created, and a plan solution for that problem was generated using a state-of-the-art planner, Madagascar (Mp) [30]. Based on that, an experience was assembled enabling to drive a task model, i.e., an activity schema. Figs. 4 and 5 represent these learned task models. SBP successfully solved all problems in each class using the learned activity schemata. The performance of SBP was evaluated and compared to two state-of-the-art planners, Mp and Fast-Forward (FF) [31], based on four measurements of time, evaluated nodes, plan length and memory (see Tables 3 and 4). In both classes of problems, SBP was extremely efficient in terms of evaluated nodes in the search tree. During the search process, features allow SBP to find the best actions, thus reducing the number of evaluated nodes. It should be noted that the time evaluation is not completely accurate, since SBP has been implemented in Prolog and Python, in contrast to other planners, which have been implemented in C++. FF was fairly fast to solve the problems in task Stack, however it failed to solve the problems in task UnstackStack. Figs. 6 and 7 summarize the performance of the three planners. SBP also showed the flexibility to solve slightly different problems to the original class. We generated a set of random problems in the class UnstackStack, in which in each problem, the blocks are not necessarily in a pile at the initial state, but some blocks are already on a table. SBP was able to ignore the loops in the given problems and solve the problems using the already learned task knowledge for this class. 7 Conclusion and future work Methods were presented for conceptualizing robot experiences containing loops, and for instanciating the learned concepts (activity schemata) with loops to solve other instances of the same class of tasks. The integration of this system in a real robotic arm was demonstrated. Additional evaluation was carried out in a complex class of problems in a simulated domain. Future work includes extensive evaluation of the proposed system on different task problems and scenarios. Acknowledgment This work is funded by the Portuguese Foundation for Science and Technology (FCT) under the grant SFRH/BD/94184/2013, and the project UID/CEC/00127/2013. References [1] M. Ghallab D. Nau P. Traverso Automated Planning: Theory & Practice 2004 Elsevier [2] V. Mokhtari L. Seabra Lopes A.J. Pinho Experience-based planning domains: an integrated learning and deliberation approach for intelligent robots J. Intell. Robot. Syst. 83 3 2016 463 483 [3] V. Mokhtari L. Seabra Lopes A.J. Pinho Experience-based robot task learning and planning with goal inference 26st International Conference on Automated Planning and Scheduling (ICAPS) 2016 AAAI Press 509 517 [4] A. Chauhan L. Seabra Lopes A.M. Tomé A. Pinho Towards supervised acquisition of robot activity experiences: an ontology-based approach 16th Portuguese Conference on Artificial Intelligence - EPIA’2013 2013 [5] V. Mokhtari G. Lim L. Seabra Lopes A.J. Pinho Gathering and conceptualizing plan-based robot activity experiences E. Menegatti N. Michael K. Berns H. Yamaguchi Intelligent Autonomous Systems 13 Advances in Intelligent Systems and Computing vol. 302 2016 Springer International Publishing 993 1005 [6] K.J. Hammond Chef: a model of case-based planning Proceedings of the Fifth National Conference on Artificial Intelligence 1986 AAAI Press 267 271 [7] D. Borrajo A. Roubíčková I. Serina Progress in case-based planning ACM Comput. Surv 47 2 2015 35:1 35:39 [8] R.E. Fikes P.E. Hart N.J. Nilsson Learning and executing generalized robot plans Artif. Intell. 3 1972 251 288 [9] L. Chrpa Generation of macro-operators via investigation of action dependencies in plans Knowl. Eng. Rev. 25 03 2010 281 297 [10] T.M. Mitchell R.M. Keller S.T. Kedar-Cabelli Explanation-based generalization: a unifying view Mach. Learn. 1 1 1986 47 80 [11] J.W. Shavlik Acquiring recursive and iterative concepts with explanation-based learning Mach. Learn. 5 1 1990 39 70 [12] S. Srivastava N. Immerman S. Zilberstein Learning generalized plans using abstract counting Proceedings of the Twenty-Third Conference on Artificial Intelligence 2008 AAAI Press 991 997 [13] S. Srivastava N. Immerman S. Zilberstein A new representation and associated algorithms for generalized planning Artif. Intell. 175 2 2011 615 647 [14] H.J. Levesque Planning with loops Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI) 2005 509 515 [15] E. Winner M.M. Veloso LoopDISTILL: learning domain-specific planners from example plans Workshop on AI Planning and Learning, ICAPS 2007 [16] U. Manber G. Myers Suffix arrays: a new method for on-line string searches SIAM J. Comput. 22 5 1993 935 948 [17] H.H. Zhuo D.H. Hu C. Hogg Q. Yang H. Munoz-Avila Learning HTN method preconditions and action models from partial observations Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI) 2009 1804 1810 [18] O. Ilghami D.S. Nau H. Munoz-Avila D.W. Aha Camel: learning method preconditions for HTN planning Proceedings of the Sixth International Conference on Artificial Intelligence Planning Systems (AIPS) 2002 131 142 [19] C. Hogg H. Munoz-Avila U. Kuter HTN-MAKER: learning HTNs with minimal additional knowledge engineering required Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence 2008 AAAI Press 950 956 [20] E. D. Sacerdoti Planning in a hierarchy of abstraction spaces Artif. Intell. 5 2 1974 115 135 [21] P. Haslum A. Botea M. Helmert B. Bonet S. Koenig Domain-independent construction of pattern database heuristics for cost-optimal planning Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence vol. 7 2007 AAAI Press 1007 1012 [22] C.A. Knoblock S. Minton O. Etzioni Integrating abstraction and explanation-based learning in PRODIGY Proceedings of the Ninth AAAI Conference on Artificial Intelligence 1991 AAAI Press 541 546 [23] R. Bergmann W. Wilke Building and refining abstract planning cases by change of representation language J. Artif. Intell. Res. 3 1995 53 118 [24] L. Seabra Lopes Failure recovery planning in assembly based on acquired experience: learning by analogy Assembly and Task Planning, 1999. (ISATP’99) Proceedings of the 1999 IEEE International Symposium on 1999 IEEE 294 300 [25] L. Seabra Lopes Failure recovery planning for robotized assembly based on learned semantic structures IFAC Workshop on Intelligent Assembly and Disassembly (IAD’2007) 2007 65 70 [26] M. Tenorth A.C. Perzylo R. Lafrenz M. Beetz Representation and exchange of knowledge about actions, objects, and environments in the RoboEarth framework IEEE Trans. Autom. Sci. Eng. 10 3 2013 643 651 [27] M. Tenorth M. Beetz Representations for robot knowledge in the KnowRob framework Artif. Intell. 2015 [28] J. Hertzberg J. Zhang L. Zhang S. Rockel B. Neumann J. Lehmann K. Dubba A. Cohn A. Saffiotti F. Pecora M. Mansouri Š. Konĕcný M. Günther S. Stock L. Seabra Lopes M. Oliveira G. Lim H. Kasaei V. Mokhtari L. Hotz W. Bohlken The RACE Project KI - Künstliche Intelligenz 28 4 2014 297 304 [29] N. Shafii S.H. Kasaei L. Seabra Lopes Learning to grasp familiar objects using object view recognition and template matching 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2016 2895 2900 [30] J. Rintanen Planning as satisfiability: heuristics Artif. Intell. 193 2012 45 86 [31] J. Hoffmann B. Nebel The ff planning system: fast plan generation through heuristic search J. Artif. Intell. Res. 14 2001 253 302 "
    },
    {
        "doc_title": "Learning and planning of robot tasks with loops",
        "doc_scopus_id": "85026865200",
        "doc_doi": "10.1109/ICARSC.2017.7964091",
        "doc_eid": "2-s2.0-85026865200",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Action sequences",
            "Planning domains",
            "Real environments",
            "Robot tasks",
            "Task knowledge"
        ],
        "doc_abstract": "© 2017 IEEE.We extend our previous work on experience-based planning domains in robotics to detect and represent loops in actions sequences that achieve certain tasks; and to generate action sequences from loop descriptions. The approach includes methods for conceptualizing robot experiences containing loops; and generating plans based on learned task knowledge with loops. This facilitates the reuse of existing task knowledge for different instances of the same task with varying numbers of objects. A demonstration of this system in a real environment illustrates how a robotic arm can learn and carry out a task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cryfa: A tool to compact and encrypt FASTA files",
        "doc_scopus_id": "85025168586",
        "doc_doi": "10.1007/978-3-319-60816-7_37",
        "doc_eid": "2-s2.0-85025168586",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Advanced Encryption Standard",
            "Compact representation",
            "Encryption methods",
            "Genomic data",
            "Large volumes",
            "Next-generation sequencing",
            "Patient data",
            "Secure protocols"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.NGS (next-generation sequencing) is bringing the need to efficiently handle large volumes of patient data, maintaining privacy laws, such as those with secure protocols that ensure patients DNA confidentiality. Although there are multiple file representations for genomic data, the FASTA format is perhaps the most used and popular. As far as we know, FASTA encryption is being addressed with general purpose encryption methods, without exploring a compact representation. In this paper, we propose Cryfa, a new fast encryption method to store securely FASTA files in a compact form. The main differences between a general encryption approach and Cryfa are the reduction of storage, up to approximately three times, without compromising security, and the possibility of integration with pipelines. The core of the encryption method uses a symmetric approach, the AES (Advanced Encryption Standard). Cryfa implementation is freely available, under license GPLv3, at https://github.com/pratas/cryfa.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Substitutional tolerant markov models for relative compression of DNA sequences",
        "doc_scopus_id": "85025129828",
        "doc_doi": "10.1007/978-3-319-60816-7_32",
        "doc_eid": "2-s2.0-85025129828",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Compression efficiency",
            "Computation time",
            "DNA data",
            "Fundamental operations",
            "Genomic sequence",
            "High-efficiency",
            "Markov model",
            "Model species"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Referential compression is one of the fundamental operations for storing and analyzing DNA data. The models that incorporate relative compression, a special case of referential compression, are being steadily improved, namely those which are based on Markov models. In this paper, we propose a new model, the substitutional tolerant Markov model (STMM), which can be used in cooperation with regular Markov models to improve compression efficiency. We assessed its impact on synthetic and real DNA sequences, showing a substantial improvement in compression, while only slightly increasing the computation time. In particular, it shows high efficiency in modeling species that have split less than 40 million years ago.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dissimilar symmetric word pairs in the human genome",
        "doc_scopus_id": "85025122728",
        "doc_doi": "10.1007/978-3-319-60816-7_30",
        "doc_eid": "2-s2.0-85025122728",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Dissimilarity measures",
            "Distance distributions",
            "Human genomes",
            "Inter-word distance",
            "Reversed complements",
            "Word-pairs"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.In this work we explore the dissimilarity between symmetric word pairs, by comparing the inter-word distance distribution of a word to that of its reversed complement. We propose a new measure of dissimilarity between such distributions. Since symmetric pairs with different patterns could point to evolutionary features, we search for the pairs with the most dissimilar behaviour. We focus our study on the complete human genome and its repeat-masked version.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the role of inverted repeats in DNA sequence similarity",
        "doc_scopus_id": "85025115709",
        "doc_doi": "10.1007/978-3-319-60816-7_28",
        "doc_eid": "2-s2.0-85025115709",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Chromosomal rearrangement",
            "Compression methods",
            "Computational approach",
            "Computational time and memory",
            "Context modeling",
            "Inverted repeat",
            "Sequence similarity",
            "Statistical characteristics"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.In this paper, we propose a computational approach to quantify inverted repeats. This is important, because it is known that the presence of inverted repeats in genomic data may be associated to certain chromosomal rearrangements. First, we present a reference-based relative compression method, which employs statistical characteristics of the genomic data. Then, for determining the similarity between genomic sequences, we use the normalized relative compression measure, which is light-weight regarding computational time and memory. Testing this approach on various species, including human, chimpanzee, gorilla, chicken, turkey and archaea genomes, we unveil unreported results that may support several evolution insights.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Impact of the acquisition time on ECG compression-based biometric identification systems",
        "doc_scopus_id": "85021253678",
        "doc_doi": "10.1007/978-3-319-58838-4_19",
        "doc_eid": "2-s2.0-85021253678",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biometric identifications",
            "Compression algorithms",
            "Context models",
            "Kolmogorov complexity",
            "Similarity metrics"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.The ECG signal conveys desirable characteristics for biometric identification (universality, uniqueness, measurability, acceptability and circumvention avoidance). However, based on the current literature review, there are no results that evaluate the number of heartbeats needed for personal identification. This information is undoubtedly useful when building a biometric identification system – any system should ask participants to provide data for identification, using the smallest time interval that is possible, for practical reasons. In this paper, we aim at exploring this topic using a measure of similarity based on the Kolmogorov Complexity, called the Normalized Relative Compression (NRC). To attain the goal, we built finite-context models to represent each individual – a compression-based approach that has been shown successful for several other pattern recognition applications like image similarity, DNA sequences or authorship attribution.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visualization of distinct DNA regions of the modern human relatively to a neanderthal genome",
        "doc_scopus_id": "85021224756",
        "doc_doi": "10.1007/978-3-319-58838-4_26",
        "doc_eid": "2-s2.0-85021224756",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ancient dnas",
            "Bloom filters",
            "Computational tools",
            "DNA patterns",
            "Genome sequences",
            "Paleogenomics",
            "Probabilistic methods",
            "Species specifics"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Species-specific DNA regions are segments that are unique or share high dissimilarity relatively to close species. Their discovery is important, because they allow the localization of evolutionary traits that are often related to novel functionalities and, sometimes, diseases. We have detected distinct DNA regions specific in the modern human, when compared to a Neanderthal high-quality genome sequence obtained from a bone of a Siberian woman. The bone is around 50,000 years old and the DNA raw data totalizes more than 418 GB. Since the data size required for localizing efficiently such events is very high, it is not practical to store the model on a table or hash table. Thus, we propose a probabilistic method to map and visualize those regions. The time complexity of the method is linear. The computational tool is available at http://pratas.github.io/chester. The results, computed in approximately two days using a single CPU core, show several regions with documented neanderthal absent regions, namely genes associated with the brain (neurotransmiters and synapses), hearing, blood, fertility and the immune system. However, it also shows several undocumented regions, that may express new functions linked with the evolution of the modern human.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the approximation of the Kolmogorov complexity for DNA sequences",
        "doc_scopus_id": "85021204963",
        "doc_doi": "10.1007/978-3-319-58838-4_29",
        "doc_eid": "2-s2.0-85021204963",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Archaea",
            "Complete genomes",
            "Finite alphabet",
            "Kolmogorov complexity",
            "Lossless",
            "Natural process",
            "Relative complexity"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.The Kolmogorov complexity furnishes several ways for studying different natural processes that can be expressed using sequences of symbols from a finite alphabet, such as the case of DNA sequences. Although the Kolmogorov complexity is not algorithmically computable, it can be approximated by lossless normal compressors. In this paper, we use a specific DNA compressor to approximate the Kolmogorov complexity and we assess it regarding its normality. Then, we use it on several datasets, that are constituted by different DNA sequences, representing complete genomes of different species and domains. We show several evolution-related insights associated with the complexity, namely that, globally, archaea have higher relative complexity than bacteria and eukaryotes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Authorship Attribution Using Relative Compression",
        "doc_scopus_id": "85010032070",
        "doc_doi": "10.1109/DCC.2016.53",
        "doc_eid": "2-s2.0-85010032070",
        "doc_date": "2016-12-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Authorship attribution",
            "Classification performance",
            "Data set",
            "Information distance",
            "Multiple discriminant analysis",
            "Normalized compression distance"
        ],
        "doc_abstract": "© 2016 IEEE.Authorship attribution is a classical classification problem. We use it here to illustrate the performance of a compression-based measure that relies on the notion of relative compression. Besides comparing with recent approaches that use multiple discriminant analysis and support vector machines, we compare it with the Normalized Conditional Compression Distance (a direct approximation of the Normalized Information Distance) and the popular Normalized Compression Distance. The Normalized Relative Compression (NRC) attained 100% correct classification in the data set used, showing consistency between the compression ratio and the classification performance, a characteristic not always present in other compression-based measures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Efficient Compression of Genomic Sequences",
        "doc_scopus_id": "85007344704",
        "doc_doi": "10.1109/DCC.2016.60",
        "doc_eid": "2-s2.0-85007344704",
        "doc_date": "2016-12-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Compression gain",
            "Context modeling",
            "Context models",
            "Genomic sequence",
            "Hardware specifications",
            "High order model",
            "Reference-free"
        ],
        "doc_abstract": "© 2016 IEEE.The number of genomic sequences is growing substantially. Besides discarding part of the data, the only efficient possibility for coping with this trend is data compression. We present an efficient compressor for genomic sequences, allowing both reference-free and referential compression. This compressor uses a mixture of context models of several orders, according to two model classes: reference and target. A new type of context model, which is capable of tolerating substitution errors, is introduced. For ensuring flexibility regarding hardware specifications, the compressor uses cache-hashes in high order models. The results show additional compression gains over several specific top tools in different levels of redundancy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experience-Based Planning Domains: an Integrated Learning and Deliberation Approach for Intelligent Robots: Robot Task Learning from Human Instructions",
        "doc_scopus_id": "84991220046",
        "doc_doi": "10.1007/s10846-016-0371-y",
        "doc_eid": "2-s2.0-84991220046",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Conceptualization",
            "Learning from human instructions",
            "Planning domains",
            "Robot tasks",
            "Task planning"
        ],
        "doc_abstract": "© 2016, Springer Science+Business Media Dordrecht.Deliberation and learning are required to endow a robot with the capabilities for acquiring knowledge, performing a variety of tasks and interactions, and adapting to open-ended environments. This paper presents the notion of experience-based planning domains (EBPDs) for task level learning and planning in robotics. EBPDs provide methods for a robot to: (i) obtain robot activity experiences from the robot’s performance in a dynamic environment; (ii) conceptualize each experience producing an activity schema; and (iii) exploit the learned activity schemata to make plans in similar situations. Experiences are episodic descriptions of plan-based robot activities including environment perception, sequences of applied actions and achieved tasks. The conceptualization approach integrates different techniques including deductive generalization, abstraction, goal inference and feature extraction. A high-level task planner was developed to find a solution for a task by following an activity schema. The proposed approach is illustrated and evaluated in a restaurant environment where a service robot learns how to carry out complex tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Progressive lossy-to-lossless compression of DNA microarray images",
        "doc_scopus_id": "84964922592",
        "doc_doi": "10.1109/LSP.2016.2547893",
        "doc_eid": "2-s2.0-84964922592",
        "doc_date": "2016-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Analysis techniques",
            "DNA microarray images",
            "Lossy to lossless compression",
            "Lossy-to-lossless",
            "Quantization",
            "Rate distortion performance",
            "Region of interest",
            "Regular structure"
        ],
        "doc_abstract": "© 1994-2012 IEEE.The analysis techniques applied to DNA microarray images are under active development. As new techniques become available, it will be useful to apply them to existing microarray images to obtain more accurate results. The compression of these images can be a useful tool to alleviate the costs associated to their storage and transmission. The recently proposed Relative Quantizer (RQ) coder provides the most competitive lossy compression ratios while introducing only acceptable changes in the images. However, images compressed with the RQ coder can only be reconstructed with a limited quality, determined before compression. In this work, a progressive lossy-to-lossless scheme is presented to solve this problem. First, the regular structure of the RQ intervals is exploited to define a lossy-to-lossless coding algorithm called the Progressive RQ (PRQ) coder. Second, an enhanced version that prioritizes a region of interest, called the PRQ-region of interest (ROI) coder, is described. Experiments indicate that the PRQ coder offers progressivity with lossless and lossy coding performance almost identical to the best techniques in the literature, none of which is progressive. In turn, the PRQ-ROI exhibits very similar lossless coding results with better rate-distortion performance than both the RQ and PRQ coders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analysis-driven lossy compression of DNA microarray images",
        "doc_scopus_id": "84959449231",
        "doc_doi": "10.1109/TMI.2015.2489262",
        "doc_eid": "2-s2.0-84959449231",
        "doc_date": "2016-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Analysis techniques",
            "Compression performance",
            "DNA microarray analysis",
            "DNA microarray images",
            "Lossy compressions",
            "Maximum relative errors",
            "Non-uniform quantization",
            "Quantization",
            "Data Compression",
            "DNA",
            "Equipment Design",
            "Image Processing, Computer-Assisted",
            "Oligonucleotide Array Sequence Analysis"
        ],
        "doc_abstract": "© 2015 IEEE.DNA microarrays are one of the fastest-growing new technologies in the field of genetic research, and DNA microarray images continue to grow in number and size. Since analysis techniques are under active and ongoing development, storage, transmission and sharing of DNA microarray images need be addressed, with compression playing a significant role. However, existing loss-less coding algorithms yield only limited compression performance (compression ratios below 2:1), whereas lossy coding methods may introduce unacceptable distortions in the analysis process. This work introduces a novel Relative Quantizer (RQ), which employs non-uniform quantization intervals designed for improved compression while bounding the impact on the DNA microarray analysis. This quantizer constrains the maximum relative error introduced into quantized imagery, devoting higher precision to pixels critical to the analysis process. For suitable parameter choices, the resulting variations in the DNA microarray analysis are less than half of those inherent to the experimental variability. Experimental results reveal that appropriate analysis can still be performed for average compression ratios exceeding 4.5:1.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A survey on data compression methods for biological sequences",
        "doc_scopus_id": "85007393441",
        "doc_doi": "10.3390/info7040056",
        "doc_eid": "2-s2.0-85007393441",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "FASTA",
            "FASTQ",
            "Multi-FASTA",
            "Protein sequences",
            "Reference-free"
        ],
        "doc_abstract": "© 2016 by the authors.The ever increasing growth of the production of high-throughput sequencing data poses a serious challenge to the storage, processing and transmission of these data. As frequently stated, it is a data deluge. Compression is essential to address this challenge-it reduces storage space and processing costs, along with speeding up data transmission. In this paper, we provide a comprehensive survey of existing compression approaches, that are specialized for biological data, including protein and DNA sequences. Also, we devote an important part of the paper to the approaches proposed for the compression of different file formats, such as FASTA, as well as FASTQ and SAM/BAM, which contain quality scores and metadata, in addition to the biological sequences. Then, we present a comparison of the performance of several methods, in terms of compression ratio, memory usage and compression/decompression time. Finally, we present some suggestions for future research on biological data compression.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experience-based robot task Learning and planning with goal inference",
        "doc_scopus_id": "84989889577",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84989889577",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Complex task",
            "Environment perceptions",
            "Planning domains",
            "Robot tasks",
            "Service robots",
            "Task levels",
            "Task modeling",
            "Task planner"
        ],
        "doc_abstract": "Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.Learning and deliberation are required to endow a robot with the capabilities to acquire knowledge, perform a variety of tasks and interactions, and adapt to open-ended environments. This paper explores the notion of experiencebased planning domains (EBPDs) for task-level learning and planning in robotics. EBPDs rely on methods for a robot to: (i) obtain robot activity experiences from the robot's performance; (ii) conceptualize each experience to a task model called activity schema; and (iii) exploit the learned activity schemata to make plans in similar situations. Experiences are episodic descriptions of plan-based robot activities including environment perception, sequences of applied actions and achieved tasks. The conceptualization approach integrates different techniques including deductive generalization, abstraction and feature extraction to learn activity schemata. A high-level task planner was developed to find a solution for a similar task by following an activity schema. In this paper, we extend our previous approach by integrating goal inference capabilities. The proposed approach is illustrated in a restaurant environment where a service robot learns how to carry out complex tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gathering and conceptualizing plan-based robot activity experiences",
        "doc_scopus_id": "84945936412",
        "doc_doi": "10.1007/978-3-319-08338-4_72",
        "doc_eid": "2-s2.0-84945936412",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Abstraction and generalization",
            "Conceptualization",
            "Ego networks",
            "Experience gathering",
            "Plan schema",
            "Plan-based"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.Learning from experiences is an effective approach to enhance robot’s competence. This paper focuses on developing capabilities for a robot to obtain robot activity experiences and conceptualize the experiences as plan schemata, which are used as heuristics for the robot to make plans in similar situations. The plan-based robot activity experiences are obtained through human-robot interactions where a teaching action from a command-line user interface triggers recording of an experience. To represent human-robot interaction activities, ontologies for experiences and user instructions are integrated into a robot ontology. Recorded experiences are episodic descriptions of the robot’s activities including relevant perceptions of the environment, the goals pursued, successes, and failures. Since the amount of experience data is large, a graph simplification algorithm based on ego networks is investigated to filter out irrelevant information in an experience. Finally, an approach to robot activity conceptualization based on deductive generalization and abstraction is presented. The proposed systemwas demonstrated in a scenario where a PR2 robot is taught how to “serve a coffee” to a guest, in the EU project RACE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ECG biometric identification: A compression based approach",
        "doc_scopus_id": "84953229210",
        "doc_doi": "10.1109/EMBC.2015.7319719",
        "doc_eid": "2-s2.0-84953229210",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Biometric Identification",
            "Data Compression",
            "Electrocardiography",
            "Signal Processing, Computer-Assisted"
        ],
        "doc_abstract": "© 2015 IEEE.Using the electrocardiogram signal (ECG) to identify and/or authenticate persons are problems still lacking satisfactory solutions. Yet, ECG possesses characteristics that are unique or difficult to get from other signals used in biometrics: (1) it requires contact and liveliness for acquisition (2) it changes under stress, rendering it potentially useless if acquired under threatening. Our main objective is to present an innovative and robust solution to the above-mentioned problem. To successfully conduct this goal, we rely on information-theoretic data models for data compression and on similarity metrics related to the approximation of the Kolmogorov complexity. The proposed measure allows the comparison of two (or more) ECG segments, without having to follow traditional approaches that require heartbeat segmentation (described as highly influenced by external or internal interferences). As a first approach, the method was able to cluster the data in three groups: identical record, same participant, different participant, by the stratification of the proposed measure with values near 0 for the same participant and closer to 1 for different participants. A leave-one-out strategy was implemented in order to identify the participant in the database based on his/her ECG. A 1- NN classifier was implemented, using as distance measure the method proposed in this work. The classifier was able to identify correctly almost all participants, with an accuracy of 99% in the database used.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Three minimal sequences found in Ebola virus genomes and absent from human DNA",
        "doc_scopus_id": "84943639957",
        "doc_doi": "10.1093/bioinformatics/btv189",
        "doc_eid": "2-s2.0-84943639957",
        "doc_date": "2015-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Disease Outbreaks",
            "DNA, Viral",
            "Ebolavirus",
            "Genome, Human",
            "Genome, Viral",
            "Hemorrhagic Fever, Ebola",
            "Humans",
            "Sequence Analysis, DNA",
            "Viral Proteins"
        ],
        "doc_abstract": "© The Author 2014. Published by Oxford University Press.Motivation: Ebola virus causes high mortality hemorrhagic fevers, with more than 25 000 cases and 10000 deaths in the current outbreak. Only experimental therapies are available, thus, novel diagnosis tools and druggable targets are needed. Results: Analysis of Ebola virus genomes from the current outbreak reveals the presence of short DNA sequences that appear nowhere in the human genome. We identify the shortest such sequences with lengths between 12 and 14. Only three absent sequences of length 12 exist and they consistently appear at the same location on two of the Ebola virus proteins, in all Ebola virus genomes, but nowhere in the human genome. The alignment-free method used is able to identify pathogen-specific signatures for quick and precise action against infectious agents, of which the current Ebola virus outbreak provides a compelling example.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An alignment-free method to find and visualise rearrangements between pairs of DNA sequences",
        "doc_scopus_id": "84929429321",
        "doc_doi": "10.1038/srep10203",
        "doc_eid": "2-s2.0-84929429321",
        "doc_date": "2015-05-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Animals",
            "Computational Biology",
            "Gene Rearrangement",
            "Genomics",
            "Humans",
            "Web Browser"
        ],
        "doc_abstract": "© 2015, Nature Publishing Group. All rights reserved.Species evolution is indirectly registered in their genomic structure. The emergence and advances in sequencing technology provided a way to access genome information, namely to identify and study evolutionary macro-events, as well as chromosome alterations for clinical purposes. This paper describes a completely alignment-free computational method, based on a blind unsupervised approach, to detect large-scale and small-scale genomic rearrangements between pairs of DNA sequences. To illustrate the power and usefulness of the method we give complete chromosomal information maps for the pairs human-chimpanzee and human-orangutan. The tool by means of which these results were obtained has been made publicly available and is described in detail.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Planning with activity schemata: Closing the loop in experience-based planning",
        "doc_scopus_id": "84933055924",
        "doc_doi": "10.1109/ICARSC.2015.35",
        "doc_eid": "2-s2.0-84933055924",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Abstract task knowledge",
            "Cognitive capability",
            "Conceptualization",
            "Effective approaches",
            "Environment perceptions",
            "Learning tasks",
            "Planning domains",
            "Task planning"
        ],
        "doc_abstract": "© 2015 IEEE.Learning task knowledge from robot activity experiences has been recognized as an effective approach to improve robot task planning performance. Cognitive capabilities are required to enable a robot to learn new activities from its human partners as well as to refine and improve already learned skills. This paper presents an approach for a robot to conceptualize plan-based robot activity experiences as activity schemata - enriched abstract task knowledge - as well as to exploit them to make plans in similar situations. The experiences are episodic descriptions of plan-based robot activities including environment perceptions, sequences of applied actions and achieved tasks. In this work, the robot activity experiences are obtained through human-robot interaction. The adopted conceptualization approach constructs an activity schema through deductive generalization, abstraction and feature extraction. A high-level task planner was developed to find a solution for a similar task by following an activity schema. The paper proposes a formalization for experience-based planning domains. The proposed learning and planning approach is illustrated in a restaurant environment where a service robot learns how to carry out complex tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mafco: A compression tool for MAF files",
        "doc_scopus_id": "84929484087",
        "doc_doi": "10.1371/journal.pone.0116082",
        "doc_eid": "2-s2.0-84929484087",
        "doc_date": "2015-03-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Data Compression",
            "Genomics",
            "Sequence Alignment",
            "Time Factors"
        ],
        "doc_abstract": "© 2015 Matos et al.In the last decade, the cost of genomic sequencing has been decreasing so much that researchers all over the world accumulate huge amounts of data for present and future use. These genomic data need to be efficiently stored, because storage cost is not decreasing as fast as the cost of sequencing. In order to overcome this problem, the most popular generalpurpose compression tool, gzip, is usually used. However, these tools were not specifically designed to compress this kind of data, and often fall short when the intention is to reduce the data size as much as possible. There are several compression algorithms available, even for genomic data, but very few have been designed to deal with Whole Genome Alignments, containing alignments between entire genomes of several species. In this paper, we present a lossless compression tool, MAFCO, specifically designed to compress MAF (Multiple Alignment Format) files. Compared to gzip, the proposed tool attains a compression gain from 34% to 57%, depending on the data set. When compared to a recent dedicated method, which is not compatible with some data sets, the compression gain of MAFCO is about 9%. Both source-code and binaries for several operating systems are freely available for non-commercial use at: http://bioinformatics.ua.pt/software/mafco.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression of microarray images using a binary tree decomposition",
        "doc_scopus_id": "84911937859",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911937859",
        "doc_date": "2014-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Binary tree decomposition",
            "Compression standards",
            "Context modeling",
            "Hierarchical organizations",
            "Intensity levels",
            "JPEG-LS",
            "Lossless compression",
            "Microarray images"
        ],
        "doc_abstract": "© 2014 EURASIP.This paper proposes a lossless compression method for microarray images, based on a hierarchical organization of the intensity levels followed by finite-context modeling. A similar approach was recently applied to medical images with success. The goal of this work was to further extend, adapt and evaluate this approach to the special case of microarray images. We performed simulations on seven different data sets (total of 254 images). On average, the proposed method attained 9% better results when compared to the best compression standard (JPEG-LS).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring deep Markov models in genomic data compression using sequence pre-analysis",
        "doc_scopus_id": "84911892507",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911892507",
        "doc_date": "2014-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression algorithms",
            "Context models",
            "Genomic data compressions",
            "Genomic sequence",
            "Hash table",
            "High compressions",
            "Markov model",
            "Textual data"
        ],
        "doc_abstract": "© 2014 EURASIP.The pressure to find efficient genomic compression algorithms is being felt worldwide, as proved by several prizes and competitions. In this paper, we propose a compression algorithm that relies on a pre-analysis of the data before compression, with the aim of identifying regions of low complexity. This strategy enables us to use deeper context models, supported by hash-tables, without requiring huge amounts of memory. As an example, context depths as large as 32 are attainable for alphabets of four symbols, as is the case of genomic sequences. These deeper context models show very high compression capabilities in very repetitive genomic sequences, yielding improvements over previous algorithms. Furthermore, this method is universal, in the sense that it can be used in any type of textual data (such as quality-scores).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On entropy-based data mining",
        "doc_scopus_id": "84927633085",
        "doc_doi": "10.1007/978-3-662-43968-5_12",
        "doc_eid": "2-s2.0-84927633085",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Approximate entropy",
            "Biomedical informatics",
            "FiniteTopEn",
            "Fuzzy entropy",
            "Sample entropy",
            "Topological entropy"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 2014.In the real world, we are confronted not only with complex and high-dimensional data sets, but usually with noisy, incomplete and uncertain data, where the application of traditional methods of knowledge discovery and data mining always entail the danger of modeling artifacts. Originally, information entropy was introduced by Shannon (1949), as a measure of uncertainty in the data. But up to the present, there have emerged many different types of entropy methods with a large number of different purposes and possible application areas. In this paper, we briefly discuss the applicability of entropy methods for the use in knowledge discovery and data mining, with particular emphasis on biomedical data. We present a very short overview of the state-of-theart, with focus on four methods: Approximate Entropy (ApEn), Sample Entropy (SampEn), Fuzzy Entropy (FuzzyEn), and Topological Entropy (FiniteTopEn). Finally, we discuss some open problems and future research challenges.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new compressor for measuring distances among images",
        "doc_scopus_id": "84921770705",
        "doc_doi": "10.1007/978-3-319-11758-4_4",
        "doc_eid": "2-s2.0-84921770705",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context models",
            "Digital Objects",
            "Feature selection and extractions",
            "Image similarity",
            "Kolmogorov complexity",
            "Measure of similarities",
            "Measuring distances"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Ideally, we would like to have a measure of similarity between images that did not require a feature selection and extraction step. In theory, this can be attained using Kolmogorov complexity concepts. In practice, because the Kolmogorov complexity of a digital object cannot be computed, one has to rely on appropriate approximations, the most successful being based on data compression. The application of these ideas to images has been more difficult than to some other areas. In this paper, we suggest a new distance and compare it with two others, showing some of their relative advantages and disadvantages, hoping to contribute to the advance of this promising line of research.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A method to detect repeated unknown patterns in an image",
        "doc_scopus_id": "84908703833",
        "doc_doi": "10.1007/978-3-319-11758-4_2",
        "doc_eid": "2-s2.0-84908703833",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context modeling",
            "Context models",
            "Extracting features",
            "Kolmogorov complexity",
            "SIFT",
            "Tampering detection",
            "Two-step procedure",
            "Visual inspection"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Consider a natural image that has been manipulated by copying, transforming and pasting back fragments of the image itself. Our goal is to detect such manipulations in the absence of any knowledge about the content of the repeated fragments or the transformations to which they might have been subject. The problem is non-trivial even in the absence of any transformations. For example, copy/paste of a textured fragment of a background can be difficult to detect even by visual inspection. Our approach to the problem is a two-step procedure. The first step consists in extracting features from the image. The second step explores the connection between image compression and complexity: a finite-context model is used to build a complexity map of the image features. Patterns that reappear, even in a somewhat modified form, are encoded with fewer bits, a fact that renders the detection of the repeated regions possible.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression-based normal similarity measures for DNA sequences",
        "doc_scopus_id": "84905270632",
        "doc_doi": "10.1109/ICASSP.2014.6853630",
        "doc_eid": "2-s2.0-84905270632",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression algorithms",
            "Compression methods",
            "Context models",
            "interleaving",
            "LRU cache",
            "Normalized compression distance",
            "Signal features",
            "Similarity measure"
        ],
        "doc_abstract": "Similarity measures based on compression assess the distance between two objects based on the number of bits needed to describe one, given a description of the other. Theoretically, compression-based similarity depends on the concept of Kol-mogorov complexity, which is non-computable. The implementations require compression algorithms that are approximately normal. The approach has important advantages (no signal features to identify and extract, for example) but the compression method must be normal. This paper proposes normal algorithms based on mixtures of finite context models. Normality is attained by combining two new ideas: the use of least-recently-used caching in the context models, to allow deeper contexts, and data interleaving, to better explore that cache. Examples for DNA sequences are given (at the human genome scale). © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The natural scale of signals: Pulse duration and superoscillations",
        "doc_scopus_id": "84905248151",
        "doc_doi": "10.1109/ICASSP.2014.6854388",
        "doc_eid": "2-s2.0-84905248151",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "High frequency HF",
            "Linear combinations",
            "Maximum frequency",
            "Pulse durations",
            "Pulsewidths",
            "scale",
            "Signal spectrum",
            "Super-oscillations"
        ],
        "doc_abstract": "Superoscillations are oscillations at frequencies above the maximum frequency in the signal spectrum. Signals of very small bandwidth can indeed oscillate at arbitrarily high frequencies, over arbitrarily long intervals. This work addresses the matter from a different angle, emphasizing scale and discussing the following question: can an arbitrarily narrow pulse be constructed by linearly combining arbitrarily wider pulses? The connection with superoscillations and approximation theory is also discussed. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information profiles for DNA pattern discovery",
        "doc_scopus_id": "84903478465",
        "doc_doi": "10.1109/DCC.2014.54",
        "doc_eid": "2-s2.0-84903478465",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "DNA patterns",
            "Fission yeast",
            "Information contents",
            "Schizosaccharomyces pombe"
        ],
        "doc_abstract": "Finite-context modeling is a powerful tool for compressing and hence for representing DNA sequences. We describe an algorithm to detect genomic regularities, within a blind discovery strategy. The algorithm uses information profiles built using suitable combinations of finite-context models. We used the genome of the fission yeast Schizosaccharomyces pombe strain 972 h-for illustration, unveiling locations of low information content, which are usually associated with DNA regions of potential biological interest. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A conditional compression distance that unveils insights of the genomic evolution",
        "doc_scopus_id": "84903434093",
        "doc_doi": "10.1109/DCC.2014.58",
        "doc_eid": "2-s2.0-84903434093",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Genomic sequence",
            "Information contents",
            "Normalized compression distance"
        ],
        "doc_abstract": "We describe a compression-based distance for genomic sequences. Instead of using the usual conjoint information content, as in the classical Normalized Compression Distance (NCD), it uses the conditional information content. To compute this Normalized Conditional Compression Distance (NCCD), we need a normal conditional compressor, that we built using a mixture of static and dynamic finite-context models. Using this approach, we measured chromosomal distances between Hominidae primates and also between Muroidea (rat and mouse), observing several insights of evolution that so far have not been reported in the literature. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "XS: A FASTQ read simulator",
        "doc_scopus_id": "84892407950",
        "doc_doi": "10.1186/1756-0500-7-40",
        "doc_eid": "2-s2.0-84892407950",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 Pratas et al.Background: The emerging next-generation sequencing (NGS) is bringing, besides the natural huge amounts of data, an avalanche of new specialized tools (for analysis, compression, alignment, among others) and large public and private network infrastructures. Therefore, a direct necessity of specific simulation tools for testing and benchmarking is rising, such as a flexible and portable FASTQ read simulator, without the need of a reference sequence, yet correctly prepared for producing approximately the same characteristics as real data. Findings: We present XS, a skilled FASTQ read simulation tool, flexible, portable (does not need a reference sequence) and tunable in terms of sequence complexity. It has several running modes, depending on the time and memory available, and is aimed at testing computing infrastructures, namely cloud computing of large-scale projects, and testing FASTQ compression algorithms. Moreover, XS offers the possibility of simulating the three main FASTQ components individually (headers, DNA sequences and quality-scores). Conclusions: XS provides an efficient and convenient method for fast simulation of FASTQ files, such as those from Ion Torrent (currently uncovered by other simulators), Roche-454, Illumina and ABI-SOLiD sequencing machines. This tool is publicly available at http://bioinformatics.ua.pt/software/xs/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mfcompress: A compression tool for fasta and multi-fasta data",
        "doc_scopus_id": "84891355058",
        "doc_doi": "10.1093/bioinformatics/btt594",
        "doc_eid": "2-s2.0-84891355058",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Data Compression",
            "Genome",
            "Genomics",
            "Humans",
            "Markov Chains",
            "Software"
        ],
        "doc_abstract": "Motivation: The data deluge phenomenon is becoming a serious problem in most genomic centers. To alleviate it, general purpose tools, such as gzip, are used to compress the data. However, although pervasive and easy to use, these tools fall short when the intention is to reduce as much as possible the data, for example, for medium-and long-term storage. A number of algorithms have been proposed for the compression of genomics data, but unfortunately only a few of them have been made available as usable and reliable compression tools.Results: In this article, we describe one such tool, MFCompress, specially designed for the compression of FASTA and multi-FASTA files. In comparison to gzip and applied to multi-FASTA files, MFCompress can provide additional average compression gains of almost 50%, i.e. it potentially doubles the available storage, although at the cost of some more computation time. On highly redundant datasets, and in comparison with gzip, 8-fold size reductions have been obtained.Availability: Both source code and binaries for several operating systems are freely available for non-commercial use at http://bioinformatics.ua. pt/software/mfcompress/.Contact: Supplementary information: Supplementary data are available at Bioinformatics online. © 2013 The Author .",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A genomic distance for assembly comparison based on compressed maximal exact matches",
        "doc_scopus_id": "84887940267",
        "doc_doi": "10.1109/TCBB.2013.77",
        "doc_eid": "2-s2.0-84887940267",
        "doc_date": "2013-11-25",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Alignment-free",
            "Assembly quality",
            "Genome assembly",
            "Genome sequencing",
            "Genomic distance",
            "Human genomes",
            "Maximal exact matches",
            "Normalized compression distance",
            "Algorithms",
            "Genome, Human",
            "Genomics",
            "Humans",
            "Sequence Analysis, DNA"
        ],
        "doc_abstract": "Genome assemblies are typically compared with respect to their contiguity, coverage, and accuracy. We propose a genome-wide, alignment-free genomic distance based on compressed maximal exact matches and suggest adding it to the benchmark of commonly used assembly quality metrics. Maximal exact matches are perfect repeats, without gaps or misspellings, which cannot be further extended to either their left- or right-end side without loss of similarity. The genomic distance here proposed is based on the normalized compression distance, an information-theoretic measure of the relative compressibility of two sequences estimated using multiple finite-context models. This measure exposes similarities between the sequences, as well as, the nesting structure underlying the assembly of larger maximal exact matches from smaller ones. We use four human genome assemblies for illustration and discuss the impact of genome sequencing and assembly in the final content of maximal exact matches and the genomic distance here proposed. © 2004-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA sequences at a glance",
        "doc_scopus_id": "84896690677",
        "doc_doi": "10.1371/journal.pone.0079922",
        "doc_eid": "2-s2.0-84896690677",
        "doc_date": "2013-11-21",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Data summarization and triage is one of the current top challenges in visual analytics. The goal is to let users visually inspect large data sets and examine or request data with particular characteristics. The need for summarization and visual analytics is also felt when dealing with digital representations of DNA sequences. Genomic data sets are growing rapidly, making their analysis increasingly more difficult, and raising the need for new, scalable tools. For example, being able to look at very large DNA sequences while immediately identifying potentially interesting regions would provide the biologist with a flexible exploratory and analytical tool. In this paper we present a new concept, the \"information profile\", which provides a quantitative measure of the local complexity of a DNA sequence, independently of the direction of processing. The computation of the information profiles is computationally tractable: we show that it can be done in time proportional to the length of the sequence. We also describe a tool to compute the information profiles of a given DNA sequence, and use the genome of the fission yeast Schizosaccharomyces pombe strain 972 h- and five human chromosomes 22 for illustration. We show that information profiles are useful for detecting large-scale genomic regularities by visual inspection. Several discovery strategies are possible, including the standalone analysis of single sequences, the comparative analysis of sequences from individuals from the same species, and the comparative analysis of sequences from different organisms. The comparison scale can be varied, allowing the users to zoom-in on specific details, or obtain a broad overview of a long segment. Software applications have been made available for non-commercial use at http://bioinformatics.ua.pt/ software/dna-at-glance. © 2013 Pinho et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The breakdown of the word symmetry in the human genome",
        "doc_scopus_id": "84880640252",
        "doc_doi": "10.1016/j.jtbi.2013.06.032",
        "doc_eid": "2-s2.0-84880640252",
        "doc_date": "2013-10-21",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Equivalence testing",
            "Oligonucleotide composition",
            "Single strand symmetry",
            "Word symmetry distance"
        ],
        "doc_abstract": "Previous studies have suggested that Chargaff's second rule may hold for relatively long words (above 10. nucleotides), but this has not been conclusively shown. In particular, the following questions remain open: Is the phenomenon of symmetry statistically significant? If so, what is the word length above which significance is lost? Can deviations in symmetry due to the finite size of the data be identified? This work addresses these questions by studying word symmetries in the human genome, chromosomes and transcriptome. To rule out finite-length effects, the results are compared with those obtained from random control sequences built to satisfy Chargaff's second parity rule. We use several techniques to evaluate the phenomenon of symmetry, including Pearson's correlation coefficient, total variational distance, a novel word symmetry distance, as well as traditional and equivalence statistical tests. We conclude that word symmetries are statistical significant in the human genome for word lengths up to 6. nucleotides. For longer words, we present evidence that the phenomenon may not be as prevalent as previously thought. © 2013 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 272314 291210 291838 31 Journal of Theoretical Biology JOURNALTHEORETICALBIOLOGY 2013-07-02 2013-07-02 2014-08-19T10:40:25 S0022-5193(13)00304-4 S0022519313003044 10.1016/j.jtbi.2013.06.032 S300 S300.3 FULL-TEXT 2015-05-14T01:44:09.644048-04:00 0 0 20131021 2013 2013-07-02T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids alllist content subj ssids 0022-5193 00225193 true 335 335 C Volume 335 16 153 159 153 159 20131021 21 October 2013 2013-10-21 2013 article fla Copyright © 2013 Elsevier Ltd. All rights reserved. BREAKDOWNWORDSYMMETRYINHUMANGENOME AFREIXO V 1 Introduction 2 Materials and methods 2.1 Materials 2.2 Methods 2.2.1 Equivalence tests 2.2.2 Word symmetry distance 3 Results and discussion 3.1 Symmetry hypothesis testing 3.2 Pearson's correlation coefficient, Kullback–Leibler divergence (KL) and total variational distance 3.3 Word symmetry distance 4 Conclusion Funding References ALBRECHTBUEHLER 2006 17828 17833 G ALBRECHTBUEHLER 2007 297 305 G ALDOUS 1999 199 213 D BAISNEE 2002 1021 1033 P BIRD 1980 1499 1504 A BUSH 2006 1343 1348 E CHARGAFF 1950 201 209 E DEMBO 1992 329 357 A JOSSE 1961 864 875 J KARKAS 1968 915 920 J KATZ 1978 469 474 D KLINE 2004 R KONG 2009 e7553 S MITCHELL 2006 90 94 D MOORE 1997 D STATISTICS NIKOLAOU 2005 23 35 C PRABHU 1993 2797 2800 V QI 2001 557 559 D RUDNER 1968 630 635 R RUDNER 1968 921 922 R THANASSOULIS 2010 2323 2334 G WATSON 1953 737 738 J ZHANG 2010 478 485 S AFREIXOX2013X153 AFREIXOX2013X153X159 AFREIXOX2013X153XV AFREIXOX2013X153X159XV item S0022-5193(13)00304-4 S0022519313003044 10.1016/j.jtbi.2013.06.032 272314 2014-08-20T00:34:32.874288-04:00 2013-10-21 true 800903 MAIN 7 55000 849 656 IMAGE-WEB-PDF 1 si0085 254 11 43 si0084 235 11 39 si0083 195 10 31 si0082 219 10 51 si0081 209 9 46 si0080 201 8 44 si0079 209 9 46 si0078 201 8 44 si0077 195 10 30 si0076 195 10 30 si0075 196 11 23 si0074 241 10 51 si0073 179 10 30 si0072 488 16 126 si0071 165 12 30 si0070 196 8 29 si0069 174 10 30 si0068 209 9 46 si0067 201 8 44 si0066 195 10 31 si0065 219 10 51 si0064 231 10 38 si0063 201 11 24 si0062 187 11 24 si0061 175 10 31 si0060 130 6 15 si0059 172 13 28 si0058 177 10 30 si0057 177 10 30 si0056 187 8 28 si0055 203 10 40 si0054 174 10 30 si0053 181 10 40 si0052 185 10 30 si0051 181 10 40 si0050 203 10 40 si0049 181 10 40 si0048 203 10 40 si0047 124 10 9 si0046 145 15 15 si0045 308 12 51 si0044 789 28 149 si0043 744 16 161 si0042 508 13 109 si0041 299 13 52 si0040 237 13 31 si0039 228 13 31 si0038 1064 12 367 si0037 236 13 31 si0036 1070 12 364 si0035 225 13 31 si0034 237 13 31 si0033 228 13 31 si0032 135 12 13 si0031 135 12 13 si0030 266 14 41 si0029 1290 32 322 si0028 118 7 10 si0027 207 14 33 si0026 253 14 53 si0025 233 13 39 si0024 1085 52 169 si0023 207 14 33 si0022 139 8 14 si0021 129 8 13 si0020 209 9 46 si0019 201 8 44 si0018 212 11 33 si0017 196 8 29 si0016 299 13 52 si0015 212 11 33 si0014 196 8 29 si0013 299 13 52 si0012 212 11 33 si0011 196 8 29 si0010 299 13 52 si0009 203 10 40 si0008 187 8 28 si0007 187 8 28 si0006 187 8 28 si0005 203 10 40 si0004 181 10 40 si0003 124 10 9 si0002 145 15 15 si0001 150 12 17 gr3 34606 688 386 gr2 19543 276 367 gr1 19041 199 377 gr3 2278 164 92 gr2 3772 164 218 gr1 2618 116 219 YJTBI 7368 S0022-5193(13)00304-4 10.1016/j.jtbi.2013.06.032 Elsevier Ltd © 2013 Elsevier Ltd. All rights reserved. Fig. 1 Percentage of symmetric pairs at a distance of at most d words in the sorted word arrangement. For convenience, only the first 50 distances are displayed. Fig. 2 Word symmetry distance in random and human chromosome and transcriptome. Note that we present the average results of different chromosomes. Fig. 3 Word symmetry distance vs chromosome size. Note that | C | represents the absolute Pearson's correlation coefficient. Table 1 Results (p-values) of goodness of fit χ 2 test for whole assembled human genome (H a ), whole assembled random control sequences (RC a ), human chromosomes (H c ), random control chromosomes (RC c ) and transcriptome of human genome (RNA). k 1 2 3 4 5 6 7 8 9 10 H a 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 RC a 0.82 0.91 0.75 0.37 0.42 0.74 0.95 0.99 0.80 0.29 H c 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 – RC c 0.28 0.50 0.53 0.59 0.55 0.67 0.67 0.78 0.71 0.72 RNA 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 – Note: symbol “–” means that at least one word with length k does not occur in at least one of the considered sequences; the values relative to chromosome sequences (random and human genome) are average p-values. Table 2 Percentage of equivalent symmetric pairs for two tolerance values ( δ ) in the human and random genome for k-mers of length up to 10nt. The chromosome results are the average of percentage of equivalent symmetric pairs. k 1 2 3 4 5 6 7 8 9 10 δ = 1.1 (%) H a 100 100 100 100 100 100 99.9 96.2 74.5 47.3 RC a 100 100 100 100 100 100 100 100 99.5 69.7 H c 100 100 100 100 99.2 93.8 69.8 36.6 5.7 0.3 RC c 100 100 100 100 100 99.8 92.4 43.6 3.6 0.0 RNA 100 75.0 56.3 47.7 36.7 28.5 18.6 7.8 0.7 0.0 δ = 1.3 (%) H a 100 100 100 100 100 100 100 99.9 97.6 84.5 RC a 100 100 100 100 100 100 100 100 100 100 H c 100 100 100 100 100 99.7 96.5 79.7 48.8 16.1 RC c 100 100 100 100 100 100 100 97.7 70.7 15.2 RNA 100 100 100 87.5 81.6 72.2 62.1 47.7 25.9 6.1 Table 3 Values and maximal ratio ( max r ), 95th percentile ratio (P 95r ) and 75th percentile ratio (P 75r ) between symmetric pairs in the human and random genome for k-mers of length up to 10nt. k 1 2 3 4 5 6 7 8 9 10 H a max r 1.001 1.003 1.004 1.006 1.011 1.034 1.114 1.400 2.505 17 P 95r 1.001 1.003 1.004 1.004 1.004 1.007 1.012 1.025 1.055 1.121 P 75r 1.001 1.001 1.001 1.001 1.002 1.003 1.004 1.007 1.013 1.027 RC a max r 1.000 1.000 1.001 1.001 1.003 1.010 1.020 1.052 1.113 1.276 P 95r 1.000 1.000 1.000 1.001 1.001 1.003 1.006 1.013 1.026 1.053 P 75r 1.000 1.000 1.000 1.000 1.001 1.001 1.002 1.005 1.010 1.020 Table 4 Percentage of non-equivalent pairs ( δ = 1.3 ) in the complete human genome (H a ) for k-mers of length 8nt to 10nt containing a certain oligonucleotide. Only the four highest percentages are displayed. k n ne Nucleotide (%) Dinucleotide (%) Trinucleotide (%) Tetranucleotide (%) 8 34 C/G 100 CG 100 CGA/TCG 56 CGCG 35 A/T 100 TA 65 ACG/CGT 47 TCGA 24 – – AC/GT 56 CGC/GCG 41 ACGT 18 – – GA/TC 56 GTA/TAC 26 CGAC/GTCG 18 9 6266 C/G 100 CG 100 CGC/GCG 47 CGCG 30 A/T 92 GC 65 CGA/TCG 46 CGAC/GTCG 17 – – AC/GT 56 ACG/CGT 41 GCGA/TCGC 15 – – GA/TC 55 CCG/CGG 26 ACGC/GCGT 14 10 162862 C/G 100 CG 100 CGC/GCG 35 CGCG 14 A/T 93 GC 64 CGA/TCG 33 CGAC/GTCG 10 – – AC/GT 54 CCG/CGG 29 GCGA/TCGC 9 – – GA/TC 52 ACG/CGT 29 GCGC 9 n ne represents the number of non-equivalent pairs. Table 5 Symmetry measures for k-mers of length up to 10nt and for random and human complete genome. k 1 2 3 4 5 6 7 8 9 10 H a C 0.99998 0.99998 0.99999 0.99999 0.99999 0.99998 0.99998 0.99997 0.99995 0.99992 KL 1E−06 2E−06 3E−06 4E−06 6E−06 9E−06 2E−05 5E−05 1.7E−04 6.3E−04 1−S 1 0.9995 0.9994 0.9992 0.9991 0.9989 0.9986 0.9981 0.9970 0.9948 0.9904 U ( A 1 ; A 2 ) 0 0 0 21 170 1393 7522 32,130 130,611 523,422 Ws 0 0 0 0.164 0.332 0.680 0.918 0.981 0.996 0.998 max d 0 0 0 4 8 42 420 2870 19,031 215,755 max ′ d 0 0 0 0.02 0.01 0.01 0.03 0.04 0.07 0.21 RC a C 1.00000 1.00000 1.00000 1.00000 1.00000 0.99999 0.99998 0.99993 0.99974 0.99907 KL 5E−11 3E−09 3E−08 1E−07 5E−07 2E−06 8E−06 3E−05 1.3E−04 5.3E−04 1−S 1 1.0000 1.0000 0.9999 0.9998 0.9997 0.9994 0.9987 0.9974 0.9948 0.9896 U ( A 1 ; A 2 ) 0 2 22 107 495 1996 8164 32,608 131,033 523,732 Ws 0 0.250 0.688 0.836 0.967 0.975 0.997 0.995 1.000 0.999 max d 0 1 19 86 300 1255 4379 17,786 64,366 257,148 max ′ d 0 0.07 0.30 0.34 0.29 0.31 0.27 0.27 0.25 0.25 C is Pearson's correlation coefficient; KL is the Kullback–Leibler divergence; S1 is the total variational distance; U ( A 1 ; A 2 ) is Ulam's metric; Ws is the word symmetry distance as defined above; max d is the maximum distance in the sorted word arrangement; and max ′ d is the normalized maximum distance in the sorted word arrangement The breakdown of the word symmetry in the human genome Vera Afreixo a ⁎ Carlos A.C. Bastos b c Sara P. Garcia b João M.O.S. Rodrigues b c Armando J. Pinho b c Paulo J.S.G. Ferreira b c a CIDMA – Center for Research and Development in Mathematics and Applications, Department of Mathematics, University of Aveiro, 3810-193 Aveiro, Portugal CIDMA - Center for Research and Development in Mathematics and Applications Department of Mathematics University of Aveiro Aveiro 3810-193 Portugal b Signal Processing Lab., IEETA, University of Aveiro, 3810-193 Aveiro, Portugal Signal Processing Lab IEETA University of Aveiro Aveiro 3810-193 Portugal c Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal Department of Electronics Telecommunications and Informatics University of Aveiro Aveiro 3810-193 Portugal ⁎ Corresponding author. Tel.: +351 234370650. Previous studies have suggested that Chargaff's second rule may hold for relatively long words (above 10nucleotides), but this has not been conclusively shown. In particular, the following questions remain open: Is the phenomenon of symmetry statistically significant? If so, what is the word length above which significance is lost? Can deviations in symmetry due to the finite size of the data be identified? This work addresses these questions by studying word symmetries in the human genome, chromosomes and transcriptome. To rule out finite-length effects, the results are compared with those obtained from random control sequences built to satisfy Chargaff's second parity rule. We use several techniques to evaluate the phenomenon of symmetry, including Pearson's correlation coefficient, total variational distance, a novel word symmetry distance, as well as traditional and equivalence statistical tests. We conclude that word symmetries are statistical significant in the human genome for word lengths up to 6nucleotides. For longer words, we present evidence that the phenomenon may not be as prevalent as previously thought. Keywords Single strand symmetry Oligonucleotide composition Word symmetry distance Equivalence testing 1 Introduction The discovery of the double helix structure of DNA (Watson and Crick, 1953) made evident that the total percentage of complementary nucleotides (A–T and C–G) in a double-stranded molecule should be equal. This property had been previously reported by Chargaff and it is accordingly known as Chargaff's (1950) first parity rule. The detailed analysis of some bacterial genomes led to the formulation of Chargaff's second parity rule, which asserts that the percentage of complementary nucleotides should also be equal in each of the two strands (Rudner et al., 1968a, 1968b; Karkas et al., 1968). This rule has been extensively confirmed in bacterial and eukaryotic genomes, including more recent results (e.g. Qi and Cuticchia, 2001; Baisnée et al., 2002; Albrecht-Buehler, 2007; Kong et al., 2009; Zhang and Huang, 2010), that use global association measures and suggest that the symmetry holds for words with up to 10nucleotides (nt). However, the universality of Chargaff's second parity rule has been questioned for organellar DNA and some viral genomes (Mitchell and Bridge, 2006). A natural extension of Chargaff's second parity rule is that, in each DNA strand, the number of occurrences of a given word should match that of its reversed complement. A genomic word, or oligonucleotide, of length k is here interchangeably denoted as a k-mer. Moreover, we refer to the reversed complement as the symmetric word and the pair constituted by both words as a symmetric pair. The symmetry phenomenon that we aim to investigate pertains to the distributions of symmetric pairs, i.e. the distribution of all words and the distribution of the corresponding reversed complements. We focus our study on the complete human genome, on each chromosome and on the transcriptome. As far as we know, previous works do not answer the following question: Are the observed discrepancies between the empirical distributions of the k-mers and their reversed complements, for each k, sufficiently small to infer that they are modeled by the same probability law, and is the phenomenon of the symmetry of DNA sequences statistically significant? Caution is required regarding the application of some traditional statistical tests, which can yield irrelevant or even misleading results, as reported in Baisnée et al. (2002). We adopt a number of independent approaches to detect the word length k above which the single strand symmetry ceases, or partially ceases, to exist. We use a number of standard statistical tools and introduce an adapted version of a multiple equivalence test. We also introduce a word symmetry distance based on Ulam's metric. The contributions in this work lead to the identification in the human genome of the range of word lengths for which symmetry is statistically significant, as well as the threshold beyond which it seems to break. 2 Materials and methods 2.1 Materials We analyze the human genome and use the reference assembly build 37.3 available from the website of the National Center for Biotechnology Information discarding all non-sequenced nucleotides (N) from the analysis. All chromosomes and the transcriptome of the human genome were processed as separate sequences. The words were counted with overlapping. We generated 24 random control sequences of the same size as the sequences of corresponding human chromosomes. The model used to generate the random sequences assumes Chargaff's second parity rule: the probability of occurrence of complement nucleotides is equal ( π A = π T and π C = π G ) and the probability of occurrence of each pair of complement nucleotides is estimated as the average of the nucleotides frequency of occurrence in the corresponding human chromosome. The sequences used in this work are denoted as follows: • human chromosomes: H c ; • random control chromosomes: RC c ; • whole assembled human genome: H a ; • whole assembled random control sequences: RC a ; • transcriptome of human genome: RNA. 2.2 Methods For a given word length k, we compare the frequency distribution of all words and the frequency distribution of their reversed complements, in order to evaluate the symmetry phenomenon. We use several methods, including some well-known measures 1. Pearson's correlation coefficient (C), 2. Kullback–Leibler divergence (KL), 3. total variational distance (S 1), which is a normalized L 1 distance (L 1 distance divided by 2) (Dembo and Karlin, 1992). We also use the well-known chi-square test to assess the statistically significance of the exact symmetry (that is, to check whether the probabilities of words and the corresponding reversed complements are equal). Furthermore, we propose two novel methods based on equivalence tests and Ulam's metric, described below. 2.2.1 Equivalence tests Let π 1 , π 2 denote the (population) proportions of one word and its corresponding reversed complement (a symmetric pair). Let p 1, p 2 denote the corresponding sample proportions. Traditional statistical hypothesis testing may be used to assess the differences between two proportions. However, it is well known that when traditional hypothesis tests are applied to large data sets, any small effect is always deemed significant (Moore, 1997; Kline, 2004; Migliorati and Ongaro, 2010). To overcome this drawback, we use equivalence tests for accepting/rejecting the equivalence hypotheses between the proportions of symmetric words. Because the word proportions may have low (or very low) values, particularly for large k, the differences between them may not be a good measure for evaluating discrepancies. Hence, we study the proportion ratio π 1 / π 2 . The statistical hypotheses for the equivalence test are H 0 : π 1 / π 2 ≥ δ or π 1 / π 2 ≤ 1 / δ vs H 1 : 1 / δ < π 1 / π 2 < δ . here δ ( > 1 ) represents a small tolerance used to conclude a practical equivalence between the proportions of symmetric words. For convenience, the hypotheses above are formulated only for one pair of symmetric words, but they represent all pairs of symmetric words. The ratio between two proportions, r = p 1 / p 2 , is an effect size measure. As in many studies, e.g. Thanassoulis and Vasan (2010), we consider the effect to be weak when it assumes values between 1.1 and 1.3 and we explore these lower effect size values as a tolerance to conclude about practical equivalence. The confidence interval for the unknown true value of π 1 / π 2 , considering the ln-method of Katz et al. (1978), for a given α , is ( p 1 p 2 e − z 1 − α / 2 1 / n 1 − 1 / N + 1 / n 2 − 1 / N ; p 1 p 2 e z 1 − α / 2 1 / n 1 − 1 / N + 1 / n 2 − 1 / N ) where n 1 and n 2 are the number of occurrences of the corresponding words and N is the total number of word occurrences. The equivalence tests procedure consists of obtaining the confidence interval for the proportion ratios and checking if it is contained within the interval ( 1 / δ , δ ) . If so, H 0 is rejected and the equivalence can be assumed. For each word length k, we performed 4 k equivalence tests. When all of the 4 k null hypotheses are rejected, we consider that the distributions of symmetric pairs (i.e. the distribution of occurrence of all words and the distribution of occurrence of their corresponding reversed complements) are equivalent. 2.2.2 Word symmetry distance For a given word length k, the distribution of original words is sorted by frequency values and the final arrangement is denoted as A 1 ( k ) . A second arrangement, A 2 ( k ) , is obtained by sorting symmetric pairs according to the largest frequency value of each pair. In this way, symmetric words are kept adjacent and elements of the pair are sorted according to their frequencies. For illustration, consider an arrangement of the dinucleotides A 1 ( 2 ) [ AA AT TT AG CT AC GT CA CG TG GA GC TC CC GG TA ] and the corresponding arrangement A 2 ( 2 ) [ AA TT AT AG CT AC GT CA TG CG GA TC GC CC GG TA ] . Under the assumption of the symmetry phenomenon, A 1 ( k ) should be equal to A 2 ( k ) . Deviations from this ideal symmetry are quantified using Ulam's metric, which is the minimum number of moves required to go from arrangement A 1 to arrangement A 2. This metric, denoted as U ( A 1 ; A 2 ) , is related with the longest increasing subsequence of a sequence (Aldous and Diaconis, 1999). For the example above U ( A 1 ( 2 ) ; A 2 ( 2 ) ) = 3 . As a consequence of the definition of A 2 and for a fixed word length k, the maximum Ulam distance is max { U ( A 1 ( k ) ; A 2 ( k ) ) } = 4 k / 2 . Therefore, we introduce a word symmetry distance given by Ws ( k ) = 2 4 k U ( A 1 ( k ) ; A 2 ( k ) ) , where Ws ∈ [ 0 , 1 ] , with values close to zero indicating that the distributions of symmetric pairs are nearly identical, and values close to one reflecting the opposite. 3 Results and discussion 3.1 Symmetry hypothesis testing To evaluate the exact symmetry hypothesis for different word lengths (that is, whether the probability of a word equals the probability of its reversed complement) we use the χ 2 goodness of fit test. For random data generated according to the hypothesis, the test should of course reveal non-significant differences. Table 1 presents the p-values of the goodness of fit test of the studied sequences. As expected, the null hypothesis is confirmed for the random sequences, but it is rejected for the human genomic sequences. For the random control sequences (RC c , RC a ), generated with equal A/T and C/G compositions, the exact symmetry phenomenon prevails for the studied word lengths. This is not observed in the human sequences (H c , H a , RNA). Since the random and human sequences have the same length, this cannot be due to the sample size. It is known that for sample sizes like those we use and for a reasonable number of categories, the differences between proportions have a large chance of being detected. The irrelevance of the traditional statistical tests when used to evaluate the symmetry phenomenon was also pointed out in Baisnée et al. (2002). Therefore, we applied statistical tests of equivalence to the data, and tested if the proportion ratios are close to one. Table 2 presents the results of equivalence tests for the proportions ratio for the studied sequences. Tolerances close to δ are often used. We present the percentage of equivalent symmetric pairs for two tolerances: δ = 1.3 and the more strict δ = 1.1 . To interpret the equivalence test results one should take into consideration the value of the tolerance and the size of the studied sequences. In order to check for the presence or importance of a possible finite-size effect, we use the random sequences as control sequences. Obviously, in an infinite random sequence generated as described before, we would expect to observe the symmetry phenomenon (similar frequencies for all symmetric pairs), regardless of the word lengths. If this is not observed in a finite sample, there is reason to suspect sample-size effects. Table 2 shows the results obtained. Consider first δ = 1.3 . The percentage of equivalent word pairs for the human sequence H a is below 100% for word lengths above 8, and below 85% for word length 10. This stands in contrast to what is observed in the random sequence RC a , which, despite having the same length as H a , shows 100% equivalent pairs for all word lengths up to 10. Thus, the decrease observed in H a cannot be due to the sample size. In the case of the shorter random sequences RC c , the table shows a decrease in the percentage of equivalent pairs, starting with length 8. However, the decrease in the case of the human sequences H c is stronger and starts for length 6. Thus, although the effect of the finite sample size cannot be entirely ruled out, the difference between the two cases remains nevertheless clear. The case δ = 1.1 confirms this. Under this tighter tolerance, deviations from 100% equivalent pairs can be noted both in the random data and in the human data. However, the differences between the human and the random cases remain considerable and cannot be entirely explained by the limited sample size. The RNA sequence shows symmetry deviations already for very small word lengths ( k > 3 when δ = 1.1 , and k > 1 when δ = 1.3 ). We observe a much higher percentage of non-equivalent pairs in comparison with the random sequences RC c , which is of a comparable size (RNA has about 134 megabases and the average size of the chromosomes is about 129 megabases). Table 3 presents a summary of the occurrence ratios for symmetric pairs. The table shows the maximum ratio ( max r ), 95th percentile ratio (P 95r ) and 75th percentile ratio (P 75r ) between the proportions of the words in each symmetric pair. For words of length 10nt, a maximum value of 17 was observed. This means that at least one word occurred 17 times more than the corresponding symmetric word. However, in all k-mers the majority of ratios assume low values: the 95th percentile ratio is lower than 1.2 and the 75th percentile ratio is lower than 1.1. In general, for the complete human genome (H a ), and for the corresponding random sequence (RC a ), symmetric pairs have ratio values close to 1. However, for k > 7 , we observe some pairs with high (or very high) ratio values which is in accordance with the equivalence test results. When k > 7 , not all word symmetric pairs can be considered equivalent. We carried out an exploratory analysis of the content of these word pairs. For each pair we searched for the presence of oligonucleotides (with 1–5 symbols). Table 4 shows the four most frequent oligonucleotides (nucleotide to tetranucleotide) present in non-equivalent pairs. For 8-mers, 9-mers and 10-mers we observe that all words in the non-equivalent pairs have a rich C/G content, i.e. all words contain C, G and CG. Moreover, a high percentage of nonequivalent pairs contains CGCG. This evidence is in accordance with the observation first made several decades ago (Josse et al., 1961) of the under-representation of CpG dinucleotides (CpGs) in the human genome, hypothesized to be a consequence of the hypermutability of CpGs due to the increased mutation rate of 5-methylcytosine followed by the inaccurate mismatch repair of deaminated 5-methylcytosine (i.e. uracil) that introduces a thymine upon replication (Bird, 1980). Finally, we note that our listing of non-equivalent pairs may represent words involved in evolutionary processes that may be distinct for different word lengths (Nikolaou and Almirantis, 2005; Bush and Lahn, 2006). 3.2 Pearson's correlation coefficient, Kullback–Leibler divergence (KL) and total variational distance Table 5 presents the results of several techniques to evaluate the word symmetry phenomenon. We present Pearson's correlation coefficient values, which suggest a strong word symmetry phenomenon, in line with the previous work in this area (Albrecht-Buehler, 2006; Prabhu, 1993). However, it is well known that this coefficient is not robust to outliers, such as poly-A and poly-T words. The Kullback–Leibler (KL) divergence and total variational distance (S 1) results are in agreement with Pearson's correlation coefficient (C): words and symmetric words distributions are close, at least for words of length up to 10nt. C and 1 − S 1 assume values between zero and one, and values higher than 0.9 correspond to a strong association. Kullback–Leibler divergence assumes values in (0, ∞ ), with greater values corresponding to larger distances. These measures, C, S 1 and KL, may be overstating the symmetry phenomenon, because they only evaluate the global agreement between all corresponding values in both distributions (words vs corresponding symmetric words) and not if this agreement is the strongest among all possible pairs. 3.3 Word symmetry distance In order to measure the strength of the symmetry phenomenon, we introduced a new word symmetry distance, Ws. In the complete human genome (H a ) we observe (see Table 5) that for smaller k-mers ( k < 4 ), the symmetric words permutation is the closest to the original distribution. For larger k-mers ( k ≥ 4 ), there are other nearest permutations. However, for k=4 and k=5, we obtain a small distance, which leads us to conclude that the phenomenon of symmetry is still reasonably strong. For k ≥ 6 the symmetric words permutation is increasingly more distant from the original distribution. To better characterize the fading of the symmetry phenomenon, we estimate the percentage of symmetric word pairs that are spaced at most d words in A 1 (the arrangement of words is sorted by their frequencies). Fig. 1 presents these percentages for d < 50 . Naturally, we observe that the distances between symmetric words increase with the word length. For the random complete genome (RC a ) we observe that word symmetry distances reach higher values than for the human genome, indicating a lower symmetry in random sequences when compared with the corresponding human genome. In Table 5, for 1 < k < 7 and for the complete human genome we obtain a word symmetry distance lower than in the random sequences. For k > 6 the random and human complete genomes present similar word symmetry distances. This result seems to contradict previous ones obtained by the equivalence and the traditional hypothesis test. However, from all these results, we observe that the differences between frequencies of symmetric words in the random sequences are lower than in the human sequences. In the sorted frequency vector the distances between symmetric words in the random sequences are higher than in the human sequences. The random control sequences were generated to model Chargaff's second parity rule ( π A = π T and π C = π G ) thus it is expected that the frequency of occurrence of any word is similar to that of its reversed complement. This similarity is expected for any word length. Since, for word lengths greater than 1 ( k > 1 ) there are groups of words whose expected frequencies are similar (e.g. AAA, AAT, ATA, ATT, TAA, TAT, TTA, TTT), we do not expect null distances between the frequencies of the symmetric word pairs in the vector sorted by the frequency of occurrence. Thus, the results obtained for the word symmetry distance of the random sequences are in agreement with the expected values. In addition, to better characterize the symmetry phenomenon, for each k-mer, we find the maximum distance between two symmetric words ( max d ) in A 1. Note that the maximum distance between two words in a general arrangement is 4 k − 1 . Because of this, we propose a normalized maximum distance max ′ d = max d / ( 4 k − 1 ) . In Table 5 we observe that in the complete human genome (H a ) the normalized maximum distance is small ( < 0.1 ) for k-mers of length 1–9nt, but larger for 10-mers. If we consider a threshold of 0.05, the symmetry phenomenon is relevant for words of length up to 8nt. In the random complete sequence (RC a ) using the normalized maximum distance with 0.05 threshold, the symmetry phenomenon is relevant only for k=1 and takes values similar to the complete human genome only for k=1 or k=10. In order to compare the symmetry results for the random control sequences with the corresponding human chromosomes and RNA sequences, we present some plots in Fig. 2 . In Fig. 2 we observe that the word symmetry distances in RNA and the random control sequences are very similar. In the human chromosome sequences the average values of the word symmetry for 1 < k < 8 suggest higher symmetry than in random or RNA sequences. Fig. 3 shows two scatter plots to study the size effect in word symmetry distance, for k=7 and k=10. For random sequences, the absolute value of Pearson's correlation coefficient between chromosome size and word symmetry distance is lower than 0.5 for k ≤ 8 and for k > 8 the absolute value of Pearson's correlation coefficient is higher than 0.80. This difference might arise from the finite sample size. These results emphasize the hypothesis that for k > 8 the effect of the sample size begins to be noticeable, as already suggested in the context of equivalence testing. 4 Conclusion We used several criteria to compare distributions of words and their reversed complements, including Pearson's correlation coefficient, total variational distance, Kullback–Leibler divergence, a novel word symmetry distance, the normalized maximum distance, as well as traditional and equivalence statistical tests. Our results for relatively short words agree with and reinforce previous evidence in the literature (Albrecht-Buehler, 2006; Baisnée et al., 2002; Qi and Cuticchia, 2001; Prabhu, 1993). The several measures used complement each other, providing a broader description of the symmetry phenomenon. In terms of frequency similarity, the symmetry was assessed with the total variational distance, Kullback–Leibler divergence, traditional and equivalence hypothesis tests. In terms of correlation the symmetry was described by Pearson's coefficient. In terms of the distance between symmetric words in sorted frequency vector the symmetry was discussed by the word symmetry distance and the normalized maximum distance. We compared the human sequences with random control sequences ( π A = π T and π C = π G ) of the same length, to understand the onset of finite sample size effects. An infinite random sequence generated subject to π A = π T and π C = π G should exhibit the symmetry phenomenon (similar frequencies for all symmetric pairs), regardless of the word length. A failure to detect this symmetry in a finite sample sequence reveals a limitation in the sample size. The human genome deviates from the symmetry more rapidly than the control sequences. For example, in the complete human genome, the equivalence testing shows that the symmetry phenomenon is statistically significant at least for words of length up to 6nt. As the word length increases, the strength of the symmetry phenomenon in the complete human genome decreases, but does so more rapidly than in the control sequences. In terms of distances between symmetric words, the symmetry remains higher than in the control sequences, at least for 1 < k < 7 . However, for k > 6 , high distance values can be found in both cases (word symmetry distance > 90 % ). The analysis of our results shows that, globally, Chargaff's second parity rule only holds for small oligonucleotides, in the human genome. However, there are some large oligonucleotides for which the extension of Chargaff's second parity holds. We also proposed new methods to evaluate the distance between symmetric words in the sorted frequency vector. The new methods evaluate the word length k s at which symmetry ceases to exist. The results obtained point to similar thresholds ( 5 ≤ k s ≤ 7 ). This suggests that the phenomenon of symmetry is not as strong as previously thought. Furthermore, in the case of RNA, this is especially notorious, even for the smallest word lengths. The deviations from perfect symmetry are more pronounced for large word lengths, for which the sample size limit might become an issue. Therefore, we were careful to subtract this effect from the analysis. We based our conclusions on a set of methods, not on a single one; statistical significance was a major concern; and we systematically used control sequences to detect the onset of problems due to limited sample sizes. We feel that our results provide a broad picture of the symmetry phenomenon in the human genome and the extent to which it holds. Funding This work was supported by FEDER funds through COMPETE–Operational Programme Factors of Competitiveness (“Programa Operacional Factores de Competitividade”) and by Portuguese funds through the Center for Research and Development in Mathematics and Applications (University of Aveiro), Institute of Electronics and Telematics Engineering of Aveiro (University of Aveiro) and the Portuguese Foundation for Science and Technology (“FCT – Fundação para a Ciência e a Tecnologia”), within projects PEst-C/MAT/UI4106/2011 with COMPETE number FCOMP-01-0124-FEDER-022690 and PEst-C/EEI/UI0127/2011 with COMPETE number FCOMP-01-0124-FEDER-022682. S.P.G. acknowledges funding from the European Social Fund and the Portuguese Ministry of Education and Science. References Albrecht-Buehler, 2006 G. Albrecht-Buehler Asymptotically increasing compliance of genomes with Chargaff's second parity rules through inversions and inverted transpositions Proc. Nat. Acad. Sci. USA 103 2006 17828 17833 Albrecht-Buehler, 2007 G. Albrecht-Buehler Inversions and inverted transpositions as the basis for an almost universal “format” of genome sequences Genomics 90 2007 297 305 Aldous and Diaconis, 1999 D. Aldous P. Diaconis Longest increasing subsequences from patience sorting to the Baik–Deift–Johansson theorem Bull. Am. Math. Soc. 36 1999 199 213 Baisnée et al., 2002 P.F. Baisnée S. Hampson P. Baldi Why are complementary DNA strands symmetric? Bioinformatics 18 2002 1021 1033 Bird, 1980 A. Bird DNA methylation and the frequency of CPG in animal DNA Nucleic Acids Res. 8 1980 1499 1504 Bush and Lahn, 2006 E.C. Bush B.T. Lahn The evolution of word composition in metazoan promoter sequence PLOS Comput. Biol. 2 2006 1343 1348 Chargaff, 1950 E. Chargaff Chemical specificity of nucleic acids and mechanism of their enzymatic degradation Experientia 6 1950 201 209 Dembo and Karlin, 1992 A. Dembo S. Karlin Poisson approximations for r-scan processes Ann. Appl. Probab. 2 1992 329 357 Josse et al., 1961 J. Josse A. Kaiser A. Kornberg Enzymatic synthesis of deoxyribonucleic acid. VIII. Frequencies of nearest neighbor base sequences in deoxyribonucleic acid J. Biol. Chem. 236 1961 864 875 Karkas et al., 1968 J.D. Karkas R. Rudner E. Chargaff Separation of B. subtilis DNA into complementary strands. II. Template functions and composition as determined by transcription with RNA polymerase Proc. Nat. Acad. Sci. USA 60 1968 915 920 Katz et al., 1978 D. Katz J. Baptista S. Azen M. Pike Obtaining confidence intervals for the risk ratio in cohort studies Biometrics 34 1978 469 474 Kline, 2004 R. Kline Beyond Significance testing reforming data analysis methods in behavioral research Am. Psychol. Assoc 2004 Kong et al., 2009 S.G. Kong W.L. Fan H.D. Chen Z.T. Hsu N. Zhou B. Zheng H.C. Lee Inverse symmetry in complete genomes and whole-genome inverse duplication PLoS ONE 4 2009 e7553 Migliorati and Ongaro, 2010 Migliorati, S., Ongaro, A., 2010. Adjusting p-values when n is large in the presence of nuisance parameters. In: Statistics for Industry and Technology, Vienna, pp. 305–318. Mitchell and Bridge, 2006 D. Mitchell R. Bridge A test of Chargaff's second rule Biochem. Biophys. Res. Commun. 340 2006 90 94 Moore, 1997 D.S. Moore Statistics Concepts and Controversies 4th ed. 1997 Freeman Nikolaou and Almirantis, 2005 C. Nikolaou Y. Almirantis “Word” preference in the genomic text and genome evolution different modes of n-tuplet usage in coding and noncoding sequences J. Mol. Evol. 61 2005 23 35 Prabhu, 1993 V.V. Prabhu Symmetry observations in long nucleotide sequences Nucleic Acids Res. 21 1993 2797 2800 Qi and Cuticchia, 2001 D. Qi A.J. Cuticchia Compositional symmetries in complete genomes Bioinformatics 17 2001 557 559 Rudner et al., 1968a R. Rudner J.D. Karkas E. Chargaff Separation of B. subtilis DNA into complementary strands. I. Biological properties Proc. Nat. Acad. Sci. USA 60 1968 630 635 Rudner et al., 1968b R. Rudner J.D. Karkas E. Chargaff Separation of B. subtilis DNA into complementary strands. III. Direct analysis Proc. Nat. Acad. Sci. USA 60 1968 921 922 Thanassoulis and Vasan, 2010 G. Thanassoulis R.S. Vasan Genetic cardiovascular risk prediction—will we get there? Circulation 122 2010 2323 2334 Watson and Crick, 1953 J. Watson F. Crick A structure for deoxyribose nucleic acid Nature 171 1953 737 738 Zhang and Huang, 2010 S.H. Zhang Y.Z. Huang Limited contribution of stem-loop potential to symmetry of single-stranded genomic DNA Bioinformatics 26 2010 478 485 "
    },
    {
        "doc_title": "An ontology-based multi-level robot architecture for learning from experiences",
        "doc_scopus_id": "84883297909",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84883297909",
        "doc_date": "2013-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Eu projects",
            "High level semantics",
            "Learning from experiences",
            "Ontology-based",
            "Robot architecture",
            "Robot performance",
            "Service robots"
        ],
        "doc_abstract": "One way to improve the robustness and flexibility of robot performance is to let the robot learn from its experiences. In this paper, we describe the architecture and knowledge-representation framework for a service robot being developed in the EU project RACE, and present examples illustrating how learning from experiences will be achieved. As a unique innovative feature, the framework combines memory records of low-level robot activities with ontology-based high-level semantic descriptions. © 2013, Association for the Advancement of artificial intelligence.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compressing resequencing data with GReEn",
        "doc_scopus_id": "84881097916",
        "doc_doi": "10.1007/978-1-62703-514-9_2",
        "doc_eid": "2-s2.0-84881097916",
        "doc_date": "2013-08-09",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Genome sequencing centers are flooding the scientific community with data. A single sequencing machine can nowadays generate more data in one day than any existing machine could have produced throughout the entire year of 2005. Therefore, the pressure for efficient sequencing data compression algorithms is very high and is being felt worldwide. Here, we describe GReEn (Genome Resequencing Encoding), a compression tool recently proposed for compressing genome resequencing data using a reference genome sequence. © 2013 Springer Science+Business Media New York.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A compression model for DNA multiple sequence alignment blocks",
        "doc_scopus_id": "84876759103",
        "doc_doi": "10.1109/TIT.2012.2236605",
        "doc_eid": "2-s2.0-84876759103",
        "doc_date": "2013-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [
            "Compression model",
            "Genomics",
            "Lossless compression",
            "Modeling approach",
            "Molecular genomics",
            "Multiple sequence alignments",
            "Per-symbol",
            "Whole genome alignment"
        ],
        "doc_abstract": "A particularly voluminous dataset in molecular genomics, known as whole genome alignments, has gained considerable importance over the last years. In this paper, we propose a compression modeling approach for the multiple sequence alignment (MSA) blocks, which make up most of these datasets. Our method is based on a mixture of finite-context models. Contrarily to other recent approaches, it addresses both the DNA bases and gap symbols at once, better exploring the existing correlations. For comparison with previous methods, our algorithm was tested in the multiz28way dataset. On average, it attained 0.94 bits per symbol, approximately 7% better than the previous best, for a similar computational complexity. We also tested the model in the most recent dataset, multiz46way. In this dataset, that contains alignments of 46 different species, our compression model achieved an average of 0.72 bits per MSA block symbol. © 1963-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The rank of random binary matrices and distributed storage applications",
        "doc_scopus_id": "84873058970",
        "doc_doi": "10.1109/LCOMM.2012.120612.122169",
        "doc_eid": "2-s2.0-84873058970",
        "doc_date": "2013-01-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Binary matrix",
            "Block angular matrix",
            "Closed-form expression",
            "Distributed storage",
            "Fountain codes",
            "Random matrices",
            "Random Matrix",
            "Rateless codes"
        ],
        "doc_abstract": "Random binary matrices appear in a variety of signal processing and encoding problems. They play an important role in rateless codes and in distributed storage applications. This paper focuses on block angular matrices, a class of random rectangular binary matrices that are particularly suited to distributed storage applications. We address one of the key issues regarding binary random matrices in general, and block angular matrices in particular: the probability of obtaining a full rank matrix, when drawing uniformly at random from the set of binary matrices with compatible structure. This paper gives a closed-form expression for this probability, as well as some bounds and approximations. © 1997-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-STOP symbol distances for the identification of coding regions.",
        "doc_scopus_id": "84902933766",
        "doc_doi": "10.1515/jib-2013-230",
        "doc_eid": "2-s2.0-84902933766",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "MLCS",
            "MLOWN"
        ],
        "doc_abstract": "In this study we explore the potential of inter-STOP symbol distances for finding coding regions in DNA sequences. We use the distance between STOP symbols in the DNA sequence and a chi-square statistic to evaluate the nonhomogeneity of the three possible reading frames and the occurrence of one long distance in one of the frames. The results of this exploratory study suggest that inter-STOP symbol distances have strong ability to discriminate coding regions in prokaryotes and simple eukaryotes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bacterial DNA sequence compression models using artificial neural networks",
        "doc_scopus_id": "84885362601",
        "doc_doi": "10.3390/e15093435",
        "doc_eid": "2-s2.0-84885362601",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "It is widely accepted that the advances in DNA sequencing techniques have contributed to an unprecedented growth of genomic data. This fact has increased the interest in DNA compression, not only from the information theory and biology points of view, but also from a practical perspective, since such sequences require storage resources. Several compression methods exist, and particularly, those using finite-context models (FCMs) have received increasing attention, as they have been proven to effectively compress DNA sequences with low bits-per-base, as well as low encoding/decoding time-per-base. However, the amount of run-time memory required to store high-order finite-context models may become impractical, since a context-order as low as 16 requires a maximum of 17.2 × 109 memory entries. This paper presents a method to reduce such a memory requirement by using a novel application of artificial neural networks (ANN) to build such probabilistic models in a compact way and shows how to use them to estimate the probabilities. Such a system was implemented, and its performance compared against state-of-the art compressors, such as XM-DNA (expert model) and FCM-Mx (mixture of finite-context models) , as well as with general-purpose compressors. Using a combination of order-10 FCM and ANN, similar encoding results to those of FCM, up to order-16, are obtained using only 17 megabytes of memory, whereas the latter, even employing hash-tables, uses several hundreds of megabytes. © 2013 by the authors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Segmentation of DNA into Coding and Noncoding Regions Based on Inter-STOP Symbols Distances",
        "doc_scopus_id": "84880349505",
        "doc_doi": "10.1007/978-3-319-00578-2_4",
        "doc_eid": "2-s2.0-84880349505",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "chi-square",
            "Chi-square statistics",
            "Coding region",
            "Exploratory studies",
            "inter-STOP symbols distance",
            "Non-coding region",
            "Nonhomogeneity"
        ],
        "doc_abstract": "In this study we set to explore the potentialities of the inter-genomic symbols distance for finding the coding regions in DNA sequences. We use the distance between STOP symbols in the DNA sequence and a chi-square statistic to evaluate the nonhomogeneity of the three possible reading frames. The results of this exploratory study suggest that inter-STOP symbols distance has strong ability to discriminate coding regions. © Springer International Publishing Switzerland 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the detection of unknown locally repeating patterns in images",
        "doc_scopus_id": "84864151066",
        "doc_doi": "10.1007/978-3-642-31295-3_19",
        "doc_eid": "2-s2.0-84864151066",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Digital image",
            "False positive",
            "Image complexity",
            "Lossless image compression",
            "Repeated patterns"
        ],
        "doc_abstract": "Detecting unknown repeated patterns that appear multiple times in a digital image is a great challenge. We have addressed this problem in a recent work and we have shown that, using a compression based approach, it is possible to find exact repetitions. In this work, we continue this study, introducing a procedure for detecting unknown repeated patterns that occur in a close vicinity. We use finite-context modelling to pinpoint the possible locations of the repetitions, by exploring the connection between lossless image compression and image complexity. Since repetitions are associated to low complexity regions, the repeating patterns are revealed and easily detected. The experimental results show that the proposed approach provides increased ability to eliminate false positives. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression of whole genome alignments using a mixture of finite-context models",
        "doc_scopus_id": "84864149326",
        "doc_doi": "10.1007/978-3-642-31295-3_42",
        "doc_eid": "2-s2.0-84864149326",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Arithmetic Coding",
            "Bit per symbols",
            "Compression methods",
            "Compression rates",
            "Data sets",
            "DNA Sequencing",
            "Genomic sequence",
            "Multiple sequence alignments",
            "Per-symbol",
            "Sequence data",
            "Whole genome alignment"
        ],
        "doc_abstract": "In the last years, advances in DNA sequencing technology have caused a giant growth in the amount of available data related with genomic sequences. One of those types of data sets is that resulting from multiple sequence alignments (MSA). In this paper, we propose a compression method for compressing these data sets, using a mixture of finite-context models and arithmetic coding. The method relies on image compression concepts, it was tested in the multiz28way data set and attained a compression rate around 0.93 bits per symbol on the sequence data, better than the ≈∈1 bit per symbol attained by a recently proposed method. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computation of the normalized compression distance of DNA sequences using a mixture of finite-context models",
        "doc_scopus_id": "84861969601",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861969601",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Comprehensive studies",
            "Compression algorithms",
            "Compression methods",
            "DNA compression",
            "Human chromosomal similarity",
            "Human genomes",
            "Internal models",
            "Normalized-compression distance",
            "Similarity measure"
        ],
        "doc_abstract": "A compression-based similarity measure assesses the similarity between two objects using the number of bits needed to describe one of them when a description of the other is available. For being effective, these measures have to rely on \"normal\" compression algorithms, roughly meaning that they have to be able to build an internal model of the data being compressed. Often, we find that good \"normal\" compression methods are slow and those that are fast do not provide acceptable results. In this paper, we propose a method for measuring the similarity of DNA sequences that balances these two goals. The method relies on a mixture of finite-context models and is compared with other methods, including XM, the state-of-the-art DNA compression technique. Moreover, we present a comprehensive study of the inter-chromosomal similarity of the human genome.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exon: A web-based software toolkit for DNA sequence analysis",
        "doc_scopus_id": "84861206336",
        "doc_doi": "10.1007/978-3-642-28839-5_25",
        "doc_eid": "2-s2.0-84861206336",
        "doc_date": "2012-05-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "DNA compression",
            "DNA sequence analysis",
            "DNA Sequencing",
            "Exponential growth",
            "Genomic-sequence data",
            "On-line analysis",
            "Web-based softwares"
        ],
        "doc_abstract": "Recent advances in DNA sequencing methodologies have caused an exponential growth of publicly available genomic sequence data. By consequence, many computational biologists have intensified studies in order to understand the content of these sequences and, in some cases, to search for association to disease. However, the lack of public available tools is an issue, specially when related to efficiency and usability. In this paper, we present Exon, a user-friendly solution containing tools for online analysis of DNA sequences through compression based profiles. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Finite-context models for image compression",
        "doc_scopus_id": "84856735864",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84856735864",
        "doc_date": "2012-02-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "DNA microarray images",
            "Image complexity",
            "Medical images"
        ],
        "doc_abstract": "Finite-context modeling has been applied to the problem of image compression with remarkable success. In this talk, we address this topic, showing examples with various types of images, including medical images of several modalities and DNA microarray images. We also show how finite-context modeling can be used for image complexity estimation and how this relates to image compression. © 2012 Taylor & Francis Group.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GReEn: A tool for efficient compression of genome resequencing data",
        "doc_scopus_id": "84857860662",
        "doc_doi": "10.1093/nar/gkr1124",
        "doc_eid": "2-s2.0-84857860662",
        "doc_date": "2012-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Research in the genomic sciences is confronted with the volume of sequencing and resequencing data increasing at a higher pace than that of data storage and communication resources, shifting a significant part of research budgets from the sequencing component of a project to the computational one. Hence, being able to efficiently store sequencing and resequencing data is a problem of paramount importance. In this article, we describe GReEn (Genome Resequencing Encoding), a tool for compressing genome resequencing data using a reference genome sequence. It overcomes some drawbacks of the recently proposed tool GRS, namely, the possibility of compressing sequences that cannot be handled by GRS, faster running times and compression gains of over 100-fold for some sequences. This tool is freely available for non-commercial use at ftp://ftp.ieeta.pt/∼ap/codecs/GReEn1.tar.gz. © 2012 The Author(s).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Minimal absent words in four human genome assemblies",
        "doc_scopus_id": "84855261012",
        "doc_doi": "10.1371/journal.pone.0029344",
        "doc_eid": "2-s2.0-84855261012",
        "doc_date": "2011-12-29",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Minimal absent words have been computed in genomes of organisms from all domains of life. Here, we aim to contribute to the catalogue of human genomic variation by investigating the variation in number and content of minimal absent words within a species, using four human genome assemblies. We compare the reference human genome GRCh37 assembly, the HuRef assembly of the genome of Craig Venter, the NA12878 assembly from cell line GM12878, and the YH assembly of the genome of a Han Chinese individual. We find the variation in number and content of minimal absent words between assemblies more significant for large and very large minimal absent words, where the biases of sequencing and assembly methodologies become more pronounced. Moreover, we find generally greater similarity between the human genome assemblies sequenced with capillary-based technologies (GRCh37 and HuRef) than between the human genome assemblies sequenced with massively parallel technologies (NA12878 and YH). Finally, as expected, we find the overall variation in number and content of minimal absent words within a species to be generally smaller than the variation between species. © 2011 Garcia, Pinho.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Symbolic to numerical conversion of DNA sequences using finite-context models",
        "doc_scopus_id": "84863762407",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863762407",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Information sequences",
            "Numerical information",
            "Numerical representation",
            "Quick Bird",
            "Signal processing technique",
            "Symbolic data",
            "Symbolic sequence"
        ],
        "doc_abstract": "Symbolic sequences can be analysed using two main approaches. One is by means of algorithms specifically designed for processing symbolic sequences. The other uses signal processing techniques, after converting the sequence from symbols to numbers. The latter approach depends on the availability of meaningful numerical representations of the sequences. In this paper, we present a technique that uses finite-context models to generate numerical information sequences from the DNA symbolic data. We give some examples that illustrate the method and show that these information sequences may reveal important structural properties of the DNA sequences. Moreover, the proposed approach is fast, allowing a quick bird's-eye view of whole chromosomes, with the aim of locating potentially interesting regions. © EURASIP, 2011.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Finding unknown repeated patterns in images",
        "doc_scopus_id": "84863760761",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863760761",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Digital image",
            "Image complexity",
            "Repeated patterns"
        ],
        "doc_abstract": "We consider the problem of finding unknown patterns that appear multiple times in a digital image. We want to find the number of repetitions and their positions in the image without constraining the nature and shape of the pattern in any way. We propose a method that is able to pinpoint the possible locations of the repetitions by exploring the connection between image compression and image complexity. The method uses a finite-context model to build a complexity map of the image in which the repeated patterns correspond to areas of low complexity, which mark the locations of the repetitions. © 2011 EURASIP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image similarity using the normalized compression distance based on finite context models",
        "doc_scopus_id": "84856259569",
        "doc_doi": "10.1109/ICIP.2011.6115866",
        "doc_eid": "2-s2.0-84856259569",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Compression algorithms",
            "Context models",
            "Image applications",
            "Image compression methods",
            "Image similarity",
            "Kolmogorov complexity",
            "Normalized compression distance",
            "Similarity measure"
        ],
        "doc_abstract": "A compression-based similarity measure assesses the similarity between two objects using the number of bits needed to describe one of them when a description of the other is available. Theoretically, compression-based similarity depends on the concept of Kolmogorov complexity but implementations require suitable (normal) compression algorithms. We argue that the approach is of interest for challenging image applications but we identify one obstacle: standard high-performance image compression methods are not normal, and normal methods such as Lempel-Ziv type algorithms might not perform well for images. To demonstrate the potential of compression-based similarity measures we propose an algorithm that is based on finite-context models and works directly on the intensity domain of the image. The proposed algorithm is compared with several other methods. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Complexity profiles of DNA sequences using finite-context models",
        "doc_scopus_id": "82155191383",
        "doc_doi": "10.1007/978-3-642-25364-5_8",
        "doc_eid": "2-s2.0-82155191383",
        "doc_date": "2011-11-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Compression methods",
            "DNA sequence analysis",
            "DNA sequence data",
            "IMPROVE-A",
            "Information sources",
            "Relative entropy",
            "Source models",
            "Storage spaces",
            "Transmission time"
        ],
        "doc_abstract": "Every data compression method assumes a certain model of the information source that produces the data. When we improve a data compression method, we are also improving the model of the source. This happens because, when the probability distribution of the assumed source model is closer to the true probability distribution of the source, a smaller relative entropy results and, therefore, fewer redundancy bits are required. This is why the importance of data compression goes beyond the usual goal of reducing the storage space or the transmission time of the information. In fact, in some situations, seeking better models is the main aim. In our view, this is the case for DNA sequence data. In this paper, we give hints on how finite-context (Markov) modeling may be used for DNA sequence analysis, through the construction of complexity profiles of the sequences. These profiles are able to unveil structures of the DNA, some of them with potential biological relevance. © 2011 Springer-Verlag Berlin.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compressing the human genome using exclusively Markov models",
        "doc_scopus_id": "80052957011",
        "doc_doi": "10.1007/978-3-642-19914-1_29",
        "doc_eid": "2-s2.0-80052957011",
        "doc_date": "2011-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Human genomes",
            "Markov model",
            "Markov models",
            "Markov property",
            "Probability estimate"
        ],
        "doc_abstract": "Models that rely exclusively on the Markov property, usually known as finite-context models, can model DNA sequences without considering mechanisms that take direct advantage of exact and approximate repeats. These models provide probability estimates that depend on the recent past of the sequence and have been used for data compression. In this paper, we investigate some properties of the finite-context models and we use these properties in order to improve the compression. The results are presented using the human genome as example. © 2011 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Distances between dinucleotides in the human genome",
        "doc_scopus_id": "80052936474",
        "doc_doi": "10.1007/978-3-642-19914-1_28",
        "doc_eid": "2-s2.0-80052936474",
        "doc_date": "2011-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Dinucleotides",
            "Distance distributions",
            "Human genomes",
            "Kullback Leibler divergence",
            "Random sequence",
            "Relative errors"
        ],
        "doc_abstract": "We developed a methodology to process DNA sequences based on the inter-dinucleotide distances and we characterized the inter-dinucleotide distance distributions of the human genome. The distance distribution of each dinucleotide was compared to the distance distribution of all the other dinucleotides using the Kullback-Leibler divergence. We found out that the divergence between the distribution of the distances of a dinucleotide and that of its reversed complement is very small, indicating that these distance distributions are very similar. This is an interesting finding that might give evidence of a stronger parity rule than the one provided by Chargaff's second parity rule. Furthermore, we also compared the distance distribution of each dinucleotide to a reference distribution, that of a random sequence generated with the same dinucleotide abundances, revealing the CG dinucleotide as the one with the highest cumulative relative error for the first 60 distances. © 2011 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bacteria DNA sequence compression using a mixture of finite-context models",
        "doc_scopus_id": "80052249011",
        "doc_doi": "10.1109/SSP.2011.5967637",
        "doc_eid": "2-s2.0-80052249011",
        "doc_date": "2011-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Bacterial genomes",
            "Compression methods",
            "DNA coding",
            "finite-context models",
            "Recursive procedure",
            "Sequence compression"
        ],
        "doc_abstract": "The ability of finite-context models for compressing DNA sequences has been demonstrated on some recent works. In this paper, we further explore this line, proposing a compression method based on eight finite-context models, with orders from two to sixteen, whose probabilities are averaged using weights calculated through a recursive procedure. The method was tested on a total of 2,338 sequences belonging to bacterial genomes, with sizes ranging from 1,286 to 13,033,779 bases, showing better compression results than the state-of-the-art XM DNA coding algorithm and also faster operation. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A probabilistic image analysis problem and random linear equations",
        "doc_scopus_id": "80052245168",
        "doc_doi": "10.1109/SSP.2011.5967693",
        "doc_eid": "2-s2.0-80052245168",
        "doc_date": "2011-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Analysis problems",
            "Decision systems",
            "Image noise",
            "Probabilistic images"
        ],
        "doc_abstract": "Consider an image that depicts two objects. Two randomly selected pixels of the image are examined. A decision system emits 0 if both pixels belong to the same object, otherwise it emits 1. Image noise and imperfections in the decision system render the outputs unreliable, that is, there will be pairs of pixels incorrectly classified. The question we address is the following: as the result of successive comparisons accumulate, can one reliably decide which pixels belong to each object? We present and discuss results that shed some light on this problem and discuss it from different angles. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA synthetic sequences generation using multiple competing Markov models",
        "doc_scopus_id": "80052209762",
        "doc_doi": "10.1109/SSP.2011.5967639",
        "doc_eid": "2-s2.0-80052209762",
        "doc_date": "2011-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational model",
            "DNA compression",
            "Markov model",
            "Sequence generation",
            "Statistical generator",
            "Synthetic DNA",
            "Synthetic sequence"
        ],
        "doc_abstract": "The development and implementation of computational models to represent DNA sequences is a great challenge. Markov models, usually known as finite-context models, have been used for a long time in DNA compression. In a previous work, we have shown that finite-context modelling can also be used for sequence generation. Furthermore, it is known that DNA is better represented by multiple finite-context models. However, the previous generator only allowed a single finite-context model to be used for generating a certain sequence. In this paper, we present results regarding a synthetic DNA generator based on multiple competing finite-context models. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Random block-angular matrices for distributed data storage",
        "doc_scopus_id": "80051629646",
        "doc_doi": "10.1109/ICASSP.2011.5946697",
        "doc_eid": "2-s2.0-80051629646",
        "doc_date": "2011-08-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "block-angular matrices",
            "full rank probability",
            "Inner product",
            "Random matrices",
            "rank",
            "rank distribution"
        ],
        "doc_abstract": "Random binary matrices have found many applications in signal processing and coding. Rateless codes, for example, are based on the random generation of codewords by means of inner products between the data and random binary vectors. But the usefulness of random binary matrices is not limited to coding: they are also well suited to distributed data storage applications. In this context, random binary matrices with block-angular structure are of particular interest because they allow cooperative encoding and decentralized models for coding and decoding, with a built-in degree of parallelism. Linear programming, LUfactorization andQRfactorization are some of the problems for which the coarse-grain parallelization inherent in the block-angular structure is of interest. This paper studies one of the most important characteristics of block-angular matrices, their rank. More precisely, we study the rank distribution and full rank probability of rectangular random binary matrices and block-angular matrices in GF(2). © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the representability of complete genomes by multiple competing finite-context (Markov) models",
        "doc_scopus_id": "79959722141",
        "doc_doi": "10.1371/journal.pone.0021588",
        "doc_eid": "2-s2.0-79959722141",
        "doc_date": "2011-07-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "A finite-context (Markov) model of order k yields the probability distribution of the next symbol in a sequence of symbols, given the recent past up to depth k. Markov modeling has long been applied to DNA sequences, for example to find gene-coding regions. With the first studies came the discovery that DNA sequences are non-stationary: distinct regions require distinct model orders. Since then, Markov and hidden Markov models have been extensively used to describe the gene structure of prokaryotes and eukaryotes. However, to our knowledge, a comprehensive study about the potential of Markov models to describe complete genomes is still lacking. We address this gap in this paper. Our approach relies on (i) multiple competing Markov models of different orders (ii) careful programming techniques that allow orders as large as sixteen (iii) adequate inverted repeat handling (iv) probability estimates suited to the wide range of context depths used. To measure how well a model fits the data at a particular position in the sequence we use the negative logarithm of the probability estimate at that position. The measure yields information profiles of the sequence, which are of independent interest. The average over the entire sequence, which amounts to the average number of bits per base needed to describe the sequence, is used as a global performance measure. Our main conclusion is that, from the probabilistic or information theoretic point of view and according to this performance measure, multiple competing Markov models explain entire genomes almost as well or even better than state-of-the-art DNA compression methods, such as XM, which rely on very different statistical models. This is surprising, because Markov models are local (short-range), contrasting with the statistical models underlying other methods, where the extensive data repetitions in DNA sequences is explored, and therefore have a non-local character. © 2011 Pinho et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Genome analysis with distance to the nearest dissimilar nucleotide",
        "doc_scopus_id": "79952139725",
        "doc_doi": "10.1016/j.jtbi.2011.01.038",
        "doc_eid": "2-s2.0-79952139725",
        "doc_date": "2011-04-21",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "DNA may be represented by sequences of four symbols, but it is often useful to convert those symbols into real or complex numbers for further analysis. Several mapping schemes have been used in the past, but most of them seem to be unrelated to any intrinsic characteristic of DNA. The objective of this work was to study a mapping scheme that is directly related to DNA characteristics, and that could be useful in discriminating between different species.Recently, we have proposed a methodology based on the inter-nucleotide distance, which proved to contribute to the discrimination among species. In this paper, we introduce a new distance, the distance to the nearest dissimilar nucleotide, which is the distance of a nucleotide to first occurrence of a different nucleotide. This distance is related to the repetition structure of single nucleotides. Using the information resulting from the concatenation of the distance to the nearest dissimilar and the inter-nucleotide distance, we found that this new distance brings additional discriminative capabilities. This suggests that the distance to the nearest dissimilar nucleotide might contribute with useful information about the evolution of the species. © 2011 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 272314 291210 291838 31 Journal of Theoretical Biology JOURNALTHEORETICALBIOLOGY 2011-02-02 2011-02-02 2011-03-03T22:09:36 S0022-5193(11)00064-6 S0022519311000646 10.1016/j.jtbi.2011.01.038 S300 S300.1 FULL-TEXT 2021-04-04T17:17:36.312313Z 0 0 20110421 2011 2011-02-02T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0022-5193 00225193 275 275 1 1 Volume 275, Issue 1 7 52 58 52 58 20110421 21 April 2011 2011-04-21 2011 article fla Copyright © 2011 Elsevier Ltd. All rights reserved. GENOMEANALYSISDISTANCENEARESTDISSIMILARNUCLEOTIDE AFREIXO V 1 Introduction 2 Materials and methods 2.1 DNA sequences 2.2 Distance to the nearest dissimilar 2.2.1 Relationship between ND and IN distances 2.2.2 Comparison with an independent random process 2.3 Numerical procedure 3 Results 3.1 Distances analysis 3.2 Analysis of multiple organisms 4 Conclusion Acknowledgments References AFREIXO 2009 3064 3070 V ALBRECHTBUEHLER 2006 17828 17833 G ALBRECHTBUEHLER 2007 297 305 G ANASTASSIOU 2001 8 20 D BRODZIK 2005 373 376 A PROCEEDINGSIEEEICASSP SYMBOLBALANCEDQUATERNIONICPERIODICITYTRANSFORMFORLATENTPATTERNDETECTIONINDNASEQUENCES BULDYREV 1995 5084 5091 S CRISTEA 2003 871 888 P DING 2010 618 623 S HODGE 2000 3353 3354 T JEFFREY 1990 2163 2170 H LIAO 2005 196 199 B NAIR 2005 A PROCEEDINGSIEEEGENOMICSIGNALPROCESSING VISUALIZATIONGENOMICDATAUSINGINTERNUCLEOTIDEDISTANCESIGNALS NING 2003 509 510 J PROCEEDINGSIEEEBIOINFORMATICSCONFERENCE PRELIMINARYWAVELETANALYSISGENOMICSEQUENCES PRASADARJUN 2008 1795 1808 B PUIGBO 2007 1556 1558 P QI 2001 557 559 D QI 2004 1 11 J RANDIC 2008 84 88 M SILVERMAN 1986 295 300 B SIMS 2009 17077 17082 G VINGA 2003 513 523 S VOSS 1992 3805 3808 R ZHANG 1994 767 782 R AFREIXOX2011X52 AFREIXOX2011X52X58 AFREIXOX2011X52XV AFREIXOX2011X52X58XV 2021-03-25T21:03:31.419Z S0022519311000646 Fundação para a Ciência e Tecnologia European Social Fund ESF European Social Fund Portuguese Ministry of Science, Technology and Higher Education This work was supported in part by the FCT (Fundação para a Ciência e Tecnologia). S.P. Garcia acknowledges funding from the European Social Fund and the Portuguese Ministry of Science, Technology and Higher Education . item S0022-5193(11)00064-6 S0022519311000646 10.1016/j.jtbi.2011.01.038 272314 2011-03-04T07:21:24.255647-05:00 2011-04-21 true 310755 MAIN 7 61221 849 656 IMAGE-WEB-PDF 1 si0047 273 10 50 si0046 232 13 30 si0045 633 31 106 si0044 1404 28 336 si0043 210 13 40 si0042 1123 15 364 si0041 151 15 16 si0040 160 15 17 si0039 156 15 17 si0038 154 15 17 si0037 161 10 29 si0036 216 12 46 si0035 146 10 15 si0034 577 40 104 si0033 715 36 149 si0032 768 36 182 si0031 143 12 14 si0030 452 31 89 si0029 440 36 68 si0028 409 36 66 si0027 251 13 60 si0026 229 12 55 si0025 202 13 40 si0024 251 13 61 si0023 192 10 48 si0022 242 14 56 si0021 175 10 33 si0020 224 13 41 si0019 184 13 38 si0018 173 12 35 si0017 191 11 33 si0016 251 13 60 si0015 229 12 55 si0014 202 13 40 si0013 192 10 48 si0012 224 10 55 si0011 175 10 33 si0010 224 13 41 si0009 184 13 38 si0008 173 12 35 si0007 683 13 223 si0006 606 15 180 si0005 658 15 201 si0004 191 11 33 si0003 132 11 13 si0002 368 15 90 si0001 395 13 88 gr1 28940 387 482 gr1 3837 164 204 gr2 31003 353 483 gr2 3666 160 219 gr3 17279 298 386 gr3 2921 164 212 gr4 27679 595 388 gr4 1488 164 107 gr5 29915 595 388 gr5 1547 164 107 gr6 14607 267 207 gr6 2080 164 127 gr7 27582 293 388 gr7 3262 164 217 YJTBI 6350 S0022-5193(11)00064-6 10.1016/j.jtbi.2011.01.038 Elsevier Ltd Fig. 1 Distribution of the four nucleotide ND distance sequences for the Homo sapiens genome. The histogram is from the observed distances and the solid line shows the reference distribution with parameters estimated from the data. Fig. 2 Relative error of the four ND distances in the complete genome of Homo sapiens (first 20 distances). Fig. 3 Relative error for the global ND distance distribution in the complete genome of Homo sapiens (first 20 distances). Fig. 4 Phylogenetic tree with the species used in this study, using the 20 ND distances. Fig. 5 Phylogenetic tree with the species used in this study, using the IN–ND concatenation method. Fig. 6 Phylogenetic tree with the 10 mammals used in this study, using the IN–ND concatenation method. Fig. 7 Normalized split distances between phylogenetic trees constructed with multiple sequence alignment and free-alignment methods. Table 1 List of DNA builds used for each species. Species Reference Homo sapiens (human) Build 36.3 Pan troglodytes (chimpanzee) Build 2.1 Macaca mulatta (Rhesus macaque) Build 1.1 Mus musculus (mouse) Build 37.1 Rattus norvegicus (brown rat) Build 4.1 Equus caballus (horse) Build 2.1 Cannis familiaris (dog) Build 2.1 Bos taurus (cow) Build 4.1 Ornithorhynchus anatinus (platypus) Build 1.1 Monodelphis domestica (opossum) Build 2 Gallus gallus (chicken) Build 2.1 Xenopus tropicalis (Western clawed frog) Build 4.1 Danio rerio (zebrafish) Build 3.1 Apis mellifera (honey bee) Build 4.1 Caenorhabditis elegans (nematode) NC003279 Vitis vinifera (grape vine) Build 1.1 Populus trichocarpa (California poplar) Build 1.0 Arabidopsis thaliana (thale cress) AGI 7.2 Saccharomyces cerevisiae str.S228C (budding yeast) SGD 1 Schizosaccharomyces pombe (fission yeast) Build 1.1 Dictyostelium discoideum str.AX4 (amoeba) Build 2.1 Plasmodium falciparum3D7 (protozoon) Build 2.1 Escherichia coli str.K12 substr.MG1655 (bacterium) NC000913 Bacillus subtilis str.168 (bacterium) NC000964 Chlamydia trachomatis str.D/UW-3/CX (bacterium) NC000117 Mycoplasma genitalium str.G37 (bacterium) NC000908 Streptococcus mutans str.UA159 (bacterium) NC004350 Streptococcus pneumoniae str.ATCC 700669 (bacterium) NC011900 Aeropyrum pernix str.K1 (archaeota) NC000854 Table 2 Kullback–Leibler divergence values between the ND distance distribution of the four nucleotides in the Homo sapiens genome. A C G T A 0.00 0.04 0.04 0.00 C 0.03 0.00 0.00 0.03 G 0.03 0.00 0.00 0.03 T 0.00 0.04 0.04 0.00 Table 3 Normalized split distances of the 29 species. ND IN Random IN–ND 0.7692 0.1154 0.9888 Genome analysis with distance to the nearest dissimilar nucleotide Vera Afreixo a ⁎ Carlos A.C. Bastos b c Armando J. Pinho b c Sara P. Garcia b Paulo J.S.G. Ferreira b c a Department of Mathematics, University of Aveiro, 3810-193 Aveiro, Portugal b Signal Processing Lab, IEETA, University of Aveiro, 3810-193 Aveiro, Portugal c Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. DNA may be represented by sequences of four symbols, but it is often useful to convert those symbols into real or complex numbers for further analysis. Several mapping schemes have been used in the past, but most of them seem to be unrelated to any intrinsic characteristic of DNA. The objective of this work was to study a mapping scheme that is directly related to DNA characteristics, and that could be useful in discriminating between different species. Recently, we have proposed a methodology based on the inter-nucleotide distance, which proved to contribute to the discrimination among species. In this paper, we introduce a new distance, the distance to the nearest dissimilar nucleotide, which is the distance of a nucleotide to first occurrence of a different nucleotide. This distance is related to the repetition structure of single nucleotides. Using the information resulting from the concatenation of the distance to the nearest dissimilar and the inter-nucleotide distance, we found that this new distance brings additional discriminative capabilities. This suggests that the distance to the nearest dissimilar nucleotide might contribute with useful information about the evolution of the species. Keywords Alignment-free genome comparison Inter-nucleotide distances Nearest dissimilar distances DNA sequences 1 Introduction DNA sequences have been converted to numerical signals using different mappings. A commonly used mapping is to consider binary sequences that describe the position of each symbol (Voss, 1992). The binary representation is certainly one of the earliest and one of the most popular mappings of DNA. However, several other different mappings have been proposed (see for example Silverman and Linsker, 1986; Jeffrey, 1990; Zhang and Zhang, 1994; Buldyrev et al., 1995; Anastassiou, 2001; Cristea, 2003; Ning et al., 2003; Brodzik and Peters, 2005; Liao et al., 2005; Akhtar et al., 2007; Randic, 2008; Nair and Mahalakshmi, 2005; Afreixo et al., 2009). Some of the mappings used in DNA processing do not have a simple numerical interpretation and others do not have biological motivation. Also, some of the representations are not reversible and do not take into account the sequence structure. Currently, there is no ideal mapping to analyze every type of correlation in DNA sequences. In a previous work, we explored the inter-nucleotide (IN) distance, the distance to the first occurrence of the same symbol, to perform a comparative analysis between species (Afreixo et al., 2009). In this work, we present a new DNA numerical profile and a new mapping to explore the correlation structure of DNA: the distance to the nearest dissimilar (ND) nucleotide. This representation converts any DNA sequence into a unique numerical sequence with lower length, where each number represents the distance of a symbol to the next occurrence of a different symbol. We introduced also four sequences, one for each nucleotide, to represent the ND distances. This allows to perform comparative analysis between the behavior of the four nucleotides distance sequences and the global sequence. From the perspective of molecular evolution, DNA sequences may reflect both the results of random mutation and selective evolution. One should subtract the random background from the simple counting result in order to highlight the contribution of selective evolution (Qi et al., 2004; Ding et al., 2010). Therefore, we present an analysis of the relative error to highlight the contribution of selective evolution of the DNA of each species. This residual analysis may be used, for example, to perform multiple organism comparisons. Phylogenetic trees reproduce the evolutionary tree that represents the historical relationships between the species. Recent phylogenetic tree algorithms use nucleotide sequences. Typically, these trees are constructed with multiple sequence alignment (Hodge and Cope, 2000), which is a computationally demanding task. Recently, alignment-free methods have been proposed and present some advantages over multiple sequences alignment methods (see for example Sims et al., 2009; Vinga and Almeida, 2003). The distance that we address in this paper seems to possess discriminating properties that might be helpful in inferring phylogenies. We do believe that this claim is supported by the examples of trees that are provided. However, we also believe that, by itself, this distance measure does not convey all necessary information for building phylogenies. Instead, it should be regarded as potentially useful for working in cooperation and complementing other measures. 2 Materials and methods 2.1 DNA sequences In this study, we used the complete DNA sequences of 29 species: 27 were obtained from the National Center for Biotechnology Information (NCBI) Populus trichocarpa (California poplar) obtained from the Joint Genome Institute and Xenopus tropicalis (Western clawed frog) from Xenbase The species used in this work are listed in Table 1 . 2.2 Distance to the nearest dissimilar Consider the alphabet A = { A , C , G , T } and let s = ( s k ) k ∈ { 1 , … , N } be a symbolic sequence defined in A . Consider a numerical sequence, w x , that represents the distance to the nearest dissimilar of symbol x ∈ A . As an example, the four ND distance sequences for the short DNA fragment CAAACCGTTTAAGTAACAGGGATATTGGCCC are w A = ( 3 , 2 , 2 , 1 , 1 , 1 ) , w C = ( 1 , 2 , 1 , 3 ) , w G = ( 1 , 1 , 3 , 2 ) , w T = ( 3 , 1 , 1 , 2 ) . The global sequence of ND distances for this example is w = ( 1 , 3 , 2 , 1 , 3 , 2 , 1 , 1 , 2 , 1 , 1 , 3 , 1 , 1 , 1 , 2 , 2 , 3 ) . Note that the ND distance of each nucleotide corresponds to the repeat length of that nucleotide. Algorithm 1 Computation of w for sequence s. p ≔ 1 p ′ ≔ 1 while p ′ ≤ N do i ≔ 0 while s p ′ = s p ′ + i do i ≔ i + 1 end while w p ≔ i p ≔ p + 1 p ′ ≔ p ′ + i end while Algorithm 2 Computation of w x with x ∈ A . p ≔ 1 p ′ ≔ 1 while p ′ ≤ N do i ≔ 0 while s p ′ + i = ′ x ′ do i ≔ i + 1 end while if i=0 then p ′ ≔ p ′ + 1 else w p ≔ i p ≔ p + 1 p ′ ≔ p ′ + i end if end while Note that (1) ∑ i = 1 L w i = N , where L is the length of w. Let n i be the number of occurrences of ND distance i, then (2) ∑ i = 1 K n i i = N , where K is the largest ND distance present in the data sequence. The mean distance is (3) ∑ i = 1 L w i L = N L . 2.2.1 Relationship between ND and IN distances In general, the ND distance distribution complements the information that is accumulated in the first IN distance. We recall that the IN distance is the distance to the first occurrence of the same nucleotide. Let n ′ i be the absolute frequency of the ith IN distance, (4) n ′ 1 = ∑ i = 2 K n i ( i − 1 ) + δ = N − L + δ and (5) ∑ i = 2 K ′ n ′ i = ∑ i = 1 K n i − δ = L − δ , where δ = 0 , s 1 ≠ s N , 1 , s 1 = s N , and K ′ is the largest IN distance present in the data sequence. The last equation can be described by the following sentence: the number of IN distances greater than one is equal to the total number of ND distances. As an illustration, the computed values for the previous DNA fragment example are N=31, n ′ 1 = 14 , L=18, δ = 1 . 2.2.2 Comparison with an independent random process In order to calculate some statistical properties of various genomes, we will study the characteristics of the ND distance distribution. Consider p A , p C , p G and p T the occurrence probabilities of nucleotides A, C, G and T, respectively. If the nucleotide sequences were generated by an independent and identically distributed (i.i.d.) random process, then each of the distances, W x , would follow a geometric distribution of parameter 1−p x . In fact, the probability distribution of the distance to the nearest dissimilar for symbol x is (6) f x ( k ) = P ( W x = k ) = P ( W = k | x ) = ( 1 − p x ) ( p x ) k − 1 , k = 1 , 2 , … Fig. 1 shows the measured and the reference distributions of the ND distance sequences for the Homo sapiens complete genome. Although the ND distance distribution from DNA shows an exponential behavior, it differs from the reference distribution. This is expected, since the reference distribution was established under the assumption of an i.i.d. random process (with constant nucleotide relative frequencies estimated from the DNA sequence). We performed the chi-square goodness of fit test with the result that the distributions are significantly different (all p values < 10 − 4 ). In order to measure the differences between the observed and the reference distributions we used the Kullback–Leibler divergence. Assuming that the DNA sequence was generated by an independent random process with constant parameters, the corresponding global ND distance sequence distribution is given by (7) f ( k ) = P ( W = k ) = ∑ x ∈ A P ( W = k | x ) P ( x ) = ∑ x ∈ A ( 1 − p x ) ( p x ) k . 2.3 Numerical procedure The histograms of the ND distance sequences were computed for each nucleotide and also for the global sequence. For large genomes and for convenience, the sequences were divided into blocks of 500000 symbols. This procedure does not influence the total nucleotide counts. For eukaryote genomes, the chromosomes were processed separately and the resulting distance histograms were stored separately. All the symbols in the sequence that did not correspond to one of the four nucleotides were removed from the sequences before further processing. We setup to investigate how similar (or different) are the observed and the reference distributions of: • the four nucleotides of Homo sapiens; • the chromosomes of Homo sapiens; • various species. In order to facilitate the visual comparison of the various distance distributions with the theoretical one and to subtract the random background, we used the relative error, as given by (8) r ( k ) = f o ( k ) − f ( k ) f o ( k ) , where f o ( k ) is the observed relative frequency of the distance k and f(k) is the relative frequency of the reference distribution. For the prokaryote species the values of the relative error of the ND distance were truncated to values between −1 and 1. We also used the IN distance relative error used in Afreixo et al. (2009) and we concatenated the first 100 IN distances and the first 20 ND distance relative errors. Only the first 20 ND distances were used, because for larger distances the distributions become sparse. We have found that the limitation to the first 20 distances, which was carried out in all numerical experiments described in this paper, provides an adequate compromise between the information content and the vector length. Note that 792 is the largest ND distance for all the studied species and that the maximum proportion of the ND distances above 20 is 0.13%. 3 Results 3.1 Distances analysis We compared the observed ND distance distributions of all Homo sapiens chromosomes and we obtained very small Kullback–Leibler divergence ( < 0.0008 ). We have also compared, using the Kullback–Leibler divergence, the ND distance distribution of the four nucleotides of the Homo sapiens complete genome. The results of the comparative tests are shown in Table 2 , showing that the ND distance distribution of nucleotide A is closer to that of nucleotide T than to the other two nucleotides, and the ND distance distribution of C is closer to that of nucleotide G than to the other two nucleotides. Notice that the DNA complementary sequence was not used in the computation of the distributions. These identical distance distributions for the nucleotides A/T and C/G are present in all the human chromosomes and also in the genome of all the other species used in this work. This may be explained by the Chargaff's second parity rule and the extension of this rule (see for example Qi and Cuticchia, 2001; Albrecht-Buehler, 2006, 2007). We used the relative error, as defined in (8), to compare the ND distance distributions, both for nucleotides and global sequences, with the reference distributions. This corresponds to removing the contribution of the random background to the ND distance distribution (Qi et al., 2004). Fig. 2 shows the relative error for the ND distance for each nucleotide of the Homo sapiens genome and Fig. 3 the relative error for the global ND distance sequence. A relative error close to zero means that the observed frequency is similar to that of the reference distribution. Values close to one imply observed distances much more frequent than in the reference distribution. For the global ND distance (Fig. 3) of the human genome, the first two distances have a lower frequency than the one corresponding to random sequences, whereas the other distances have higher frequencies. The relative frequencies of distances above two are higher than in the reference distribution. The repetition of three or more nucleotides is more frequent in the human genome than in a random sequence. This pervasive existence of repeats is a well known characteristic of the human genome. 3.2 Analysis of multiple organisms The ND distance relative error vectors of each complete genome may be used as a genomic signature that identifies each species, thus allowing the comparison of species (e.g. by building phylogenetic trees). We used the UPGMA programs in the PHYLIP package to build the phylogenetic trees (the similarity matrix was computed using the Euclidean distance). The comparison of the various phylogenetic trees was carried out with TOPD/FMTS software (Puigbo et al., 2007) and using the split distance (a low split distance value is synonymous of a high number of common branches between the two trees). Hierarchical clustering was applied to a matrix composed by the first 20 ND distances for all the species used in this study. Fig. 4 shows the phylogenetic tree for all the species in this study. The phylogenetic tree (obtained with complete linkage) displays a first branching between prokaryotes and eukaryotes, showing most of the mammals together. Moreover, all vertebrates are in the same cluster, the only two non-vertebrates in that cluster are A. mellifera and V. vinifera. Fungi are also grouped together. The bacteria are displayed close together and the archaeota further apart. With the ND distance relative errors we obtained an interesting tree that shows some of the evolutive features. However, we could point several unexpected clusters which may be due to the use of small vectors (20 elements) to characterize the species. In parallel, we have developed methodologies based on the IN distances that can be complemented with information from the ND distances. These two methodologies are complementary: both describe distances between nucleotides, but in two different ways (see Section 2.2.1). Fig. 5 shows the phylogenetic tree constructed with a species vector that corresponds to the concatenation of the IN (removing the first IN distance) and ND relative errors. We compare the results obtained by the IN–ND concatenation method with the IN, ND and random methods in Table 3 . The concatenation method obtains a tree topology similar to the IN method, but the changes introduced produced a better phylogenetic tree, including a higher fungi and archaeota differentiation. The differences between Figs. 4 and 5 may arise from the use of larger vectors in Fig. 5 (119 elements), thus conveying more information than the ND distance vectors (Fig. 4). Moreover, as already mentioned, the ND distance complements the information accumulated in the first IN distance. In order to compare alignment and alignment-free algorithms to construct phylogenetic trees (Sims et al., 2009) used 10 mammals. Fig. 6 shows the phylogenetic tree constructed for the same set of mammals as used by Sims et al. (2009), using the IN–ND concatenation method. The topology of the phylogenetic tree obtained using the IN–ND concatenation method is similar to the phylogenetic trees presented by Sims et al. (2009) for alignment based algorithms (Type I, Type II, Prasad Arjun et al., 2008). Fig. 7 shows the normalized split distance between alignment algorithms and alignment-free algorithms (Sims's methods and our methods). The normalized split distance is the number of splits in one tree, but not present in the other, divided by the total number of splits in the tree. The IN–ND concatenation method shows a small split distance value (similar to those obtained by Sims et al., 2009) which reveals that the IN–ND concatenation contains relevant information about the phylogenetic evolution. 4 Conclusion The ND distance mapping contains information about nucleotide repetition structure in the DNA sequence and characterizes the lengths of the single nucleotide repeats. An interesting feature of the ND distance approach is the strong similarity found for the A/T and C/G nucleotides. This may be explained by the existence of inverted repeats and by the second Chargaff parity rule and its extensions (Qi and Cuticchia, 2001; Albrecht-Buehler, 2006, 2007). Since the ND distance vectors are small (around 20 elements) and the IN distance characterizes the distance between groups of single nucleotide repeats, we concatenated the IN and ND distances in order to obtain a vector that better differentiates species. The results obtained in this work suggest that, for the addressed species, there is a genetic signature, a vector with the concatenation of the relative error of the IN and ND distances, that is a distinguishing characteristic of each species. Acknowledgments This work was supported in part by the FCT (Fundação para a Ciência e Tecnologia). S.P. Garcia acknowledges funding from the European Social Fund and the Portuguese Ministry of Science, Technology and Higher Education. References Afreixo et al., 2009 V. Afreixo C.A.C. Bastos A.J. Pinho S.P. Garcia P.J.S.G. Ferreira Genome analysis with inter-nucleotide distances Bioinformatics 25 2009 3064 3070 Akhtar et al., 2007 Akhtar, M., Epps, J., Ambikairajah, E., 2007. On DNA numerical representation for period-3 based exon prediction. In: Fifth International Workshop on Genomic Signal Processing and Statistics. Albrecht-Buehler, 2006 G. Albrecht-Buehler Asymptotically increasing compliance of genomes with Chargaff's second parity rules through inversions and inverted transpositions Proceedings of the National Academy of Sciences of the United States of America 103 2006 17828 17833 Albrecht-Buehler, 2007 G. Albrecht-Buehler Inversions and inverted transpositions as the basis for an almost universal “format” of genome sequences Genomics 90 2007 297 305 Anastassiou, 2001 D. Anastassiou Genomic signal processing IEEE Signal Processing Magazine 18 2001 8 20 Brodzik and Peters, 2005 A.K. Brodzik O. Peters Symbol-balanced quaternionic periodicity transform for latent pattern detection in DNA sequences Proceedings of IEEE ICASSP 2005 373 376 Buldyrev et al., 1995 S. Buldyrev A. Goldberger S. Havlin R. Mantegna M. Matsa Long-range correlation properties of coding and noncoding DNA sequences: GenBank analysis Physical Review E 51 1995 5084 5091 Cristea, 2003 P.D. Cristea Large scale features in DNA genomic signals Signal Processing 83 2003 871 888 Ding et al., 2010 S. Ding Q. Dai H. Liu T. Wang A simple feature representation vector for phylogenetic analysis of DNA sequences Journal of Theoretical Biology 265 2010 618 623 Hodge and Cope, 2000 T. Hodge M.J.T.V. Cope A myosin family tree Journal of Cell Science 113 2000 3353 3354 Jeffrey, 1990 H.J. Jeffrey Chaos game representation of gene structure Nucleic Acids Research 18 1990 2163 2170 Liao et al., 2005 B. Liao M. Tan K. Ding Application of 2-d graphical representation of DNA sequence Chemical Physics Letters 401 2005 196 199 Nair and Mahalakshmi, 2005 A.S.S. Nair T. Mahalakshmi Visualization of genomic data using inter-nucleotide distance signals Proceedings of IEEE Genomic Signal Processing 2005 Ning et al., 2003 J. Ning N. Moore J.C. Nelson Preliminary wavelet analysis of genomic sequences Proceedings of IEEE Bioinformatics Conference 2003 509 510 Prasad Arjun et al., 2008 B. Prasad Arjun W. Allard Marc D. Green Eric Confirming the phylogeny of mammals by use of large comparative sequence data sets Journal of Molecular Evolution 25 2008 1795 1808 Puigbo et al., 2007 P. Puigbo S. Garcia-Vallve J.O. McInerney TOPD/FMTS: a new software to compare phylogenetic trees Bioinformatics 23 2007 1556 1558 Qi and Cuticchia, 2001 D. Qi A.J. Cuticchia Compositional symmetries in complete genomes Bioinformatics 17 2001 557 559 Qi et al., 2004 J. Qi B. Wang B.I. Hao Whole proteome prokaryote phylogeny without sequence alignment: a K-string composition approach Journal of Molecular Evolution 58 2004 1 11 Randic, 2008 M. Randic Another look at the chaos-game representation of DNA Chemical Physics Letters 456 2008 84 88 Silverman and Linsker, 1986 B.D. Silverman R. Linsker A measure of DNA periodicity Journal of Theoretical Biology 118 1986 295 300 Sims et al., 2009 G.E. Sims S.R. Jun G.A. Wu S.H. Kim Whole-genome phylogeny of mammals: evolutionary information in genic and nongenic regions Proceedings of the National Academy of Sciences of the United States of America 106 2009 17077 17082 Vinga and Almeida, 2003 S. Vinga J. Almeida Alignment-free sequence comparison—a review Bioinformatics 19 2003 513 523 Voss, 1992 R.F. Voss Evolution of long-rang fractal correlations and 1/f noise in DNA base sequences Physical Review Letters 68 1992 3805 3808 Zhang and Zhang, 1994 R. Zhang C.T. Zhang Z curves, an intuitive tool for visualising and analysing the DNA sequences Journal of Biomolecular Structure and Dynamics 11 1994 767 782 "
    },
    {
        "doc_title": "Minimal absent words in prokaryotic and eukaryotic genomes",
        "doc_scopus_id": "79551646687",
        "doc_doi": "10.1371/journal.pone.0016065",
        "doc_eid": "2-s2.0-79551646687",
        "doc_date": "2011-02-09",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Minimal absent words have been computed in genomes of organisms from all domains of life. Here, we explore different sets of minimal absent words in the genomes of 22 organisms (one archaeota, thirteen bacteria and eight eukaryotes). We investigate if the mutational biases that may explain the deficit of the shortest absent words in vertebrates are also pervasive in other absent words, namely in minimal absent words, as well as to other organisms. We find that the compositional biases observed for the shortest absent words in vertebrates are not uniform throughout different sets of minimal absent words. We further investigate the hypothesis of the inheritance of minimal absent words through common ancestry from the similarity in dinucleotide relative abundances of different sets of minimal absent words, and find that this inheritance may be exclusive to vertebrates. © 2011 Garcia et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-dinucleotide distances in the human genome: an analysis of the whole-genome and protein-coding distributions.",
        "doc_scopus_id": "84855542023",
        "doc_doi": "10.1515/jib-2011-172",
        "doc_eid": "2-s2.0-84855542023",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "MLCS",
            "MLOWN"
        ],
        "doc_abstract": "We study the inter-dinucleotide distance distributions in the human genome, both in the whole-genome and protein-coding regions. The inter-dinucleotide distance is defined as the distance to the next occurrence of the same dinucleotide. We consider the 16 sequences of inter-dinucleotide distances and two reading frames. Our results show a period-3 oscillation in the protein-coding inter-dinucleotide distance distributions that is absent from the whole-genome distributions. We also compare the distance distribution of each dinucleotide to a reference distribution, that of a random sequence generated with the same dinucleotide abundances, revealing the CG dinucleotide as the one with the highest cumulative relative error for the first 60 distances. Moreover, the distance distribution of each dinucleotide is compared to the distance distribution of all other dinucleotides using the Kullback-Leibler divergence. We find that the distance distribution of a dinucleotide and that of its reversed complement are very similar, hence, the divergence between them is very small. This is an interesting finding that may give evidence of a stronger parity rule than Chargaff's second parity rule. Copyright 2011 The Author(s). Published by Journal of Integrative Bioinformatics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An efficient omnidirectional vision system for soccer robots: From calibration to object detection",
        "doc_scopus_id": "79952628276",
        "doc_doi": "10.1016/j.mechatronics.2010.05.006",
        "doc_eid": "2-s2.0-79952628276",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Multi-robot systems",
            "Omnidirectional vision system",
            "Robotic soccer team",
            "Robotic vision",
            "Self localization",
            "Shape based",
            "Technical challenges",
            "Vision systems"
        ],
        "doc_abstract": "Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-06-19 2010-06-19 2011-03-08T22:21:19 S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 S300 S300.1 FULL-TEXT 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-06-19T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 6 399 410 399 410 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright © 2010 Elsevier Ltd. All rights reserved. EFFICIENTOMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTSCALIBRATIONOBJECTDETECTION NEVES A 1 Introduction 2 Architecture of the vision system 3 Calibration of the vision system 3.1 Self-calibration of the digital camera parameters 3.1.1 Proposed algorithm 3.1.2 Experimental results 3.2 Distance map calibration 4 Color-based object detection 4.1 Color extraction 4.2 Object detection 4.3 Experimental results 5 Arbitrary ball detection 5.1 Related work 5.2 Proposed approach 5.3 Experimental results 6 Conclusions Acknowledgment References LIMA 2001 87 102 P ASTROM 1995 K PIDCONTROLLERSTHEORYDESIGNTUNING BAKER 1999 175 196 S BRESENHAM 1965 25 30 J TREPTOW 2004 41 48 A GRIMSON 1990 1255 1274 W NEVESX2011X399 NEVESX2011X399X410 NEVESX2011X399XA NEVESX2011X399X410XA item S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 true 1828663 MAIN 12 84396 849 656 IMAGE-WEB-PDF 1 si4 1540 53 289 si3 1436 46 293 si2 1250 46 238 si1 1092 46 192 gr10 45233 182 377 gr10 14653 106 219 gr11 18028 138 376 gr11 2966 81 219 gr12 43502 281 757 gr12 6024 81 219 gr13 31126 208 377 gr13 11667 121 219 gr14 7740 60 489 gr14 1994 27 219 gr15 19330 223 373 gr15 3452 131 219 gr16 43365 197 765 gr16 5658 56 219 gr17 11813 182 264 gr17 4751 151 219 gr18 9049 172 358 gr18 2922 105 219 gr19 8089 119 217 gr19 4702 121 219 gr2 27937 199 271 gr2 19723 161 219 gr20 23395 140 484 gr20 5238 63 219 gr21 24408 220 361 gr21 4625 133 219 gr3 57308 455 703 gr3 8598 142 219 gr4 25516 139 381 gr4 10182 80 219 gr5 22239 199 575 gr5 3068 76 219 gr6 26663 137 374 gr6 10562 80 219 gr7 61745 405 533 gr7 15882 164 215 gr8 53286 374 533 gr8 11776 154 219 gr9 33410 179 378 gr9 13228 104 219 gr1 39196 225 378 gr1 14502 130 219 MECH 1160 S0957-4158(10)00086-3 10.1016/j.mechatronics.2010.05.006 Elsevier Ltd Fig. 1 The CAMBADA team playing at RoboCup 2009, Graz, Austria. Fig. 2 On the left, a detailed view of the CAMBADA vision system. On the right, one of the robots. Fig. 3 Some experiments using the automated calibration procedure. At the top, results obtained starting with all the parameters of the camera set to zero. At the bottom, results obtained with all the parameters set to the maximum value. On the left, the initial image acquired. In the middle, the image obtained after applying the automated calibration procedure. On the right, the graphics showing the evolution of the parameters along the time. Fig. 4 On the left, an example of an image acquired with the camera parameters in auto-mode. On the right, an image acquired after applying the automated calibration algorithm. Fig. 5 The histogram of the intensities of the two images presented in Fig. 4. In (a) it is shown the histogram of the image obtained with the camera parameters set to the maximum value. (b) Shows the histogram of the image obtained after applying the automated calibration procedure. Fig. 6 On the left, an image acquired outdoors using the camera in auto-mode. As it is possible to observe, the colors are washed out. This happens because the camera’s auto-exposure algorithm tries to compensate the black region around the mirror. On the right, the same image with the camera calibrated using our algorithm. As can be seen, the colors and the contours of the objects are much more defined. Fig. 7 A screenshot of the tool developed to calibrate some important parameters of the vision system, namely the inverse distance map, the mirror and robot center and the regions of the image to be processed. Fig. 8 A screenshot of the interface to calibrate some important parameters need to obtain the inverse distance map (these parameters are described in [17]). Fig. 9 Acquired image after reverse-mapping into the distance map. On the left, the map was obtained with all misalignment parameters set to zero. On the right, after automatic correction. Fig. 10 A 0.5m grid, superimposed on the original image. On the left, with all correction parameters set to zero. On the right, the same grid after geometrical parameter extraction. Fig. 11 On the left, the position of the radial search lines used in the omnidirectional vision system, after detecting the center of the robot in the image using the tool described in this section. On the right, an example of a robot mask used to select the pixels to be processed, obtained with the same tool. White points represent the area that will be processed. Fig. 12 The software architecture of the omnidirectional vision system. Fig. 13 A screenshot of the application used to calibrate the color ranges for each color class using the HSV color space. Fig. 14 An example of a transition. “G” means green pixel, “W” means white pixel and “X” means pixel with a color different from green or white, for example resulting due to some noise or a not perfect color calibration. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 15 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 16 On the left, an example of an original image acquired by the omnidirectional vision system. In the center, the corresponding image of labels. On the right, the color blobs detected in the images. Marks over the ball point to the mass center. The several marks near the white lines (magenta) are the position of the white lines. The cyan marks are the position of the obstacles. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 17 Experimental results obtained by the omnidirectional system using the color ball detection. In this experiment, the ball was positioned in the center of the field, position (0,0). The robot performed a predefined trajectory while the position of the ball and the robot was recorded. Both axes in the graphics are in meters. Fig. 18 The circular Hough transform. a and b represent the parameter space that in this application are the radius of the ball and the distance to robot, respectively. Fig. 19 Example of a circle detection through the use of the circular Hough transform. Fig. 20 Example of a captured image using the proposed approach. The cross over the ball points out the detected position. In (b) the image (a), with the Canny edge detector applied. In (c), the image (b) after applying the circular Hough transform. Fig. 21 Experimental results obtained by the omnidirectional system using the morphological ball detection. In this experience, the ball was positioned in the penalty mark of the field. The robot performed a predefined trajectory while the position of the ball was recorded. Both axes in the graphics are in meters. Table 1 Statistical measures obtained for the images presented in Figs. 3 and 4. The initial values refer to the images obtained with the camera before applying the proposed automated calibration procedure. The final values refer to the images acquired with the cameras configured with the proposed algorithm. Experiment – ACM μ E MSV Parameters set to zero Initial 111.00 16.00 0.00 1.00 Final 39.18 101.95 6.88 2.56 Parameters set to maximum Initial 92.29 219.03 2.35 4.74 Final 42.19 98.59 6.85 2.47 Camera in auto-mode Initial 68.22 173.73 6.87 3.88 Final 40.00 101.14 6.85 2.54 An efficient omnidirectional vision system for soccer robots: From calibration to object detection António J.R. Neves ⁎ Armando J. Pinho Daniel A. Martins Bernardo Cunha ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. Keywords Robotic vision Omnidirectional vision systems Color-based object detection Shape-based object detection Vision system calibration 1 Introduction The Middle Size League (MSL) of RoboCup is a forum where several research areas have been challenged for proposing solutions to well-defined practical problems. The robotic vision is one of those areas and, for most of the MSL teams, it has become the only way of sensing the surrounding world. From the point of view of a robot, the playing field during a game provides a fast-changing scenery, where the teammates, the opponents and the ball move quickly and often in an unpredictable way. The robots have to capture these scenes through their cameras and have to discover where the objects of interest are located. There is no time for running complex algorithms. Everything has to be computed and decided in a small fraction of a second, for allowing real-time operation; otherwise, it becomes useless. Real-time is not the only challenge that needs to be addressed. Year after year, the initially well controlled and robot friendly environment where the competition takes place has become increasingly more hostile. Conditions that previously have been taken for granted, such as controlled lighting or easy to recognize color coded objects, have been relaxed or even completely suppressed. Therefore, the vision system of the robots needs to be prepared for adapting to strong lighting changes during a game, as well as, for example, for ball-type changes across games. In this paper, we provide a comprehensive description of the vision system of the MSL CAMBADA team (Fig. 1 ). Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture (CAMBADA) is the RoboCup MSL soccer team of the Institute of Electronics and Telematics Engineering of Aveiro (IEETA) research institute, University of Aveiro, Portugal. The team, which started officially in October 2003, won the 2008 MSL RoboCup World Championship and ranked 3rd in the 2009 edition. We start by presenting and explaining the hardware architecture of the vision system used by the robots of the CAMBADA team, which relies on an omnidirectional vision system (Section 2). Then, we proceed with the description of the approach that we have adopted regarding the calibration of a number of crucial parameters and in the construction of auxiliary data structures (Section 3). Concerning the calibration of the intrinsic parameters of the digital camera, we propose an automated calibration algorithm that is used to configure the most important features of the camera, namely, the saturation, exposure, white-balance, gain and brightness. The proposed algorithm uses the histogram of intensities of the acquired images and a black and a white area, known in advance, to estimate the referred parameters. We also describe a general solution to calculate the robot centered distances map, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. The soccer robots need to locate several objects of interest, such as the ball, the opponent robots and the teammates. Moreover, they also need to collect information for self-localization, namely, the position of the field white lines. For these tasks, we have developed fast and efficient algorithms that rely on color information. The color extraction algorithms are based on lookup tables and use a radial model for color object detection. Due to the severe restrictions imposed by the real-time constraint, some of the image processing tasks are implemented using a multi-threading approach and use special data structures to reduce the processing time. Section 4 provides a detailed description of these algorithms. As previously mentioned, the color codes assigned to the objects of interest tend to disappear as the competition evolves. For example, the usual orange ball used in the MSL will soon be replaced by an arbitrary FIFA ball, increasing the difficulty in locating one of the most important objects in the game. Anticipating this scenario, we developed a fast method for detecting soccer balls independently of their colors. In Section 5, we describe a solution based on the morphological analysis of the image. The algorithm relies on edge detection and on the circular Hough transform, attaining a processing time almost constant and complying with the real-time constraint. Its appropriateness has been clearly demonstrated by the results obtained in the mandatory technical challenge of the RoboCup MSL: 2nd place in 2008 and 1st place in 2009. 2 Architecture of the vision system The CAMBADA robots [1] use a catadioptric vision system, often named omnidirectional vision system, based on a digital video camera pointing at a hyperbolic mirror, as presented in Fig. 2 . We are using a digital camera Point Grey Flea 2, 1 Last accessed: 18/02/2010. 1 FL2-08S2C with a 1/3” CCD Sony ICX204 that can deliver images up to 1024×768 pixels in several image formats, namely RGB, YUV 4:1:1, YUV 4:2:2 or YUV 4:4:4. The hyperbolic mirror was developed by IAIS Fraunhofer Gesellschaft 2 Last accessed: 18/02/2010. 2 (FhG-AiS). Although the mirror was designed for the vision system of the FhG Volksbot 3 Last accessed: 18/02/2010. 3 we are achieving also an excellent result with it in our vision system. The use of omnidirectional vision systems have captured much interest in the last years, because it allows a robot to attain a 360° field of view around its central vertical rotation axis, without having to move itself or its camera. In fact, it has been a common solution for the main sensorial element in a significant number of autonomous mobile robot applications, as is the case of the MSL, where most of the teams have adopted this approach [2–9]. A catadioptric vision system ensures an integrated perception of all major target objects in the surrounding area of the robot, allowing a higher degree of maneuverability. However, this also implies higher degradation in the resolution with growing distances away from the robot, when compared to non-isotropic setups. 3 Calibration of the vision system An important task in the MSL is the calibration of the vision system. This includes the calibration of intrinsic parameters of the digital camera, the computation of the inverse distance map, the detection of the mirror and robot center and the definition of the regions of the image that have to be processed. Calibration has to be performed when environmental conditions change, such as playing in a different soccer field or when the lighting conditions vary over time. Therefore, there are adjustments that have to be made almost continuously, for example if the playing field is unevenly illuminated, or less frequently, when the playing field changes. Moreover, a number of adjustments have also to be performed when some of the vision hardware of the robot is replaced, such as the camera or the mirror. All these calibrations and adjustments should be robust, i.e., they should be as much as possible insensitive to small environmental variations, they should be fast to perform and they should be simple to execute, so that no special calibration expert is required to operate them. 3.1 Self-calibration of the digital camera parameters In a near future, it is expected that the MSL robots will have to play under natural lighting conditions and in outdoor fields. This introduces new challenges. In outdoor fields, the illumination may change slowly during the day, due to the movement of the sun, but also may change quickly in short periods of time due to a partial and temporally varying covering of the sun by clouds. In this case, the robots have to adjust, in real-time, both the color segmentation values as well as some of the camera parameters, in order to adapt to new lighting conditions [10]. The common approach regarding the calibration of the robot cameras in the MSL has been based on manual adjustments, that are performed prior to the games, or through some automatic process that runs offline using a pre-acquired video sequence. However, most (or even all) of the parameters remain fixed during the game. We propose an algorithm that does not require human interaction to configure the most important parameters of the camera, namely the exposure, the white-balance, the gain and the brightness. Moreover, this algorithm runs continuously, even during the game, allowing coping with environmental changes that often occur when playing. We use the histogram of intensities of the acquired images and a black and a white area, which location is known in advance, to estimate the referred parameters of the camera. Note that this approach differs from the well known problem of photometric camera calibration (a survey can be found in [11]), since we are not interested in obtaining the camera response values, but only to configure its parameters according to some measures obtained from the acquired images. The self-calibration process for a single robot requires a few seconds, including the time necessary to start the application. This is significantly faster than the usual manual calibration by an expert user, for which several minutes are needed. 3.1.1 Proposed algorithm The proposed calibration algorithm processes the image acquired by the camera and analyzes a white area in the image (a white area in a fixed place on the robot body, near the camera in the center of the image), in order to calibrate the white-balance. A black area (we use a part of the image that represents the robot itself, actually a rectangle in the upper left side of the image) is used to calibrate the brightness of the image. Finally, the histogram of the image intensities is used to calibrate the exposure and gain. The histogram of the intensities of an image is a representation of the number of times that each intensity value appears in the image. For an image represented using 8 bits per pixel, the possible values are between 0 and 255. Image histograms can indicate some aspects of the lighting conditions, particularly the exposure of the image and whether if it is underexposed or overexposed. The assumptions used by the proposed algorithm are the following: (i) The white area should appear white in the acquired image. In the YUV color space, this means that the average value of U and V should be close to 127, that is to say, the chrominance components of the white section should be as close to zero as possible. If the white-balance is not correctly configured, these values are different from 127 and the image does not have the correct colors. The white-balance parameter is composed by two values, WB_BLUE and WB_RED, directly related to the values of U and V, respectively. (ii) The black area should be black. In the RGB color space, this means that the average values of R, G and B should be close to zero. If the brightness parameter is too high, it is observed that the black region becomes blue, resulting in a degradation of the image. (iii) The histogram of intensities should be centered around 127 and should span all intensity values. Dividing the histogram into regions, the left regions represent dark colors, while the right regions represent light colors. An underexposed image will be leaning to the left, while an overexposed image will be leaning to the right in the histogram (for an example, see Fig. 5a). The values of the gain and exposure parameters are adjusted according to the characteristic of the histogram. Statistical measures can be extracted from the images to quantify the image quality [12,13]. A number of typical measures used in the literature can be computed from the image gray level histogram, namely, the mean (1) μ = ∑ i = 0 N - 1 iP i , μ ∈ [ 0 , 255 ] , the entropy (2) E = - ∑ i = 0 N - 1 P i log ( P i ) , E ∈ [ 0 , 8 ] , the absolute central moment (3) ACM = ∑ i = 0 N - 1 | i - μ | P i , ACM ∈ [ 0 - 127 ] and the mean sample value (4) MSV = ∑ j = 0 4 ( j + 1 ) x j ∑ j = 0 4 x j , MSV ∈ [ 0 - 5 ] , where N is the number of possible gray values in the histogram (typically, 256), P i is the relative frequency of each gray value and x j is the sum of the gray values in region j of the histogram (in the proposed approach we divided the histogram into five regions). When the histogram values of an image are uniformly distributed in the possible values, then μ ≈127, E ≈8, ACM ≈60 and MSV ≈2.5. In the experimental results we use these measures to analyze the performance of the proposed calibration algorithm. Moreover, we use the information of MSV to calibrate the exposure and the gain of the camera. The algorithm is depicted next. do do acquire image calculate the histogram of intensities calculate the MSV value if MSV<2.0 OR MSV>3.0 apply the PI controller to adjust exposure else apply the PI controller to adjust gain set the camera with new exposure and gain values while exposure or gain parameters change do acquire image calculate average U and V values of the white area apply the PI controller to adjust WB_BLUE apply the PI controller to adjust WB_RED set the camera with new white-balance parameters while white-balance parameters change do acquire image calculate average R, G and B values of the black area apply the PI controller to adjust brightness set the camera with new brightness value while brightness parameter change while any parameter changed The calibration algorithm configures one parameter at a time, proceeding to the next one when the current one has converged. For each of these parameters, a PI controller was implemented. PI controllers are used instead of proportional controllers as they result in better control, having no stationary error. The coefficients of the controller were obtained experimentally: first, the proportional gain was increased until the camera parameter started to oscillate. Then, it was reduced to about 70% of that value and the integral gain was increased until an acceptable time to reach the desired reference was obtained [14]. The algorithm stops when all the parameters have converged. More details regarding this algorithm can be found in [15]. 3.1.2 Experimental results To measure the performance of this calibration algorithm, tests have been conducted using the camera with different initial configurations. In Fig. 3 , results are presented both when the algorithm starts with the parameters of the camera set to zero, as well as when set to the maximum value. As can be seen, the configuration obtained after running the proposed algorithm is approximately the same, independently of the initial configuration of the camera. Moreover, the algorithm is fast to converge (it takes between 60 and 70 frames). In Fig. 4 , it is presented an image acquired with the camera in auto-mode. As can be seen, the image obtained using the camera with the parameters in auto-mode is overexposed and the white balance is not configured correctly. This is due to the fact that the camera analyzes the entire image and, as can be observed in Fig. 3, there are large black regions corresponding to the robot itself. Our approach uses a mask to select the region of interest, in order to calibrate the camera using exclusively the valid pixels. Moreover, and due to the changes in the environment when the robot is moving, leaving the camera in auto-mode leads to undesirable changes in the parameters of the camera, causing color classification problems. Table 1 presents the values of the statistical measures described in (1)–(4), regarding the experimental results presented in Fig. 3. These results confirm that the camera is correctly configured after applying the automated calibration procedure, since the results obtained are close to the optimal. Moreover, the algorithm converges always to the same set of parameters, independently of the initial configuration. According to the experimental results presented in Table 1, we conclude that the MSV measure is the best one for classifying the quality of an image. This is due to the fact that it is closer to the optimal values when the camera is correctly calibrated. Moreover, this measure can distinguish between two images that have close characteristics, as is the case when the camera is used in auto-mode. The good results of the automated calibration procedure can also be confirmed in the histograms presented in Fig. 5 . The histogram of the image obtained after applying the proposed automated calibration procedure (Fig. 5b) is centered near the intensity 127, which is a desirable property, as shown in Fig. 3 in the middle images. The histogram of the image acquired using the camera with all the parameters set to the maximum value (Fig. 5a) shows that the image is overexposed, leading that the majority of the pixels have bright colors. This algorithm has also been tested outdoors, under natural light. Fig. 6 shows that it works well even when the robot is under very different lighting conditions, showing its robustness. 3.2 Distance map calibration For most practical applications, the setup of the vision system requires the translation of the planar field of view at the camera sensor plane, into real world coordinates at the ground plane, using the robot as the center of this system. In order to simplify this non-linear transformation, most practical solutions adopted in real robots choose to create a mechanical geometric setup that ensures a symmetrical solution for the problem by means of a single viewpoint (SVP) approach. This, on the other hand, calls for a precise alignment of the four major points comprising the vision setup: the mirror focus, the mirror apex, the lens focus and the center of the image sensor. Furthermore, it also demands the sensor plane to be both parallel to the ground field and normal to the mirror axis of revolution, and the mirror foci to be coincident with the effective viewpoint and the camera pinhole respectively [16]. Although tempting, this approach requires a precision mechanical setup. In this section, we briefly present a general solution to calculate the robot centered distances map on non-SVP catadioptric setups, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. A detailed description of the algorithms can be found in [17] and a screenshot of the application is presented in Figs. 7 and 8 . This solution effectively compensates for the misalignment that may result either from a simple mechanical setup or from the use of low cost video cameras. The method can also extract most of the required parameters from the acquired image itself, allowing it to be used for self-calibration purposes. In order to allow further trimming of these parameters, two simple image feedback tools have been developed. The first one creates a reverse mapping of the acquired image into the real world distance map. A fill-in algorithm is used to integrate image data in areas outside pixel mapping on the ground plane. This produces a plane vision from above, allowing visual check of line parallelism and circular asymmetries (Fig. 9 ). The second generates a visual grid with 0.5m distances between both lines and columns, which is superimposed on the original image. This provides an immediate visual clue for the need of possible further distance correction (Fig. 10 ). With this tool, it is also possible to determine some other important parameters, namely the mirror center and the area of the image that will be processed by the object detection algorithms (Fig. 11 ). 4 Color-based object detection The algorithms that we propose for object detection can be split into three main modules, namely the Utility Sub-System, the Color Processing Sub-System and the Morphological Processing Sub-System, as shown in Fig. 12 . In the Color Processing Sub-System, proper color classification and extraction processes were developed, along with an object detection process to extract information from the acquired image, through color analysis. The Morphological Processing Sub-System presented in Section 5, is used to detect arbitrary FIFA balls independently of their colors. In order to satisfy the real-time constrains in the proposed image processing system, we implemented efficient data structures to process the image data [18,19]. Moreover, we use a two-thread approach to perform the most time consuming operations in parallel, namely the color classification and the color extraction, taking advantage of the dual core processor used by the laptop computers of our robots. 4.1 Color extraction Image analysis in the MSL is simplified, since objects are color coded. Black robots play with an orange ball on a green field that has white lines. Thus, the color of a pixel is a strong hint for object segmentation. We exploit this fact by defining color classes, using a look-up table (LUT) for fast color classification. The table consists of 16,777,216 entries (224, 8 bits for red, 8 bits for green and 8 bits for blue), each 8 bits wide, occupying a total of 16 MByte. Note that for other color spaces the table size would be the same, changing only the meaning of each component. Each bit expresses whether the color is within the corresponding class or not. This means that a certain color can be assigned to several classes at the same time. To classify a pixel, we first read the pixel’s color and then use the color as an index into the table. The 8-bit value read from the table is called the “color mask” of that pixel. The color calibration is performed in the HSV (Hue, Saturation and Value) color space, since it provides a single, independent, color spectrum variable. In the current setup, the image is acquired in RGB or YUV format and then is converted to an image of labels using the appropriate LUT. Fig. 13 presents a screenshot of the application used to calibrate the color ranges for each color class, using the HSV color space and a histogram based analysis. Certain regions of the image are excluded from analysis. One of them is the part in the image that reflects the robot itself. Other regions are the sticks that hold the mirror and the areas outside the mirror. These regions are found using the algorithm described in Section 3.2. An example is presented on the right of Fig. 11, where the white pixels indicate the area that will be processed. With this approach, we can reduce the time spent in the conversion and searching phases and we also eliminate the problem of finding erroneous objects in those areas. To extract color information from the image we use radial search lines, instead of processing the whole image. A radial search line is a line that starts at the center of the robot, with some angle, and ends at the limits of the image. In an omnidirectional system, the center of the robot is approximately the center of the image (see left of Fig. 11). The search lines are constructed based on the Bresenham line algorithm [20]. They are constructed once, when the application starts, and saved in a structure in order to improve the access to these pixels in the color extraction module. For each search line, we iterate through its pixels to search for transitions between two colors and areas with specific colors. The use of radial search lines accelerates the process of object detection, due to the fact that we only process part of the valid pixels. This approach has a processing time almost constant, independently of the information that is captured by the camera. Moreover, the polar coordinates, inherent to the radial search lines, facilitate the definition of the bounding boxes of the objects in omnidirectional vision systems. We developed an algorithm for detecting areas of a specific color which eliminates the possible noise that could appear in the image. For each radial scanline, it is performed a median filtering operation. Each time a pixel is found with a color of interest, the algorithm analyzes the pixels that follow (a predefined number). If it does not find more pixels of that color, it discards the pixel found and continues. When a predefined number of pixels with that color is found, it considers that the search line has that color. Regarding the ball detection, we created an algorithm to recover lost orange pixels due to the ball shadow cast over itself. As soon as we find a valid orange pixel in the radial sensor, the shadow recovery algorithm tries to search for darker orange pixels previously discarded in the color segmentation analysis. The search is conducted in each radial sensor, starting at the first orange pixel found when searching towards the center of the robot, limited to a maximum number of pixels. For each pixel analyzed, a comparison is performed using a wider region of the color space, in order to being able to accept darker orange pixels. Once a different color is found or the maximum number of pixels is reached, the search along the current sensor is completed and the next sensor is processed. In Fig. 16, we can see the pixels recovered by this algorithm (the orange blobs contain pixels that were not originally classified as orange). To accelerate the process of calculating the position of the objects, we put the color information that was found in each of the search lines into a list of colors. We are interested in the first pixel (in the corresponding search line) where the color was found and with the number of pixels with that color that have been found in the search line. Then, using the previous information, we separate the information of each color into blobs (Fig. 16 shows an example). After this, it is calculated the blob descriptor that will be used for the object detection module, which contains the following information: – Distance to the robot. – Closest pixel to the robot. – Position of the mass center. – Angular width. – Number of pixels. – Number of green and white pixels in the neighborhood of the blob. 4.2 Object detection The objects of interest that are present in a MSL game are: a ball, obstacles and the green field with white lines. Currently, our system detects efficiently all these objects with a set of simple algorithms that, using the color information collected by the radial search lines, calculate the object position and/or its limits in a polar representation (distance and angle). The algorithm that searches for the transitions between green pixels and white pixels is described next. If a non-green pixel is found in a radial scanline, we search for the next green pixel, counting the number of non-green pixels and the number of white pixels that meanwhile appeared. If these values are greater than a predefined threshold, the center of this region is considered a transition point corresponding to a position of a soccer field line. The algorithm is illustrated in Fig. 14 with an example. A similar approach has been described in [21]. The ball is detected using the following algorithm: (i) Separate the orange information into blobs. (ii) For each blob, calculate the information described previously. (iii) Perform a first validation of the orange blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only balls inside the field are detected. (iv) Validate the remaining orange blobs according to the number of pixels. As illustrated in Fig. 15 , it is known the relation between the pixel size at the ground plane and the distance to the center of the robot. Using this knowledge, we estimate the number of pixels that a ball should have according to the distance. (v) Following the same approach, the angular width is also used to validate the blobs. (vi) The ball candidate is the valid blob closest to the robot. The position of the ball is the mass center of the blob. To calculate the position of the obstacles around the robot, we use the following algorithm: (i) Separate the black information into blobs. (ii) Calculate the information for each blob. (iii) Perform a simple validation of the black blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only obstacles inside the field are detected. (iv) The position of the obstacle is given by the distance of the blob relatively to the robot. The limits of the obstacle are obtained using the angular width of the blob. More details regarding the detection and identification of obstacles can be found in [22]. Fig. 16 presents an example of an acquired image, the corresponding segmented image and the detected color blobs. As can be seen, the objects are correctly detected. The position of the white lines, the position of the ball and the information about the obstacles are then sent to the Real-time Database [1,23] and used, afterward, by the high level process responsible for the behaviors of the robots [24,25,22,26]. 4.3 Experimental results To experimentally measure the efficiency of the proposed algorithms, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. Note that the results in this test may be affected by errors in the localization algorithm and by some bumps while the robot is moving. The separate study of these sources of error has being left outside this experimental evaluation. However, they should be performed, for better understanding the several factors that influence the correct localization of the ball. The robot path across the field may be seen in Fig. 17 , along with the measured ball position. According to that data, it is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the effectiveness of the proposed algorithms. Our measures show a very high detection ratio (near 95%), and a good accuracy, with the average measures very close to the real ball position. In our experiments, we verified that the robots are able to detect the ball up to 6m with regular light conditions and a good color calibration, easy to obtain after applying the proposed automated calibration algorithm described in Section 3. The proposed algorithm has an almost constant processing time, independently of the environment around the robot, typically around 6ms. It needs approximately 35MBytes of memory. The experimental results were obtained using a camera resolution of 640×480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz and 1GB of memory. 5 Arbitrary ball detection The color codes tend to disappear as the competition evolves, increasing the difficulty posed to the vision algorithms. The color of the ball, currently orange, is the next color scheduled to become arbitrary. In this section, we propose a solution for overcoming this new challenge, i.e., a method for detecting balls independently of their colors. This solution is based on a morphological analysis of the image, being strictly directed to detect round objects in the field with specific characteristics, in this case the ball. Morphological object recognition through image analysis has became more robust and accurate in the past years, whereas still very time consuming even to modern personal computers. Because RoboCup is a real-time environment, available processing time can become a serious constraint when analyzing large amounts of data or executing complex algorithms. This section presents an arbitrary FIFA ball recognition algorithm, based on the use of image segmentation and the circular Hough transform. The processing time is almost constant and allows real-time processing. As far as we know, this approach has never been proposed. The experimental results obtained, as well as the classifications obtained by the CAMBADA team, seem to be very promising. Regarding the vision system described in Fig. 12, it is possible to specify whether to use the Morphological sub-system to detect the ball or the current color-based approach. Currently, in the MSL, the shape-based detection is only necessary in the mandatory challenge of the competition, although it will be incorporated in the rules in the next years. 5.1 Related work Many of the algorithms proposed during previous research work showed their effectiveness but, unfortunately, their processing time is in some cases over one second per video frame [27]. In [28], the circular Hough transform was presented in the context of colored ball detection as a validation step. However, no details about the implementation and experimental results have been presented. Hanek et al. [29] proposed a Contracting Curve Density algorithm to recognize the ball without color labeling. This algorithm fits parametric curve models to the image data by using local criteria based on local image statistics to separate adjacent regions. This method can extract the contour of the ball even in cluttered environments under different illumination, but the vague position of the ball should be known in advance. The global detection cannot be realized by this method. Treptow et al. [30] proposed a method to detect and track a ball without color information in real-time, by integrating the Adaboost Feature Learning algorithm into a condensation tracking framework. Mitri et al. [31] presented a scheme for color invariant ball detection, in which the edged filtered images serve as the input of an Adaboost learning procedure that constructs a cascade of classification and regression trees. This method can detect different soccer balls in different environments, but the false positive rate is high when there are other round objects in the environment. Coath et al. [32] proposed an edge-based arc fitting algorithm to detect the ball for soccer robots. However, the algorithm is used in a perspective camera vision system in which the field of view is far smaller and the image is also far less complex than that of the omnidirectional vision system used by most of the robotic soccer teams. More recently, Lu et al. [33] considered that the ball on the field can be approximated by an ellipse. They scan the color variation to search for the possible major and minor axes of the ellipse, using radial and rotary scanning, respectively. A ball is considered if the middle points of a possible major axis and a possible minor axis are very close to each other in the image. However, this method has a processing time that can achieve 150ms if the tracking algorithm fails, which might cause problems in real-time applications. 5.2 Proposed approach The proposed approach is presented in the top layer of Fig. 12. The search for potential ball candidates is conducted taking advantage of morphological characteristics of the ball (round shape), using a feature extraction technique known as the Hough transform. This is a technique for identifying the locations and orientations of certain types of features in a digital image [34]. The Hough transform algorithm uses an accumulator and can be described as a transformation of a point in the x, y-plane to the parameter space. The parameter space is defined according to the shape of the object of interest, in this case, the ball presents a rounded shape. First used to identify lines in images, the Hough transform has been generalized through the years to identify positions of arbitrary shapes by a voting procedure [35–37]. Fig. 18 shows an example of a circular Hough transform, for a constant radius, from the x, y-space to the parameter space. In Fig. 19 , we show an example of circle detection through the circular Hough transform. We can see the original image of a dark circle (known radius r) on a bright background (see Fig. 19a). For each dark pixel, a potential circle-center locus is defined by a circle with radius r and center at that pixel (see Fig. 19b). The frequency with which image pixels occur in the circle-center loci is determined (see Fig. 19c). Finally, the highest-frequency pixel represents the center of the circle with radius r. To feed the Hough transform process, it is necessary a binary image with the edge information of the objects. This image, Edges Image, is obtained using an edge detector operator. In the following, we present an explanation of this process and its implementation. To be possible to use this image processing system in real-time, and increase time efficiency, a set of data structures to process the image data has been implemented [18,19]. The proposed algorithm is based on three main operations: (i) Edge detection: this is the first image processing step in the morphological detection. It must be as efficient and accurate as possible in order not to compromise the efficiency of the whole system. Besides being fast to calculate, the intended resulting image must be absent of noise as much as possible, with well defined contours, and be tolerant to the motion blur introduced by the movement of the ball and the robots. Some popular edge detectors were tested, namely Sobel [38,39], Laplace [40,41] and Canny [42]. The tests were conducted under two distinct situations: with the ball standing still and with the ball moving fast through the field. The test with the ball moving fast was performed in order to study the motion blur effect in the edge detectors, on high speed objects captured with a frame rate of 30 frames per second. For choosing the best edge detector for this purpose, the results from the tests were compared taking into account the image of edges and processing time needed by each edge detector. On one hand, the real-time capability must be assured. On the other hand, the algorithm must be able to detect the edges of the ball independently of its motion blur effect. According to our experiments, the Canny edge detector was the most demanding in terms of processing time. Even so, it was fast enough for real-time operation and, because it provided the most effective contours, it was chosen. The parameters of the edge detector were obtained experimentally. (ii) Circular Hough transform: this is the next step in the proposed approach to find points of interest containing eventual circular objects. After finding these points, a validation procedure is used for choosing points containing a ball, according to our characterization. The voting procedure of the Hough transform is carried out in a parameter space. Object candidates are obtained as local maxima of a denoted Intensity Image (Fig. 20 c), that is constructed by the Hough Transform block (Fig. 12). Due to the special features of the Hough circular transform, a circular object in the Edges Image would produce an intense peak in Intensity Image corresponding to the center of the object (as can be seen in Fig. 20c). On the contrary, a non-circular object would produce areas of low intensity in the Intensity Image. However, as the ball moves away, its edge circle size decreases. To solve this problem, information about the distance between the robot center and the ball is used to adjust the Hough transform. We use the inverse mapping of our vision system [17] to estimate the radius of the ball as a function of distance. (iii) Validation: in some situations, particularly when the ball is not present in the field, false positives might be produced. To solve this problem and improve the ball information reliability, we propose a validation algorithm that discards false positives based on information from the Intensity Image and the Acquired Image. This validation algorithm is based on two tests against which each ball candidate is put through. In the first test performed by the validation algorithm, the points with local maximum values in the Intensity Image are considered if they are above a distance-dependent threshold. This threshold depends on the distance of the ball candidate to the robot center, decreasing as this distance increases. This first test removes some false ball candidates, leaving a reduced group of points of interest. Then, a test is made in the Acquired Image over each point of interest selected by the previous test. This test is used to eliminate false balls that usually appear in the intersection of the lines of the field and other robots (regions with several contours). To remove these false balls, we analyze a square region of the image centered in the point of interest. We discard this point of interest if the sum of all green pixels is over a certain percentage of the square area. Note that the area of this square depends on the distance of the point of interest to the robot center, decreasing as this distance increases. Choosing a square where the ball fits tightly makes this test very effective, considering that the ball fills over 90% of the square. In both tests, we use threshold values that were obtained experimentally. Besides the color validation, it is also performed a validation of the morphology of the candidate, more precisely a circularity validation. Here, from the candidate point to the center of the ball, it is performed a search of pixels at a distance r from the center. For each edge found between the expected radius, the number of edges at that distance are determined. By the size of the square which covers the possible ball and the number of edge pixels, it is calculated the edges percentage. If the edges percentage is greater than 70, then the circularity of the candidate is verified. The position of the detect ball is then sent to the Real-time Database, together with the information of the white lines and the information about the obstacles to be used, afterward, by the high level process responsible for the behaviors of the robots. 5.3 Experimental results Fig. 20 presents an example of the Morphological Processing Sub-System. As can be observed, the balls in the Edges Image (Fig. 20b) have almost circular contours. Fig. 20c) shows the resulting image after applying the circular Hough transform. Notice that the center of the balls present a very high peak when compared to the rest of the image. The ball considered was the closest to the robot, due to the fact that it has the high peak in the image. To ensure good results in the RoboCup competition, the system was tested with the algorithms described above. For that purpose, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. The results in this test may be affected by the errors in the localization algorithm and by the robot bumps while moving. These external errors are out of the scope of this study. The robot path in the field may be seen in Fig. 21 , along with the measured ball position. It is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the accuracy of the proposed algorithms. We obtained a very high detection ratio (near 90%) and a false positive rate around 0%, which is a very significant result. With the proposed approach, the omnidirectional vision system can detect the ball within this precision until distances up to 4 meters. The average processing time of the proposed approach was approximately 16ms. It needs approximately 40MBytes of memory. The experimental results have been obtained using a camera resolution of 640×480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz. 6 Conclusions This paper presents the omnidirectional vision system developed for the CAMBADA MSL robotic soccer team, from the calibration to the object detection. We presented several algorithms for the calibration of the most important parameters of the vision system and we proposed efficient color-based algorithms for object detection. Moreover, we proposed a solution for the detection of arbitrary FIFA balls, one of the current challenges in the MSL. The CAMBADA team won the last three editions of the Portuguese Robotics Festival, ranked 5th in RoboCup 2007, won the RoboCup 2008 and ranked 3rd in RoboCup 2009, demonstrating the effectiveness of our vision algorithms in a competition environment. As far as we know, no previous work has been published describing all the steps of the design of an omnidirectional vision system. Moreover, some of the algorithms presented in this paper are state-of-the-art, as demonstrated by the first place obtained in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. We are currently working in the automatic calibration of the inverse distance mapping and in efficient algorithms for autonomous color calibration, based on region growing. Regarding the object detection algorithms, as we have reduced the processing time to a few milliseconds, we are working on the acquisition of higher resolution images, capturing only a region of interest. The idea of work with higher image resolutions is to improve the object detection at higher distances. Moreover, we continue the development of algorithms for shape-based object detection, also to incorporate as a validation of the color-based algorithms. Acknowledgment This work was supported in part by the FCT (Fundação para a Ciência e a Tecnologia). References [1] Neves A, Azevedo J, Cunha NLB, Silva J, Santos F, Corrente G, et al. CAMBADA soccer team: from robot architecture to multiagent coordination. In: Vladan Papic editor. Robot soccer. Vienna, Austria: I-Tech Education and Publishing; 2010 [chapter 2]. [2] Zivkovic Z, Booij O. How did we built our hyperbolic mirror omni-directional camera-practical issues and basic geometry. Tech. rep. Intelligent Systems Laboratory, University of Amsterdam; 2006. [3] Wolf J. Omnidirectional vision system for mobile robot localization in the robocup environment. Master’s thesis. Graz University of Technology; 2003. [4] Menegatti E, Nori F, Pagello E, Pellizzari C, Spagnoli D. Designing an omnidirectional vision system for a goalkeeper robot. In: Proc of RoboCup 2001. Lecture notes in computer science, vol. 2377. Springer; 2001. p. 78–87. [5] Menegatti E, Pretto A, Pagello E. Testing omnidirectional vision-based monte carlo localization under occlusion. In: Proc of the IEEE intelligent robots and systems, IROS 2004; 2004. p. 2487–93. [6] P. Lima A. Bonarini C. Machado F. Marchese C. Marques F. Ribeiro Omni-directional catadioptric vision for soccer robots Robot Auton Syst 36 2–3 2001 87 102 [7] Liu F, Lu H, Zheng Z. A robust approach of field features extraction for robot soccer. In: Proc of the 4th IEEE Latin America robotic symposium, Monterry, Mexico; 2007. [8] Lu H, Zheng Z, Liu F, Wang X. A robust object recognition method for soccer robots. In: Proc of the 7th world congress on intelligent control and automation, Chongqing, China; 2008. [9] Voigtlrande A, Lange S, Lauer M, Riedmiller M. Real-time 3D ball recognition using perspective and catadioptric cameras. In: Proc of the 3rd European conference on mobile robots, Freiburg, Germany; 2007. [10] Mayer G, Utz H, Kraetzschmar G. Playing robot soccer under natural light: a case study. In: Proc of the RoboCup 2003. Lecture notes on artificial intelligence, vol. 3020. Springer; 2003. [11] Krawczyk G, Goesele M, Seidel H. Photometric calibration of high dynamic range cameras. Research Report MPI-I-2005-4-005. Max-Planck-Institut für Informatik, Stuhlsatzenhausweg 85, 66123 Saarbrücken, Germany; April 2005. [12] Shirvaikar MV. An optimal measure for camera focus and exposure. In: Proc of the IEEE southeastern symposium on system theory, Atlanta (USA); 2004. [13] Nourani-Vatani N, Roberts J. Automatic camera exposure control. In: Proc of the 2007 Australasian conference on robotics and automation, Brisbane, Australia; 2007. [14] K. Åström T. Hågglund PID controllers: theory, design, and tuning 2nd ed. 1995 Instruments Society of America [15] Neves AJR, Cunha AJPB, Pinheiro I. Autonomous configuration of parameters in robotic digital cameras. In: Proc of the 4th Iberian conference on pattern recognition and image analysis, IbPRIA-2009. Lecture notes in computer science, vol. 5524. Póvoa do Varzim, Portugal: Springer; 2009. p. 80–7. [16] S. Baker S.K. Nayar A theory of single-viewpoint catadioptric image formation Int J Comput Vis 2 1999 175 196 [17] Cunha B, Azevedo JL, Lau N, Almeida L. Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system. In: Proc of the RoboCup 2007. Lecture notes in computer science, vol. 5001. Atlanta (USA): Springer; 2007. p. 417–24. [18] Neves AJR, Martins DA, Pinho AJ. A hybrid vision system for soccer robots using radial search lines. In: Proc of the 8th conference on autonomous robot systems and competitions. Portuguese robotics open – ROBOTICA’2008, Aveiro, Portugal; 2008. p. 51–5. [19] Neves AJR. Corrente G, Pinho AJ. An omnidirectional vision system for soccer robots. In: Proc of the 2nd international workshop on intelligent robotics, IROBOT 2007. Lecture notes in artificial intelligence, vol. 4874. Springer; 2007. p. 499–507. [20] J.E. Bresenham Algorithm for computer control of a digital plotter IBM Syst J 4 1 1965 25 30 [21] Merke A, Welker S, Riedmiller M. Line base robot localisation under natural light conditions. In: Proc of the ECAI workshop on agents in dynamic and real-time environments, Valencia, Spain; 2002. [22] Silva J, Lau N, Rodrigues J, Azevedo JL, Neves AJR. Sensor and information fusion applied to a robotic soccer team. In: RoboCup 2009: robot soccer world cup XIII. Lecture notes in artificial intelligence. Springer; 2009. [23] Almeida L, Santos F, Facchinetti T, Pedreira P, Silva V, Lopes LS. Coordinating distributed autonomous agents with a real-time database: the CAMBADA project. In: Proc of the 19th international symposium on computer and information sciences, ISCIS 2004. Lecture notes in computer science, vol. 3280. Springer; 2004. p. 878–86. [24] Lau N, Lopes LS, Corrente G, Filipe N. Roles, positionings and set plays to coordinate a msl robot team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT’09. Lecture notes in computer science, vol. 5816. Aveiro, Portugal: Springer; 2009. p. 323–37. [25] Lau N, Lopes LS, Corrente G, Filipe N. Multi-robot team coordination through roles, positioning and coordinated procedures. In: Proc of the IEEE/RSJ international conference on intelligent robots and systems, MO, USA: St. Louis; 2009. p. 5841–48. [26] Silva J, Lau N, Neves AJR, Rodrigues J, Azevedo JL. Obstacle detection, identification and sharing on a robotic soccer team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT’09. Lecture notes in computer science, LNAI 5816. Aveiro, Portugal: Springer; 2009. p. 350–60. [27] Mitri S, Frintrop S, Pervolz K, Surmann H, Nuchter A. Robust object detection at regions of interest with an application in ball recognition. In: Proc of the 2005 IEEE international conference on robotics and automation, ICRA 2005, Barcelona, Spain; 2005. p. 125–30. [28] Jonker P, Caarls J, Bokhove W. Fast and accurate robot vision for vision based motion. In: RoboCup 2000: robot soccer world cup IV. Lecture notes in computer science. Springer; 2000. p. 149–58. [29] Hanek R, Schmitt T, Buck S. Fast image-based object localization in natural scenes. In: Proc of the 2002 IEEE/RSJ international conference on intelligent robotics and systems, Lausanne, Switzerland; 2002. p. 116–22. [30] A. Treptow A. Zell Real-time object tracking for soccer-robots without color information Robot Auton Syst 48 1 2004 41 48 [31] Mitri S, Pervolz K, Surmann H, Nuchter A. Fast color independent ball detection for mobile robots. In: Proc of the 2004 IEEE international conference on mechatronics and robotics, Aechen, Germany; 2004. p. 900–5. [32] Coath G, Musumeci P. Adaptive arc fitting for ball detection in RoboCup. In: Proc of the APRS workshop on digital image computing, WDIC 2003, Brisbane, Australia; 2003. p. 63–8. [33] Lu H, Zhang H, Zheng Z. Arbitrary ball recognition based on omni-directional vision for soccer robots. In: Proc of RoboCup 2008; 2008. [34] Nixon M, Aguado A. Feature extraction and image processing. 1st ed. Linacre House, Jordan Hill, Oxford OX2 8DP 225 Wildwood Avenue, Woburn, MA 01801-2041: Reed Educational and Professional Publishing Ltd.; 2002. [35] Ser PK, Siu WC. Invariant hough transform with matching technique for the recognition of non-analytic objects. In: IEEE international conference on acoustics, speech, and signal processing, ICASSP 1993, vol. 5; 1993. p. 9–12. [36] Zhang YJ, Liu ZQ. Curve detection using a new clustering approach in the hough space. In: IEEE international conference on systems, man, and cybernetics, 2000, vol. 4; 2000. p. 2746–51. [37] W.E.L. Grimson D.P. Huttenlocher On the sensitivity of the hough transform for object recognition IEEE Trans Pattern Anal Mach Intell 12 1990 1255 1274 [38] Zou J, Li H, Liu B, Zhang R. Color edge detection based on morphology. In: First international conference on communications and electronics, ICCE 2006; 2006. p. 291–3. [39] Zin TT, Takahashi H, Hama H. Robust person detection using far infrared camera for image fusion. In: Second international conference on innovative computing, information and control, ICICIC 2007; 2007. p. 310. [40] Zou Y, Dunsmuir W. Edge detection using generalized root signals of 2-d median filtering. In: Proc of the international conference on image processing, 1997, vol. 1; 1997. p. 417–9. [41] Blaffert T, Dippel S, Stahl M, Wiemker R. The laplace integral for a watershed segmentation. In: Proc of the international conference on image processing, 2000, vol. 3; 2000. p. 444–7. [42] Canny JF. A computational approach to edge detection. IEEE Trans Pattern Anal Mach Intell 8 (6). "
    },
    {
        "doc_title": "Exploring homology using the concept of three-state entropy vector",
        "doc_scopus_id": "78049460831",
        "doc_doi": "10.1007/978-3-642-16001-1_14",
        "doc_eid": "2-s2.0-78049460831",
        "doc_date": "2010-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data models",
            "DNA coding",
            "Encoding proteins",
            "Entropy value",
            "Markov model",
            "State entropy",
            "DNA coding",
            "Encoding proteins",
            "Entropy value",
            "Markov model",
            "State entropy"
        ],
        "doc_abstract": "The three-base periodicity usually found in exons has been used for several purposes, as for example the prediction of potential genes. In this paper, we use a data model, previously proposed for encoding protein-coding regions of DNA sequences, to build signatures capable of supporting the construction of meaningful dendograms. The model relies on the three-base periodicity and provides an estimate of the entropy associated with each of the three bases of the codons. We observe that the three entropy values vary among themselves and also from species to species. Moreover, we provide evidence that this makes it possible to associate a three-state entropy vector with each species, and we show that similar species are characterized by similar three-state entropy vectors. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA coding using finite-context models and arithmetic coding",
        "doc_scopus_id": "70349192866",
        "doc_doi": "10.1109/ICASSP.2009.4959928",
        "doc_eid": "2-s2.0-70349192866",
        "doc_date": "2009-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Arithmetic coding",
            "Coding methods",
            "Context models",
            "DNA basis",
            "DNA coding",
            "DNA encoding",
            "Finite-context modeling",
            "Genomic database",
            "Lossless compression",
            "Low order",
            "Source coding",
            "Standard compression algorithms"
        ],
        "doc_abstract": "The interest in DNA coding has been growing with the availability of extensive genomic databases. Although only two bits are sufficient to encode the four DNA bases, efficient lossless compression methods are still needed due to the size of DNA sequences and because standard compression algorithms do not perform well on DNA sequences. As a result, several specific coding methods have been proposed. Most of these methods are based on searching procedures for finding exact or approximate repeats. Low order finite-context models have only been used as secondary, fall back mechanisms. In this paper, we show that finite-context models can also be used as main DNA encoding methods. We propose a coding method based on two finite-context models that compete for the encoding of data, on a block by block basis. The experimental results confirm the effectiveness of the proposed method. ©2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Genome analysis with inter-nucleotide distances",
        "doc_scopus_id": "75949102506",
        "doc_doi": "10.1093/bioinformatics/btp546",
        "doc_eid": "2-s2.0-75949102506",
        "doc_date": "2009-09-16",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Motivation: DNA sequences can be represented by sequences of four symbols, but it is often useful to convert the symbols into real or complex numbers for further analysis. Several mapping schemes have been used in the past, but they seem unrelated to any intrinsic characteristic of DNA. The objective of this work was to find a mapping scheme directly related to DNA characteristics and that would be useful in discriminating between different species. Mathematical models to explore DNA correlation structures may contribute to a better knowledge of the DNA and to find a concise DNA description. Results: We developed a methodology to process DNA sequences based on inter-nucleotide distances. Our main contribution is a method to obtain genomic signatures for complete genomes, based on the inter-nucleotide distances, that are able to discriminate between different species. Using these signatures and hierarchical clustering, it is possible to build phylogenetic trees. Phylogenetic trees lead to genome differentiation and allow the inference of phylogenetic relations. The phylogenetic trees generated in this work display related species close to each other, suggesting that the inter-nucleotide distances are able to capture essential information about the genomes. To create the genomic signature, we construct a vector which describes the inter-nucleotide distance distribution of a complete genome and compare it with the reference distance distribution, which is the distribution of a sequence where the nucleotides are placed randomly and independently. It is the residual or relative error between the data and the reference distribution that is used to compare the DNA sequences of different organisms. © The Author(s) 2009. Published by Oxford University Press.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Variable order finite-context models in DNA sequence coding",
        "doc_scopus_id": "68749102414",
        "doc_doi": "10.1007/978-3-642-02172-5_59",
        "doc_eid": "2-s2.0-68749102414",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biological research",
            "Coding methods",
            "Context models",
            "DNA structure",
            "Encoded sequences",
            "Predictive function",
            "Side information",
            "Variable order"
        ],
        "doc_abstract": "Being an essential key in biological research, the DNA sequences are often shared between researchers and digitally stored for future use. As these sequences grow in volume, it also grows the need to encode them, thus saving space for more sequences. Besides this, a better coding method corresponds to a better model of the sequence, allowing new insights about the DNA structure. In this paper, we present an algorithm capable of improving the encoding results of algorithms that depend of low-order finite-context models to encode DNA sequences. To do so, we implemented a variable order finite-context model, supported by a predictive function. The proposed algorithm allows using three finite-context models at once without requiring the inclusion of side information in the encoded sequence. Currently, the proposed method shows small improvements in the encoding results when compared with same order finite-context models. However, we also present results showing that there is space for further improvements regarding the use variable order finite-context models for DNA sequence coding. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
        "doc_scopus_id": "68749090046",
        "doc_doi": null,
        "doc_eid": "2-s2.0-68749090046",
        "doc_date": "2009-08-20",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Autonomous configuration of parameters in robotic digital cameras",
        "doc_scopus_id": "68749090043",
        "doc_doi": "10.1007/978-3-642-02172-5_12",
        "doc_eid": "2-s2.0-68749090043",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous configuration",
            "Camera parameter",
            "Initial configuration",
            "Making decision",
            "Robotic applications",
            "Surrounding environment"
        ],
        "doc_abstract": "In the past few years, the use of digital cameras in robotic applications has been increasing significantly. The main areas of application of these robots are the industry and military, where these cameras are used as sensors that allow the robot to take the relevant information of the surrounding environment and making decisions. To extract information from the acquired image, such as shapes or colors, the configuration of the camera parameters, such as exposure, gain, brightness or white-balance, is very important. In this paper, we propose an algorithm for the autonomous setup of the most important parameters of digital cameras for robotic applications. The proposed algorithm uses the intensity histogram of the images and a black and a white area, known in advance, to estimate the parameters of the camera. We present experimental results that show the effectiveness of our algorithms. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On finding minimal absent words",
        "doc_scopus_id": "67650878985",
        "doc_doi": "10.1186/1471-2105-10-137",
        "doc_eid": "2-s2.0-67650878985",
        "doc_date": "2009-05-08",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "DNA data",
            "Linear time"
        ],
        "doc_abstract": "Background: The problem of finding the shortest absent words in DNA data has been recently addressed, and algorithms for its solution have been described. It has been noted that longer absent words might also be of interest, but the existing algorithms only provide generic absent words by trivially extending the shortest ones. Results: We show how absent words relate to the repetitions and structure of the data, and define a new and larger class of absent words, called minimal absent words, that still captures the essential properties of the shortest absent words introduced in recent works. The words of this new class are minimal in the sense that if their leftmost or rightmost character is removed, then the resulting word is no longer an absent word. We describe an algorithm for generating minimal absent words that, in practice, runs in approximately linear time. An implementation of this algorithm is publicly available at ftp://www.ieeta.pt/~ap/maws. Conclusion: Because the set of minimal absent words that we propose is much larger than the set of the shortest absent words, it is potentially more useful for applications that require a richer variety of absent words. Nevertheless, the number of minimal absent words is still manageable since it grows at most linearly with the string size, unlike generic absent words that grow exponentially. Both the algorithm and the concepts upon which it depends shed additional light on the structure of absent words and complement the existing studies on the topic. © 2009 Pinho et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless compression of microarray images using image-dependent finite-context models",
        "doc_scopus_id": "59449096695",
        "doc_doi": "10.1109/TMI.2008.929095",
        "doc_eid": "2-s2.0-59449096695",
        "doc_date": "2009-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Arithmetic coding",
            "Finite-context modeling",
            "Image coding standards",
            "Lossless image compression",
            "Microarray images"
        ],
        "doc_abstract": "The use of microarray expression data in state-of-the-art biology has been well established. The widespread adoption of this technology, coupled with the significant volume of data generated per experiment, in the form of images, has led to significant challenges in storage and query retrieval. In this paper, we present a lossless bitplane-based method for efficient compression of microarray images. This method is based on arithmetic coding driven by image-dependent multibitplane finite-context models. It produces an embedded bitstream that allows progressive, lossy-to-lossless decoding. We compare the compression efficiency of the proposed method with three image compression standards (JPEG2000, JPEG-LS, and JBIG) and also with the two most recent specialized methods for microarray image coding. The proposed method gives better results for all images of the test sets and confirms the effectiveness of bitplane-based methods and finite-context modeling for the lossless compression of microarray images. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Progressive lossless compression of medical images",
        "doc_scopus_id": "70349437371",
        "doc_doi": "10.1109/icassp.2009.4959607",
        "doc_eid": "2-s2.0-70349437371",
        "doc_date": "2009-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression methods",
            "Context models",
            "Embedded bitstreams",
            "Lossless compression",
            "Lossless image coding",
            "Lossy-to-lossless",
            "Medical image database",
            "Progressive transmission"
        ],
        "doc_abstract": "This paper describes a lossless compression method for medical images that produces an embedded bit-stream, allowing progressive lossy-to-lossless decoding with L-infinity oriented rate-distortion. The experimental results show that the proposed technique produces better average lossless compression results than several other compression methods, including JPEG2000, JPEG-LS and JBIG, in a publicly available medical image database containing images from several modalities. ©2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A bitplane based algorithm for lossless compression of DNA microarray images",
        "doc_scopus_id": "84863764744",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863764744",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Arithmetic Coding",
            "Bit stream",
            "Bitplane",
            "Compression efficiency",
            "DNA microarray images",
            "Image compression standards",
            "JPEG 2000",
            "JPEG-LS",
            "Lossless",
            "Lossless compression",
            "Lossy-to-lossless",
            "Microarray images",
            "Microarray technologies",
            "Test sets"
        ],
        "doc_abstract": "During the past years, the development of microarray technology has been remarkable, and it is becoming a daily tool in many genomic research laboratories. The widespread adoption of this technology, coupled with the significant volume of data generated per experiment, in the form of images, have led to significant challenges in storage and query-retrieval. In this paper, we present a lossless bitplane based method for efficient compression of microarray images. This method is based on arithmetic coding driven by image-dependent multi-bitplane finite-context models. It produces an embedded bitstream that allows progressive, lossy-to-lossless decoding. We compare the compression efficiency of the proposed method with three image compression standards (JPEG2000, JPEG-LS and JBIG) and also with the two most recent specialized methods for microarray image coding. The proposed method gives better results for all images of the test sets and confirms the effectiveness of bitplane based methods and finite-context modeling for the lossless compression of microarray images. copyright by EURASIP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inverted-repeats-aware finite-context models for DNA coding",
        "doc_scopus_id": "84863762123",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863762123",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "DNA coding",
            "Encoding methods",
            "Inverted repeat",
            "Model order",
            "Model updating",
            "Sequence compression"
        ],
        "doc_abstract": "Finite-context models have been used for DNA sequence compression as secondary, fall back mechanisms, the generalized opinion being that models with order larger than two or three are inappropriate. In this paper we show that finite-context models can also be used as the main encoding method, and that they are effective for model orders at least as higher as thirteen. Moreover, we propose a new model updating scheme that takes into account inverted repeats, a common characteristic in DNA sequences. copyright by EURASIP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "L-infinity progressive image compression",
        "doc_scopus_id": "84898065095",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84898065095",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Constrained decoding",
            "Integer arithmetic",
            "L-infinity",
            "Lossless image coding",
            "Operational rate-distortion",
            "Progressive image compression",
            "Progressive transmission",
            "Reconstructed image"
        ],
        "doc_abstract": "This paper presents a lossless image coding approach that produces an embedded bit-stream optimized for L∞-constrained decoding. The decoder is implementable using only integer arithmetic and is able to deduce from the bit-stream the L∞ error that affects the reconstructed image at an arbitrary point of decoding. The lossless coding performance is compared with JPEG-LS and JPEG2000. Operational rate-distortion curves, in the L∞ sense, are presented and compared with JPEG2000.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An omnidirectional vision system for soccer robots",
        "doc_scopus_id": "38349011052",
        "doc_doi": "10.1007/978-3-540-77002-2_42",
        "doc_eid": "2-s2.0-38349011052",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color classification",
            "Image visualization",
            "Object detection",
            "Omnidirectional vision",
            "Soccer robots"
        ],
        "doc_abstract": "This paper describes a complete and efficient vision system developed for the robotic soccer team of the University of Aveiro, CAMBADA (Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture). The system consists on a firewire camera mounted vertically on the top of the robots. A hyperbolic mirror placed above the camera reflects the 360 degrees of the field around the robot. The omnidirectional system is used to find the ball, the goals, detect the presence of obstacles and the white lines, used by our localization algorithm. In this paper we present a set of algorithms to extract efficiently the color information of the acquired images and, in a second phase, extract the information of all objects of interest. Our vision system architecture uses a distributed paradigm where the main tasks, namely image acquisition, color extraction, object detection and image visualization, are separated in several processes that can run at the same time. We developed an efficient color extraction algorithm based on lookup tables and a radial model for object detection. Our participation in the last national robotic contest, ROBOTICA 2007, where we have obtained the first place in the Medium Size League of robotic soccer, shows the effectiveness of our algorithms. Moreover, our experiments show that the system is fast and accurate having a maximum processing time independently of the robot position and the number of objects found in the field. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossy-to-lossless compression of images based on binary tree decomposition",
        "doc_scopus_id": "78649882735",
        "doc_doi": "10.1109/ICIP.2006.312812",
        "doc_eid": "2-s2.0-78649882735",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Arithmetic Coding",
            "Binary tree decomposition",
            "Compression gain",
            "Compression standards",
            "Context-based",
            "JPEG 2000",
            "JPEG-LS",
            "Lossless compression",
            "Lossy to lossless compression",
            "Lossy-to-lossless"
        ],
        "doc_abstract": "A new lossy-to-lossless quality-progressive method for image coding is presented. This method is based on binary tree decomposition and context-based arithmetic coding. Experimental results obtained with the eighteen 8-bit ISO images show that the proposed method attains an average lossless compression gain of 7.9% with respect to JPEG2000. Compared to JPEG-LS, the compression gain is 3.1%, but this compression standard does not allow lossy-to-lossless decoding. ©2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless compression of microarray images",
        "doc_scopus_id": "48149100015",
        "doc_doi": "10.1109/ICIP.2006.312802",
        "doc_eid": "2-s2.0-48149100015",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "3D Context Model",
            "Arithmetic Coding",
            "Bio-medical image processing",
            "Image coding standard",
            "JPEG 2000",
            "JPEG-LS",
            "Lossless",
            "Lossless compression",
            "Microarray experiments",
            "Microarray images",
            "Microarray technologies",
            "Progressive decoding",
            "Test sets"
        ],
        "doc_abstract": "Microarray experiments are characterized by a massive amount of data in the form of images. Since the interest in microarray technology is growing nowadays, a large number of microarray images is currently being produced. In this paper, we present a lossless method for efficiently compress microarray images based on arithmetic coding using a 3D context model. Our method produces an embedded bit-stream that allows progressive decoding. We present the compression results using a large set of images and compare these results with three image coding standards, namely, lossless JPEG2000, JBIG and JPEG-LS. We also compare our results with the two most recent specialized methods, using a common set of images. The proposed method gives better results for all images of the test set. ©2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring three-base periodicity for DNA compression and modeling",
        "doc_scopus_id": "33947690256",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33947690256",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression gain",
            "DNA model",
            "Finite-context model"
        ],
        "doc_abstract": "To explore the three-base periodicity often found in proteincoding DNA regions, we introduce a DNA model based on three deterministic states, where each state implements a finite-context model. The results obtained show compression gains in relation to the single finite-context model counterpart. Additionally, and potentially more interesting than the compression gain on its own, is the observation that the entropy associated to each of the three states differs and that this variation is not the same among the organisms analyzed. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A bit-plane approach for lossless compression of color-quantized images",
        "doc_scopus_id": "33947662496",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33947662496",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Bit-plane based encoders",
            "Color-quantized images",
            "Lossless compression"
        ],
        "doc_abstract": "Palette reordering is a well-known and effective approach for improving the compression of color-indexed images. The performance of most of the existing palette reordering methods has been evaluated using the compression rates attained by image coders which are based on predictive or transform coding technology, such as JPEG-LS or JPEG2000. In this paper, we show that image coding technology based on bit-plane coding, such as the one used by JBIG, provides a competitive performance in this class of images. Also, we present a palette reordering algorithm, adapted for bit-plane based encoders, that gives good results and is faster than Mention's or the modified Zeng's methods, the two most effective palette reordering algorithms. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A three-state model for DNA protein-coding regions",
        "doc_scopus_id": "33750311852",
        "doc_doi": "10.1109/TBME.2006.879477",
        "doc_eid": "2-s2.0-33750311852",
        "doc_date": "2006-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Compression gain",
            "DNA compression",
            "DNA sequence modeling",
            "Finite-context models"
        ],
        "doc_abstract": "It is known that the protein-coding regions of DNA are usually characterized by a three-base periodicity. In this paper, we exploit this property, studying a DNA model based on three deterministic states, where each state implements a finite-context model. The experimental results obtained confirm the appropriateness of the proposed approach, showing compression gains in relation to the single finite-context model counterpart. Additionally, and potentially more interesting than the compression gain on its own, is the observation that the entropy associated to each of the three base positions of a codon differs and that this variation is not the same among the organisms analyzed. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the relation between Memon's and the modified Zeng's palette reordering methods",
        "doc_scopus_id": "33646866379",
        "doc_doi": "10.1016/j.imavis.2006.02.005",
        "doc_eid": "2-s2.0-33646866379",
        "doc_date": "2006-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Color-indexed images",
            "JPEG-LS",
            "JPEG2000",
            "Lossless image compression",
            "Palette recording"
        ],
        "doc_abstract": "Palette reordering has been shown to be a very effective approach for improving the compression of color-indexed images by general purpose continuous-tone image coding techniques. In this paper, we provide a comparison, both theoretical and experimental, of two of these methods: the pairwise merging heuristic proposed by Memon et al. and the recently proposed modification of Zeng's method. This analysis shows how several parts of the algorithms relate and how their performance is affected by some modifications. Moreover, we show that Memon's method can be viewed as an extension of the modified version of Zeng's technique and, therefore, that the modified Zeng's method can be obtained through some simplifications of Memon's method. © 2006 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271526 291210 291718 291872 291874 31 Image and Vision Computing IMAGEVISIONCOMPUTING 2006-04-17 2006-04-17 2010-03-29T03:54:28 S0262-8856(06)00085-0 S0262885606000850 10.1016/j.imavis.2006.02.005 S300 S300.1 FULL-TEXT 2015-05-14T06:05:42.241066-04:00 0 0 20060501 2006 2006-04-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0262-8856 02628856 24 24 5 5 Volume 24, Issue 5 12 534 540 534 540 20060501 1 May 2006 2006-05-01 2006 article fla Copyright © 2006 Elsevier B.V. All rights reserved. RELATIONBETWEENMEMONSMODIFIEDZENGSPALETTEREORDERINGMETHODS PINHO A 1 Introduction 2 Palette reordering 2.1 Memon's method 2.2 Modified Zeng's method 3 Analysis of the algorithms 4 Experimental results 5 Conclusions Acknowledgements References WEINBERGER 2000 1309 1324 M SKODRAS 2001 36 58 A TAUBMAN 2002 D JPEG2000IMAGECOMPRESSIONFUNDAMENTALSSTANDARDSPRACTICE AUSBECK 2000 1779 1789 P CHEN 2002 904 908 X PINHO 2004 1411 1418 A MEMON 1996 1522 1527 N PINHO 2004 232 234 A PINHOX2006X534 PINHOX2006X534X540 PINHOX2006X534XA PINHOX2006X534X540XA item S0262-8856(06)00085-0 S0262885606000850 10.1016/j.imavis.2006.02.005 271526 2010-10-08T02:41:52.245775-04:00 2006-05-01 true 132019 MAIN 7 69700 849 656 IMAGE-WEB-PDF 1 si47 1176 35 325 si9 885 46 141 si8 133 15 21 si7 125 15 17 si64 1423 46 293 si63 1195 46 222 si62 219 17 50 si61 224 19 51 si60 450 17 118 si6 1092 35 225 si59 407 17 92 si58 323 18 84 si57 331 18 82 si56 255 16 48 si55 240 15 51 si54 416 17 106 si53 95 10 12 si52 104 12 12 si51 970 35 248 si50 741 35 153 si5 1060 36 267 si49 104 13 12 si48 297 19 76 si46 1074 38 266 si45 745 36 172 si44 700 19 226 si43 249 15 71 si42 322 18 84 si41 362 17 85 si40 338 19 84 si4 630 45 102 si39 771 36 162 si38 1846 85 355 si37 633 36 127 si36 1981 93 286 si35 1516 41 355 si34 1444 47 259 si33 531 23 118 si32 104 13 12 si31 104 12 12 si30 104 13 12 si29 994 35 250 si28 972 35 248 si27 886 35 215 si26 798 35 185 si25 104 12 12 si24 503 21 135 si23 739 36 161 si22 297 19 76 si21 104 12 12 si20 1448 125 112 si19 253 19 64 si18 2072 100 179 si17 354 19 84 si16 426 18 125 si15 443 18 128 si14 130 15 21 si13 125 15 17 si12 130 15 21 si11 125 15 17 si10 875 19 266 gr1 3978 68 266 gr1 776 32 125 IMAVIS 2368 S0262-8856(06)00085-0 10.1016/j.imavis.2006.02.005 Elsevier B.V. Fig. 1 Configurations of neighboring pixels, in relation to the pixel in gray, for constructing function w(i, j). Table 1 Comparison of the coding performance using JPEG-LS and lossless JPEG2000 for different constructions of the w(i, j) function Dither Colors Memon mZeng (a) (b) (c) (d) (a) (b) (c) (d) JPEG-LS No 64 2.641 2.660 2.647 2.663 2.709 2.719 2.709 2.727 128 3.441 3.461 3.461 3.469 3.559 3.569 3.552 3.565 256 4.204 4.253 4.225 4.247 4.473 4.482 4.475 4.505 Yes 64 3.341 3.329 3.307 3.327 3.436 3.445 3.420 3.441 128 4.112 4.150 4.122 4.125 4.239 4.226 4.226 4.238 256 4.870 4.890 4.865 4.887 5.107 5.115 5.120 5.138 Global average 3.768 3.791 3.771 3.786 3.921 3.926 3.917 3.936 JPEG2000 No 64 2.981 2.999 2.985 3.002 3.052 3.061 3.046 3.064 128 3.793 3.816 3.812 3.827 3.925 3.930 3.914 3.925 256 4.576 4.617 4.595 4.611 4.868 4.872 4.870 4.899 Yes 64 3.590 3.568 3.553 3.565 3.672 3.677 3.653 3.671 128 4.354 4.386 4.359 4.355 4.481 4.461 4.458 4.470 256 5.149 5.153 5.138 5.146 5.381 5.378 5.388 5.406 Global average 4.074 4.090 4.074 4.084 4.230 4.230 4.222 4.239 The configurations (a)–(d) are depicted in Fig. 1. Table 2 Comparison of the coding performance using JPEG-LS and lossless JPEG2000 for different initializations of Memon's and mZeng's reordering methods Dither Colors Memon MemonZi mZeng mZengMi bpp bpp % bpp bpp % JPEG-LS No 64 2.641 2.642 0.0 2.709 2.744 −1.3 128 3.441 3.443 −0.1 3.552 3.570 −0.5 256 4.204 4.206 −0.1 4.475 4.494 −0.4 Yes 64 3.341 3.341 0.0 3.420 3.438 −0.5 128 4.112 4.117 −0.1 4.226 4.258 −0.8 256 4.870 4.864 0.1 5.120 5.140 −0.4 JPEG2000 No 64 2.981 2.982 0.0 3.046 3.085 −1.3 128 3.793 3.791 0.1 3.914 3.932 −0.5 256 4.576 4.577 0.0 4.870 4.885 −0.3 Yes 64 3.590 3.590 0.0 3.653 3.671 −0.5 128 4.354 4.358 −0.1 4.458 4.499 −0.9 256 5.149 5.144 0.1 5.388 5.417 −0.5 ‘MemonZi’ refers to Memon's method using the initialization of mZeng's method, whereas ‘mZengMi’ refers to mZeng's method using the initialization of Memon's method. Table 3 Comparison of the coding performance using JPEG-LS and lossless JPEG2000 for several modifications of Memon's and mZeng's methods: ‘-Grp” indicates that the selection phase always comprises the largest set and one set of size one; ‘-INS’ indicates that only the merging options defined in (5) are allowed, whereas ‘+INS’ indicates the inclusion of the merging configurations defined in (6) Dither Colors Memon mZeng -GRP -INS -GRP -INS +INS bpp bpp % bpp % bpp % bpp % bpp % JPEG-LS No 64 2.641 2.655 −0.5 2.748 −4.1 2.722 −3.1 2.709 −2.6 2.660 −0.7 128 3.441 3.450 −0.3 3.616 −5.1 3.574 −3.9 3.552 −3.2 3.463 −0.6 256 4.204 4.256 −1.2 4.519 −7.5 4.481 −6.6 4.475 −6.4 4.322 −2.8 Yes 64 3.341 3.338 0.1 3.489 −4.4 3.460 −3.6 3.420 −2.4 3.321 0.6 128 4.112 4.122 −0.2 4.298 −4.5 4.263 −3.7 4.226 −2.8 4.112 0.0 256 4.870 4.951 −1.7 5.138 −5.5 5.130 −5.3 5.120 −5.1 4.926 −1.2 JPEG2000 No 64 2.981 3.000 −0.6 3.096 −3.9 3.068 −2.9 3.046 −2.2 2.996 −0.5 128 3.793 3.808 −0.4 3.995 −5.3 3.945 −4.0 3.914 −3.2 3.817 −0.6 256 4.576 4.643 −1.5 4.923 −7.6 4.872 −6.5 4.870 −6.4 4.711 −2.9 Yes 64 3.590 3.589 0.0 3.734 −4.0 3.700 −3.1 3.653 −1.8 3.565 0.7 128 4.354 4.364 −0.2 4.549 −4.5 4.511 −3.6 4.458 −2.4 4.352 0.0 256 5.149 5.225 −1.5 5.425 −5.4 5.404 −5.0 5.388 −4.6 5.192 −0.8 Percentages are relative to Memon's method. On the relation between Memon's and the modified Zeng's palette reordering methods Armando J. Pinho ⁎ António J.R. Neves Signal Processing Lab, DET/IEETA, University of Aveiro, Campus Universitario de Santiago, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Fax: +351 23 4370545. Palette reordering has been shown to be a very effective approach for improving the compression of color-indexed images by general purpose continuous-tone image coding techniques. In this paper, we provide a comparison, both theoretical and experimental, of two of these methods: the pairwise merging heuristic proposed by Memon et al. and the recently proposed modification of Zeng's method. This analysis shows how several parts of the algorithms relate and how their performance is affected by some modifications. Moreover, we show that Memon's method can be viewed as an extension of the modified version of Zeng's technique and, therefore, that the modified Zeng's method can be obtained through some simplifications of Memon's method. Keywords Color-indexed images Palette recording Lossless image compression JPEG-LS JPEG2000 1 Introduction A color-indexed image is represented by a matrix of indexes (the index image) and by a color-map or palette. Each index points to a color-map entry, establishing the corresponding color of the pixel. For a particular image, the mapping between index values and colors is not unique—it can be arbitrarily permuted, under the condition that the corresponding index image is changed accordingly. However, although equivalent in terms of representation, for most continuous-tone image coding techniques, such as JPEG-LS [1,2] or lossless JPEG2000 [3–5], different mappings may imply dramatic variations in the compression performance. Moreover, despite the existence of specialized approaches for coding color-indexed images (see, for example [6–9]), it remains an important issue to ensure that standard techniques are able to produce acceptable results within this class of images. Palette reordering is a class of preprocessing methods aiming at finding a permutation of the color palette such that the resulting image of indexes is more amenable for compression (for a survey, see [10]). These preprocessing techniques have the advantage of not requiring post-processing and of being cost-less in terms of side information. However, if the optimal configuration is sought, then the computational complexity involved can be high. In fact, the number of possible configurations for a table of M colors corresponds to the number of permutations of M objects, which equals M! Clearly, exhaustive search is impractical for most of the interesting cases, which motivated several sub-optimal, lower complexity, proposals. In this paper, we address two of the most effective palette reordering methods: Memon's method [11] and the modified Zeng's method [12] (for short, we will refer to the modified Zeng's method as mZeng's method). A comparison of the performance and computational complexity of these two methods is presented in [10]. The main objective of this paper is to provide a detailed analysis and comparison of both techniques, showing how they relate, which are their similarities and fundamental differences. A particularly interesting and potentially useful finding is that mZeng's method can be viewed as a simplified version of Memon's method. The remainder of the paper is organized as follows. In Section 2, both reordering methods are described, using an unifying notation, with the aim of exposing similarities and differences between them. In Section 3, a detailed comparison is performed. Section 4 proceeds with the comparison using experimental data. Finally, in Section 5, some conclusions are drawn. 2 Palette reordering Palette reordering methods can be classified into two main classes. To one of those classes, to which we refer as ‘color-based methods’, belong methods that are characterized by relying only on the information provided by the color palette. The other class characterizes techniques relying only on the statistical information conveyed by the index image, independently of its meaning in terms of color representation. We refer to the techniques in this class as ‘index-based methods’. The main idea behind index-based methods for palette reordering is that colors that occur frequently close to each other should have close indexes. Therefore, based on this principle, the assignment of the indexes is usually guided by some function, w(i, j), measuring the number of occurrences corresponding to pixels with index i that are spatially adjacent to pixels with index j, according to some predefined neighborhood. In the remainder of this section we describe two techniques that fall under the class of index-based methods, and which are among the best known palette reordering methods [10]: Memon's method [11] and the modified Zeng's method [12]. 2.1 Memon's method Memon et al. formulated the problem of palette reordering within the framework of linear predictive coding [11]. In that context, the objective is to minimize the zero-order entropy of the prediction residuals. They noticed that, for image data, the prediction residuals are often well modeled by a Laplacian distribution and that, in this case, minimizing the absolute sum of the prediction residuals leads to the minimization of the zero-order entropy of those residuals. For the case of a first-order prediction scheme, the absolute sum of the prediction residuals reduces to (1) ∑ i = 1 M ∑ j = 1 M c i j | i − j | , where c ij denotes the number of times index i is used as the predicted value for a pixel whose color is indexed by j. The problem of finding a palette reordering that minimizes (1) can be formulated as the optimization version of the linear ordering problem (also known as the minimum linear arrangement), whose decision version is known to be NP-complete [11]. In fact, if we consider a complete non-directed weighted graph G(V, E, w), where each vertex in V={v 1, v 2, …,v M }, corresponds to a palette color, and w(v i , v j )=c ij +c ji , (v i , v j )∈E, corresponds to the weight associated to the edge defined between vertices v i and v j , then the goal is to find a one-to-one mapping (permutation) σ: V→{1, 2,…,M}, among all possible permutations σ n , satisfying: (2) σ = arg min σ n ∑ ( v i , v j ) ∈ E w ( v i , v j ) | σ n ( v i ) − σ n ( v j ) | . With the aim of seeking approximate solutions for (2), Memon et al. proposed two heuristics: one based on simulated annealing, the other, faster to compute, based on a technique called ‘pairwise merging’. Essentially, the pairwise merging heuristic consists on repeatedly merging ordered sets of colors until obtaining a single set. Initially, each color (graph vertex) is assigned to a different set. Then, each iteration consists of two steps: Step 1: From all possible pairs of sets, choose the one satisfying: (3) ( S u , S v ) = arg max ( S a , S b ) ∑ a ∈ S a ∑ b ∈ S b w ( a , b ) . Step 2: Among all possible merging combinations of S u and S v , choose the one minimizing (4) ∑ i = 1 | S m | ∑ j > i | S m | ( j − i ) w ( m i , m j ) , where S m = { m 1 , m 2 , … , m | S m | } , | S m | = | S u | + | S v | , is the set under evaluation, obtained from a particular merging of S u and S v . The m i represent, therefore, the elements of sets S u and S v . To alleviate the computational burden involved in selecting the best way of merging the two ordered sets, Memon et al. proposed a reduced number of configurations [11]. If S u = { u 1 , u 2 , … , u | S u | } and S v = { v 1 , v 2 , … , v | S v | } are the two sets under evaluation, and if | S u | , | S v | > 1 , then the following configurations are considered: (5) { u 1 , u 2 , … , u | S u | , v 1 , v 2 , … , v | S v | } { u | S u | , … , u 2 , u 1 , v 1 , v 2 , … , v | S v | } { v 1 , v 2 , … , v | S v | , u 1 , u 2 , … , u | S u | } { v 1 , v 2 , … , v | S v | , u | S u | , … , u 2 , u 1 } . Alternatively, if one of the sets has size one, then the following configurations are tested (without loss of generality, we consider | S u | = 1 ) : (6) { u 1 , v 1 , v 2 , … , v | S v | } { v 1 , u 1 , v 2 , … , v | S v | } { v 1 , v 2 , u 1 , … , v | S v | } ⋮ { v 1 , v 2 , … , v | S v | , u 1 } . 2.2 Modified Zeng's method The palette reindexing method proposed by Zeng et al. [13] is based on an one-step look-ahead greedy approach, which aims at increasing the lossless compression efficiency of color-indexed images. In [12], a modification of Zeng's algorithm was proposed, relying on a Laplacian model for the distribution of first order prediction residuals, and on the assumption that the entropy of the absolute differences between neighboring pixels is a good indicator of the degree of compressibility of an image. The algorithm starts by finding the index that is most frequently located adjacent to other (different) indexes, and the index that is most frequently found adjacent to it. This pair of indexes is the starting base for an ordered set S that will be constructed, one index at a time, during the operation of the reindexing algorithm. We denote by v i the indexes already assigned to the ordered set (i indicates the position of the index in the ordered set and, therefore, its distance to the left side of the set) and by u those still unassigned. Therefore, just before starting the iterations, S = { v 1 , v 2 } , where (7) v 1 = arg max u j ∑ u i ≠ u j w ( u i , u j ) and (8) v 2 = arg max u w ( v 1 , u ) . The function w(i, j)=w(j, i) denotes the number of occurrences (measured on the initial index image) corresponding to pixels with index i that are spatially adjacent to pixels with index j. New indexes can only be attached to the left or to the right extremity of the ordered set. It is well-known that, for a memoryless source, the number of bits required to represent the occurrence of a given symbol s is given by −log2 P(s), where P(s) denotes the probability of occurrence of s. Therefore, we start by defining the estimated code length implied by placing index u on the left side of S (9) l L ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( i ) , and by placing it one position farther away (10) l L + ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( i + 1 ) . We also calculate similar estimates for the right side (11) l R ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( | S | − i + 1 ) , and (12) l R + ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( | S | − i + 2 ) . According to the greedy strategy of Zeng's algorithm, the next index, u ¯ , that should integrate S is the one that implies the largest increase in code length if its choice is postponed to the next iteration. Then, the new index, u ¯ , should satisfy (13) u ¯ = arg max u ∉ S Δ l ( u ) , with (14) Δ l ( u ) = { l L + ( u ) − l L ( u ) , if l L ( u ) < l R ( u ) l R + ( u ) − l R ( u ) , otherwise . In words, for each candidate index, u, its best position (left or right) is chosen, i.e. the one that minimizes the code length. Then, among all those indexes, we pick the one producing the largest increase in code length if its choice is postponed to the next iteration. Using (9) and (10), we can write (15) l L + ( u ) − l L ( u ) = ∑ v i ∈ S w ( u , v i ) log 2 P ( i ) P ( i + 1 ) = ∑ v i ∈ S α i w ( u , v i ) , if the best position for index u is the left side, or, using (11) and (12) (16) l R + ( u ) − l R ( u ) = ∑ v i ∈ S w ( u , v i ) log 2 P ( | S | − i + 1 ) P ( | S | − i + 2 ) = ∑ v i ∈ S α | S | − i + 1 w ( u , v i ) , if the best position is the right side, where (17) α k = log 2 P ( k ) P ( k + 1 ) , and where P(k) denotes the probability of occurrence of a difference of k units between two neighboring pixels. Moreover, we can also write (18) l R ( u ) − l L ( u ) = ∑ v i ∈ S w ( u , v i ) ( log 2 P ( i ) − log 2 P ( | S | − i + 1 ) ) = ∑ v i ∈ S β i w ( u , v i ) , where (19) β k = log 2 P ( k ) P ( | S | − k + 1 ) . The new ordered set will be { u ¯ , v 1 , … , v | S | } , if l L ( u ¯ ) < l R ( u ¯ ) , or { v 1 , … , v | S | , u ¯ } , otherwise. This iterative process should continue until assigning all indexes. Finally, the re-indexed image is constructed by applying the mapping v i ↦ ( i − 1 ) to all image pixels, and changing the color-map accordingly. For exponentially distributed residuals, i.e. considering (20) P ( k ) = A θ k , 0 < θ < 1 , 0 ≤ k < M , Eq. (17) reduces to (21) α k = log 2 A θ k A θ k + 1 = − log 2 θ , and (19) reduces to (22) β k = log 2 A θ k A θ | S | − k + 1 = ( 2 k − | S | − 1 ) log 2 θ , i.e. the parameter β k decreases linearly with k (note that log2 θ<0), and α k can be set to an arbitrary positive value, e.g. one. Moreover, note that, in this case: (23) Δ l ( u ) = l L + ( u ) − l L ( u ) = l R + ( u ) − l R ( u ) = ∑ v i ∈ S w ( a , v i ) . Summarizing, the mZeng's algorithm is composed of an initialization phase, and two iterative steps: Initialization: Construct the initial set S = { v 1 , v 2 } , as given by (7) and (8). Step 1: Compute the next index, u ¯ , using (24) u ¯ = arg max a ∉ S ∑ v i ∈ S w ( a , v i ) . Step 2: Compute (25) l R ( u ¯ ) − l L ( u ¯ ) = ∑ v i ∈ S ( | S | − 2 i + 1 ) w ( u ¯ , v i ) and choose the left side of S to attach u ¯ if l R ( u ¯ ) − l L ( u ¯ ) > 0. 3 Analysis of the algorithms In Section 2, we presented Memon's and mZeng's techniques for palette reordering. We now proceed with a comparative discussion of both algorithms, with the aim of identifying similarities and differences among them. To do so, we will focus our analysis mainly in two points, corresponding to steps 1 and 2 of the algorithms, and to which we refer as the selection and merging phases, respectively. However, before we address these two points, we will discuss some issues regarding the construction of the w(i, j) function and also regarding the initialization phase of the algorithms. The construction of the w(i, j) function, responsible for conveying the information of how frequently the pairs of neighboring pixels occur in the image, has typically been left somewhat vague by the authors. Memon referred that this function should reflect the number of times a given index i is used as the predicted value of a pixel having index j [11]. However, he did not mention if that hypothetical first-order predictor uses the left pixel, the upper pixel or even the upper-left pixel. Zeng referred that function w(i, j) should represent the number of occurrences of pixels with index i that are spatially adjacent to pixels with index j [13]. However, the type of neighborhood, 4-connected or 8-connected, was not specified. To better understand how a particular choice of this function affects the performance of the reordering algorithms and, therefore, the compression efficiency of the coding methods applied after reordering, we performed some experiments using the four configurations depicted in Fig. 1 . Detailed results are reported in Table 1 , and show that, according to the experiments performed, the best configuration for Memon's method is that presented in Fig. 1(a), whereas for mZeng's method is the one depicted in Fig. 1(c). Regarding the initialization phase, while it is explicit for mZeng's method, defined by (7) and (8), for Memon's technique it coincides with step 1 of the algorithm. In general, the first pair of indexes generated by the two techniques differs, because the most frequent pair, i.e. the one chosen by Memon's method, does not necessarily contain the most frequent index. Once more, it is not evident of how a particular choice of the first pair of indexes might influence the final outcome of the algorithms. To get some indication on this matter, we ran a simple test consisting on permuting the two initialization approaches between the two algorithms (detailed results are given in Table 2 ). The outcome of this experiment was that for Memon's method the effect was virtually null. However, plugging the initialization approach of Memon's method into mZeng's technique produced an average decrease in lossless compression of around 0.6%. The main conclusion that can be drawn from this simple experiment is that Memon's method seems to be more tolerant to the initialization than mZeng's method. In fact, this is not surprising, since whereas the former has the possibility of splitting apart the first pair at a later time, therefore correcting for poor initializations, the latter does not have this flexibility. Step 1, the selection phase, shows some apparently significant differences. As can be observed from the comparison of (3) and (24), whereas (3) seeks the best pair of sets, ( S u , S v ) , among all possible pairs, (24) seeks the best pair, ( { u ¯ } , S ) , but only among the pairs formed by one particular set (which is always the largest) and all the other sets (which are always of size one). Therefore, Memon's method has a greater freedom for picking up the next couple of sets to be merged. In practice, and based on the experimental data set described in Section 4, we found that, on average, the best pair obtained with (3) contains the largest set together with a set of size one for about 80% of the time. In other words, we found that in four out of five iterations, step 1 of Memon's method provides the same output as step 1 of mZeng's method. Step 2, the merging phase, differs considerably in both algorithms. In fact, whereas for mZeng's technique only two possibilities are considered, i.e. (26) { u ¯ , v 1 , … , v | S | } and (27) { v 1 , … , v | S | , u ¯ } , Memon's method considers, most often, max ( | S u | , | S v | ) merging configurations. This claim is related to the observation already pointed out when step 1 was discussed, i.e. that frequently min ( | S u | , | S v | ) = 1 , which forces the algorithm to use the combinations described in (6). Moreover, if those combinations are removed and only those described in (5) are allowed, then the merging phase of both algorithms becomes equivalent in terms of combinations tested (note that, when | S u | = 1 or | S v | = 1 , then only two of the four configurations described in (5) are different, and those coincide with (26) and (27)). Still regarding step 2 of the algorithms, it remains to be shown that (4) and (25) are, in fact, equivalent. We start by noting that, for configuration (26), Eq. (4) can be written as (28) ∑ i = 1 | S | ∑ j > i | S | ( j − i ) w ( v i , v j ) + ∑ i = 1 | S | i w ( u , v i ) and, for configuration (27), it results in (29) ∑ i = 1 | S | ∑ j > i | S | ( j − i ) w ( v i , v j ) + ∑ i = 1 | S | ( | S | − i + 1 ) w ( u , v i ) . Subtracting (28) from (29) we obtain (25), which shows that, for the two configurations represented in (26) and (27), step 2 of both algorithms are equivalent. The main objective of this section was to show that, although developed using different approaches, the algorithms underlying Memon's and mZeng's techniques share some common points. Effectively, despite the fact of both being based on a Laplacian model of the differences among neighboring pixels, they proposed different heuristic algorithms to address the optimization problem. Moreover, the analysis that we performed showed that Memon's method can be viewed as an extension of mZeng's method. In the remainder of this paper we proceed with the comparison of both methods, but now with a more experimental character. We will provide detailed experimental results regarding the already addressed aspects of the construction of the w(i, j) function and the initialization phase. Also, we will present and discuss results obtained after changing some parts of both algorithms, in order to better understand, in practice, their similarities and differences. 4 Experimental results In this section, we present experimental results based on the set of the 23 ‘kodak’ 768×512 true color images. 1 1 These images can be obtained from Color quantization was applied, both with and without Floyd–Steinberg color dithering, creating images with 256, 128 and 64 colors. Image manipulations have been performed using version 1.2.3 of the ‘Gimp’ program. 2 2 The color-quantized images can be obtained from Compression results are given for JPEG-LS 3 3 Using V2.2 of the SPMG JPEG-LS codec with default parameters and for lossless JPEG2000. 4 4 Using the JasPer 1.700.2 JPEG2000 codec with default parameters The compression figures presented in Tables 1–3 , in bits per pixel (bpp), represent average results over the 23 images, and include the sizes of the uncompressed color-maps. The first set of experiments, presented in Table 1, evaluates the impact of each of the four configurations depicted in Fig. 1, used to build the function w(i, j)=w(j, i), on the compressibility of the reordered images. As can be seen, configuration (a) is the one that globally shows the best results for Memon's technique. For mZeng's method, the experimental results indicate configuration (c) as the best. However, the results also show that the gain provided by these configurations is only marginal. Table 2 presents detailed results regarding the initialization phase of both reordering methods. As can be seen, the impact of using mZeng's initialization on Memon's method (denoted by ‘MemonZi’ in the Table 2) is minimal. On the contrary, the effect of using the initialization procedure of Memon's method in mZeng's technique deserves to be considered. In fact, in this case, the average loss in compression performance is around 0.6%, showing a higher sensibility of mZeng's method to the appropriateness of the first pair of indexes that is created. As mentioned in Section 3, this is due to the reduced flexibility of mZeng's method when compared to Memon's method in terms of the merging capabilities. In Table 3, we show how the methods behave when some modifications are performed in what we denoted as steps 1 and 2 of the algorithms. The first column of results in Table 3 is devoted to Memon's method (using its own initialization procedure and the configuration of Fig. 1(a) for the construction of w(i, j)). The percentages included in Table 3 have been calculated in relation to the compression values displayed in the first column. The results presented under label ‘-GRP’ have been generated by suppressing the capability of Memon's method to select an arbitrary pair of sets, limiting the choice only to the largest set and to a size-one set. In other words, this corresponds of using step 1 of mZeng's method. As can been observed, in general, there is a decrease in compression performance, somewhat more accentuated for images with more colors. The results in the column labeled ‘-INS’ have been obtained by suppressing the ability of Memon's method to insert size-one sets into the other set, during the merging phase. This corresponds to restricting the merging combinations to only those in (5), i.e. corresponds to using step 2 of mZeng's method. As can be seen, in this case the decrease in compression performance is much larger than in the previous case, showing that the possibility of interleaving indexes during the merging phase is a major difference between the two methods. The results of a final modification of Memon's method are reported in the column with label ‘-GRP -INS’, where the two modifications just described have been combined. Surprisingly, in this case the results are better than for the ‘-INS’ case. Our conjecture regarding this behavior is that allowing the selection of arbitrary pairs of sets, as defined by step 1 of Memon's method, without allowing breaking some sets in the future, prevents the correction of some poor selections made in previous iterations. Therefore, from the experimental results obtained, it seems that for step 1 of Memon's method to be effective, then step 2 should include the merging configurations of (6). It is worthwhile to note that if the initialization procedure and the w(i, j) function of mZeng's method are used in Memon's method with the ‘-GRP -INS’ modifications then both algorithms become equivalent. Although, we do not have included those results in Table 3, we verified that, in fact, they are equal. Instead, we include in Table 3 the results of modifying mZeng's method through the inclusion of the capability of putting new indexes not only in the extremities of the set, but also inside it (see column ‘+INS’). This corresponds to using step 2 of Memon's method in mZeng's method. As can be seen, this modification provides a considerable improvement. However, this is also the part of the algorithm that imposes a complexity of o(M 4), instead of the o(M 3) complexity of the original mZeng's method [10]. 5 Conclusions In this paper, we provided a detailed description and analysis of two of the most effective palette reordering methods, used for improving the compression of color-indexed images by general purpose continuous-tone lossless image coding techniques. Our objective was to show how these two methods relate and how different parts of their corresponding algorithms contribute to their performance. The main conclusion of this study was that Memon's method can be viewed as an extension of mZeng's method, the latter being included into the former. To achieve this objective, we performed a step by step comparison of both methods using an unifying notation. Moreover, we provided a discussion on issues related to the construction of the function that conveys the neighboring information and also on the effect of the initialization phase. With this work, we believe having contributed to a better understanding of how state-of-the-art techniques for palette reordering work, easing the task of those seeking further improvements. Acknowledgements This work was supported in part by the Fundação para a Ciência e a Tecnologia (FCT). References [1] ISO/IEC, Information technology—Lossless and near-lossless compression of continuous-tone still images, ISO/IEC 14495-1 and ITU Recommendation T.87, 1999. [2] M.J. Weinberger G. Seroussi G. Sapiro The LOCO-I lossless image compression algorithm: principles and standardization into JPEG-LS IEEE Transactions on Image Processing 9 8 2000 1309 1324 [3] ISO/IEC, Information technology—JPEG 2000 image coding system, ISO/IEC International Standard 15444-1, ITU-T Recommendation T.800, 2000. [4] A. Skodras C. Christopoulos T. Ebrahimi The JPEG 2000 still image compression standard IEEE Signal Processing Magazine 18 5 2001 36 58 [5] D.S. Taubman M.W. Marcellin JPEG 2000: Image Compression Fundamentals, Standards and Practice 2002 Kluwer Dordecht MA [6] P.J. Ausbeck Jr. The piecewise constant image model Proceedings of the IEEE 88 11 2000 1779 1789 [7] Y. Yoo, Y.G. Kwon, A. Ortega, Embedded image-domain compression using context models, in: Proceedings of the Sixth IEEE International Conference on Image Processing, ICIP-99, vol. I, Kobe, Japan, October 1999, pp. 477–481. [8] V. Ratnakar, RAPP: Lossless image compression with runs of adaptive pixel patterns, in: Proceedings of the 32nd Asilomar Conference on Signals, Systems, and Computers, 1998, vol. 2, 1998, pp. 1251–1255. [9] X. Chen S. Kwong J.-F. Feng A new compression scheme for color-quantized images IEEE Transactions on Circuits and Systems for Video Technology 12 10 2002 904 908 [10] A.J. Pinho A.J.R. Neves A survey on palette reordering methods for improving the compression of color-indexed images IEEE Transactions on Image Processing 13 11 2004 1411 1418 [11] N.D. Memon A. Venkateswaran On ordering color maps for lossless predictive coding IEEE Transactions on Image Processing 5 5 1996 1522 1527 [12] A.J. Pinho A.J.R. Neves A note on Zeng's technique for color reindexing of palette-based images IEEE Signal Processing Letters 11 2 2004 232 234 [13] W. Zeng, J. Li, S. Lei, An efficient color re-indexing scheme for palette-based compression, in: Proceedings of the Seveth IEEE International Conference on Image Processing, ICIP-2000, vol. III, Vancouver, Canada, September 2000, pp. 476–479. "
    },
    {
        "doc_title": "On the use of standards for microarray lossless image compression",
        "doc_scopus_id": "33344465888",
        "doc_doi": "10.1109/TBME.2005.869782",
        "doc_eid": "2-s2.0-33344465888",
        "doc_date": "2006-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Image coding standards",
            "JBIG",
            "JPEG-LS",
            "JPEG2000",
            "Lossless image compression",
            "Lossy-to-lossless compression",
            "Microarray images"
        ],
        "doc_abstract": "The interest in methods that are able to efficiently compress microarray images is relatively new. This is not surprising, since the appearance and fast growth of the technology responsible for producing these images is also quite recent. In this paper, we present a set of compression results obtained with 49 publicly available images, using three image coding standards: lossless JPEG2000, JBIG, and JPEG-LS. We concluded that the compression technology behind JBIG seems to be the one that offers the best combination of compression efficiency and flexibility for microarray image compression. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancing the reactivity of the vision subsystem in autonomous mobile robots using real-time techniques",
        "doc_scopus_id": "37249064834",
        "doc_doi": "10.1007/11780519_33",
        "doc_eid": "2-s2.0-37249064834",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous Mobile Robot",
            "Computation resources",
            "Mobile autonomous agents",
            "Modular architectures",
            "Proposed architectures",
            "Real-time techniques",
            "Robotic soccer team",
            "Vision based algorithms"
        ],
        "doc_abstract": "Interest on using mobile autonomous agents has been growing, recently, due to their capacity to cooperate for diverse purposes, from rescue to demining and security. In many of these applications the environments are inherently unstructured and dynamic, requiring substantial computation resources for gathering enough sensory input data to allow a safe navigation and interaction with the environment. As with humans, who depend heavily on vision for these purposes, mobile robots employ vision frequently as the primary source of input data when operating in such environments. However, vision-based algorithms are seldom developed with reactive and real-time concerns, exhibiting large variations in the execution time and leading to occasional periods of black-out or vacant input data. This paper addresses this problem in the scope of the CAMBADA robotic soccer team developed at the University of Aveiro, Portugal. It presents an evolution from a monolithic to a modular architecture for the vision system that improves its reactivity. With the proposed architecture it is possible to track different objects with different rates without losing any frames. © springer-Verlag Berlin Heidelberg 2006.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless bit-plane compression of microarray images using 3D context models",
        "doc_scopus_id": "84887250729",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887250729",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Bitplane coding",
            "Context models",
            "Image coding standard",
            "Lossless image compression",
            "Lossy-to-lossless",
            "Microarray images"
        ],
        "doc_abstract": "With the recent growth of interest in microarray technology and the improvement of the technology responsible for producing these images, massive amounts of microarray images are currently being produced. In this paper, we present a new lossless method for efficiently compress microarray images based on arithmetic coding using a 3D context model. Our method produces an embedded bitstream that allows progressive decoding. We present the compression results using 49 publicly available images and we compare these results with three image coding standards: lossless JPEG2000, JBIG and JPEG-LS. The proposed method gives better results for all images of the test set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A context adaptation model for the compression of images with a reduced number of colors",
        "doc_scopus_id": "33749587629",
        "doc_doi": "10.1109/ICIP.2005.1530161",
        "doc_eid": "2-s2.0-33749587629",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Binary tree representation",
            "Color quantized images",
            "Context based arithmetic coding"
        ],
        "doc_abstract": "Recently, Chen et al. proposed a method for compressing color-quantized images that is based on a binary tree representation of colors and on context-based arithmetic coding with variable size templates. In this paper, we address the problem of context adaptation and we propose a model that provides improvements in the lossless compression capabilities of Chen's method. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Palette reordering under an exponential power distribution model of prediction residuals",
        "doc_scopus_id": "20444459122",
        "doc_doi": "10.1109/ICIP.2004.1418800",
        "doc_eid": "2-s2.0-20444459122",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Color-maps",
            "Exponents",
            "Palette recording",
            "Power distribution models"
        ],
        "doc_abstract": "Palette reordering is one of the most effective approaches for improving the compression of color-indexed images. Recently, a theoretically motivated modification of a reordering technique proposed by Zeng et al. was suggested, based on an exponential distribution model of the prediction residuals. In this paper, we develop this theoretical analysis further, exploiting a broader model based on exponential power distributions. ©2004 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Variable size block-based histogram packing for lossless coding of color-quantized images",
        "doc_scopus_id": "11144267772",
        "doc_doi": null,
        "doc_eid": "2-s2.0-11144267772",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Color-quantized images",
            "Histogram packing",
            "JPEG-LS",
            "Lossless image compression"
        ],
        "doc_abstract": "Previous work has shown that the lossless compression of color-quantized images, by general purpose continuoustone encoders, can be improved if histogram packing is performed on a block by block basis, prior to compression. In this paper, we extend this idea, proposing an algorithm that adaptively finds a more appropriate region size for histogram packing. With this new algorithm, a balance between the effectiveness of the histogram packing operation and the overhead spent in representing the mapping tables is automatically sought, providing a further increase in the lossless compression gains.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A survey on palette reordering methods for improving the compression of color-indexed images",
        "doc_scopus_id": "7444260917",
        "doc_doi": "10.1109/TIP.2004.836168",
        "doc_eid": "2-s2.0-7444260917",
        "doc_date": "2004-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Color-indexed images",
            "Joint Photographic Experts Group (JPEG)",
            "Lossless image compression",
            "Palette reordering"
        ],
        "doc_abstract": "Palette reordering is a well-known and very effective approach for improving the compression of color-indexed images. In this paper, we provide a survey of palette reordering methods, and we give experimental results comparing the ability of seven of them in improving the compression efficiency of JPEG-LS and lossless JPEG 2000. We concluded that the pairwise merging heuristic proposed by Memon et al. is the most effective, but also the most computationally demanding. Moreover, we found that the second most effective method is a modified version of Zeng's reordering technique, which was 3%-5% worse than pairwise merging, but much faster. © 2004 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A note on Zeng's technique for color reindexing of palette-based images",
        "doc_scopus_id": "0442295580",
        "doc_doi": "10.1109/LSP.2003.821758",
        "doc_eid": "2-s2.0-0442295580",
        "doc_date": "2004-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Color indexed image compression",
            "Color reindexing",
            "Lossless image compression",
            "Palette based images"
        ],
        "doc_abstract": "Palette reindexing is a well-known and very effective approach for improving the compression of color-indexed images. In this letter, we address the reindexing technique proposed by Zeng et al. and we show how its performance can be improved through a theoretically motivated choice of parameters. Experimental results show the practical appropriateness of the proposed modification.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of some reordering techniques for image VQ index compression",
        "doc_scopus_id": "35048904504",
        "doc_doi": "10.1007/978-3-540-30125-7_38",
        "doc_eid": "2-s2.0-35048904504",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color indexed images",
            "Degree of correlations",
            "Lossless compression",
            "Lossless data compression",
            "Lossless image coding",
            "Palette reordering",
            "Reordering techniques",
            "VQ index compression"
        ],
        "doc_abstract": "Frequently, it is observed that the sequence of indexes generated by a vector quantizer (VQ) contains a high degree of correlation, and, therefore, can be further compressed using lossless data compression techniques. In this paper, we address the problem of codebook reordering regarding the compression of the image of VQ indexes by general purpose lossless image coding methods, such as JPEG-LS or CALIC. We present experimental results showing that techniques available for palette reordering of color-indexed images can also be used successfully for improving the lossless compression of images of VQ indexes. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless compression of color-quantized images using block-based palette reordering",
        "doc_scopus_id": "35048840166",
        "doc_doi": "10.1007/978-3-540-30125-7_35",
        "doc_eid": "2-s2.0-35048840166",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Block packing",
            "Color indexed images",
            "Color-quantized images",
            "Compression gain",
            "Lossless compression",
            "Lossless image coding",
            "Palette reordering",
            "Reordering techniques"
        ],
        "doc_abstract": "It is well-known that the lossless compression of color-indexed images can be improved if a suitable reordering of the palette is performed before encoding the images. In this paper, we show that, if this reordering is made in a block basis, then further compression gains can be attained. Moreover, we show that the use of block-based palette reordering can outperform a previously proposed block packing procedure. Experimental results using a JPEG-LS encoder are presented, showing how different reordering methods behave. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "JPEG 2000 coding of color-quantized images",
        "doc_scopus_id": "0345566188",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0345566188",
        "doc_date": "2003-12-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Color quantization",
            "Histograms"
        ],
        "doc_abstract": "The efficiency of compressing color-quantized images using general purpose lossless image coding methods depends on the degree of smoothness of the index images. A wellknown and very effective approach for increasing smoothness relies on palette reordering techniques. In this paper, we show that these reordering methods may leave some room for further improvements in the compression performance. More precisely, we provide experimental results showing that the JPEG 2000 lossless compression of palette reordered color-quantized natural images can be further improved if histogram packing is applied on a regional basis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the efficiency of luminance-based palette reordering of color-quantized images",
        "doc_scopus_id": "35248849911",
        "doc_doi": "10.1007/978-3-540-44871-6_89",
        "doc_eid": "2-s2.0-35248849911",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color indexed images",
            "Color-quantized images",
            "Complex methods",
            "Experimental evidence",
            "Natural images",
            "Palette reordering"
        ],
        "doc_abstract": "Luminance-based palette reordering is often considered less efficient than other more complex approaches, in what concerns improving the compression of color-indexed images. In this paper, we provide experimental evidence that, for color-quantized natural images, this may not be always the case. In fact, we show that, for dithered images with 128 colors or more, luminance-based reordering outperforms other more complex methods. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Block-based histogram packing of color-quantized images",
        "doc_scopus_id": "11144291958",
        "doc_doi": "10.1109/ICME.2003.1220924",
        "doc_eid": "2-s2.0-11144291958",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Color tables",
            "Color-quantized images",
            "Compression efficiency",
            "Effective approaches",
            "Histogram packing",
            "Indexed image",
            "Natural images",
            "Preprocessing techniques"
        ],
        "doc_abstract": "© 2003 IEEE.Reordering the color table of color-quantized images is a very effective approach towards the improvement of the compression efficiency of color-indexed images. In this paper, we provide experimental results showing that, for images generated by color-quantization of full color natural images, histogram packing applied on a block basis is able to provide additional compression improvements of around 15%. In other words, we show that the improvements provided by a global color table reordering can be further increased by applying a local preprocessing technique which packs the histograms of regions (blocks) of the index image.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Why does histogram packing improve lossless compression rates?",
        "doc_scopus_id": "0036699055",
        "doc_doi": "10.1109/LSP.2002.803018",
        "doc_eid": "2-s2.0-0036699055",
        "doc_date": "2002-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Context-based adaptive lossless image coding (CALIC)",
            "Histogram packing"
        ],
        "doc_abstract": "The performance of state-of-the-art lossless image coding methods [such as JPEG-LS, lossless JPEG-2000, and context-based adaptive lossless image coding (CALIC)] can be considerably improved by a recently introduced preprocessing technique that can be applied whenever the images have sparse histograms. Bitrate savings of up to 50% have been reported, but so far no theoretical explanation of the fact has been advanced. This letter addresses this issue and analyzes the effect of the technique in terms of the interplay between histogram packing and the image total variation, emphasizing the lossless JPEG-2000 case.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Histogram packing, total variation, and lossless image compression",
        "doc_scopus_id": "84960921800",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84960921800",
        "doc_date": "2002-03-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Bit-rate savings",
            "Compression algorithms",
            "Histogram packing",
            "Lossless image compression",
            "Preprocessing techniques",
            "State of the art",
            "Theoretical foundations",
            "Total variation"
        ],
        "doc_abstract": "© 2002 EUSIPCO.State-of-the-art lossless image compression methods, such as JPEG-LS, lossless JPEG-2000, and CALIC, perform considerably better on images with sparse histograms when a recently introduced preprocessing technique is used. Bitrate savings of up to 50% have been reported, but so far there is no firm theoretical foundation for this surprising performance. In this paper we address this issue, and attempt to explain how the preprocessing stage, which basically packs the histogram of the images, affects the image total variation, and as a result the ability of the compression algorithms to work more effectively.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improvement of the lossless compression of images with quasi-sparse histograms",
        "doc_scopus_id": "84960901151",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84960901151",
        "doc_date": "2002-03-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Histogram packing",
            "Lossless compression"
        ],
        "doc_abstract": "© 2002 EUSIPCO.Off-line histogram packing is a known method which is capable of producing improvements if applied prior to the lossless compression of images having sparse histograms. However, this technique becomes useless if the image possesses a quasi-sparse histogram, even if it differs from the strictly sparse case only by a minimal margin. In this paper, we present a technique that is able to overcome this drawback and we present results showing its effectiveness.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preprocessing techniques for improving the lossless compression of images with quasi-sparse and locally sparse histograms",
        "doc_scopus_id": "32344433123",
        "doc_doi": "10.1109/ICME.2002.1035861",
        "doc_eid": "2-s2.0-32344433123",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Archeology (arts and humanities)",
                "area_abbreviation": "ARTS",
                "area_code": "1204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Lossless compression",
            "Preprocessing techniques"
        ],
        "doc_abstract": "© 2002 IEEE.Among the characteristics found relatively frequently in computer-generated images, but that are usually not found in natural images, is intensity histogram sparseness. The difficulties shown by state-of-the-art image coding algorithms in properly compressing images with sparse histograms have been pointed out in some recent works. In this paper, we address not only the problem of compressing images belonging to this class, but also the problem of compressing images that, although not possessing histograms that are strictly sparse, can be classified as quasi-sparse or locally sparse. We propose some simple preprocessing techniques that may lead to some dramatic improvements in the compression ratios attained by state-of-the-art image coding techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A comparison of methods for improving the lossless compression of images with sparse histograms",
        "doc_scopus_id": "0036452011",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0036452011",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Lossless compression",
            "Sparse histograms"
        ],
        "doc_abstract": "The importance of efficiently compressing images that are characterized by having sparse histograms is been increasing. A good indicator of this fact is the recent introduction of an extended prediction mode in Part 2 of the JPEG-LS standard, with the aim of improving the compression of this class of images. In this paper, we describe and compare several methods that share the same objective: to overcome the lack of compression efficiency that most of the general purpose image compression techniques usually show when handling images that have sparse histograms. This comparison allow us to conclude that a recently proposed online preprocessing technique, albeit quite simple, is also the most effective and versatile.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An online preprocessing technique for improving the lossless compression of images with sparse histograms",
        "doc_scopus_id": "0036222746",
        "doc_doi": "10.1109/97.988715",
        "doc_eid": "2-s2.0-0036222746",
        "doc_date": "2002-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Histograms",
            "Online preprocessing"
        ],
        "doc_abstract": "The problem of improving the efficiency of lossless compression of images with sparse histograms was discussed. Online preprocessing techniques were suggested for the problems. The reversible discrete wavelet transform which JPEG-2000 standards used for lossless compression provided a considerable improvement over the prediction based lossless mode of JPEG. Three sets of images were used to assess the efficiency of the preprocessing technique. The technique provided improvements in compression ratio of images and good robustness.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Region-based near-lossless image compression",
        "doc_scopus_id": "0034855017",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034855017",
        "doc_date": "2001-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Context modeled prediction residuals",
            "Entropy encoding",
            "Image partition",
            "Region based encoder",
            "Sparse histograms"
        ],
        "doc_abstract": "We present a near-lossless technique for the compression of images, which is based on the partitioning of the image into regions of constant intensity. The boundary information associated with the image partition is encoded with the method of the transition points. The compression of the intensities of the regions is based on the usual entropy encoding of the context-modeled prediction residuals. The experimental results show that this approach is able to provide significant compression improvements in images having sparse histograms, for small L∞ errors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the impact of histogram sparseness on some lossless image compression techniques",
        "doc_scopus_id": "0035173498",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0035173498",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Histogram sparseness",
            "Lossless image compression",
            "Offline histogram packing",
            "Online histogram packing"
        ],
        "doc_abstract": "Most of the image compression techniques currently available were designed mainly with the aim of compressing continuous-tone natural images. However, if this assumption is not verified, such as in the case of histogram sparseness, a degradation in compression performance may occur. In this paper, we analyze the impact of histogram sparseness in three state-of-the-art lossless image compression techniques: JPEG-LS, CALIC and lossless JPEG-2000. Moreover, we propose a simple procedure for on-line histogram packing, which holds nearly the same improvement as offline histogram packing. Results of its effectiveness when associated with JPEG-LS are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptive context-based arithmetic coding of arbitrary contour maps",
        "doc_scopus_id": "0035129080",
        "doc_doi": "10.1109/97.889634",
        "doc_eid": "2-s2.0-0035129080",
        "doc_date": "2001-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Adaptive context based arithmetic coding",
            "Contour maps",
            "Differential chain coding",
            "Motion picture experts group",
            "Partition coding",
            "Shape coding"
        ],
        "doc_abstract": "An efficient technique for encoding arbitrary contours is presented. It is based on the concept of transition point of a contour map and relies on a new four-symbol adaptive context-based arithmetic encoder (CAE) that calculates contexts in the (binary) domain of the contour map. The results obtained with this new technique applied to the compression of image partitions are substantially better than those previously attained, showing a clear superiority over adaptive CAE-based differential chain coding and also over the MPEG-4 shape coder.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "High quality region-based video coding",
        "doc_scopus_id": "0034442224",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034442224",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Region based video coding",
            "Video codec"
        ],
        "doc_abstract": "Traditionally, region-based image and video coding have been addressed only in the context of low and very low bit-rate coding. However, one of the most interesting by-products of region-based image coding is the possibility of manipulation of image content at a reduced additional cost, allowing simple integration of content-based functionalities. In this paper, we address the topic of high quality region-based video coding and present experimental results showing that, using some recent encoding techniques, this goal can be achieved. An important part of the video codec, that one responsible for partition coding, relies on the recently proposed contour coding technique based on transition points, which is characterized by a good performance on high complexity contours.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A JBIG-based approach to the encoding of contour maps",
        "doc_scopus_id": "0033732224",
        "doc_doi": "10.1109/83.841538",
        "doc_eid": "2-s2.0-0033732224",
        "doc_date": "2000-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Chain coding",
            "Contour coding",
            "Joint bilevel image expert group",
            "Partition coding",
            "Region based image coding",
            "Shape coding",
            "Transition points"
        ],
        "doc_abstract": "Recently, a new technique for the lossless encoding of contour maps was introduced, based on the concept of \"transition points.\" In this paper, we show that by using a simple representation for the transition points, i.e., the map of transition points, the problem of encoding arbitrary contour maps can be easily converted into a problem of lossless image coding. Experimental results show that the joint bilevel image experts group (JBIG) image coding standard can be used successfully for encoding maps of transition points, outperforming, in most cases, differential chain-coding. Moreover, if we take into account that JBIG codecs are available as \"off the shelf\" components, then the effort required to implement the proposed method is small. © 2000 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Simplification of image partitions for region-based image coding",
        "doc_scopus_id": "84937057524",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84937057524",
        "doc_date": "2000-03-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Coding techniques",
            "Compression efficiency",
            "Encoding techniques",
            "Image partition",
            "Majority filters",
            "Reconstruction error",
            "Simplification method",
            "Transition point"
        ],
        "doc_abstract": "© 2000 EUSIPCO.In this paper we address the problem of lossy image partition encoding, and compare the compression efficiency of two contour coding techniques associated with appropriate contour simplification methods: the majority filter for the case of chain coding; the reduction of the number of transition points for the encoding technique based on transition points. The experimental results show that chain coding only perform better when unconstrained simplification is allowed, i.e., when the reconstruction error is unlimited. In all other cases, for which the error is bounded, the encoding technique based on transition points shows superior compression efficiency.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Constrained and unconstrained simplification of image partitions encoded with the method of transition points",
        "doc_scopus_id": "0034445530",
        "doc_doi": "10.1109/icip.2000.899861",
        "doc_eid": "2-s2.0-0034445530",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Contour maps",
            "Image partitions",
            "Simplification",
            "Transition point"
        ],
        "doc_abstract": "The encoding of image partitions plays a key role in the overall efficiency of the so-called region-based image coding techniques. Recently, a method based on the novel concept of \"transition point\" was proposed for the lossless encoding of arbitrary contour maps. This paper addresses the problem of partition simplification in the context of this new encoding method. A new simplification approach is introduced and results concerning its compression efficiency under constrained and unconstrained conditions are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Encoding of image partitions using a standard technique for lossless image compression",
        "doc_scopus_id": "0032649118",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0032649118",
        "doc_date": "1999-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Recently, a new technique for the lossless encoding of boundary maps was introduced, which is based on the concept of `transition points'. In this paper we show that, using a simple representation for the transition points, it is possible to use the JBIG image coding standard for the encoding of image partitions. Moreover, this new approach outperforms, in most cases, differential chain-coding both in efficiency and simplicity of implementation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Encoding of closed boundaries using transition points",
        "doc_scopus_id": "0032305920",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0032305920",
        "doc_date": "1998-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Boundary maps",
            "Compression ratio",
            "Transition points"
        ],
        "doc_abstract": "Transition points occur at locations where rows or columns of edge elements change state. Based on these occurrences, it was proposed, recently, a method for encoding boundary maps. In this paper we review the fundamental ideas of this technique and we describe a new encoding strategy for closed boundaries, which compares favorably with the generic strategy originally proposed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A method for encoding region boundaries based on transition points",
        "doc_scopus_id": "0032024276",
        "doc_doi": "10.1016/s0262-8856(97)00071-1",
        "doc_eid": "2-s2.0-0032024276",
        "doc_date": "1998-03-16",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Boundary coding",
            "Boundary maps"
        ],
        "doc_abstract": "In this paper we present an encoding algorithm for boundary maps containing only closed contours. The method relies on the location and encoding of 'transition points', i.e., points where rows or columns of edge elements change state. Experimental results show that this technique offers better compression ratios than differential chain-coding for boundary maps having medium or large complexity. © 1998 Elsevier Science B.V.",
        "available": true,
        "clean_text": "serial JL 271526 291210 291718 291872 291874 31 Image and Vision Computing IMAGEVISIONCOMPUTING 1999-10-27 1999-10-27 2010-03-25T23:26:51 S0262-8856(97)00071-1 S0262885697000711 10.1016/S0262-8856(97)00071-1 S350 S350.1 HEAD-AND-TAIL 2015-05-14T06:05:42.241066-04:00 0 0 19980316 1998 1999-10-27T00:00:00Z rawtext articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype volfirst volissue affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst pubtype ref alllist content subj ssids 0262-8856 02628856 16 16 3 3 Volume 16, Issue 3 6 213 218 213 218 19980316 16 March 1998 1998-03-16 1998 converted-article sco Copyright © 1998 Published by Elsevier B.V. AMETHODFORENCODINGREGIONBOUNDARIESBASEDTRANSITIONPOINTS PINHO A KUNT 1985 549 574 M PROCEEDINGSIEEE SECONDGENERATIONIMAGECODINGTECHNIQUES KUNT 1987 1306 1336 M BIGGAR 1988 121 132 M IEEPROCFCOMMUNICATIONSRADARSIGNALPROCESSING SEGMENTEDIMAGECODINGPERFORMANCECOMPARISONDISCRETECOSINETRANSFORM EGGER 1996 747 750 O SIGNALPROCESSINGVIIITHEORIESAPPLICATIONS REGIONBASEDCODINGSCHEMESCALABILITYFEATURES FREEMAN 1961 260 268 H KANEKO 1985 697 707 T EDEN 1985 381 386 M MARQUES 1993 18.6.1 18.6.2 F PICTURECODINGSYMPOSIUM SHAPELOCATIONCODINGFORCONTOURIMAGES MINAMI 1986 269 276 T CHRISTOPOULOS 1996 336 339 V SIGNALPROCESSINGVIIITHEORIESAPPLICATIONS ANEWCONTOURSIMPLIFICATIONFILTERFORREGIONBASEDCODING JAIN 1989 A FUNDAMENTALSDIGITALIMAGEPROCESSING SALEMBIER 1996 125 170 P VIDEOCODINGSECONDGENERATIONAPPROACH CODINGPARTITIONSEQUENCES BALLARD 1982 D COMPUTERVISION PINHOX1998X213 PINHOX1998X213X218 PINHOX1998X213XA PINHOX1998X213X218XA item S0262-8856(97)00071-1 S0262885697000711 10.1016/S0262-8856(97)00071-1 271526 2010-09-22T20:54:09.697868-04:00 1998-03-16 true 603078 MAIN 6 75224 849 656 IMAGE-WEB-PDF 1 ELSEVIER Image and Vision Computing 16 (1998) 213-218 Short Communication A method for encoding region boundaries based on transition points Armando J. Pinho' Dep. Electrica e Telecomunicacs, INESC, Universidade de Aveiro, 3810 Aveiro, Portugal Received 26 June 1997; accepted 14 October 1997 Abstract In this paper we present an encoding algorithm for boundary maps containing only closed contours. The method relies on the location and encoding of 'transition points', i.e., points where rows or columns of edge elements change state. Experimental results show that this technique offers better compression ratios than differential chain-coding for boundary maps having medium or large complexity. 1998 Eisevier Science B.V. Keywords: Boundary coding; Image coding; Image segmentation 1. Introduction The necessity of encoding boundary maps arises in several applications. One of the most known and increas- ingly important is region-based (or 'second-generation') image coding. This technique relies on the partition of the image into variable-shape and variable-size regions resem- bling, more or less, the objects that can be found in the picture [1-4]. Therefore, the encoding of the shape (bound- aries) of the regions plays a key role in this image coding approach. Nevertheless, some other important applications requiring the encoding of boundary maps can be pointed out, such as the encoding of maps containing descriptions of coastlines or terrain contours. Several methods have been proposed for the encoding of region boundaries, chain-coding [5] probably being the most popular technique. Variations based on chain-coding, some of them for lossless encoding [6-8], others for lossy compression [9], have also been suggested. Contour simpli- fication [ 10] and approximations using splines, polygons or Fourier descriptors are also lossy techniques frequently used for the efficient representation of object boundaries [11]. A recent review on this topic can be found in [12]. In this paper, we propose a lossless method for the encod- ing of boundary maps. Its main characteristic is the design specifically adapted for the encoding of boundary maps consisting of closed contours, such as those produced by image segmentation techniques, or found in maps ' Tel: + 351 34 370520; Fax: + 351 34 370545; E-mail ap@inesca.pt 2 Obviously, this is not possible at the image borders. describing coastlines or terrain contours. Pig. 1 displays one example of such boundary maps, in this case produced by a region growing segmentation operation. Another char- acteristic is that the proposed encoding algorithm is not based on some variation of chain-coding. Instead, it relies on a completely different philosophy. Our definition of a boundary map is based on the inter- pixel sites ('cracks' [13]) of an image (black rectangles in Fig. 2). We call each of those cracks an edge element, which can be active (if it belongs to a boundary) or inactive (if not). Moreover, the particular nature of the boundary maps addressed here, i.e., representing (closed) boundaries of regions, imply that each active edge element should be con- nected, whenever possible2, to at least one active edge on each side. Let us consider the set of sites represented in Fig. 2 by white squares and circles, and let us call them 'nodes'. Therefore, an image having A^ rows and N c columns has N^ (Nc - 1) + N c (N.^ - 1) edge elements and N^ N c - 1 nodes. Also, we refer to the nodes repre- sented by white circles in Fig. 2 as 'starting nodes' (a total of N.^ + A?( 2). Note that starting nodes are distributed asymmetrically, i.e., they are only present on the top and left borders. This is due to the mode of operation of the proposed algorithm, explained in detail below, which per- forms scans from top to bottom and from left to right. In the following section, we describe the proposed encod- ing algorithm. Next, we present some experimental results obtained with this technique and also with differential chain-coding. Finally, we draw some conclusions based on those experimental results. 0262-8856/98/$19.00 1998 Eisevier Science B.V. All rights reserved Pil S0262-8856(97)00071-l 214 A.J. Pinko/Image and Vision Computing 16 (1998) 213-218 Fig. l. Example of a boundary map obtained after a segmentation of an image into a set of regions. 2. The proposed algorithm If we scan a complete row (or column) of the boundary map, beginning at the corresponding starting node, we are able to encode the boundary contents of that row (or col- umn) by storing the position of the nodes where the sequence of edge elements changes state, i.e., where it goes from a sequence of active edge elements to a sequence of inactive edge elements or vice versa. We call these points 'transition points' of the boundary map. Given that transi- tion points are located on node points, we may have, at most, N.^Nc 1 transition points. An algorithm based on this approach can be easily implemented using, for example, run-length encoding along rows and columns. Moreover, if boundaries are closed, which is the case V(S) V(S) Fig. 2. Location of edge elements (black rectangles) and nodes (white circles and squares). Nodes represented by white circles are called 'starting nodes'. Shaded squares indicate pixel positions. Numbering is used for the description of the algorithm. H(S) Fig. 3. Example of a small boundary map and corresponding location of its transition points. addressed here, some transition points are common both to vertical and horizontal boundaries. The main implication of this fact is that the encoding of those transition points may be performed in a more efficient way than when they are considered unrelated. The algorithm that we propose in this paper is based on this property. We can identify three types of transition points: comers ('C'), vertical tees ('V') and horizontal tees ('H'). A comer is a point where a change of boundary state occurs both in the vertical and horizontal directions. Vertical or horizontal tees are characterized by changes only in the vertical or horizontal direction, respectively. Fig. 3 presents a small example that helps to illustrate the proposed method. In this figure we show a boundary map together with its reproduction having the corresponding set of transition points marked on it. Note that we marked the occurrence of transition points at starting nodes also with a 'S'. The reason for this is that transition points occurring at starting nodes on the top row are always vertical tees, while transi- tion points occurring at starting nodes on the left column are always horizontal tees. Therefore, this characteristic can be taken into account in order to improve the efficiency of the code. The encoding algorithm is quite simple and can be described by the following pseudo-code: n=0; for node = 1 to tartingNodes if(StartingNodeType(node) = = S) { EncodeNumber(n) ; n=0; } else n-r-r- ; n=0; for node = 1 to nNonStartingNodes { symbol = NonStartingNodeType(node); A.J. Pinho/Image and Vision Computing 16 (1998) 213-218 215 if(symbol = = C II symbol = = H symbol = == V) { EncodeNumber(n) ; 72=0; EncodeSymbol(symbol); Table 1 Example of construction of the variable length 'shift' code used in the implementation of the encoding algorithm else n++; Note that in case of transition points located at starting nodes, the corresponding code does not need to be stored, since it can be deduced, as explained above. The decoding phase is also very simple. First, the transi- tion points are marked on the map. Then, the following reconstruction algorithm is executed (we present only the algorithm for the horizontal scanning; the vertical pass is obtained by changing 'H' by 'V', 'rowOfNodes' by 'colOf- Nodes' and 'leftToRight' by 'topToBottom'): foreach rowOfNodes draw == NO; foreach node from leftToRight { if(NodeType(node) = = S II NodeType(node) = == C I NodeType(node) = = H) if(draw = = NO) draw = YES; else draw = NO; if(draw = = YES) DrawEdgeElement( ) ; 3. Experimental results As can be seen from the encoding algorithm described above, the code string is composed of two types of compo- nents: (1) numbers, representing distances between transi- tion points; (2) symbols, representing the type of transition point (i.e., 'C', 'H' or 'V'). To encode the numbers (dis- tances) we use a variable length 'shift' code constructed in the following way: Number 5= 1, / = 1 B =2, / = 1 5= !,/= 2 L Code L Code Z. ( Code 0 1 0 l 00 1 C 3 1 2 100 l 01 2 000 2 2 101 l 10 2 001 3 2 110 2 11000 2 010 4 3 111000 2 11001 2 011 5 3 111001 2 11010 2 100 6 3 111010 2 11011 2 101 7 3 111011 2 11100 2 110 8 3 111100 2 11101 3 11100000 9 3 111101 2 11110 3 11100001 10 3 111110 3 111110000 3 11100010 11 Let us start by defining a base, B, and an increment, /. The first level (level 1 ) of code-words consists of &11 words of B bits, except for the word having only ones (prefix for the next level). The next level of code-words (level 2) consists of all words of B + I bits, except for the worid having only ones, appended to the prefix (all ones) obtained from the previous level. Generalising, level L consists of all words of B + I(L - 1) bits, except for the word having only ones, appended to the prefix obtained from level L l. Table 1 shows some examples of this type of code. Obviously, the efficiency of the code depends on the choice of B and /. Currently, the only way we have to find the optimal values of these parameters is through trial and error, by testing the encoding efficiency obtained for a few combinations of B and /. Although time-consuming during encoding (proportional to the number of combinations tested), this code presents the advantage of requiring only a few bits in order to represent the values of B and / used in each case, instead of more or less complex coding tables. The three different symbols were encoded in the follow- ing way: symbol 'C' with code 'O', symbol 'V\" with code '10' and symbol 'H' with code 'II'. Typically, the comer transition point is the most frequent and, therefore, we assigned it the shortest code. For comparison purposes we use the well-known differential chain-coding algorithm, described briefly here. In a 4-connected grid (which is the case for the boundary maps addressed here) there are only three possible direc- tions of continuation in a chain of edge elements: 'go ahead', 'turn left' and 'turn right'. However, one additional symbol is needed, due to the ramifications that are normally present in the path. Typically, at a ramification node the coordinates of the point are stored in a stack for posterior processing of the other branches. Therefore, this symbol can be viewed as a 'push' stack operation. The corresponding 'pop' operations can be deduced during decoding. In chain-coding it is a requirement to encode 216 A.J. Pinko/Image and Vision Computing 16 (1998) 213-218 Fig. 4. Images used in the experiments: 'mri', 'lena', 'fish' and 'apollo'. the starting nodes3 of each set of connected active edge elements. The distribution of the coordinate values of the starting nodes is quite uniform, which led us to use a fixed length code to encode them. On the other hand, we used the fol- lowing association of codes and symbols: symbol 'go ahead' with code 'OO', symbol 'turn left' with code 'Ol', symbol 'turn right' with code '10' and symbol 'push' with code 'II'. Finally, the bit strings generated by both techniques were further compressed using a Lempel-Ziv- based encoding tool. To assess the compression efficiency of the proposed algorithm we used a simple region growing technique for Note that here the concept of 'starting node' is different from the one we used previously in the context of transition points. In the context of chain- coding a starting node can be any node and not only the nodes represented by white circles in Fig. 2. Tr Ch 150000 125000 100000 75000 S 's 50000 25000 2 O O 20000 40000 60000 Number of active edge elements Fig. 5. Plots of the number of bits, by a function of the number of active edge elements, needed to encode several boundary maps of the 'mri' image, using the proposed algorithm (Tr) and also differential chain-coding (Ch). l3UUUU Tr o Ch - ^5 125000 0 u c u100000 ' ^^^e> vi3 75000 ,''^' VI .^y\"^ ' ^ 4-10 50000 ^.' 1-1 S ^'\"\"\" l 25000 - A-' ^ , 0 ( -) 20000 40000 600 Number of active edge elements A.J. Pinko/Image and Vision Computing 16 (1998) 213-218 150000 217 Fig. 6. Plots of the number of bits, by a function of the number of active edge elements, needed to encode several boundary maps of the 'lena' image, using the proposed algorithm (Tr) and also differential chain-coding (Ch). ouuuu Tr -, wc Ch -125000 - g 100000 - .--^ -0 ( 3 75000 .''-\"\"'' - VI .'^s^ ' i^ 4-1 0 50000 . ' ^ e 25000 /^ Z n / 0 20000 40000 60000 Number of active edge elements Fig. 8. Plots of the number of bits, by a function of the number of active edge elements, needed to encode several boundary maps of the 'apollo' image, using the proposed algorithm (Tr) and also differential chain-coding (Ch). producing several segmentations of four different 256 X 256 images: 'lena', 'fish', 'mri' and 'apollo' (see Fig. 4). Plots of the number of bits needed to encode the boundary maps, as a function of the number of active edge elements, together with the results obtained using differential chain- coding, are displayed in Figs. 5-8. As a final example, we refer to the boundary map dis- played in Fig. 1. This map, which contains 19813 active edge elements, was encoded with 41 192 bits using the algo- rithm based on transition points. On the other hand, it required 48072 bits to be encoded using differential chain-coding, i.e., 16.7% more bits. 4. Conclusions In this paper we presented an encoding algorithm for l3UUUU Tr - 00 Ch i..- c ' 125000 - 0 ,*' c 100000 ,f' - ,' S 75000 /\"'^^r^ ^ yi / 'sr .S .;;/ 4^ 0 50000 ^ - l ^ 6 25000 ^ -*' n \"' 0 20000 40000 60000 Number of active edge elements Fig. 7. Plots of the number of bits, by a function of the number of active edge elements, needed to encode several boundary maps of the 'fish' image, using the proposed algorithm (Tr) and also differentia] chain-coding (Ch). boundary maps containing only closed contours. The method relies on the location and encoding of 'transition points', i.e., of points where rows or columns of edge elements change state (go from inactive to active or vice versa). The results displayed in Figs. 5-8 show that the proposed method behaves better than differential chain-coding when the number of active edge elements is relatively large. How- ever, for boundary maps having a small number of active edge elements, it was found that differential chain-coding is more efficient. The point, in terms of the number of active edge elements, for which the two methods exhibit similar performance differs from image to image. This suggests that one of them may be more suited to some class of images while the other may operate better for some other class. However, this is only conjecture that needs to be further investigated. Fig. 5 shows an interesting property of the encoding method based on transition points, which is the fact that increasing the number of active edge elements of the bound- ary map does not always imply an increase in the number of bits needed to encode it. This can be easily verified if we think of the limit situation of having all the edge elements active or, in other words, of restricting all the regions to contain only one pixel. In that case there are no interior transition points at all (only at starting nodes), and the encoding can be performed with only a few bits. Neverthe- less, this phenomenon only happens for high numbers of active edge elements, typically beyond the limit of practical usefulness. One important difference between the proposed method and chain-coding is related to the handling of 'isolated chains', i.e., of boundaries of regions or groups of regions that are completely surrounded by a single neighbouring region. In order to encode those kinds of boundaries, using chain-coding, the location of a point (typically a 218 A.J. Pinho/Image and Vision Computing 16 (1998) 213-218 pair of coordinates) from where the description of the chain begins has to be stored. If the boundary map is characterised by a large number of isolated chains then the efficiency of chain-coding is reduced. As can be easily understood, this problem does not affect the encoding method based on tran- sition points. Based on this observation we make the following remark regarding the experimental results presented in this paper. As mentioned above, the boundary maps were built using a simple region growing procedure, producing, therefore, 'unconstrained' maps. If a constraint is introduced, limiting the number of isolated chains, in particular, then the results produced by chain-coding will be better, while the results produced by the proposed method will remain virtually unchanged (for some constant number of active edge elements). This line of thought leads us to another important problem: the applicability of the proposed encoding algorithm to region-based image coding. Considering that to be effective high compression region-based image coding techniques require simple boundary maps (having a small number of active edge elements and also having constraints during map construction), then the method based on transition points is probably not the best one. Nevertheless, for medium and low com- pression region-based image coding the method may prove to be useful, although this remains to be studied. Moreover, we should not restrict the range of potential applications to the case of region-based image coding. Some other important applications, such as the encoding of coastline and terrain contour maps, which are typically unconstrained and/or contain a large number of isolated chains, may benefit from the proposed algorithm. References [l] M. Kunt, A. Ikonomopoulos, M, Kocher, Second-generation image- coding-techniques, Proceedings of the IEEE 73 (4) (1985) 549-574. [2] M. Kunt, M. Benard, R. Leonardi, Recent results in high-compression image coding, IEEE Trans, on Circuits and Systems 34 (l l) (1987) 1306-1336. [3] M.J. Biggar, O.J. Morris, A.G. Constantinides, Segmented-image cod- ing: performance comparison with the discrete cosine transform, IEE Proc. - F, Communications, Radar and Signal Processing 135 (2) (1988)121-132. [4] 0. Egger, F. Bossen, T. Ebrahimi, Region based coding scheme with scalability features, in: Signal Processing VIIIheories and Appli- cations, Proc. of the 8th European Signal Processing Conf., EUSIPCO-96, Edizioni LINT Trieste, Trieste, Italy, 1996, pp. 747-750. [5] H. Freeman, On the encoding of arbitrary geometric configurations, IRE Trans, on Electron. Comput. 10 (1961) 260-268. [6] T. Kaneko, M. Okudaira, Encoding of arbitrary curves based on the chain code representation, IEEE Trans, on Communications 33 (7) (1985)697-707. [7] M. Eden, M. Kocher, On the performance of a contour coding algo- rithm in the context of image coding. Part I: contour segment coding, Signal Processing 8 (4) (1985) 381-386. [8] F. Marqu, J. Sauleda, A. Gasull, Shape and location coding for contour images. Picture Coding Symposium, Lausanne, Switzerland, March 1993, pp. 18.6.1-18.6,2. [9] T. Minami, K. Shinohara, Encoding of line drawings with a multiple grid chain code, IEEE Trans, on Pattern Analysis and Machine Intel- ligence 8 (2)(1986) 269-276. [10] V.A. Christopoulos, C.A. Christopoulos, J. Comelis, A.N. Skodras, A new contour simplification filter for region-based coding, in: Signal Processing VIIIheories and Applications, Proc. of the 8th Eur- opean Signal Processing Conf., EUSIPCO-96, Edizioni LINT Trieste, Trieste, Italy, 1996, pp. 336-339. [11] A.K. Jain, Fundamentals of digital image processing, Prentice-Hall, Inc., 1989. [12] P. Salembier, F. Marqu, A. Gasull, Coding of partition sequences, in: Video Coding: The second generation approach, Kluwer Academic Publishers, 1996, pp. 125-170. [13] D.H. Ballard, C.M. Brown, Computer vision, Prentice-Hall, Inc., 1982. IMAVIS 97000711 S0262-8856(97)00071-1 10.1016/S0262-8856(97)00071-1 Short communication A method for encoding region boundaries based on transition points Armando J. Pinho 1 Dep. Electrónica e Telecomunicações, INESC, Universidade de Aveiro, 3810 Aveiro, Portugal 1 Tel: + 351 34 370520; Fax: + 351 34 370545. In this paper we present an encoding algorithm for boundary maps containing only closed contours. The method relies on the location and encoding of ‘transition points’, i.e., points where rows or columns of edge elements change state. Experimental results show that this technique offers better compression ratios than differential chain-coding for boundary maps having medium or large complexity. Keywords Boundary coding Image coding Image segmentation References [1] M. Kunt A. Ikonomopoulos M. Kocher Second-generation image-coding-techniques Proceedings of the IEEE 73 1985 549 574 (4) [2] M. Kunt M. Benard R. Leonardi Recent results in high-compression image coding IEEE Trans. on Circuits and Systems 34 11 1987 1306 1336 [3] M.J. Biggar O.J. Morris A.G. Constantinides Segmented-image coding: performance comparison with the discrete cosine transform IEE Proc. — F, Communications, Radar and Signal Processing 135 1988 121 132 (2) [4] O. Egger F. Bossen T. Ebrahimi Region based coding scheme with scalability features Signal Processing VIII—Theories and Applications Proc. of the 8th European Signal Processing Conf., EUSIPCO-96 1996 Edizioni LINT Trieste Trieste, Italy 747 750 [5] H. Freeman On the encoding of arbitrary geometric configurations IRE Trans. on Electron. Comput. 10 1961 260 268 [6] T. Kaneko M. Okudaira Encoding of arbitrary curves based on the chain code representation IEEE Trans. on Communications 33 7 1985 697 707 [7] M. Eden M. Kocher On the performance of a contour coding algorithm in the context of image coding. Part I: contour segment coding Signal Processing 8 4 1985 381 386 [8] F. Marqués J. Sauleda A. Gasull Shape and location coding for contour images Picture Coding Symposium Lausanne, Switzerland March 1993 18.6.1 18.6.2 [9] T. Minami K. Shinohara Encoding of line drawings with a multiple grid chain code IEEE Trans. on Pattern Analysis and Machine Intelligence 8 2 1986 269 276 [10] V.A. Christopoulos C.A. Christopoulos J. Cornelis A.N. Skodras A new contour simplification filter for region-based coding Signal Processing VIII—Theories and Applications Proc. of the 8th European Signal Processing Conf., EUSIPCO-96 1996 Edizioni LINT Trieste Trieste, Italy 336 339 [11] A.K. Jain Fundamentals of digital image processing 1989 Prentice-Hall, Inc., [12] P. Salembier F. Marqués A. Gasull Coding of partition sequences Video Coding: The second generation approach 1996 Kluwer Academic Publishers 125 170 [13] D.H. Ballard C.M. Brown Computer vision 1982 Prentice-Hall, Inc., "
    },
    {
        "doc_title": "Improvement of region-based image coding by neural networks",
        "doc_scopus_id": "0031634788",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0031634788",
        "doc_date": "1998-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [],
        "doc_keywords": [],
        "doc_abstract": "© 1998 IEEE.Region-based image coding is a very promising technique that is gaining considerable interest. One of the criticisms often associated to this class of image compression algorithms is its lack of realism in what concerns textures. In this paper we study an approach intended to attack this problem, relying on the encoding of the residual image (obtained after region-based encoding) using neural networks. Although preliminary, the results that we obtained, part of them presented in this paper, show that the proposed association of the two techniques is promising. Moreover, due to its cascaded operation, incremental transmission arises naturally.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Asymptotic approximation power for neural networks",
        "doc_scopus_id": "0031628559",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0031628559",
        "doc_date": "1998-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Asymptotic approximation power",
            "Radial basis function (RBF) neural networks"
        ],
        "doc_abstract": "This paper studies the asymptotic approximation power of radial basis function neural networks in the sup norm. The methods used are constructive and based on discretizaion of approximate identities. The effect of the kernel on the approximation order is discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Contextual edge detection using a recurrent neural network",
        "doc_scopus_id": "84957702838",
        "doc_doi": "10.1007/3-540-63508-4_130",
        "doc_eid": "2-s2.0-84957702838",
        "doc_date": "1997-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Contextual information",
            "Inter-dependences"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 1997.If we consider edge detection as a classification problem, then it seems reasonable that context should play an important role in its study. In fact, it is frequent that neighboring pixels exhibit a strong inter-dependence. In this paper we propose a recurrent neural network for edge detection, which uses a special architecture intended to incorporate contextual information during operation. Some experimental results are presented, showing its effectiveness.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the partition of binary edge maps as a first step for quantitative quality evaluation",
        "doc_scopus_id": "0030398674",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030398674",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Binary edge maps",
            "Cartesian system"
        ],
        "doc_abstract": "We present a method to obtain a partition of binary edge maps. The edge maps are partitioned into three subsets of active elements, according to the following classification: matched edge elements, smeared edge elements, and false edge elements. Based on this partition, figures of merit for the quantitative evaluation of edge maps can be derived, as described in [1].",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Figures of merit for quality assessment of binary edge maps",
        "doc_scopus_id": "0030398658",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030398658",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Binary edge maps",
            "Image distortions"
        ],
        "doc_abstract": "We present a novel quality evaluation method for binary edge maps. This method is composed of two different steps: (1) the partition of the edge map into three subsets of active edge elements and (2) the computation of figures of merit, covering five types of distortions, which are errors due to: displacement, smearing, false responses, missing elements, and lack of connectivity.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Example of tuned neural network based noise reduction filters for images",
        "doc_scopus_id": "0029771971",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0029771971",
        "doc_date": "1996-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Image distortion function",
            "Median filters"
        ],
        "doc_abstract": "This paper presents some results on noise reduction in digital images using artificial neural networks. The design is based on the known capacity of supervised neural networks to learn from examples, avoiding the need for explicit knowledge about the image distortion function. The filter is implemented using current back-propagation feed-forward neural networks, and works on the first differences calculated between neighbor pixels. The filtered gray level images are obtained from the output of the filter using an iterative reconstruction algorithm. We give some experimental results which show that the neural network filter provides an increased reduction in noise variance, when compared to the median filters.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Edge detection filters based on artificial neural networks",
        "doc_scopus_id": "71049172745",
        "doc_doi": "10.1007/3-540-60298-4_252",
        "doc_eid": "2-s2.0-71049172745",
        "doc_date": "1995-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Edge detection filters",
            "Edge quality",
            "Figure of merits",
            "Gaussian edge",
            "Neural network edge detection",
            "Neural network filters",
            "Quantitative result"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 1995.This paper presents quantitative results on the problem of edge detection using neural network filters. These results are compared with the results provided by the derivative of the Gaussian edge detection filter. A new figure of merit for edge quality, based on Pratt’s figure of merit, is introduced. The results displayed in this paper give evidence that neural network edge detection filters can perform better than the linear “optimal” filters.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Errorless restoration algorithms for band-limited images",
        "doc_scopus_id": "84999648359",
        "doc_doi": "10.1109/ICIP.1994.413868",
        "doc_eid": "2-s2.0-84999648359",
        "doc_date": "1994-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Band-limited images",
            "Discrete images",
            "In-band",
            "Iterative algorithm",
            "Non-iterative",
            "Randomly distributed",
            "Restoration algorithm"
        ],
        "doc_abstract": "© 1994 IEEE.Considers the problem of restoring randomly distributed sets of missing pixels in band-limited discrete images, and give non-iterative and iterative algorithms capable of error-free restoration. The methods discussed have minimum dimension, that is, the size of the matrices and vectors which appear in the algorithm is determined by the number of unknown pixels. This is a characteristic which an alternative iterative formulation, based on the Papoulis-Gerchberg iteration, does not have. Convergence proofs for both the basic algorithms and a number of accelerated iterative methods are included as well. The performance of the methods is demonstrated with examples.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Some results on edge enhancement with neural networks",
        "doc_scopus_id": "84999252419",
        "doc_doi": "10.1109/ICIP.1994.413717",
        "doc_eid": "2-s2.0-84999252419",
        "doc_date": "1994-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Edge enhancements",
            "Linear filters",
            "Natural images",
            "Traditional approaches"
        ],
        "doc_abstract": "© 1994 IEEE.Presents some results on edge enhancement with artificial neural networks. The main motivation of this work is to show, experimentally, that nonlinear filters implemented with neural networks can be superior to the commonly used linear filters. Frequent problems that are found in traditional approaches are edge misplacement, poor handling of corners and blurring (or even suppression) of edges. The implementation described exhibits a reasonable idea of the capacity of neural networks to reduce or avoid some of these drawbacks, both on synthetic and natural images.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling non-linear edge detectors using artificial neural networks",
        "doc_scopus_id": "0027844833",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027844833",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "ANN",
            "Image segmentation",
            "Non linear edge detectors"
        ],
        "doc_abstract": "This paper we present some results on the modeling of non-linear edge detectors using the framework of artificial neural networks. The method is described and some of its characteristics are discussed. Finally, an example of application is provided and a comparison is made with one popular edge detection technique.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exact reconstruction algorithms for band-limited images",
        "doc_scopus_id": "0027815272",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027815272",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Band limited images",
            "Error free restoration",
            "Exact reconstruction"
        ],
        "doc_abstract": "A problem often found in image processing is that of restoring sets of missing pixels in band-limited discrete images. For medical applications, accuracy is an obviously important factor. In this paper we propose non-iterative and iterative algorithms which are capable of error-free restoration. The performance of the iterative method is demonstrated with a realistic example.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image compression based on quadtree segmentation and artificial neural networks",
        "doc_scopus_id": "0027588481",
        "doc_doi": "10.1049/el:19930688",
        "doc_eid": "2-s2.0-0027588481",
        "doc_date": "1993-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Encoding",
            "Optimization",
            "Pixels",
            "Segmentation method",
            "Vector quantization"
        ],
        "doc_abstract": "A new method for digital image compression is introduced that combines two non-traditional techniques: adaptive block size image segmentation and artificial neural networks. An algorithm for image compression using these techniques is proposed and some experimental results are presented. © 1993, The Institution of Electrical Engineers. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptive ECG data compression based on multiple artificial neural networks",
        "doc_scopus_id": "85066934453",
        "doc_doi": "10.1109/IEMBS.1992.5761214",
        "doc_eid": "2-s2.0-85066934453",
        "doc_date": "1992-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Average data",
            "Compression system",
            "ECG data compression",
            "Local characteristics",
            "Reconstruction error",
            "Reconstruction systems",
            "Total energy"
        ],
        "doc_abstract": "© 1992 IEEE.In this paper we suggest an approach to ECG data compression. The compression and reconstruction systems are implemented using multiple feedforward neural networks. By continuously monitoring the reconstruction error the compression system dynamically adapts to the local characteristics of the signal. Experimental results show that an average data rate of less than 100 bits/second can be sustained with a total energy error of about 0.5 percent.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quality-controlled image compression",
        "doc_scopus_id": "85024194119",
        "doc_doi": "10.1109/IEMBS.1992.5762238",
        "doc_eid": "2-s2.0-85024194119",
        "doc_date": "1992-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Adaptive block sizes",
            "Reconstruction error"
        ],
        "doc_abstract": "© 1992 IEEE.In this paper we show that a combination of adaptive block size image segmentation, continuous reconstruction error monitoring and artificial neural network compressors can be used in a cooperative way to provide quality-controlled medical image compression.",
        "available": false,
        "clean_text": ""
    }
]