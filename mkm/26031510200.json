[
    {
        "doc_title": "Feature-Based Classification of Archaeal Sequences Using Compression-Based Methods",
        "doc_scopus_id": "85129799575",
        "doc_doi": "10.1007/978-3-031-04881-4_25",
        "doc_eid": "2-s2.0-85129799575",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambients",
            "Archaeal",
            "Archaeal sequence",
            "Archaeon",
            "Carbon fixation",
            "Feature-based classification",
            "Features selection",
            "Nitrogen-cycling",
            "Single-celled organisms",
            "Taxonomic identifications"
        ],
        "doc_abstract": "© 2022, Springer Nature Switzerland AG.Archaea are single-celled organisms found in practically every habitat and serve essential functions in the ecosystem, such as carbon fixation and nitrogen cycling. The classification of these organisms is challenging because most have not been isolated in a laboratory and are only found in ambient samples by their gene sequences. This paper presents an automated classification approach for any taxonomic level based on an ensemble method using non-comparative features. This methodology overcomes the problems of reference-based classification since it classifies sequences without resorting directly to the reference genomes, using the features of the biological sequences instead. Overall we obtained high results for classification at different taxonomic levels. For example, the Phylum classification task achieved 96% accuracy, whereas 91% accuracy was achieved in the genus identification task of archaea in a pool of 55 different genera. These results show that the proposed methodology is a fast, highly-accurate solution for archaea identification and classification, being particularly interesting in the applied case due to the challenging classification of these organisms. The method and complete study are freely available, under the GPLv3 license, at https://github.com/jorgeMFS/Archaea2.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Naprt expression regulation mechanisms: Novel functions predicted by a bioinformatics approach",
        "doc_scopus_id": "85121650571",
        "doc_doi": "10.3390/genes12122022",
        "doc_eid": "2-s2.0-85121650571",
        "doc_date": "2021-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [
            "Alternative Splicing",
            "Cell Differentiation",
            "Cell Line, Tumor",
            "Computational Biology",
            "Databases, Genetic",
            "Humans",
            "Pentosyltransferases",
            "Transcriptional Activation"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.The nicotinate phosphoribosyltransferase (NAPRT) gene has gained relevance in the research of cancer therapeutic strategies due to its main role as a NAD biosynthetic enzyme. NAD metabolism is an attractive target for the development of anti-cancer therapies, given the high energy requirements of proliferating cancer cells and NAD-dependent signaling. A few studies have shown that NAPRT expression varies in different cancer types, making it imperative to assess NAPRT expression and functionality status prior to the application of therapeutic strategies targeting NAD. In addition, the recent finding of NAPRT extracellular form (eNAPRT) suggested the involvement of NAPRT in inflammation and signaling. However, the mechanisms regulating NAPRT gene expression have never been thoroughly addressed. In this study, we searched for NAPRT gene expression regulatory mechanisms in transcription factors (TFs), RNA binding proteins (RBPs) and microRNA (miRNAs) databases. We identified several potential regulators of NAPRT transcription activation, downregulation and alternative splicing and performed GO and expression analyses. The results of the functional analysis of TFs, RBPs and miRNAs suggest new, unexpected functions for the NAPRT gene in cell differentiation, development and neuronal biology.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A two-stage workflow to extract and harmonize drug mentions from clinical notes into observational databases",
        "doc_scopus_id": "85109166153",
        "doc_doi": "10.1016/j.jbi.2021.103849",
        "doc_eid": "2-s2.0-85109166153",
        "doc_date": "2021-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Biomedical science",
            "Common data model",
            "Computational analysis",
            "Database schemas",
            "Electronic health record",
            "Observational study",
            "Standard definitions",
            "Unstructured data",
            "Databases, Factual",
            "Delivery of Health Care",
            "Electronic Health Records",
            "Humans",
            "Pharmaceutical Preparations",
            "Workflow"
        ],
        "doc_abstract": "© 2021 Elsevier Inc.Background: The content of the clinical notes that have been continuously collected along patients’ health history has the potential to provide relevant information about treatments and diseases, and to increase the value of structured data available in Electronic Health Records (EHR) databases. EHR databases are currently being used in observational studies which lead to important findings in medical and biomedical sciences. However, the information present in clinical notes is not being used in those studies, since the computational analysis of this unstructured data is much complex in comparison to structured data. Methods: We propose a two-stage workflow for solving an existing gap in Extraction, Transformation and Loading (ETL) procedures regarding observational databases. The first stage of the workflow extracts prescriptions present in patient's clinical notes, while the second stage harmonises the extracted information into their standard definition and stores the resulting information in a common database schema used in observational studies. Results: We validated this methodology using two distinct data sets, in which the goal was to extract and store drug related information in a new Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database. We analysed the performance of the used annotator as well as its limitations. Finally, we described some practical examples of how users can explore these datasets once migrated to OMOP CDM databases. Conclusion: With this methodology, we were able to show a strategy for using the information extracted from the clinical notes in business intelligence tools, or for other applications such as data exploration through the use of SQL queries. Besides, the extracted information complements the data present in OMOP CDM databases which was not directly available in the EHR database.",
        "available": true,
        "clean_text": "serial JL 272371 291210 291682 291870 291901 31 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2021-06-30 2021-06-30 2021-07-06 2021-07-06 2021-08-12T07:55:59 S1532-0464(21)00178-7 S1532046421001787 10.1016/j.jbi.2021.103849 S300 S300.1 FULL-TEXT 2021-08-12T07:17:07.657509Z 0 0 20210801 20210831 2021 2021-06-30T08:53:51.992008Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst pubtype ref specialabst 1532-0464 15320464 true 120 120 C Volume 120 17 103849 103849 103849 202108 August 2021 2021-08-01 2021-08-31 2021 Original research papers article fla © 2021 Elsevier Inc. All rights reserved. ATWOSTAGEWORKFLOWEXTRACTHARMONIZEDRUGMENTIONSCLINICALNOTESOBSERVATIONALDATABASES ALMEIDA J 1 Introduction 2 Related work 2.1 Extracting patient information from clinical notes 2.2 Data harmonisation into relational models 3 Methods 3.1 Clinical notes analysis 3.1.1 Annotating clinical notes 3.1.2 Post-processing 3.1.2.1 Annotation disambiguation 3.1.2.2 Extraction of additional drug information 3.1.3 Storing extracted information 3.1.4 Practical example 3.2 Data Harmonisation 3.2.1 Data schema 3.2.2 Migration workflow 3.2.2.1 Vocabulary loading stage 3.2.2.2 Concept mapping stage 4 Results 4.1 Use case overview 4.2 Analysing results 4.2.1 Medication extraction 4.2.2 Migrated data 4.3 Error analysis 4.4 Data exploration 5 Discussion 5.1 Main findings 5.2 System synopsis 5.3 Future directions 6 Conclusion CRediT authorship contribution statement Acknowledgements References CHENG 2014 371 H KATEHAKIS 2006 D ELECTRONICHEALTHRECORD PIWOWAR 2010 148 156 H PETERSON 2020 103541 K HRIPCSAK 2016 7329 7336 G SHEIKHALISHAHI 2019 e12239 S NELSON 2011 441 S PIVOVAROV 2015 938 947 R FU 2020 103526 S SOHN 2014 858 865 S WEEKS 2020 407 418 H HENRY 2019 3 12 S WEI 2019 13 21 Q CHEN 2019 56 64 L ARONSON 2010 229 236 A HARRIS 2020 221 230 D SAVOVA 2010 507 513 G MATOS 2018 68 S RANGANATHAN 2018 184 P MURPHY 2010 124 130 S MCMURRY 2013 e55811 A HRIPCSAK 2015 574 G OVERHAGE 2011 54 60 J BADGER 2019 103185 J LIU 2020 e17376 S PARK 2021 e23983 J GARZA 2016 333 341 M BODENREIDER 2004 D267 D270 O UZUNER 2010 514 518 O BURN 2020 1 11 E MARKUS 2021 103655 A ALMEIDAX2021X103849 ALMEIDAX2021X103849XJ 2022-07-06T00:00:00.000Z 2022-07-06T00:00:00.000Z © 2021 Elsevier Inc. All rights reserved. 2021-07-09T23:48:40.537Z ACES ACES College of Agricultural, Consumer and Environmental Sciences, University of Illinois at Urbana-Champaign FCT PD/BD/142878/2018 SFRH/BD/147837/2019 FCT Fundação para a Ciência e a Tecnologia EU/EFPIA 806968 IMI Innovative Medicines Initiative USF Despertar This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. João Figueira Silva and João Rafael Almeida are funded by the FCT - Foundation for Science and Technology (national funds) under the grants PD/BD/142878/2018 and SFRH/BD/147837/2019 respectively. The authors would like to acknowledge with gratitude, the support provided by Ana Isabel Morais, GP at USF Despertar, ACES, Gondomar, Portugal and Guilherme Oliveira, GP at USF Esgueira, Aveiro, Portugal, in the validation of the medical information present in this manuscript. This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. Jo?o Figueira Silva and Jo?o Rafael Almeida are funded by the FCT - Foundation for Science and Technology (national funds) under the grants PD/BD/142878/2018 and SFRH/BD/147837/2019 respectively. The authors would like to acknowledge with gratitude, the support provided by Ana Isabel Morais, GP at USF Despertar, ACES, Gondomar, Portugal and Guilherme Oliveira, GP at USF Esgueira, Aveiro, Portugal, in the validation of the medical information present in this manuscript. item S1532-0464(21)00178-7 S1532046421001787 10.1016/j.jbi.2021.103849 272371 2021-08-12T07:17:07.657509Z 2021-08-01 2021-08-31 true 1083397 MAIN 11 56273 849 656 IMAGE-WEB-PDF 1 gr3 47336 421 428 gr4 34899 293 512 gr1 27612 245 511 gr2 30743 231 510 ga1 true 27138 184 500 gr3 8508 163 166 gr4 7730 125 219 gr1 6740 105 219 gr2 6609 99 219 ga1 true 6871 81 219 gr3 369230 1864 1894 gr4 220153 1300 2269 gr1 180226 1086 2263 gr2 184092 1023 2259 ga1 true 156535 814 2213 am false 1366485 YJBIN 103849 103849 S1532-0464(21)00178-7 10.1016/j.jbi.2021.103849 Elsevier Inc. Fig. 1 Migration pipeline used in the EMIF-AD project aiming to harmonise and validate health data, recorded from patients suffering from Alzheimer’s Diseases, into the OMOP CDM schema. Although the EMIF-AD datasets were migrated in the format of CSV files, this approach can be used directly from an EHR database (grey dashed box). Fig. 2 Overview of the extraction of information from clinical text into the matrix format. The red box represents an initial setup phase where the vocabularies are processed and imported in the Neji annotator. Fig. 3 Overview of an example clinical note processed with the first part of the presented pipeline. The clinical note is firstly annotated with Neji; for illustrative purposes, the detected drug mentions are shown highlighted in Neji’s graphical interface. The post-processing module searches the clinical note further for additional drug related information such as dosage, strength and route. Finally, all annotations are restructured into a matrix to be forwarded to the second part of the pipeline. Fig. 4 Overview of the data harmonisation pipeline used to read the extracted data matrix, process and harmonise its concepts into a relational database using the OMOP CDM data schema. The red box represents the vocabulary loading process that can be executed in the setup stage. Table 1 Dataset statistics for the 2009 i2b2 medication extraction challenge full dataset, which is provided with the train and test partitions combined. Concept Valid annotations Drug 9 003 Route 3 406 Dosage 4 482 Table 2 Dataset statistics detailing the number of annotated concepts and relations in the 2018 n2c2 ADE and medication extraction challenge dataset. Concept Relations to Drug Training Test Total Training Test Total Drug 16 225 10 575 26 800 Strength 6 691 4 230 10 921 6 702 4 244 10 946 Route 5 476 3 513 8 989 5 538 3 546 9 084 Dosage 4 221 2 681 6 902 4 225 2 695 6 920 Table 3 Evaluation results from the medication extraction component applied in the validation datasets. PP: Post Processed. 2018 n2c2 2009 i2b2 Source Precision Recall F1-Score Precision Recall F1-Score Neji 0.426 0.797 0.555 0.544 0.776 0.640 PP 0.802 0.705 0.751 0.667 0.556 0.607 Table 4 Results of the mapped concepts in the second part of the methodology, including the database entries in the Drug Exposure table and the predicted amount of entries that were not mapped. 2018 n2c2 2009 i2b2 Unique concepts 961 1049 Mapped (score equals to 1.0) 470 (48.9%) 448 (42.7%) Mapped (score less than 1) 87 (9.1%) 94 (9.0%) Mapped (manually) 221 (23%) 215 (20.5%) Not mapped 183 (19%) 292 (27.8%) Database entries 5316 8998 Discarded entries 1246 3855 Table 5 Analysis of some of the false positives and false negatives annotated by the proposed system. Mentions annotated by the system are highlighted in bold. Missing information Sentence from the clinical note aspirin/ by mouth The patient is taking aspirin and enalapril by mouth. ins (Insulin)/ p.o (by mouth)/ 3 cap (capsules) 5. ins: 3 Cap p.o 10 milligrams 3. lasix: 10 miligrams po iv The patient took an iv dosage after the breakfast containing aspart insulin. PO 2. omeprazole 20 mg Capsule, Delayed Release (E.C.) Sig: Two (2) Capsule, Delayed Release (E.C.) PO DAILY (Daily). Original Research A two-stage workflow to extract and harmonize drug mentions from clinical notes into observational databases João Rafael Almeida Conceptualization Methodology Software Writing - original draft a b ⁎ João Figueira Silva Conceptualization Methodology Software Writing - original draft a 1 Sérgio Matos Writing - review & editing Supervision a José Luís Oliveira Writing - review & editing Supervision a a DETI/IEETA, University of Aveiro, Aveiro, Portugal DETI/IEETA University of Aveiro Aveiro Portugal DETI/IEETA, University of Aveiro, Aveiro, Portugal b Department of Computation, University of A Coruña, A Coruña, Spain Department of Computation University of A Coruña A Coruña Spain Department of Computation, University of A Coruña, A Coruña, Spain ⁎ Corresponding author at: DETI/IEETA, University of Aveiro, Aveiro, Portugal. DETI/IEETA University of Aveiro Aveiro Portugal 1 Equal contribution with first author to this research. Graphical abstract Background The content of the clinical notes that have been continuously collected along patients’ health history has the potential to provide relevant information about treatments and diseases, and to increase the value of structured data available in Electronic Health Records (EHR) databases. EHR databases are currently being used in observational studies which lead to important findings in medical and biomedical sciences. However, the information present in clinical notes is not being used in those studies, since the computational analysis of this unstructured data is much complex in comparison to structured data. Methods We propose a two-stage workflow for solving an existing gap in Extraction, Transformation and Loading (ETL) procedures regarding observational databases. The first stage of the workflow extracts prescriptions present in patient’s clinical notes, while the second stage harmonises the extracted information into their standard definition and stores the resulting information in a common database schema used in observational studies. Results We validated this methodology using two distinct data sets, in which the goal was to extract and store drug related information in a new Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database. We analysed the performance of the used annotator as well as its limitations. Finally, we described some practical examples of how users can explore these datasets once migrated to OMOP CDM databases. Conclusion With this methodology, we were able to show a strategy for using the information extracted from the clinical notes in business intelligence tools, or for other applications such as data exploration through the use of SQL queries. Besides, the extracted information complements the data present in OMOP CDM databases which was not directly available in the EHR database. Keywords EHR Clinical NLP Clinical notes Information extraction Observational studies OMOP CDM ETL 1 Introduction Medicine has long enjoyed the benefits of technological developments and so has the quality of life of the population in general. Healthcare improvements were accompanied by the creation of new tools and data sources, which brought new knowledge and capabilities to physicians and impacted aspects such as disease prevention, diagnosis, treatment and patient follow-up [1]. These new resources brought the possibility of improving areas such as health research studies, which are composed of many time-consuming and expensive stages (e.g. identifying and recruiting subjects that consent the study, and monitoring them over lengthy periods of time), by lowering their cost and time through the exploration of already existing data, such as data obtained from previous studies or data stored in health-related registry systems [2]. However, challenges also arose with the need to cope with the resulting scale and diversity of medical data. Electronic Health Records (EHR) were created to provide an electronic infrastructure capable of storing administrative and medical data from diverse modalities, centralising data at the patient level [3] and providing a longitudinal view of the patient medical history. The resulting healthcare databases can be explored in health research studies to help increasing the quality of the research, especially when combining data from several databases [4]. Despite the interest in merging data from different databases, this process is severely hindered by the fact that database implementations can greatly vary among institutions. However, some efforts have already been made to tackle this issue, such as Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) that provides specifications aiming at the standardisation of the healthcare data representation [5]. The concept of data harmonisation for clinical studies has been explored over the last years with the objective of optimising and reusing EHR data. This process gained increased importance given the possibility of inferring medical knowledge by analysing the history of populations sharing similar characteristics [6]. With this in mind, some institutions have made efforts to standardise observational studies using EHR databases, with the objective of enabling the creation of distributed studies using data collected from different medical institutions [7]. However, these strategies only explored the structured patient information present in EHR databases. Apart from structured information (e.g. form fields), data in EHRs can also be stored in unstructured form. Unstructured text is frequently found in clinical notes and is written in natural language, enabling physicians to document complete descriptions of the patient medical status and its progress through time. Owing to this fact free text makes up a significant part of the data stored in EHRs, specially in chronic diseases in which clinical notes outsize structured data [8], and can contain unique information that is not detectable in other data sources [9]. While some of this data can be extracted and structured using coding standards such as RxNorm [10], Drugbank [11] or SNOMED-CT (Systematized Nomenclature of Medicine - Clinical Terms) [12], this mapping task can be extremely complex and time consuming. Moreover, the acknowledged challenging nature of free text makes it difficult to develop automatic information extraction solutions for clinical text. Nonetheless, since clinical text poses great interest, some approaches have been developed for extracting relevant information. Even though this process has historically consisted of having clinical experts manually review clinical notes, a process which cannot scale with the growing rate of generation of medical data [8], much research has been made during the past years in domains such as clinical natural language processing (NLP) with the objective of creating systems capable of automatically annotating and summarising important text content in clinical notes [13,14]. Since distributed observational studies do not explore the large amounts of data stored in clinical notes, there exists an opportunity to leverage those documents to complement structured data with additional information. The present paper explores a methodology to reuse information on patient medication present in the clinical notes by storing this information in a relational data model currently used in distributed observational studies. The presented methodology was validated using two public datasets of clinical notes and is currently available at Summarily, our main contributions in this paper are the following: • This work proposes a methodology to extract relevant concepts from clinical notes to enrich structured OMOP CDM databases, enabling the use of SQL queries for analysing the clinical text information and thus helping in the definition of patient cohorts sharing similar characteristics; • The resulting methodology is available in an open-source implementation and is prepared to be adopted and integrated in the current EHR databases to OMOP CDM migration pipelines, that currently do not explore information stored in clinical notes; • Finally, this work also introduces a new strategy in this scope for semi-automatically harmonising and validating medical concepts present in clinical notes into their standard definition. 2 Related work The proposed methodology uses improved and adapted versions of strategies which have already been used and validated in other scenarios. This section is divided into: 1) an analysis of related work on medication extraction from clinical notes; and 2) the study of strategies to migrate and harmonise semi-structured concepts into standard data schemas. 2.1 Extracting patient information from clinical notes Clinical notes are an important resource as they let physicians document patient status with full descriptions throughout time, which enables the monitoring of the patient trajectory. These notes can contain vast amounts of relevant information such as family history, prescribed medication and medication intake, diagnosis and followed procedures. While this wealth of knowledge stored in clinical free-text remains underexplored, developments in NLP can help leveraging this source of data by effectively extracting and structuring relevant information contained in clinical narratives [8]. Information extraction can typically be divided in two components. The first one is Named Entity Recognition (NER) and consists in the detection of entities of interest in text. In clinical text, these entities can involve mentions from family history, prescribed medication, disorders, laboratory measurements, and others. The second component is Named Entity Normalisation (NEN) and aims to further structure the extracted text by normalising entities according to coding standards [15]. When dealing with clinical text, this process can leverage existing medical terminologies such as RxNorm or Drugbank. In the present work, we focused on extracting information regarding medication from clinical narratives. Similarly to other NLP problems, medication extraction from clinical narratives can currently follow two main paths: heuristics based solutions and machine/deep learning based solutions. Examples of the former are MedXN [16] and MedExtractR [17], two rule-based and dictionary-based solutions designed to extract information regarding medication from clinical notes. MedXN is a reckoned open-source system implemented using Apache’s Unstructured Information Management Architecture (UIMA) that aims to extract and normalise medication mentions using the RxNorm medical terminology [16], whereas MedExtractR is an R programming language package that follows a similar approach but sacrifices some generalising capability by narrowing down the scope of drugs to search for [17]. Regarding deep learning based solutions, recent challenges on adverse drug events and medication extraction such as MADE 2018 [18] and n2c2 2018 track 2 [19] showed a clear growing trend in the use of these solutions, with a clear dominance in model variations based on bidirectional Long Short Term Memory (BiLSTM) networks coupled with attention mechanisms and/or Conditional Random Fields (CRFs) [20,21]. Other model architectures such as the Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) have also been combined obtaining promising results, particularly when using rule-based post-processing mechanisms which can improve system performance [21]. Despite their good performances, the previously mentioned systems still struggle when annotating certain information such as duration, adverse drug events and reasons, similarly to what human annotators experience [19]. More recently, researchers developed a system to process prescription instructions (commonly mentioned as “sigs”) using MetaMap [22] to normalise medication information [23]. However, this system has the limitation of focusing only on electronic prescriptions, disregarding the remaining content in clinical notes which may contain information concerning medication intake. Since our objective was to extract patient information from complete clinical notes, we opted for customisable frameworks capable of generalising and extracting medical concepts from diversified clinical notes. MedXN [16] and cTAKES [24] are examples of open-source UIMA based solutions for information extraction, with cTAKES being a modular and extensible framework and MedXN being a solution specifically designed for medication extraction. Another flexible and modular framework for text processing and annotation is Neji [25]. This open-source system provides annotation services that can be easily configured and extended with new dictionaries and machine learning models, having already been used in previous work to extract family history information from clinical narratives [26]. Resources from Neji and MedXN were used in this work to extract drug related information, as described in more detail in Section 3.1. 2.2 Data harmonisation into relational models Observational studies started being the subject of great interest over the past years due to the increase in information stored in EHRs. An observational study consists in having researchers document the relationship between the exposure and outcome in the study without performing any active intervention on the patients [27]. Since this is a powerful way to advance in the field of medicine, several organisations have invested in this type of research. To do that, these organisations had to build structures, methodologies and tools for querying multiple EHR databases. The i2b2 [28] project was launched with the goal of aggregating databases from multiple institutions without creating a centralised database. The teams involved in this project developed tools to simplify the cohort estimation and feasibility determination of the aggregated databases [29]. Observational Health Data Sciences and Informatics (OHDSI) 2 2 is an organisation whose goal is to develop tools and strategies to support large-scale distributed observational studies in health care data. OHDSI currently supports a community with several open-source solutions to perform medical product safety surveillance using observational databases [30]. In addition to those tools, OHDSI adopted a database schema that uses anonymised patient information and supports standard vocabularies in its structure [31]. Therefore, observational databases structured using the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) can be part of a medical research network, in which studies can be extended to other health institutions. This schema has the patient as central entity of the model, and all the remaining clinical information such as procedures, drugs, measurements and conditions are associated with the patient [32]. Moreover, this model uses an extensive ontology that captures information from various standard medical terminologies to harmonise stored information [33]. Although this schema was designed to store patient information extracted from EHR databases, there exist initiatives to reuse parts of this schema in more specific medical scenarios. A case of success where this data schema was used to harmonise and store patient information collected in cohorts studies was the European Medical Information Framework (EMIF) Alzheimer’s Diseases (AD) study [34]. However, disease-specific scenarios only require the use of a few tables from the CDM. The workflow represented in Fig. 1 shows an overview of the migration pipeline used in the EMIF-AD project for Alzheimer’s Diseases’ cohorts. The strategy was performed using the best practices of Extraction, Transformation and Loading (ETL) procedures, which encouraged its adoption in this work. This workflow begins with the Extraction stage, where patient data stored in CSV files is converted into a proper format to be processed in the pipeline. Then, in the Transformation stage, a semi-automatic concept mapping is done, i.e., all concepts are automatically mapped to their standard definition using string similarity metrics, with the resulting mappings being validated and approved by a clinical researcher familiarised with the original concepts. In the OHDSI community this mapping is performed using Usagi, an open-source tool with a user-friendly interface to simplify the validation of the mappings [35]. Finally, in the Loading stage the cohorts are loaded into the OMOP CDM database. Recently, other authors invested in leveraging health databases with clinical note information [36,37]. Liu et al. [36] proposed a proof-of-concept system for cohort retrieval of clinical data using the OMOP CDM to enhance model portability. In this work, cTAKES was used to extract relevant unstructured concepts from clinical notes. The goal of this system was to retrieve information from clinical note repositories using filters that can only be applied in structured information, and to use this information for the retrieval of more relevant cohorts. Park et al. [37] proposed a framework for the annotation of unstructured information present in EHR databases. The authors used topic modelling to extract information from clinical notes and stored this information in annotations under JSON (JavaScript Object Notation) format. The resulting information is used for statistical analysis and cohort exploration. Although both works used the OMOP CDM schema, neither approach integrate the resulting data with the OMOP CDM database, having the limitation of only indexing this information in unstructured databases. Although the OMOP CDM was initially designed for a smaller purpose, over the last years this data schema was updated and became a complex structure capable of storing different types of medical information focused on the patient. Currently, this data schema contains two tables specially designed for clinical notes. However, the information stored in these tables is not commonly used in observational studies. In comparison with other data schemes used in medical studies, OMOP CDM may be the most used and complete [38], which was also a reason for its use in our proposal. 3 Methods We devised a pipeline that can be divided in two main sections. During the first stage of the proposed methodology clinical notes are annotated to extract relevant information, whilst in the second stage that information is harmonised and migrated into an observational database structured using OMOP CDM. Detailed descriptions of the processes of annotating clinical notes and harmonising resulting data are further provided in Sections 3.1 and 3.2, respectively. The system was divided into two independent components to provide the possibility of swapping one of the components at any time without interfering with the remaining one. In other words, it is possible to change to other annotation techniques for clinical notes information extraction without affecting the migration component, and the same applies to the migration component which can be changed to use a different output data schema without affecting the information extraction component. 3.1 Clinical notes analysis The first part of the pipeline is responsible for the extraction of relevant medical information from free text in clinical notes, and for the storage of extracted data in a matrix structure to be used in the second part of the pipeline. Fig. 2 illustrates an overview of this process. Here, a system reader firstly receives clinical notes as input, reads their content and stores it according to a fixed structure. This reader is implemented using the factory programming pattern, thus a new dataset reader needs to be implemented whenever a new clinical note dataset is to be used. After reading the clinical notes, a Neji web service is used to annotate medication entities in each note, and the resulting annotations are stored and post-processed. Finally, the extracted information is stored in a matrix to be used in Section 3.2. Although we used Neji to annotate the clinical notes, other tools/frameworks can be integrated in this pipeline, or used to replace Neji. Furthermore, this pipeline is not dependent on a specific technology. The main condition is that the output provided at the end of this first part of the pipeline should match the expected input format of the harmonization component. To further facilitate the use of different strategies in the annotation component, the post-processing module incorporates a programmatic parser that can be easily extended to reformat the annotation output into the expected format. 3.1.1 Annotating clinical notes The red dashed box presented in Fig. 2 concerns the setting up of the annotation mechanisms. Our goal was to create a pipeline capable of generalising and working with any type of clinical note, hence we needed a flexible framework for text processing and annotation that could be easily configured with new resources, such as dictionaries and machine learning models. Neji [25] fulfilled the above mentioned requirements and provided an annotation viewer along with easy access to its annotation mechanisms through web services. Furthermore, Neji can be installed locally and used through the command line interface or through a web service, so that users can use it to annotate sensitive data without depending on external services. This was a key aspect for its integration in the pipeline, considering the privacy concerns associated with the manipulation of sensitive patient information. To set up Neji as a medication annotator, we firstly extracted three drug related medical terminologies from the Unified Medical Language System (UMLS) Metathesaurus [39]: RxNorm, DrugBank and AOD (Alcohol and Other Drug Thesaurus). However, these terminologies cover many semantic types and groups, thus to narrow down the scope of the dictionaries we filtered them keeping only entries from the “Chemicals & Drugs” semantic group. The resulting dictionaries were imported to Neji, and a Neji annotation service was configured for the extraction of drug mentions in clinical text. After passing all clinical notes through the system reader, the Neji web service was used to annotate medication entities in each note and the resulting annotations were stored. 3.1.2 Post-processing To filter annotations and perform a further search for additional drug related information, namely drug strength, dosage and route, a post-processing module was developed. This module explores specific vocabularies and integrates resources from Athena and from MedXN [16], namely vocabularies and regular expressions. 3.1.2.1 Annotation disambiguation The post-processing module begins by checking for ambiguous annotations. Since Neji was supplied with three different drug related dictionaries, which may have concept overlap, it is possible to have ambiguous situations where Neji creates multiple annotations for a mention. As an example, in the sentence “the patient took aspirin 600 mg orally”, Neji can annotate “aspirin” with a DrugBank code and “aspirin 600 mg” with a RxNorm code. When there exist multiple annotations associated with a mention, the post-processing module gives higher priority to RxNorm annotations as they are more complex and specific, enabling the distinction of mentions that have strength information incorporated. Since there may exist irrelevant entries in the dictionaries, which results in the annotation of many false positives, disambiguated annotations are subjected to an additional filtering process were possible false positives are removed by checking each annotation against a false positive vocabulary. This vocabulary was manually compiled and integrates part of the MedXN [16] vocabulary along with a list of common medical abbreviations used in clinical text. 3.1.2.2 Extraction of additional drug information Afterwards, considering the sentence or line where Neji detected an entity, the post-processing module uses a vocabulary of possible administration routes to search for the route used to administer the drug within the sentence. This vocabulary was compiled from three main sources: the MedXN vocabulary, a manual list of common route abbreviations and their expansion, and finally a list of SNOMED routes retrieved from Athena which was obtained by searching codes with type “Routes”. Route annotation is of utmost importance since drug administration route is a mandatory field in the Drug Exposure table from OMOP CDM. Therefore, when the post-processing module cannot detect a route for a drug annotation, this field is annotated with “N/A”. The final post-processing step is responsible for extracting strength and dosage information. To extract drug strength, the system firstly checks if the annotated drug mention contains strength information, and if so it directly extracts the strength component, whereas if not the system uses an adjusted version of a MedXN regular expression to try to identify drug strength in the full sentence. Finally, the sentence is processed with a list of regular expressions to detect the presence of dosage information. 3.1.3 Storing extracted information Once the information extraction process is completed, all extracted information is stored in a matrix structured by patient and drug, where each cell holds information on a drug mention (strength, dosage and route). The reason for storing extracted data in this particular format was the fact that the resulting structure is similar to that already used in cohort studies, greatly simplifying the process of migrating it into an OMOP CDM database, as described in the next section. 3.1.4 Practical example Fig. 3 presents the annotation and conversion into a structured matrix of an example clinical note. The clinical note is firstly annotated using Neji, as demonstrated in the second element of the image extracted from the Neji interface. Then the post processing stage searches for additional drug related information in the clinical note, such as dosage, strength and route. The resulting annotations are finally cleaned and restructured in a matrix format, as shown in the bottom element of Fig. 3. 3.2 Data Harmonisation Concept extraction from the text is only the first part of this process. Since the goal is to reuse the extracted information, the second part of the pipeline is responsible for gathering the extracted information from the matrix and storing it into the OMOP CDM data schema. Despite having the data represented in the previously defined structured format, this information still needs to be harmonised and cleaned, which is one of the main tasks of this second component in the proposed methodology. 3.2.1 Data schema The final output of this pipeline is a relational database that adopts the OMOP CDM standards. Therefore, our methodology requires the following tables: • Person: contains the patients’ personal information (i.e. birthday, race, gender and ethnicity). • Drug Exposure: captures the records related with the utilisation of a drug by the patient. • Visit Occurrence: contains the interval times of a Person that received medical services. In our scenario, we may not be able to define the time span due to the end date of those visits. • Note: stores the clinical note in the database. It keeps some information that characterises the note, and in one field it captures the unstructured information recorded by the provider about a patient in free text format. • Note NLP: encodes all output from NLP processes on clinical notes. Each row represents an extracted term. Regarding the Drug Exposure table, it is important to highlight some of its characteristics as these may impact on the text extraction procedure. The Drug Exposure table stores patient information associated with written prescriptions, orders, pharmacy dispensing, among other situations concerning the patient-drug relation. Its structure contains several mandatory fields such as “drug_exposure_id”, “person_id”, “drug_concept_id”, “drug_exposure_start_datetime”, “drug_type_concept_id”, “route_concept_id” and “drug_source _concept_id”. Some of these fields contain references to the standard vocabularies, which helps keeping the record harmonised. Moreover, the table contains additional fields that can be used to characterise drug utilisation, yet these are not mandatory. In addition to this information, the OMOP CDM schema also has a set of tables named “Standardised Vocabularies” which are designed to store the standard vocabularies as well as additional information, such as hierarchical concept relations for example. Additionally, OHDSI provides the Athena 3 3 web platform, which contains the most common vocabularies available in the OMOP CDM schema and facilitates selecting the desired vocabularies to be used in migrated databases. 3.2.2 Migration workflow The component in the proposed methodology responsible for migrating the data to a relational database followed similar principles as represented in Fig. 1. We improved this pipeline to a more generic solution and for this scenario, we used different output tables. Therefore, as presented in Fig. 4 , this second part is divided into two stages: 1) vocabulary loading and 2) raw data harmonisation. 3.2.2.1 Vocabulary loading stage The vocabulary loading stage requires an initial manual procedure, where the user needs to download from the Athena platform the desired vocabularies to use in the methodology. These vocabularies are used in the OHDSI Databases Network in order to allow federated and distributed queries over multiple databases from different countries. In case of building a database only for clinical notes, which is not very common, this stage will load those vocabularies automatically. The vocabularies used in this methodology were RxNorm, which provides normalised names for clinical drugs, and SNOMED, which contains medical terms particularly useful for the standard definition of routes among others. Vocabularies were also useful in the second stage to feed the Usagi tool, which maps the concepts in raw data to their standard definition. However, this second stage is complex because the information needs to be harmonised on different levels: 1) the standard definition for drugs; 2) the standard definition for routes; and 3) the correct field in the data schema. This last harmonisation level is attained automatically by the system, based on the structure of the matrix resulting from the clinical notes analysis component. The remaining harmonisation levels are achieved using Usagi, as this tool already provides suggestions for each concept based on the textual similarity with standard concepts. 3.2.2.2 Concept mapping stage The mapping stage is the most time consuming part of this pipeline. However, it allows a full validation of mapped concepts, while also discarding wrongly annotated concepts. Despite requiring the health professional to validate each mapping individually, empirical experience shows that Usagi’s suggestions are correct for a large portion of the cases with those cases requiring very little time to validate. The tool also provides search mechanisms to simplify the correction of the remaining mappings, in order to accelerate the process. The proposed methodology was built to be integrated in the OHDSI ETL (Extract, Transformation and Loading) procedures. In these procedures, concepts existing in the database are mapped to their standard definition using Usagi. The bottleneck in those procedures is the mapping stage due to the large amount of concepts. However, our proposal is focused on medications extracted from clinical notes, thus it is possible to reuse some of the mappings made in a previous EHR migration (in case such migration was performed), which reduces the time required in our approach. Another aspect concerning concept mapping is that terms are aggregated, i.e., even though a term can occur multiple times in the whole dataset, it is only mapped once in Usagi. Upon completing the mapping stage, the system receives the Usagi output and creates the mappings. While this is the only file being currently used as input, in more complex scenarios an ontology containing more information about the concepts could also be used. An example of such could be the use of range-of-values in order to automatically validate drug strength for each entry of a specific drug. Despite not being explored in our use case, the proposed system was designed taking into account this possibility. Another requirement for this part of the methodology is the need for some of the patient personal information, i.e., birth date, ethnicity, race, location, provider and death date (in case of dead patients). This information is already part of the EHR structured data and can be collected together with the clinical notes processing. 4 Results The proposed system was validated on a medication extraction use case using two public datasets from previous text mining research challenges. It is important to note that the system was not implemented focusing on a particular dataset, i.e., the methodology was tested on these datasets without any prior training on them. This section presents the selected use cases in more detail, followed by an analysis of the performance of the two major components in the pipeline, as well as the overall results. 4.1 Use case overview The present work focused on extracting information regarding medication from clinical narratives, an area of great interest which has been promoted by several international research challenges. For instance, the 2009 i2b2 medication extraction challenge had the objective of extracting medications, dosages, modes of administration, frequency of administration and reason for administration [40], while the n2c2 2018 track 2 on adverse drug events (ADE) and medication extraction in EHRs, also added to that information the relations between drugs and adverse drug events (ADEs) [19]. To validate our proposal, we used the datasets provided in these two challenges. The objective of this work was not to develop a top performing NLP approach for information extraction, but to abstract and have a generalisable annotating system for extracting information from clinical notes, which enabled the validation of the pipeline as a whole. Therefore, we used the full datasets to validate the system. The 2009 i2b2 dataset contains 1249 discharge summaries from which only 252 have gold standard annotations. Even though this dataset has 9003 drug annotations each with additional information (e.g. dosage, route), the challenge enabled the annotation of additional information with “N/A” whenever that information was not present in the text. Table 1 provides statistics on the number of annotated entities in this dataset. The 2018 n2c2 dataset contains 505 discharge summaries from the MIMIC-III database, annotated regarding entities and relations, and was originally split in train and test partitions containing 303 and 202 annotated documents, respectively. Even though the dataset contains annotations for a wider variety of drug related information, in this work we only focused on drugs, dosage, strength and route, and on the relations between drugs and the remaining entity types. Table 2 provides statistics on the number of annotated concepts and relations present in the dataset. The results obtained after running the full pipeline over both datasets are presented in the following section. 4.2 Analysing results 4.2.1 Medication extraction The medication annotation component was designed to extract as many drug entries as possible to populate the OMOP CDM data schema. Since the route field in the Drug Exposure table is mandatory, a “N/A” route is attributed whenever it is not possible to detect the route used to administer a drug. While the 2009 dataset provided the possibility of annotating various entities with “N/A”, the 2018 dataset did not. Therefore, to perform a consistent validation throughout both datasets, we decided to remove all “N/A” annotated entities during the validation process. Moreover, since this component was developed considering the extraction of data from different datasets, it was necessary to develop a single evaluator for assessing system performance across various datasets. The resulting evaluator assesses extraction performance based solely on extracted drug and route mentions, as these two fields are mandatory for the OMOP CDM data schema (dosage and strength are informative yet not mandatory). Therefore, its analysis only considers as true positives those drugs which have an associated route annotated in the gold standard, which considerably reduces the amount of true positives. For instance, despite the existence of 26800 annotated drugs in the 2018 dataset, there are only 8989 annotated routes and 9084 annotated route-drug relations, which translates to a final number of valid drug annotations close to a third of the total annotated drugs. Similarly, the 2009 dataset contains 9003 drug annotations from which only 3406 drug annotations possess valid route annotations. Results obtained from the evaluation of the extraction component in all validation datasets are presented in Table 3 . It is possible to observe in all datasets that Neji annotations have higher recall, while post-processed annotations suffer a decrease in recall but an increase in precision. This behaviour is expected since Neji is used to detect drug mentions indiscriminately (high recall), whereas the post-processing module is responsible for connecting drugs with their respective routes and filtering out drugs without mention of route. Furthermore, Table 3 shows that the system obtained a significantly higher final F1-score in the 2018 dataset than in the 2009 dataset. However, a manual revision of some gold standard annotations of the 2009 dataset revealed some annotation inconsistencies, which can impact the performance of this component. This could be partly explained by the fact that gold standard annotations were not created by a group of medical experts but instead by challenge participants after the challenge terminated. The obtained results represent the baseline of our annotator. These results can be improved, by training models for each data set, which was not our main goal in this work. Results obtained with the developed component were already quite promising, and showed consistency since in both datasets the F1-score was very similar (a difference of 0.03 points was observed). Importantly, these data sets are very different, thus allowing to verify that the proposed annotator is flexible enough to serve as baseline for this methodology. 4.2.2 Migrated data The second part of the methodology was validated differently. In this case, there is no available gold standard to calculate the usual performance metrics. However, the idea of harmonising patient data from raw data stored in the matrix format into OMOP CDM databases has already been explored in other scenarios. In the OHDSI approach, the validation of this migration process is usually performed through manual searches in the resulting database. The cohort harmonization pipeline used in the EMIF-AD project had the goal of converting the data into a common schema which was not compliant with the OMOP CDM. The ETL core of our methodology was redesigned to be fully aligned with the CDM so that one can cope with new versions of this model. Overall, both pipelines share identical processes at a high-level context, i.e., the use of a similar structure for the data source (matrix) and the manual validation by health professionals. In the EMIF-AD scenario, the migration was validated using specific queries associated to a valid range of field values. However, our use case is different as we do not have information for the range-of-values associated with the mapped concepts. Following the proposed pipeline, the information extracted from the datasets was used in the second part of the system to validate this methodology. Table 4 presents some metrics regarding the data harmonisation component. When using Usagi, we defined filters to ensure that only the Drug and Route domains from the RxNorm and SNOMED vocabularies were used. This procedure is important as it disables similarity searches with other concepts which could have a high level of textual similarity but are in fact not related with the medication domain. The mappings were divided into categories since some required more effort to validate than others. These categories are: 1) mappings with score 1, where the health professional only needs to confirm the automatic mapping; 2) direct mappings that had a similarity score lower than 1; and 3) the concepts that required a manual search in the tool’s dictionaries. With the help of two experts, we were able to obtain the mapping values presented in Table 4. As observed in this table, there still exists a percentage of concepts for which the health professionals were not able to provide a mapping. Since both professionals were not familiar with the datasets used in this work, in case of ambiguous mappings or uncertainty about the concept meaning, their decision was to not perform the mapping, hence resulting in a subset of unmapped entries. 4.3 Error analysis The proposed system was built to be specialised in extracting medication information from clinical notes, without being designed for a specific dataset. Despite this goal, analysing the results on validation datasets allows identifying possible limitations in the annotation component. Table 5 presents some examples of the most frequent errors that represent the system limitations. The first example of these limitation concerns is due to the presence of coordinations, which are not currently being processed. In some situations, the text contains more than one medication and only references the way that these are administered at the end, because all mentioned drugs share the same administration route. In this example, the system only detects the route for the last drug, whereas the initial drug (aspirin) is annotated without any route, which leads to false positives and false negatives in the evaluation metrics. The problem represented in the second example is related to unknown abbreviations, which are employed by institutional staff and most commonly present in enumerated points. Since Neji performs exact matching, these are only extracted by the annotation component if they are present in the vocabularies used. Another issue concerning abbreviations or misspelled words is during the extraction of the drug strength or dosage. In this situation we followed two different approaches. The first involved mention disambiguation prioritising RxNorm annotations, when those integrated detailed information such as drug strength. Since this technique only works in specific situations, the other approach consisted in using a conditional regular expression. However, the vocabulary used in this regular expression does not consider spelling errors, such as the one presented in the third example of Table 5. This example shows the impact of the missing “l” that affects the detection of the lasix’s strength. The last two examples in Table 5 are related with the window size used before and after concepts annotated by Neji. These words are used to identify more information about the medication (such as the route, dosage, strength, among others). The first example shows the occurrence of “iv” outside the word window considered before a drug annotation. Similarly, the second example illustrates missed information (“PO”) occurring outside the word window considered after a drug annotation. 4.4 Data exploration Relational databases allow the use of SQL (Structured Query Language) queries to analyse and explore database content. In this format, Business Intelligence (BI) tools can be used to reorganize the information and extract knowledge from the data. With the drug information stored in a relational model and using a standard data schema, one can use other strategies to analyse the data. For instance, the user can easily perform queries over the extracted information, or, in a more distributed scenario, can share these queries with other institutions that adopted this standard model. Therefore, in this section, we present examples of the application of this methodology, even though many other possibilities exist. The use of OMOP CDM as the data schema and the adoption of the standard principles defined by the OHDSI community in an ETL procedure allow the use of these OHDSI community tools. For instance, it is possible to define a study design using the Atlas 4 4 web platform, and then locally run different types of studies such as: 1) Database-Level and Cohort Characterization, as performed on a recent study with COVID-19 patients [41]; 2) Population-Level Estimation, as demonstrated by a study focused on comparing the effectiveness of antihypertensive drugs [42]; and 3) Patient-Level Prediction, which can explore several artificial intelligence models, as described in [43]. Atlas also integrates analytical features, with charts that offer an overview of the database and allow highlighting important details about the database and each table associated to the patient in this data schema. Using the system proposed in this work, cohorts defined using Atlas can consider both the information extracted from clinical notes and the data available in observational databases. The data can also be explored for tracking patient health status, i.e., although prescribed medication is recorded in the EHR systems, clinical notes can contain information about medication that the patient took without being prescribed in the clinical facilities. This can help physicians more closely tracking the medication taken by the patient without needing to sift through large amounts of clinical text records. Even though in the proposed scenario we were interested in mapping only drug related information, this methodology can be applied to other types of concepts that can be found in clinical notes, if such concept types are considered by the OMOP CDM data schema. For instance, concepts such as procedures, observations, examinations among others can be accommodated and analysed using this format. 5 Discussion 5.1 Main findings The proposed methodology creates new opportunities for reusing the information present in clinical notes, namely in research studies. Currently, during the migration of EHR databases into Observational databases, clinical notes are migrated but the information stored in them is not used. Although the OMOP CDM schema has two tables for NLP extracted concepts, study design does not usually consider information stored in these tables because they contain less detailed information. One of the reasons for this is the fact that information stored in these table is not focused on a specific domain, since these tables can store any kind of medical data that is extracted from clinical notes. This property makes it difficult to replicate studies in distributed databases, which is one of the core OHDSI fundamentals. The system herein described was validated in a scenario focused on drug extraction and this system is capable of harmonising extracted information following the OHDSI principles. With this information mapped to its standard definition following the validation practices using Usagi, there is no reason to not use this data in research studies. In fact, if this system is applied to support the migration from an already existing table, some of the Usagi mappings can be reused from the EHR migration, since the source data is the same. While reusing previous EHR mappings does not guarantee all concepts are directly mapped, mainly because of abbreviations and free-text annotations present in clinical notes, this process can considerably reduce the number of concepts in the mapping stage. Although in our proposal we only mapped drug information, if we consider a different scenario where only clinical notes are migrated, as we did to validate the system, it is now possible to perform research studies using information extracted from clinical notes alone. The dataset will be inferior, which may reduce the relevance of the findings due to the level of detail that can be extracted. However, this type of research is possible and using OHDSI tools a new study can be easily performed. Besides this novel concept regarding the use of clinical notes to improve observational studies, it is possible, with the text information stored in homogeneous data schemas and mapped to their standard definitions, to simply analyse the dataset using SQL queries or BI tools. 5.2 System synopsis The system was carefully designed to divide its responsibilities into two components, clinical notes annotation and data harmonisation. The main reason for this strong separation of responsibilities was to provide the possibility of performing future improvements in each component individually whilst maintaining a fully functional pipeline. Since NLP techniques are progressing at a rapid pace, this flexibility enables the information extraction component to be constantly updated, thus helping maintain the complete system up to date. For instance, this enables the future integration of deep learning-based approaches, which have already shown to be successful in clinical text extraction tasks. In this paper we used an English clinical text annotator that was not trained on any dataset since our goal was not to develop a state-of-the-art annotating system. Instead, our objective was to have a generic annotator capable of producing satisfactory and consistent results in various datasets. This way, we could solve a current problem and create new opportunities in the exploration of information available in clinical notes. However, to be useful in many realistic scenarios, it is necessary to be able to switch the English text annotator by an annotator designed for a language other than English. In fact, the OHDSI community is currently spread over the world with many OMOP CDM databases existing in several non-English speaking countries. Based on the error analysis performed in Section 4.3, we were able to identify a few points in which the system can be optimised. Although we used Neji for the information extraction task and this system does not currently support deep learning models, our methodology was designed to incorporate multiple annotators as well. The decision to use a matrix for storing extracted information was made based on previous experience. In the past, we needed to migrate patient clinical data collected in medical studies that was stored in spreadsheets. After studying different alternatives for performing a clean and solid data harmonisation, the principles that we applied in the second component of our methodology were the most aligned with ETL procedures. One key aspect in this methodology is the manual validation using a graphical interface, which cleans wrongly annotated concepts and facilitates the correction of concepts that were incorrectly mapped to other standard definition. As previously mentioned, this is the current procedure used when migrating EHR databases into the OMOP CDM schema. We tried to optimise this procedure as much as possible in our methodology, since it requires a manual interaction with the system. The idea was to incorporate the possibility of loading other mappings in the system, such as previous mappings made in the institution during an EHR migration. An additional possibility would be to develop a pre-mapping in the annotator component, which would then be loaded in Usagi. With this approach, Usagi suggestions would be skipped and only the annotation features would be used. However, this tool has already been validated in several migration procedures and its operation considers mappings based on a hierarchy defined in the standard vocabularies, an aspect that is not so deeply explored in text annotators including Neji. An illustrative example of this was the existence of the term “marijuana” in the 2018 n2c2 dataset. Usagi was able to map with a suggestive mapping score of 0.815 to “Cannabis sativa seed oil” because the vocabulary contains synonyms for this standard concept that are more similar to the mention than the proposed mapping. This type of feature could be developed in the Neji annotator, however this would result in losing future community contributions in the Usagi tool. Overall, the proposed system enhances the information present in observational databases that use the OMOP CDM data schema. Examples of the most recent and similar approaches to the proposed one were already described in the Related Work Section 2.2. Although both works were focused on using NLP to leverage the information of OMOP CDM databases, neither of these integrated the resulting data with the data already existing and extracted from the relational model of the EHR system. The work of Liu et al. [36] is very useful to retrieve clinical notes from the repository based on conditions defined in a cohort. Park et al. [37] used the OMOP CDM database to extract the notes from a standard schema in free-text to be then annotated. However, the extracted information was not loaded in the database in the respective tables. 5.3 Future directions The presented work uses several important concepts currently being studied in the health informatics field. Although we were able to create a methodology respecting the standard principles and using tools already validated in other scenarios, it was possible to identify some future directions and necessities in this subject. Neji currently supports machine learning modules and vocabularies, however, this tool does not support deep-learning models. This feature could be a very useful asset in specific scenarios, namely if a richly annotated dataset with similar characteristics to the target data was available or could be compiled. In this case, it would be possible to train a model and possibly obtain better results in the information extraction stage. Another interesting feature would be the integration of the post-processing features directly in Neji. This way, the public annotating service would be able to provide the final annotations without the need for posterior processing steps like in our approach. The modular architecture of Neji facilitates such adaptations and extensions. Another aspect to be improved in this methodology is the handling of negations in the text by the annotator. Our baseline does not consider the occurrence of negations, which could require the use of dataset-specific regex rules in the post processing stage. Preliminary tests with negation handling resulted in a loss in baseline flexibility, thus we decided to not incorporate this feature. Finally, we identified the need for a tool that aggregates Neji and Usagi features in a single solution. We were not able to find such tool, and we believe that merging these features in a unique and collaborative tool could simplify concept extraction and mappings. This could also provide a different support to the annotators during the mapping stage of EHR databases migration pipelines. This is mainly because by having these features merged, it would be easier to add customizable vocabularies for specific institutions without affecting the standard vocabularies provided by Athena. 6 Conclusion Clinical text enriches and expands physicians’ knowledge about their patients. During patient admission, important information is recorded in the clinical notes which is not currently exploited in medical studies. Although several initiatives to improve information extraction in clinical text exist, this data is not commonly used to reach new health findings in observational studies. In this paper, we propose a methodology that was implemented in the Python programming language aiming at 1) the extraction of medication information in clinical notes and 2) the migration of extracted information into a relational standard data model. The system was divided into two main components, the first being responsible for extracting data from the text, and the second for the mapping, harmonisation and storage of data in the standard data schema. In our case, we used the OMOP CDM schema due to its impact on medical findings through the use of observational studies. The proposed system is currently available at During this work, we demonstrated the potential of the developed system using two different datasets. We were able to show a new, different approach for exploring and visualising extracted information through the use of SQL queries and business intelligence tools. With this, we created a new strategy for analysing medical text which facilitates comparative analyses of different datasets, and that can solve an existing gap in ETL procedures regarding observational databases. CRediT authorship contribution statement João Rafael Almeida: Conceptualization, Methodology, Software, Writing - original draft. João Figueira Silva: Conceptualization, Methodology, Software, Writing - original draft. Sérgio Matos: Writing - review & editing, Supervision. José Luís Oliveira: Writing - review & editing, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. João Figueira Silva and João Rafael Almeida are funded by the FCT - Foundation for Science and Technology (national funds) under the grants PD/BD/142878/2018 and SFRH/BD/147837/2019 respectively. The authors would like to acknowledge with gratitude, the support provided by Ana Isabel Morais, GP at USF Despertar, ACES, Gondomar, Portugal and Guilherme Oliveira, GP at USF Esgueira, Aveiro, Portugal, in the validation of the medical information present in this manuscript. References [1] S.J. Nass, L.A. Levit, L.O. Gostin, et al., The value, importance, and oversight of health research, National Academies Press (US), 2009. [2] H.G. Cheng M.R. Phillips Secondary analysis of existing data: opportunities and implementation Shanghai Arch. Psychiatry 26 6 2014 371 H.G. Cheng, M.R. Phillips, Secondary analysis of existing data: opportunities and implementation, Shanghai archives of psychiatry 26 (6) (2014) 371. [3] D.G. Katehakis M. Tsiknakis Electronic health record 2006 Wiley 10.1002/9780471740360.ebs1440 D.G. Katehakis, M. Tsiknakis, Electronic health record, Wiley, 2006. URL [4] H.A. Piwowar W.W. Chapman Public sharing of research datasets: a pilot study of associations J. Informet. 4 2 2010 148 156 H.A. Piwowar, W.W. Chapman, Public sharing of research datasets: a pilot study of associations, Journal of informetrics 4 (2) (2010) 148–156. [5] K.J. Peterson G. Jiang H. Liu A corpus-driven standardization framework for encoding clinical problems with hl7 fhir J. Biomed. Inform. 110 2020 103541 10.1016/j.jbi.2020.103541 K.J. Peterson, G. Jiang, H. Liu, A corpus-driven standardization framework for encoding clinical problems with hl7 fhir, Journal of Biomedical Informatics 110 (2020) 103541. doi:10.1016/j.jbi.2020.103541. [6] G. Hripcsak P.B. Ryan J.D. Duke N.H. Shah R.W. Park V. Huser M.A. Suchard M.J. Schuemie F.J. DeFalco A. Perotte Characterizing treatment pathways at scale using the ohdsi network Proc. Nat. Acad. Sci. 113 27 2016 7329 7336 G. Hripcsak, P.B. Ryan, J.D. Duke, N.H. Shah, R.W. Park, V. Huser, M.A. Suchard, M.J. Schuemie, F.J. DeFalco, A. Perotte, et al., Characterizing treatment pathways at scale using the ohdsi network, Proceedings of the National Academy of Sciences 113 (27) (2016) 7329–7336. [7] J.R. Almeida, O. Fajarda, A. Pereira, J.L. Oliveira, Strategies to access patient clinical data from distributed databases, in: Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5 HEALTHINF: HEALTHINF, INSTICC, SciTePress, 2019, pp. 466–473. [8] S. Sheikhalishahi R. Miotto J.T. Dudley A. Lavelli F. Rinaldi V. Osmani Natural language processing of clinical notes on chronic diseases: Systematic review JMIR Med Inform 7 2 2019 e12239 10.2196/12239 URL S. Sheikhalishahi, R. Miotto, J.T. Dudley, A. Lavelli, F. Rinaldi, V. Osmani, Natural language processing of clinical notes on chronic diseases: Systematic review, JMIR Med Inform 7 (2) (2019) e12239. doi:10.2196/12239. URL [9] K. Jensen, C. Soguero-Ruiz, K. Oyvind Mikalsen, R.-O. Lindsetmo, I. Kouskoumvekaki, M. Girolami, S. Olav Skrovseth, K.M. Augestad, Analysis of free text in electronic health records for identification of cancer patient trajectories, Sci. Rep. 7 (46226). doi: 10.1038/srep46226. [10] S.J. Nelson K. Zeng J. Kilbourne T. Powell R. Moore Normalized names for clinical drugs: RxNorm at 6 years J. Am. Med. Inform. Assoc. 18 4 2011 441 10.1136/amiajnl-2011-000116 S.J. Nelson, K. Zeng, J. Kilbourne, T. Powell, R. Moore, Normalized names for clinical drugs: RxNorm at 6 years, Journal of the American Medical Informatics Association 18 (4) (2011) 441. URL [11] D. Wishart, C. Knox, A. Guo, S. Shrivastava, M. Hassanali, P. Stothard, Z. Chang, J. Woolsey, Drugbank: a comprehensive resource for in silico drug discovery and exploration, Nucleic Acids Res. 34(Database Issue) (2006) D668–D672. doi:10.1093/nar/gkj067. [12] M.Q. Stearns, C. Price, Kent A. Spackman, A.Y. Wang, SNOMED clinical terms: overview of the development process and project status, in: Proceedings of the AMIA Symposium, American Medical Informatics Association, Washington, DC, USA, 2001, pp. 662–666. [13] R. Pivovarov N. Elhadad Automated methods for the summarization of electronic health records J. Am. Med. Inform. Assoc. 22 5 2015 938 947 10.1093/jamia/ocv032 R. Pivovarov, N. Elhadad, Automated methods for the summarization of electronic health records, Journal of the American Medical Informatics Association 22 (5) (2015) 938–947. URL [14] J. Liang, C.-H. Tsou, A. Poddar, A novel system for extractive clinical note summarization using ehr data, in: Proceedings of the 2nd Clinical Natural Language Processing Workshop, 2019, pp. 46–54. [15] S. Fu D. Chen H. He S. Liu S. Moon K.J. Peterson F. Shen L. Wang Y. Wang A. Wen Y. Zhao S. Sohn H. Liu Clinical concept extraction: A methodology review J. Biomed. Inform. 109 2020 103526 10.1016/j.jbi.2020.103526 S. Fu, D. Chen, H. He, S. Liu, S. Moon, K.J. Peterson, F. Shen, L. Wang, Y. Wang, A. Wen, Y. Zhao, S. Sohn, H. Liu, Clinical concept extraction: A methodology review, Journal of Biomedical Informatics 109 (2020) 103526. doi:10.1016/j.jbi.2020.103526. [16] S. Sohn C. Clark S.R. Halgrim S.P. Murphy C.G. Chute H. Liu MedXN: an open source medication extraction and normalization tool for clinical text J. Am. Med. Inform. Assoc. 21 5 2014 858 865 10.1136/amiajnl-2013-002190 S. Sohn, C. Clark, S.R. Halgrim, S.P. Murphy, C.G. Chute, H. Liu, MedXN: an open source medication extraction and normalization tool for clinical text, Journal of the American Medical Informatics Association 21 (5) (2014) 858–865. doi:10.1136/amiajnl-2013-002190. [17] H.L. Weeks C. Beck E. McNeer M.L. Williams C.A. Bejan J.C. Denny L. Choi medExtractR: A targeted, customizable approach to medication extraction from electronic health records J. Am. Med. Inform. Assoc. 27 3 2020 407 418 10.1093/jamia/ocz207 H.L. Weeks, C. Beck, E. McNeer, M.L. Williams, C.A. Bejan, J.C. Denny, L. Choi, medExtractR: A targeted, customizable approach to medication extraction from electronic health records, Journal of the American Medical Informatics Association 27 (3) (2020) 407–418. doi:10.1093/jamia/ocz207. [18] A. Jagannatha, F. Liu, W. Liu, H. Yu, Overview of the First Natural Language Processing Challenge for Extracting Medication, Indication, and Adverse Drug Events from Electronic Health Record Notes (MADE 1.0), Drug Saf. 42 (1) (2019) 99–111. doi:10.1007/s40264-018-0762-z. [19] S. Henry K. Buchan M. Filannino A. Stubbs O. Uzuner 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records J. Am. Med. Inform. Assoc. 27 1 2019 3 12 10.1093/jamia/ocz166 S. Henry, K. Buchan, M. Filannino, A. Stubbs, O. Uzuner, 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records, Journal of the American Medical Informatics Association 27 (1) (2019) 3–12. doi:10.1093/jamia/ocz166. [20] Q. Wei Z. Ji Z. Li J. Du J. Wang J. Xu Y. Xiang F. Tiryaki S. Wu Y. Zhang C. Tao H. Xu A study of deep learning approaches for medication and adverse drug event extraction from clinical text J. Am. Med. Inform. Assoc. 27 1 2019 13 21 10.1093/jamia/ocz063 Q. Wei, Z. Ji, Z. Li, J. Du, J. Wang, J. Xu, Y. Xiang, F. Tiryaki, S. Wu, Y. Zhang, C. Tao, H. Xu, A study of deep learning approaches for medication and adverse drug event extraction from clinical text, Journal of the American Medical Informatics Association 27 (1) (2019) 13–21. doi:10.1093/jamia/ocz063. [21] L. Chen Y. Gu X. Ji Z. Sun H. Li Y. Gao Y. Huang Extracting medications and associated adverse drug events using a natural language processing system combining knowledge base and deep learning J. Am. Med. Inform. Assoc. 27 1 2019 56 64 10.1093/jamia/ocz141 L. Chen, Y. Gu, X. Ji, Z. Sun, H. Li, Y. Gao, Y. Huang, Extracting medications and associated adverse drug events using a natural language processing system combining knowledge base and deep learning, Journal of the American Medical Informatics Association 27 (1) (2019) 56–64. doi:10.1093/jamia/ocz141. [22] A.R. Aronson F.-M. Lang An overview of MetaMap: historical perspective and recent advances J. Am. Med. Inform. Assoc. 17 3 2010 229 236 10.1136/jamia.2009.002733 A.R. Aronson, F.-M. Lang, An overview of MetaMap: historical perspective and recent advances, Journal of the American Medical Informatics Association 17 (3) (2010) 229–236. URL [23] D.R. Harris D.W. Henderson A. Corbeau sig2db: a workflow for processing natural language from prescription instructions for clinical data warehouses, AMIA Joint Summits on Translational Science proceedings AMIA Joint Summits Translat. Sci. 2020 2020 221 230 URL D.R. Harris, D.W. Henderson, A. Corbeau, sig2db: a workflow for processing natural language from prescription instructions for clinical data warehouses, AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science 2020 (2020) 221–230. URL [24] G.K. Savova J.J. Masanz P.V. Ogren J. Zheng S. Sohn K.C. Kipper-Schuler C.G. Chute Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications J. Am. Med. Inform. Assoc. 17 5 2010 507 513 10.1136/jamia.2009.001560 G.K. Savova, J.J. Masanz, P.V. Ogren, J. Zheng, S. Sohn, K.C. Kipper-Schuler, C.G. Chute, Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications, Journal of the American Medical Informatics Association 17 (5) (2010) 507–513. doi:10.1136/jamia.2009.001560. [25] S. Matos Configurable web-services for biomedical document annotation J. Cheminformat. 10 1 2018 68 10.1186/s13321-018-0317-4 S. Matos, Configurable web-services for biomedical document annotation, Journal of cheminformatics 10 (1) (2018) 68. doi:10.1186/s13321-018-0317-4. [26] J.R. Almeida, S. Matos, Rule-based extraction of family history information from clinical notes, in: Proceedings of the 35th Annual ACM Symposium on Applied Computing, SAC ’20, Association for Computing Machinery, New York, NY, USA, 2020, p. 670–675. doi:10.1145/3341105.3374000. [27] P. Ranganathan R. Aggarwal Study designs: Part 1–an overview and classification Perspect. Clin. Res. 9 4 2018 184 P. Ranganathan, R. Aggarwal, Study designs: Part 1–an overview and classification, Perspectives in clinical research 9 (4) (2018) 184. [28] S.N. Murphy G. Weber M. Mendis V. Gainer H.C. Chueh S. Churchill I. Kohane Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2) J. Am. Med. Inform. Assoc. 17 2 2010 124 130 S.N. Murphy, G. Weber, M. Mendis, V. Gainer, H.C. Chueh, S. Churchill, I. Kohane, Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2), Journal of the American Medical Informatics Association 17 (2) (2010) 124–130. [29] A.J. McMurry S.N. Murphy D. MacFadden G. Weber W.W. Simons J. Orechia J. Bickel N. Wattanasin C. Gilbert P. Trevvett Shrine: enabling nationally scalable multi-site disease studies PloS One 8 3 2013 e55811 A.J. McMurry, S.N. Murphy, D. MacFadden, G. Weber, W.W. Simons, J. Orechia, J. Bickel, N. Wattanasin, C. Gilbert, P. Trevvett, et al., Shrine: enabling nationally scalable multi-site disease studies, PloS one 8 (3) (2013) e55811. [30] G. Hripcsak J.D. Duke N.H. Shah C.G. Reich V. Huser M.J. Schuemie M.A. Suchard R.W. Park I.C.K. Wong P.R. Rijnbeek Observational health data sciences and informatics (ohdsi): opportunities for observational researchers Studies Health Technol. Informat. 216 2015 574 G. Hripcsak, J.D. Duke, N.H. Shah, C.G. Reich, V. Huser, M.J. Schuemie, M.A. Suchard, R.W. Park, I.C.K. Wong, P.R. Rijnbeek, et al., Observational health data sciences and informatics (ohdsi): opportunities for observational researchers, Studies in health technology and informatics 216 (2015) 574. [31] J.M. Overhage P.B. Ryan C.G. Reich A.G. Hartzema P.E. Stang Validation of a common data model for active safety surveillance research J. Am. Med. Inform. Assoc. 19 1 2011 54 60 J.M. Overhage, P.B. Ryan, C.G. Reich, A.G. Hartzema, P.E. Stang, Validation of a common data model for active safety surveillance research, Journal of the American Medical Informatics Association 19 (1) (2011) 54–60. [32] R. Makadia, P.B. Ryan, Transforming the premier perspective hospital database into the observational medical outcomes partnership (omop) common data model, Egems 2(1) (2014). [33] J. Badger E. LaRose J. Mayer F. Bashiri D. Page P. Peissig Machine learning for phenotyping opioid overdose events J. Biomed. Inform. 94 2019 103185 10.1016/j.jbi.2019.103185 J. Badger, E. LaRose, J. Mayer, F. Bashiri, D. Page, P. Peissig, Machine learning for phenotyping opioid overdose events, Journal of Biomedical Informatics 94 (2019) 103185. [34] S. Lovestone, E. Consortium, The european medical information framework: A novel ecosystem for sharing healthcare data across europe, Learning Health Syst. 4(2) (2020) e10214. [35] OHDSI, Usagi (2020). [36] S. Liu Y. Wang A. Wen L. Wang N. Hong F. Shen S. Bedrick W. Hersh H. Liu Implementation of a cohort retrieval system for clinical data repositories using the observational medical outcomes partnership common data model: Proof-of-concept system validation JMIR Med. Informat. 8 10 2020 e17376 S. Liu, Y. Wang, A. Wen, L. Wang, N. Hong, F. Shen, S. Bedrick, W. Hersh, H. Liu, Implementation of a cohort retrieval system for clinical data repositories using the observational medical outcomes partnership common data model: Proof-of-concept system validation, JMIR medical informatics 8 (10) (2020) e17376. [37] J. Park S.C. You E. Jeong C. Weng D. Park J. Roh D.Y. Lee J.Y. Cheong J.W. Choi M. Kang A framework (socratex) for hierarchical annotation of unstructured electronic health records and integration into a standardized medical database: Development and usability study JMIR Med. Informat. 9 3 2021 e23983 J. Park, S.C. You, E. Jeong, C. Weng, D. Park, J. Roh, D.Y. Lee, J.Y. Cheong, J.W. Choi, M. Kang, et al., A framework (socratex) for hierarchical annotation of unstructured electronic health records and integration into a standardized medical database: Development and usability study, JMIR Medical Informatics 9 (3) (2021) e23983. [38] M. Garza G. Del Fiol J. Tenenbaum A. Walden M.N. Zozus Evaluating common data models for use with a longitudinal community registry J. Biomed. Informat. 64 2016 333 341 M. Garza, G. Del Fiol, J. Tenenbaum, A. Walden, M.N. Zozus, Evaluating common data models for use with a longitudinal community registry, Journal of biomedical informatics 64 (2016) 333–341. [39] O. Bodenreider The Unified Medical Language System (UMLS): integrating biomedical terminology Nucleic Acids Res. 32 suppl_1 2004 D267 D270 10.1093/nar/gkh061 O. Bodenreider, The Unified Medical Language System (UMLS): integrating biomedical terminology, Nucleic Acids Research 32 (suppl_1) (2004) D267–D270. doi:10.1093/nar/gkh061. [40] Ö. Uzuner I. Solti E. Cadag Extracting medication information from clinical text J. Am. Med. Inform. Assoc. 17 5 2010 514 518 10.1136/jamia.2010.003947 Ö. Uzuner, I. Solti, E. Cadag, Extracting medication information from clinical text, Journal of the American Medical Informatics Association 17 (5) (2010) 514–518. doi:10.1136/jamia.2010.003947. [41] E. Burn S.C. You A.G. Sena K. Kostka H. Abedtash M.T.F. Abrahão A. Alberga H. Alghoul O. Alser T.M. Alshammari Deep phenotyping of 34,128 adult patients hospitalised with covid-19 in an international network study Nature Commun. 11 1 2020 1 11 E. Burn, S.C. You, A.G. Sena, K. Kostka, H. Abedtash, M.T.F. Abrahão, A. Alberga, H. Alghoul, O. Alser, T.M. Alshammari, et al., Deep phenotyping of 34,128 adult patients hospitalised with covid-19 in an international network study, Nature communications 11 (1) (2020) 1–11. [42] M.A. Suchard, M.J. Schuemie, H.M. Krumholz, S.C. You, R. Chen, N. Pratt, C.G. Reich, J. Duke, D. Madigan, G. Hripcsak, P.B. Ryan, Comprehensive comparative effectiveness and safety of first-line antihypertensive drug classes: a systematic, multinational, large-scale analysis, The Lancet 394 (10211) (2019) 1816–1826, supplementary material. doi: 10.1016/S0140-6736(19)32317-7. [43] A.F. Markus J.A. Kors P.R. Rijnbeek The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies J. Biomed. Inform. 113 2021 103655 10.1016/j.jbi.2020.103655 A.F. Markus, J.A. Kors, P.R. Rijnbeek, The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies, Journal of Biomedical Informatics 113 (2021) 103655. URL "
    },
    {
        "doc_title": "A recommender system to help refining clinical research studies",
        "doc_scopus_id": "85107238346",
        "doc_doi": "10.3233/SHTI210174",
        "doc_eid": "2-s2.0-85107238346",
        "doc_date": "2021-07-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Cohort Studies",
            "Information Storage and Retrieval",
            "Search Engine"
        ],
        "doc_abstract": "© 2021 European Federation for Medical Informatics (EFMI) and IOS Press. All rights reserved.The process of refining the research question in a medical study depends greatly on the current background of the investigated subject. The information found in prior works can directly impact several stages of the study, namely the cohort definition stage. Besides previous published methods, researchers could also leverage on other materials, such as the output of cohort selection tools, to enrich and to accelerate their own work. However, this kind of information is not always captured by search engines. In this paper, we present a methodology, based on a combination of content-based retrieval and text annotation techniques, to identify relevant scientific publications related to a research question and to the selected data sources. © 2021 European Federation for Medical Informatics (EFMI) and IOS Press.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Patient trajectory modelling in longitudinal data: A review on existing solutions",
        "doc_scopus_id": "85110884057",
        "doc_doi": "10.1109/CBMS52027.2021.00057",
        "doc_eid": "2-s2.0-85110884057",
        "doc_date": "2021-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Clinical decision",
            "Clinical decision support systems",
            "Disease management",
            "Electronic health record",
            "Patient trajectories",
            "Preventive medicines",
            "Research interests",
            "Temporal information"
        ],
        "doc_abstract": "© 2021 IEEE.Physicians use electronic health records to monitor patient health and make more accurate prognoses, diagnoses and clinical decisions. However, with the ever increasing amounts of information stored for each patient, manually processing and digesting all this information becomes increasingly challenging, which opens an opportunity for developing clinical decision support systems such as patient trajectory modelling solutions. Patient trajectory modelling is a topic of growing research interest due to its potential to help improving health care quality by fostering preventive medicine practices, since an earlier disease diagnosis can enable better disease management and earlier intervention, along with an improved resource allocation. In this paper, we review recent approaches for patient trajectory prediction, performing a comparison based on three key aspects: 1) what is the core of the developed approach, 2) what type of data is used in the work, and 3) how is temporal information handled in the proposed solution. The resulting selection of works herein presented illustrates the current paradigm in patient trajectory modelling, and provides an overview on some of the existing challenges in this field.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic analysis of artistic paintings using information-based measures",
        "doc_scopus_id": "85100446723",
        "doc_doi": "10.1016/j.patcog.2021.107864",
        "doc_eid": "2-s2.0-85100446723",
        "doc_date": "2021-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Artistic paintings",
            "Automatic analysis",
            "Block decomposition",
            "Community IS",
            "Computational analysis",
            "Correlation function",
            "Hidden patterns",
            "Local information"
        ],
        "doc_abstract": "© 2021 Elsevier LtdThe artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website (http://panther.web.ua.pt) for fast author characterization and authentication.",
        "available": true,
        "clean_text": "serial JL 272206 291210 291718 291872 291874 31 Pattern Recognition PATTERNRECOGNITION 2021-02-01 2021-02-01 2021-02-08 2021-02-08 2021-03-02T12:42:57 S0031-3203(21)00051-0 S0031320321000510 10.1016/j.patcog.2021.107864 S300 S300.1 FULL-TEXT 2022-06-12T13:56:47.120829Z 0 0 20210601 20210630 2021 2021-02-01T16:28:18.995615Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref vitae 0031-3203 00313203 true 114 114 C Volume 114 18 107864 107864 107864 202106 June 2021 2021-06-01 2021-06-30 2021 Regular papers Objects and image analysis article fla © 2021 Elsevier Ltd. All rights reserved. AUTOMATICANALYSISARTISTICPAINTINGSUSINGINFORMATIONBASEDMEASURES SILVA J 1 Introduction 2 Related work 3 Methods 3.1 Information-based measures 3.1.1 Normalized compression (NC) 3.1.2 Normalized block decomposition method (NBDM) 3.1.3 Local complexity analysis using the normalized compression 3.2 Two-point height difference correlation function 3.2.1 Assessment pipeline 3.2.2 Finding an effective data compressor 4 Results 4.1 Comparison of NC and BDM 4.2 Information-based measures in images of artistic paintings 4.2.1 Global measures analysis 4.2.2 Combining the NC with the roughness exponent of HDC function 4.2.3 Local complexity of paintings 4.2.4 Evaluation of measures for classification purposes 5 Discussion 6 Conclusions Website CRediT authorship contribution statement Acknowledgements Appendix A Supplementary materials References WEISBERG 2006 R CREATIVITYUNDERSTANDINGINNOVATIONINPROBLEMSOLVINGSCIENCEINVENTIONARTS HERTZMANN 2018 18 A ARTS CANCOMPUTERSCREATEART KHAN 2014 1385 1397 F LYU 2004 17006 17010 S KIM 2014 7370 D ZHANG 2017 34 H ZENIL 2018 605 H DELAHAYE 2012 63 77 J SOLERTOSCANO 2014 F SMIERS 2003 J ARTSUNDERPRESSUREPROTECTINGCULTURALDIVERSITYINAGEGLOBALISATION FERREIRA 2014 12 19 P INTERNATIONALCONFERENCEIMAGEANALYSISRECOGNITION AMETHODDETECTREPEATEDUNKNOWNPATTERNSINIMAGE PINHO 2011 584 588 A 201119THEUROPEANSIGNALPROCESSINGCONFERENCE FINDINGUNKNOWNREPEATEDPATTERNSINIMAGES PRATAS 2012 158 165 D INTERNATIONALCONFERENCEIMAGEANALYSISRECOGNITION DETECTIONUNKNOWNLOCALLYREPEATINGPATTERNSINIMAGES ROMASHCHENKO 2002 111 123 A NIVEN 2009 49 63 R MANTACI 2008 411 429 S SHANNON 1948 379 423 C KOLMOGOROV 1965 1 7 A SOLOMONOFF 1964 1 22 R SOLOMONOFF 1964 224 254 R CHAITIN 1966 547 569 G SOLERTOSCANO 2017 F GAUVRIT 2017 N LI 2001 149 154 M CILIBRASI 2005 1523 1545 R CILIBRASI 2006 2309 2313 R 2006IEEEINTERNATIONALSYMPOSIUMINFORMATIONTHEORY AUTOMATICEXTRACTIONMEANINGWEB CEBRIAN 2007 1895 1900 M COHEN 2014 1602 1614 A PRATAS 2017 259 266 D IBERIANCONFERENCEPATTERNRECOGNITIONIMAGEANALYSIS APPROXIMATIONKOLMOGOROVCOMPLEXITYFORDNASEQUENCES MANICCAM 2004 475 486 S LU 2016 Z LOSSLESSINFORMATIONHIDINGINIMAGES PRATAS 2016 231 240 D 2016DATACOMPRESSIONCONFERENCEDCC EFFICIENTCOMPRESSIONGENOMICSEQUENCES PRATAS 2017 265 272 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS SUBSTITUTIONALTOLERANTMARKOVMODELSFORRELATIVECOMPRESSIONDNASEQUENCES PINHO 2008 1 5 A 200816THEUROPEANSIGNALPROCESSINGCONFERENCE INVERTEDREPEATSAWAREFINITECONTEXTMODELSFORDNACODING RISSANEN 1978 465 471 J LI 2004 3250 3264 M LI 2008 M INTRODUCTIONKOLMOGOROVCOMPLEXITYAPPLICATIONS TAYLOR 1999 R JOHNSON 2008 37 48 C LI 2004 340 353 J BRESSAN 2008 113 116 M 200815THIEEEINTERNATIONALCONFERENCEIMAGEPROCESSING ANALYSISRELATIONSHIPBETWEENPAINTERSBASEDWORK OLSHAUSEN 2010 1027 B HUGHES 2010 1279 1283 J STORK 2008 68100J D COMPUTERIMAGEANALYSISINSTUDYART IMAGEANALYSISPAINTINGSBYCOMPUTERGRAPHICSSYNTHESISINVESTIGATIONILLUMINATIONINGEORGESDELATOURSCHRISTINCARPENTERSSTUDIO LETTNER 2008 68100C M COMPUTERIMAGEANALYSISINSTUDYART ESTIMATINGORIGINALDRAWINGTRACEPAINTEDSTROKES SHAHRAM 2008 68100D M COMPUTERIMAGEANALYSISINSTUDYART RECOVERINGLAYERSBRUSHSTROKESTHROUGHSTATISTICALANALYSISCOLORSHAPEAPPLICATIONVANGOGHSSELFPORTRAITGREYFELTHAT HEDGES 2008 681009 S COMPUTERIMAGEANALYSISINSTUDYART IMAGEANALYSISRENAISSANCECOPPERPLATEPRINTS PETROV 2002 197 202 V MACHADO 2019 614 626 J PENG 2015 3057 3061 K 2015IEEEINTERNATIONALCONFERENCEIMAGEPROCESSINGICIP CROSSLAYERFEATURESINCONVOLUTIONALNEURALNETWORKSFORGENERICCLASSIFICATIONTASKS MAO 2017 1183 1191 H PROCEEDINGS25THACMINTERNATIONALCONFERENCEMULTIMEDIA DEEPARTLEARNINGJOINTREPRESENTATIONSVISUALARTS CHU 2018 2491 2502 W HAMMER 2000 442 464 D HENRIQUES 2013 1101 1106 T TERWIJN 2009 S RYBALOV 2007 268 270 A BLOEM 2014 336 350 P INTERNATIONALCONFERENCEALGORITHMICLEARNINGTHEORY ASAFEAPPROXIMATIONFORKOLMOGOROVCOMPLEXITY SOKAL 1958 1409 1438 R KRUSKAL 1956 48 50 J LLOYD 1982 129 137 S TAUBMAN 2002 D HOSSEINI 2019 68 76 M CLEARY 1984 396 402 J MAHONEY 2005 M TECHNICALREPORT ADAPTIVEWEIGHINGCONTEXTMODELSFORLOSSLESSDATACOMPRESSION RISSANEN 1979 149 162 J MOFFAT 1998 256 294 A KNOLL 2012 377 386 B 2012DATACOMPRESSIONCONFERENCE AMACHINELEARNINGPERSPECTIVEPREDICTIVECODINGPAQ8 WANG 2017 3462 3471 X IEEECVPR CHESTXRAY8HOSPITALSCALECHESTXRAYDATABASEBENCHMARKSWEAKLYSUPERVISEDCLASSIFICATIONLOCALIZATIONCOMMONTHORAXDISEASES SHAPIRO 1978 175 214 C ROSENBERG 1994 H TRADITIONNEW GARRARD 2007 L COLOURFIELDPAINTINGMINIMALCOOLHARDEDGESERIALPOSTPAINTERLYABSTRACTARTSIXTIESPRESENT HOCKNEY 2006 D SECRETKNOWLEDGEREDISCOVERINGLOSTTECHNIQUESOLDMASTERS YANG 2016 1 6 S 2016EIGHTHINTERNATIONALCONFERENCEQUALITYMULTIMEDIAEXPERIENCEQOMEX COMPUTATIONALMODELINGARTISTICINTENTIONQUANTIFYLIGHTINGSURPRISEFORPAINTINGANALYSIS FICHNERRATHUS 2011 L FOUNDATIONSARTDESIGNENHANCEDMEDIAEDITION CHEN 2016 785 794 T PROCEEDINGS22NDACMSIGKDDINTERNATIONALCONFERENCEKNOWLEDGEDISCOVERYDATAMINING XGBOOSTASCALABLETREEBOOSTINGSYSTEM NANNI 2017 158 172 L SILVAX2021X107864 SILVAX2021X107864XJ 2023-02-08T00:00:00.000Z 2023-02-08T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 2022-06-07T13:51:03.718Z Scientific Employment Stimulus CI-CTTI-94-ARH/2019 Foundation for Science and Technology SFRH/BD/137000/2018 SFRH/BD/141851/2018 UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia Fundalo para a Ciłncia e a Tecnologia This work was funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019 and the research grants SFRH/BD/141851/2018 and SFRH/BD/137000/2018 for J.M.S and R.A, respectively. D.P. is funded by national funds through FCT - Fundalo para a Ciłncia e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019. item S0031-3203(21)00051-0 S0031320321000510 10.1016/j.patcog.2021.107864 272206 2021-04-24T03:25:50.185202Z 2021-06-01 2021-06-30 true 3739076 MAIN 13 55894 849 656 IMAGE-WEB-PDF 1 gr1 15979 221 382 gr2 78329 280 760 gr3 193217 586 756 gr4 252946 579 760 gr5 56425 375 756 gr6 230839 574 760 gr7 204219 761 761 gr1 4886 127 219 gr2 7699 81 219 gr3 24575 164 211 gr4 27479 164 215 gr5 7038 109 219 gr6 28301 164 217 gr7 12502 164 164 gr1 124625 1174 2032 gr2 845953 1489 4039 gr3 2593723 3113 4017 gr4 3278848 3076 4039 gr5 538446 1994 4016 gr6 2253011 3050 4039 gr7 2190689 4042 4042 mmc1 false 6905460 APPLICATION si7 3360 si17 2830 si31 4158 si1 1201 si40 2942 si50 5831 si8 3227 si9 4679 si10 1707 si11 1524 si12 9785 si13 2878 si14 1151 si15 5646 si16 13543 si18 3320 si19 13222 si2 687 si20 4753 si30 4648 si21 4784 si22 5274 si23 3195 si24 1272 si25 1568 si26 891 si27 11153 si28 4355 si29 3865 si3 875 si32 1743 si33 1573 si34 2100 si35 10893 si36 1389 si37 930 si38 1800 si39 3102 si4 1910 si41 1085 si42 1241 si43 13196 si44 1004 si45 1734 si46 1486 si47 3197 si48 17639 si49 6487 si5 1429 si51 3688 si52 1170 si53 871 si54 2793 si55 2746 si6 3709 am 10718106 PR 107864 107864 S0031-3203(21)00051-0 10.1016/j.patcog.2021.107864 Elsevier Ltd Fig. 1 Benchmark of lossless data compression tools specifically for the processed dataset of artistic paintings. The y -axis depicts the sum of the number of bytes to compress the dataset, where each image was compressed individually using each tool. Fig. 1 Fig. 2 Information-based measures evaluation. (A) Impact of increasing pseudo-random substitution on information-based measures: NC (approximated using the PAQ8 algorithm) and two BDM normalizations (NBDM 1 and NBDM 2 ). (B) Values of the NC and NBDM 1 for different types of images. (C) Image transformation pipeline leading to BDM underestimation of the amount of information contained in the transformed object. Fig. 2 Fig. 3 Examples of artistic paintings with different levels of complexity where painting images were quantized to 8 bits. The NC and NBDM 1 values of each painting are displayed in its lower right corner. Fig. 3 Fig. 4 Average Normalized Block Decomposition Method using NBDM 1 (A), and Average Normalized Compression (B) for each author where images of paintings where quantized for 4, 6, and 8 bits. The authors are sort given the value of NBDM 1 and NC, respectively. To see this result in more detail, please visit the website associated with the article. Fig. 4 Fig. 5 Combining the HDC with NC. (A) Average and standard deviation for each style in NC and α , respectively. (B) Results grouped by styles using average NC and average α of HDC for each artist labeled on the dataset. Fig. 5 Fig. 6 Heat maps of the local complexity matrix (fingerprint) of some authors, computed with the NC. This fingerprint shows the author’s range of complexity and the locations in the canvas painted with more detail (or complexity). To see all matrices, please visit the website associated with this article. Fig. 6 Fig. 7 Artists’ phylogenetic tree computed recurring to the UPGMA algorithm. Each artist has a sample painting and a colour associated with one of his styles (the colour was chosen based on nearest leaves) assigned to him, as well as a description of some styles usually associated with the author. To obtain an improved view of the tree, please visit the website related to this article. Fig. 7 Table 1 Accuracy results obtained for the test set in style and author classification task using state-of-the-art (SoA), state-of-the-art with regional complexity (RC) and ensemble with our measures (RC and HDC). Table 1 Classification Task Number of Classes Number of Images SoA Baseline SoA Baseline + RC SoA Baseline + RC + HDC Style 13 2338 0.622 0.644 0.650 Author 91 4266 0.480 0.490 0.500 Automatic analysis of artistic paintings using information-based measures Automatic analysis of artistic paintings using information-based measures Jorge Miguel Silva Conceptualization Validation Writing - original draft Writing - review & editing ⁎ a b Diogo Pratas Conceptualization Validation Writing - original draft Writing - review & editing a b c Rui Antunes Conceptualization Validation Writing - original draft Writing - review & editing a b Sérgio Matos Conceptualization Validation Writing - original draft Writing - review & editing a b Armando J. Pinho Conceptualization Validation Writing - original draft Writing - review & editing a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal b Department of Electronics, Telecomunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecomunications and Informatics University of Aveiro Portugal Department of Electronics, Telecomunications and Informatics, University of Aveiro, Portugal c Department of Virology, University of Helsinki, Finland Department of Virology University of Helsinki Finland Department of Virology, University of Helsinki, Finland ⁎ Corresponding author at: Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal. Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal The artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website for fast author characterization and authentication. Keywords Image analysis Data compression BDM Artistic paintings Algorithmic information theory 1 Introduction Artistic paintings are concrete visual expressions of human evolution and creativity to share emotions, values, visions, beliefs, and trends of history and culture. The creation, interpretation, and analysis of artistic paintings are social, contextual, subjective, passive, and, beyond superficial characteristics, complex to compute and automatize [1]. In particular, it is theorized that art is an output of social agents, particularly a human experience, that can only be imitated by machines [2]. One of the non-trivial characteristic analysis of artistic paintings is related to the process of measuring the information contained in those paintings. Artistic paintings contain information related to schools, periods, and artists [3]. The artistic community widely uses automatic computational analysis of artistic paintings for authentication of artistic paintings [4,5]. Currently, this process does not substitute human experts completely; however, it is an essential additional control for fraud and mislead detections [6]. Furthermore, applying new techniques and pre-existing ones that are new to the field, can be useful not only for authorship attribution and fraud detection but also for art style categorization and organization, and even for art content explanation. In this paper, we introduce novel solutions for automatic computational analysis of artistic paintings and for the problem of artist authentication. When addressing artist authentication, several questions arise: What defines a painter’s style? How does the author expose information? How does the author differs and relates to other artists? Furthermore, taking inspiration from information theory: How do we best quantify information in a painting? How is the information utilized across the canvas? Moreover, what can information quantification tell us about the author’s style, way of painting, and relationships with other authors? These complex questions are at the core of this paper’s development, where we describe and compare solutions for unsupervised measures of probabilistic and algorithmic information in images (2D) of artistic paintings. Our contributions are as follows: • We perform a direct comparison between state-of-the-art unsupervised probabilistic and algorithmic information measures to specify each measure’s strengths and weaknesses. • We show that hidden patterns and relationships present in artistic paintings can be identified by analysing their complexity. • We show an efficient stylistic descriptor by combining the Normalized Compression and a measure of the paintings’ roughness. • We propose a new descriptor of the artists’ style, artistic influences, and shared techniques. • We show that average local complexity describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. • We demonstrate that these measures can serve as useful auxiliary features capable of improving current methodologies in the classification of artistic paintings. To explain how we achieve this, we first compare the Normalized Compression (NC), employing a data compression tool chosen after a competitive benchmark, with the Block Decomposition Method (BDM) [7], and the inherent Coding Theorem Method (CTM) measures [8,9]. The BDM is an information-based measure that uses small Turing machines to approximate the algorithmic information, approximating to the Shannon entropy as a fallback mechanism. After this comparison, we make use of the average NC of each artist together with the roughness exponent α of the two-point height difference correlation function (HDC), to group artists by style. Furthermore, we provide a local complexity matrix that characterizes each artist using the NC and use it to construct a phylogenetic tree that portraits the relationship between artists in terms of exposing information to the observer. Finally, we use the regional complexity fingerprints and the roughness exponent α as useful auxiliary features that, combined with state-of-the-art approaches, improve the results of style and artist classification tasks. The remaining of this paper is organized as follows. In the next section, we describe related work, followed by a description of the methods. We present the major results in the next section, with further results presented in Supplementary Material. Finally, we discuss the results obtained, draw final conclusions, and point out possible future lines of work. 2 Related work Measuring the information contained in paintings requires fast, efficient, and automatic computation due to the diversity and large quantity of the existing artistic paintings [10]. To measure the information (or complexity) contained in paintings, we first need to define what is the quantity of information of an image. We define the quantity of information of an image as the smallest number of bits required by a model to represent an image losslessly. To perform this task, the model searches for unknown patterns of similarity between sub-regions of the image [11–13] and uses this information to create this compressed representation of the image, relying exclusively in the patterns of the two-dimensional pixels without using exogenous information. There are several approaches to quantify the amount of information. Kolmogorov described three, namely combinatorial [14–16], probabilistic [17], and algorithmic [18]. Independently, the works of Solomonoff [19,20] and Chaitin [21] addressed the same lines. While the Kolmogorov complexity is non-computable, it can be approximated with programs for such purpose, such as data compressors, using probabilistic and algorithmic schemes. Practical applications to approximate the Kolmogorov complexity for multiple dimensional digital objects have been developed using Turing machines [7,9,22,23] and data compressors [24–29]. Recently, Zenil et al. have shown that this methodology has a closer connection to algorithmic information than other measures based on statistical regularities [7], namely fast lossless compression methods, for sources that follow algorithmic schemes. The majority of the lossless compression algorithms are limited to finding simple statistical regularities as they have been designed for fast storage reduction [30,31]; accordingly, they provide slight improvements over the Shannon entropy [17]. However, there are several which are designed for efficient compression at the expense of more computational resources. For example, lossless compression algorithms, such as GeCo [32], are hybrids between probabilistic and algorithmic schemes. Besides having several context models of different orders, GeCo uses sub-programs that allow substitution [33] and reverse complement modeling [34]. These last two are sub-programs of probabilistic and algorithmic information nature. Another example is PAQ8 [35], a general-purpose compressor that combines multiple context models using a neural network, transform functions, secondary structure predictors, and other simple sub-programs. Usually, the problem is how to find fast and efficient algorithmic models for data compression. Lossless data compressors are tightly related to the concept of minimal description length [36] and algorithmic probability [25,37,38]. Therefore, representative algorithms can be efficiently embedded in these data compressors, including small Turing machines. The idea of automatic computational analysis of artistic paintings is mature [4,5], and the artistic community has widely relied on it for authentication of artistic paintings. Specifically, the characteristics of artistic paintings have been analysed through several statistical techniques and properties, namely fractal [39], wavelet-based [4], hidden Markov models [40,41], Fisher kernel based [42], sparse coding model [43,44], color and brightness [5], illumination [45], stroke [46,47], Print Index [48], and entropy-based analysis [49,50]. Recently, the work of Machado and Lopes [50], using fractional calculus, showed the potentiality of measures based on entropy to describe hierarchical clustering of paintings and their correlation with artistic movements. Regarding style and author classification, several recent works have proposed the usage of Convolutional Neural Networks (CNNs). A straightforward approach is to combine features extracted from multiple CNN layers, such as proposed by Peng et al. [51]. Another more effective approach is based on representing images by the principal components of a Gram matrix that captures correlations across the different feature maps obtained from a convolutional layer of a pretrained deep CNN, such as VGG16 or VGG19. Mao et al. [52] combine this representation with the features from all the five convolutional blocks of the VGG16, learning a joint representation that can simultaneously capture content and style of visual arts. On the other hand, Chu et al. [53] apply a support vector machine (SVM) to the Gram representation to perform author and style classification. Then, they improve the results by automatically learning correlations between feature maps. 3 Methods In this section, we describe the measures used, their normalizations, the methodology, and the compression benchmark performed. 3.1 Information-based measures Algorithmic information [18–21] differs from a perspective of pure probabilistic information [17] because it considers that the source, rather than generating symbols from a probabilistic ergodic function, creates structures that represent algorithmic schemes [54,55]. Therefore, to reverse the problem, there is the need to identify the program(s) and parameter(s) that generate the outcome(s) [18,21,38]. However, the algorithmic information, K ( x ) , is non-computable [56], mostly because of the halting problem [57]. Therefore, we have to rely on approximations. Namely, in this subsection, we describe the Normalized Compression and two BDM normalizations. Then, we establish the local application of the Normalized Compression to create a complexity matrix for each author and the methods used to create a distance matrix and the phylogenetic tree. Finally, we describe a non-information-based measure, the two-point height difference correlation function. 3.1.1 Normalized compression (NC) An efficient compressor, C ( x ) , gives a possible approximation for the Kolmogorov complexity ( K ( x ) ), where K ( x ) < C ( x ) ≤ | x | ( | x | is the length of string x in the appropriate scale). Usually, an efficient data compressor is a program that approximates both probabilistic and algorithmic sources using affordable computational resources (Time and RAM). Although the algorithmic nature may be more complex to model, data compressors may have embedded sub-programs to handle this nature. For a definition of safe approximation, see [58]. The normalized version, known as the Normalized Compression (NC), is defined by (1) NC ( x ) = C ( x ) | x | log 2 | A | = C ( x ) | x | , where x is a string, C ( x ) is the compressed size of x in bits, | A | the number of different elements in x (size of the alphabet) and | x | the length of x . Since we consider a binary matrix of each image, | A | = 2 , log 2 2 = 1 . Given the normalization, the NC enables to compare the information contained in the strings independently from their sizes [29]. If the compressor is efficient, then the compressor can approximate the quantity of probabilistic-algorithmic information in data using affordable computational resources. 3.1.2 Normalized block decomposition method (NBDM) Another possible approximation to the Kolmogorov complexity is given by the use of small Turing machines, where these small computer programs approximate the components of a broader representation. The Coding Theorem Method uses the algorithmic probability between a string’s production frequency from a random program and its algorithmic complexity. As such, the more frequent a string is, the lower Kolmogorov complexity it has; and strings of lower frequency have higher Kolmogorov complexity. The Block Decomposition Method (BDM) extends the power of a CTM, approximating local estimations of algorithmic information based on Solomonoff-Levin’s algorithmic probability theory. In practice, it approximates the algorithmic information and, when it loses accuracy, it approximates the Shannon entropy. Since in this article we intend to perform a direct comparison of both measures, we first considered the normalization of the BDM (NBDM 1 ), given by the number of elements (length) of the digital object as (2) NBDM 1 ( x ) = B D M ( x ) | x | log 2 | A | = B D M ( x ) | x | . However, the normalization of the BDM is usually performed using a minimum complexity object (BDM M i n ) and a maximum complexity object (BDM M a x ). A minimum complexity object is filled with only one symbol, like a binary string of only zeros. In contrast, a maximum complexity object is an object that, when decomposed (by a given decomposition algorithm), yields slices that cover the highest CTM values and are repeated only after all possible slices of a given shape have been used once. Using these two objects, the NBDM 2 for a given string can be computed as (3) NBDM 2 ( x ) = B D M ( x ) − B D M M i n B D M M a x − B D M M i n , where B D M ( x ) is the BDM value of that string, B D M M i n is the minimum complexity object, and B D M M a x is the maximum complexity object. Kolmogorov complexity is invariant only up to a constant factor, which depends on the choice of a description language K = K ′ + L , where K is the total complexity, K ′ is the description of the object and L is the description of the language. As such, by performing the normalization according to Eq. (3), the normalization is aiming to remove the constant factor as (4) K − K M i n K M a x − K M i n = K ′ + L − K M i n ′ − L K M a x ′ + L − K M i n ′ − L = K ′ − K M i n ′ K M a x ′ − K M i n ′ , where K M a x and K M i n are the maximum and minimum Kolmogorov complexity objects and K M a x ′ and K M i n ′ are the maximum and minimum Kolmogorov complexity description of the objects. In this article, we perform a direct comparison between the NC and the NBDM 1 . Furthermore, we compare the two types of BDM normalization and their impact on the results. 3.1.3 Local complexity analysis using the normalized compression The Normalized Compression (NC) was used to approximate the local (or regional) complexity of images of artistic paintings. To that end, all of the dataset images were divided into 16 × 16 blocks (256 equal regions) and the NC was computed for each block, generating a complexity matrix. Other patch sizes were also tested, specifically patch sizes of 8 × 8 and 32 × 32 blocks. Following this operation, the average complexity matrix was generated for each author, using the complexity matrices of their paintings. The average complexity matrices were then used to obtain a similarity matrix, in which the distance between matrices was determined as (5) d ( A , B ) = ∑ i = 0 n ∑ j = 0 n ∣ a i j − b i j ∣ , where d is the distance between the complexity matrix A and B , and a i j and b i j are the complexity values at the index i and j of matrices A and B , respectively. Subsequently, using the similarity matrix, a phylogenetic tree was computed recurring to two methods, namely UPGMA (unweighted pair group method with arithmetic mean) [59] and the Kruskal minimum spanning tree algorithm [60], in order to portrait complexity relationships among different authors. 3.2 Two-point height difference correlation function The two-point height difference correlation (HDC) function was computed to quantify brightness contrast as (6) HDC ( r ) = [ h ( x → + r → ) − h ( x → ) ] 2 ¯ = 1 N r ∑ x → , | r → | = r [ h ( x → + r → ) − h ( x → ) ] 2 , where the r is the distance between two-pixel points, over-bar represents the spatial average at a fixed distance r for all possible points; N r is the number of possible pairs at a distance r , h ( x ) is pixel intensity at the position x . Using the HDC function, its roughness exponent α was determined as (7) α = log 10 ( H D C ( r f i n a l ) ) − log 10 ( H D C ( r i n i t i a l ) ) log 10 ( r f i n a l ) − log 10 ( r i n i t i a l ) , where ( α ) is the slope of the HDC curve in a double logarithmic plot of the surface growth model. The slope was calculated from r i n i t i a l = 10 to r f i n a l , which matches the point where the HDC function saturates, approximately 30% of the image’s width. 3.2.1 Assessment pipeline In order to fairly evaluate the information-based measures, we designed a pipeline for processing images. It respects the following steps: Obtaining the dataset images; converting the images to PGM format; quantization of the images to 8 bits (256 levels) using the Lloyd-Max algorithm; binarization of the images (conversion to 01 format in ASCII) and finally, applying the information-based measurements (NC, NBDM 1 and NBDM 2 ). Quantization was performed using the Lloyd-Max algorithm [61,62] since reducing the precision of the pixels (alphabet) in images enables the filtering of small variations that might occur during the digitalization process. Binarization to 01 format in ASCII was performed since the BDM currently only supports a small alphabet. 3.2.2 Finding an effective data compressor To compute the NC, we have to find an effective data compressor, meaning, a compressor that best represents each image, while using reasonable resources. Since our aim is later to apply this measure to a dataset of artistic painting, we compared seven compression tools, namely GZIP [63], BZIP2 [64], XZ [65], LZMA [66], AC [67], PPMD [68], and PAQ8 [35]. As depicted in Fig. 1 , the PAQ8 tool shows the best compression ratio for this dataset. In fact, it shows an improvement of ≈ 26 % to the second best tool (XZ). The disadvantage is the use of higher RAM and substantially more computational time. Nevertheless, since our purpose is to find the number of bits of a shortest program to reproduce the image, it is affordable to spend these computational resources. Therefore, we used the PAQ8 tool to compress each of the quantized images. The code was compiled using the package provided from Buchner [69]. The PAQ8 version used was kx v7. PAQ8kx v7 is an archiver that achieves the highest compression rates at the expense of speed and memory (approximately 1,6 GB of RAM for this dataset). We used the mode that usually provides the highest compression ratio (command parameter: “-8”). The PAQ8 compressor uses a context mixing algorithm between a large number of models independently predicting each quantized pixel’s next bit [70]. The predictions are combined using a neural network and arithmetic coding [71,72]. For automatic installation, use the script Install.sh, while for more information of PAQ, see the work of Knoll and Freitas [73]. The computations ran in a single core Ubuntu Linux computer running at 2.13 GHz with 1.6 GB of RAM. Using this machine, the compression of the whole dataset with PAQ8 required approximately 270 h of real-time, without parallelization. 4 Results 4.1 Comparison of NC and BDM In order to compare NC with BDM, we performed three types of tests. Namely, we compared the robustness of both measures according to increasing rates of random pixel changes in paintings, tested their application on different types of images, and made an assessment of the minimal information bounds. In the first test, we assessed the impact of an increasing rate of pixel editions using a pseudo-random uniform distribution and compared both information-based measures. This approach is not identical to image noise, but rather a pure edition of pixels. For the purpose, for each of the three authors (Theodore Gericault, Marc Chagall, and Rene Magritte) we select a painting, making 50 adulterated copies of each painting with increasing edition rate (from 1 to 50%). Finally, we measured the NC (Eq. (1)), the NBDM 1 (Eq. (2)), and NBDM 2 (Eq. (3)) in all the paintings. Fig. 2 (A) depicts the values obtained for the NC and BDM. The results show that, when using the same type of normalization, NC is more robust to the increment of pixel edition than NBDM (NBDM 1 ). On the other hand, whereas NBDM 1 considers the normalization by the length of the input object, NBDM 2 performs a normalization that aims to mimic the removal of the constant factor related to Kolmogorov complexity (see Eq. (4)). Since the NBDM 2 normalization does not take into account the constant of the description language, it shows a more robust behavior than NBDM 1 , which increases rapidly with the increase of pixel edition. Since NC and NBDM 1 have the same type of normalization, we will focus on comparing these normalizations from now on. In the second test, we applied both measures to six datasets with distinct nature (9 images each) to understand how NBDM 1 and NC behave with different types of images. The six datasets were: artistic images from 2 different datasets [3,74]; cellular automata images; diabetic retinopathy images [75]; chest computed radiography (CR) images [76] and photographic images [77]. The results are depicted in Fig. 2(B). Overall, the majority of the datasets show similar behavior regarding the NC and NBDM 1 . The exceptions to this are the CR and cellular automata datasets, which exhibit a more algorithmic behavior. The latter dataset is constituted by images created with small programs with simple rules. Whereas the compressor has difficulty compressing this type of images, the BDM can point to their algorithmic nature, and, thus attribute them with minimal value. This outcome shows the importance of the BDM in the detection of simple algorithmic outputs embedded into data. In the last test, we selected one of the most complex images identified by the NBDM in the last subsection to test if the BDM could accommodate specific data alterations. This test is depicted in Fig. 2 (C). After the binarization process, we performed a super-sample image transformation where each char was amplified to a 4 × 4 representation. This value was selected since the BDM has the default block size value of 4 × 4 in 2D structures. After this operation, the BDM was computed for the original and the super-sampled image. While the original image was measured with 370981 bits, the super-sampled image had only 79 bits. This abrupt decrease in the complexity value indicates that the BDM underestimates the amount of information contained in the object. The BDM analyses object information in blocks instead of looking at the whole object. Specifically, blocks analysed by the BDM (default block size value of 4 × 4 in 2D structures) have the same size as the super-sample image transformation (each char was amplified to a 4 × 4 representation); therefore, the complexity attributed to each block is approximately zero (since each block is composed of all zeros or ones), and hence the overall value attributed to the complexity of the object will drop dramatically. This analysis shows that BDM is not prepared to deal with the information associated with the choice of the model, unlike the NC. The NC relies on the use of a lossless data compressor, bounded by a maximum information channel capacity. From these three tests, we are able to notice some advantages and limitations of both measures. Ranking these measures is not a fair task because they have different characteristics and nature. Therefore, in the remainder of the article, we use the NC and NBDM in a combined mode to recover insights and characteristics from the images of the artistic paintings. 4.2 Information-based measures in images of artistic paintings Herein, we investigate the use of information measures to analyse a dataset of artistic paintings. This dataset [3] contains 4,266 images of artistic paintings from 91 authors, with approximate geometric sizes. The 91 authors are well-known painters, such as Claude Monet, Frida Kahlo, Henri Matisse, Jackson Pollock, Picasso, Rembrandt, and Salvador Dali. In the following subsections, we present the results of applying the measures, combining the NC with the HDC function, measuring local complexity for different authors and constructing a phylogenetic tree, as well as using these features to improve style and artist classification. We also measure the impact of normalizing these images by performing image normalization and then applying the measures mentioned above in the dataset. Afterwards, we compared the average variation difference and the percentage difference between the results obtained for each author. The results are shown in the Supplementary Material in Section A.1. 4.2.1 Global measures analysis In this subsection, we measure an approximation to the Kolmogorov complexity for the dataset of artistic paintings. The same pipeline, described in the methods section, was used, with the difference that the Lloyd-Max algorithm quantization was set to 16, 64, and 256 levels (4, 6, and 8 bits respectively). Important to note that Lloyd-Max algorithm forced normalization of the images for the 16 and 64 levels, while the 256 level was the original level of the images, and, as such, these images were not normalized. This process was performed to evaluate the impact of the quantization on the measures used to approximate the Kolmogorov complexity in artistic painting images. From the results obtained from the measures, we show unknown characteristics and insights into temporal traits. In general, the complexity of each painting follows the example of Fig. 3 . Paintings with low complexity are classified as abstract and minimalist, following simple patterns. As the complexity increases, we start to recognize paintings with different local complexities, meaning, there are regions with high complexity and detail (generally on the center/bottom of the paintings) surrounded by low complexity regions (same color background) namely known as chiaroscuro. This pattern begins fading, as the complexity increases since the highest complexity paintings are also the most irregular, detailed, and convoluted. Regarding the average complexity values for each artist, Fig. 4 shows the average of NBDM 1 and NC, respectively. Each artist has an associated color, and lines of the same color illustrate its relative positional deviation in different quantizations. The same results for NBDM 2 are exposed and discussed in the Supplementary Material. Noticeably, quantization impacts the NBDM 1 more than the NC, since the relative positioning between authors varies more in the former. On average, the variation is 13.4 ± 11.37 relative positions of each author in NBDM 1 , while in NC, the variation is 4.9 ± 4.3 positions. Despite the higher variation present in the NBDM 1 , both measures are capable of detecting styles with low and high complexity. Artists such as Mark Rothko, Lucio Fontana, Piet Mondrian, El Lissitzky can be easily identified on the low side of the complexity spectrum. Minimalism, Abstract Expressionism, and Constructivism movements are associated with these styles. On the other hand, artists from Abstract Expressionism, such as Willem de Kooning, Jackson Pollock, and Jasper Johns, characterize the highest complexity side of the spectrum, as well as other artists with a more detailed and convoluted style, like Gustav Klimt and Vincent van Gogh. Abstract Expressionism is characterized by aggressive features combined with random and geometric features and spontaneity [78]. The reason for Abstract Expressionism artists being present at both extremes of the complexity spectrum is because this style itself divided into two opposites, Action Painting and Color Field. In Action Painting, the paint was thrown directly on the canvas, through instinctive gestures, where chance and randomness determined the evolution of painting [79]. This style is characteristic of artists like Jackson Pollock (known for the technique of ǣdrippingǥ) and Willem de Kooning. On the other hand, Color Field is more mystical and meditative. This style of painting has few elements in the frames, indefinite limits, and explores the sensory effects of color, as well as the subtlety of chromatic relations [80]. A specific example of an artist that followed this trend was Mark Rothko. In all cases, Jackson Pollock had complexity values utterly different from other artists, the average complexity of his paintings being approximate to random (normalized value close to 1). Although he denied his paintings were random, similar results were also found in previous work, which defined Jackson Pollock’s dripping paintings as not typical artworks [5]. 4.2.2 Combining the NC with the roughness exponent of HDC function We used the average NC together with the roughness exponent ( α ) of the two-point height difference correlation (HDC) function, which measures the roughness exponents of brightness surfaces, to assess the ability of these measures to distinguish different styles. Accordingly, we made usage of style labeled paintings available in the dataset. From these labeled images, we computed for its author the average NC and the value of α . The roughness exponent was used as an additional measure since it has proven to be capable of some differentiation between styles [5]. We discarded the usage of BDM due to quantization impacting it more than the NC. Using the average NC and α of each labeled painter, we created a scatter plot (Fig. 5 ) and represented each artistic movement as an ellipse, with the center in the points’ center of mass and with a width corresponding to the standard deviation. As shown in Fig. 5(A), both measures alone are not capable of efficiently separating styles, However, when combined, the styles are well confined into different regions (except for Abstract Expressionism), showing that together these measures are representatives of artistic movements. The roughness exponent α captures the level of brightness and relative spatial position and is correlated to variations in painting techniques and genres [5]. The NC adds to the level of brightness and relative spatial position provided by the HDC, the notion of average information present in each artist’s painting. This amount of information differs depending on the artistic movement and historical circumstances. Interestingly, similar to NC, the roughness exponent of the HDC varies greatly in Abstract Expressionism, being that in this artistic movement, there is an inverse correlation between the NC and α . Namely, artists like Jackson Pollock and Willem de Kooning (Action Painting) presented a high average NC and a low α , whereas, Mark Rothko (Color Field) had polar results. This atypical behavior corroborates the big difference between the two currents of Abstract Expressionism. The Action Painting usage of instinctive gestures and randomness creates high NC values and spatial correlation approaching a random image. In contrast, in Color Field, we get more minimalist images with high spatial contrast between regions but low complexity. 4.2.3 Local complexity of paintings In this section, we divided the images into identical quadrilateral sizes and measure the algorithmic information for each one ( 16 × 16 blocks). Then, we computed the average of each quadrilateral for all the paintings for each painter. The results are shown in Fig. 6 , illustrating the same authors as those in Fig. 3. Note however that matrices of Fig. 6 were computed using all the authors’ paintings present in the dataset. The complete results are available on the website associated with this article. The same computation was repeated for blocks of sizes 8 × 8 and 32 × 32 . Analysis of these results, included in the Supplementary Material, show that 16 × 16 is the minimum patch size for which the differences in the compression rate are noticeable and can therefore be used as a measure between paintings. All artists have a unique complexity matrix (fingerprint). This fingerprint shows, on average, where artists paint with more detail and give more emphasis as well as the average range of complexity the artist operates. For instance, Jackson Pollock and Jasper Johns show high complexity values dispersed over the canvas. At the same time, artists like Francis Bacon and George de la Tour focus more on the center of the canvas, and Mark Rothko and Piet Mondrian give their highest complexities around the borders of paintings. Since the 16 × 16 fingerprints conveyed the best results regarding detail and differentiation (see Supplementary Material in Section A.2), the phylogenetic trees were constructed utilizing the distance computed from the fingerprints with block size. Concretely, two phylogenetic trees were constructed to portray the relations between different artists. One tree was constructed using the UPGMA algorithm, which is illustrated in Fig. 7 , and another tree was build using the Kruskal minimum spanning tree algorithm [60], which is depicted in the Figure S3 of the Supplementary Material in Section A.4. The tree shows the fingerprint’s capacity of grouping artists from the same artistic movements mutually. Broad groupings of artists from styles are present in the tree, namely, Renaissance, Baroque, Romanticists, Impressionists, Surrealism, Cubism, and Abstract Expressionism. Also, the tree shows smaller groupings of sister leaf-nodes with the same style. On the other hand, the tree depicts relationships of influence between authors of different artistic movements. This relation is seen in the case of Titian, who influenced Diego Velazquez; Caravaggio, who influenced Francisco de Zurbarán; Frida Kahlo, who influenced Amedeo Modigliani; Sandro Botticelli who influenced William Blake; Claude Lorrain who influenced Joseph Mallord William Turner; and Peter Paul Rubens who influenced Jean-Antoine Watteau. On the other hand, some authors seem unrelated in style and influence, for instance, Francis Bacon and Georges de la Tour, George Braque and Hieronymus Bosch, Peter Paul Rubens and Frida Kahlo, Max Ernst and Giorgione, and Rembrandt van Rijn and Roy Lichtenstein. There can be many reasons for this to occur, for instance, the number of regions the images were divided can be sub-optimal for some images of artistic paintings, decreasing the sensitivity of the measure and jeopardizing the tree’s construction. On the other hand, the algorithm used to measure the similarity between matrices or the algorithm used to construct the tree (UPGMA) may not be the most appropriate for all cases, however, we have tested the Kruskal minimum spanning tree algorithm which yielded similar results (see Supplementary Material in section ). Additionally, these seemingly unrelated connections could reveal undiscovered elements and relationships. For instance, one of Roy Lichtenstein’s early artistic idols was Rembrandt van Rijn. Moreover, if artists are not related regarding the artistic movement or influence, the vicinity among them could be representing another property. This aspect is not necessarily related to the period or movement the artists were inserted in, but rather, the way authors projected their compositions, ideas, and impressions onto the canvas. Complexity can be approximated by the total number of properties transmitted by an object and detected by an observer. By dividing images into blocks of equal size and evaluating its local complexity, we are quantifying the local information being transmitted. On the other hand, by averaging the canvas results per artist, we obtain a matrix that describes how the author exposes information to the observer. This information intertwines various notions critical to how the work is perceived, such as composition which describes where the artist places the subject and how the background elements support it, as well as the unity, balance, movement, rhythm, focus, contrast, pattern, and proportion of the painting. For instance, the proximity between Hans Holbein and Vermeer could be due to both of them having used optics to achieve precise positioning in their compositions, namely by performing a combination of curved mirrors, camera obscura, and camera lucida [81]. Another example that this information can convey is space by depicting where positive (subject itself, which is usually more detailed) and negative (the area of painting around it) spaces are on the canvas. Artists can play with a balance between these two spaces to further influence how viewers interpret their work. Therefore, the similarity between different artists concerning the regional (local) complexity can reflect the similarity in thought regarding their approaches to painting. For instance, the proximity between Francis Bacon and Georges de la Tour could be due to the former being heavily influenced by the Baroque style and having made dramatic use of contrasts of light and shadow. These methods are characteristic of the chiaroscuro principle and its radicalization in the Tenebrista school (signature style of Georges de la Tour) [82,83]. The intense contrasts of light and shadow highlight the characters, and although exaggerated, it is lighting that increases the feeling of realism, making the muscles and facial expressions more evident. Simultaneously, the presence of large blackened areas highlights the chromatic research and the illuminated space, which acquire their value as elements of the composition. We conclude that this novel technique is a unique descriptor of the authors’ paintings since it not only aggregates authors of the same style close to each other and demonstrates the influences that authors had on others. It also serves as an insight into the way the artist projects its art. 4.2.4 Evaluation of measures for classification purposes To quantitatively evaluate the use of these measures for classification purposes, we assessed their impact when used as additional features to improve state-of-the-art classification methods. For this purpose, we recreated a recently published state-of-the-art method as a baseline and improved the results by combining our proposed measures. Based on current methods [52,53], we extracted a Gram representation using the first convolutional layer from the fifth convolutional block of the VGG16 network which was pre-trained with the imagenet dataset (no significant result difference was found between the use of VGG16 or VGG19). Principal component analysis (PCA) was applied to the Gram matrix to reduce the dimensionality and, finally, this vector was provided to an SVM to perform classification. Afterwards, the features obtained from computing the HDC and the regional complexity were used for author and style classification using the XGBoost classifier [84] and combined with the baseline classifier via a Voting Classifier ensemble. The results of the baseline and ensemble classifiers, applied to the Paintings91 dataset in the author and style classification task using the labels provided in the dataset, are shown in Table 1 . The results show that the inclusion of the Regional Complexity, increased the accuracy of the results 2.2 p.p. and 1.0 p.p in the style and author classification tasks respectively. Moreover, the overall inclusion of the proposed measures (HDC + RC) increased the accuracy in both classification tasks by 2.8 p.p. and, 2.0 p.p. in the style and author classification tasks, respectively. These results indicate that these predictors are useful auxiliary features capable of improving current methodologies in the classification of artistic paintings. This is congruent with the results obtained with Nanny et al. [85], since handcrafted features and non-handcrafted features seem to extract different information from the input images and, as a result, the fusion of the two types of features improves the results obtained when using non-handcrafted features only. Furthermore, regional complexity (RC) has a higher impact on the improvement of the accuracy than the HDC features, demonstrating the importance and distinction of Regional Complexity as a feature. 5 Discussion In this work, we develop, use, and compare unsupervised pattern recognition techniques to quantify information in images of artistic paintings. We rely on two approaches, namely data compression using the Normalized Compression (NC), and the Block Decomposition Method (BDM), to estimate information of both probabilistic and algorithmic sources. To approximate the NC, we benchmark a set of data compressors, where we show that the most effective for this dataset is PAQ8. Subsequently, this article is organized into two broad sections. The first is the evaluation and comparison of information-based measures; the second is applying these information-based measures to a dataset of artistic paintings. On the measure evaluation section, we assessed the NC and BDM using three tests. In the first test, we evaluated the NC and two normalizations of the BDM, regarding their robustness when images undergo uniform pixel editing and their behavior when applied to different types of datasets. We found that in terms of uniform pixel editing, the NC is more robust than BDM with the same kind of normalization. The NC is a measure of compression (in this case, using the PAQ compressor) that makes use of the digital object in its entirety to create the shortest possible representation without loss of information. In contrast, BDM divides the digital object into blocks and, based on the complexity of the blocks, estimates the image complexity in its entirety. This means that BDM cannot determine the information shared between the blocks, which causes it to increase, when compared to the NC, with the increase in uniform pixel editing. In the second test, we compared both measures using different image natures. We found that the results of the NC and NBDM are similar, except for the computed radiography and the cellular automata dataset, which exhibited a more algorithmic behavior. The cellular automata data was created with small programs with simple rules. While the compressor had difficulty compressing this data, BDM could approximate their algorithmic nature and thus assign them a value close to a minimal complexity value. The ability to identify an algorithmic nature incorporated in the data demonstrates the relevance of BDM as a measure. In the third test, we found that a super sample image transformation causes an underestimation of the amount of information contained in the object by BDM. Again, this is due to BDM analysing the object in blocks, instead of using the object in its entirety. Since the ampliation size was the same as the blocks analysed by BDM, the complexity attributed to each block was approximately zero. Consequently, the overall value attributed to the image complexity decreased dramatically. This aspect demonstrated that BDM cannot handle information contained between each block and can easily underestimate the amount of information present in a digital object. In the second phase, we applied these measures to estimate the complexity of a dataset of paintings. We calculated the NC and NBDM in this dataset with different quantizations and assessed the results in terms of average complexity per author. Afterward, we combined the NC with the exponent of the roughness of the HDC function in the labeled paintings of the dataset. Finally, we computed the average regional complexity of each author regarding their paintings and built a phylogenetic tree. We found that paintings with low complexity are abstract, minimalist, and follow simple patterns. Paintings with a slightly higher average complexity possess different regional complexities, specifically, a region with high complexity and detail surrounded by a background of low complexity. With more complexity, this noticeable pattern begins to fade, and the most complex paintings are globally irregular, detailed, and convoluted. Regarding the average complexity values for each artist, we found that NC and NBDM behave similarly, where quantization impacted more the NBDM. We also found that the low side of the complexity spectrum was characterized by Abstract Expressionism, Minimalism, Constructivism movements, with authors such as Mark Rothko, Lucio Fontana, Piet Mondrian, and El Lissitzky. Also, artists from Abstract Expressionism characterized the high complexity side of the spectrum, such as Willem de Kooning, Jackson Pollock, and Jasper Johns, as well as other artists with a more detailed and convoluted style, like Gustav Klimt and Vincent van Gogh. Due to two different currents (Color Field with authors with low average complexity and Action Painting with authors with high complexity), Abstract Expressionism was present at the polar ends of the spectrum. In all cases, Jackson Pollock had average complexity values that were utterly different from other artists, being the average complexity of his paintings close to random. Although he denied being a creator of random paintings, this result and others [5] seem to indicate that Jackson Pollock’s dripping paintings are not typical artworks, and this is possibly related to the inclusion of many symbolic layers and dispersion intentions over the canvas by the author. When evaluating the artists’ average NC together with the roughness exponent ( α ) of the HDC function in the labels images of the dataset, we found that styles are well confined into different regions, showing that the combination of these measures gives a robust representation of artistic movements. The NC adds to the level of brightness and relative spatial position evidenced by the roughness exponent, the notion of average information present in each artist’s painting, which is consistent within the same style and historical circumstances. We also find that in Abstract Expressionism, the NC is inversely correlated to α . Concretely, artists related to Colour Field painting presented a high α and low NC, whereas artists related to Action Painting presented the exact polar results (low α and high NC). Finally, we divided the image into equal quadrilateral parts and estimated the local complexity of each painting on the dataset and used it to ascertain each artists average regional matrix (fingerprint). Complexity can be thought of as a measure of the total number of properties transmitted by an object and detected by an observer. By dividing images into blocks of equal size and evaluating its local complexity, we quantified the local information being transmitted. Furthermore, by averaging the canvas results per artist, we obtain a unique fingerprint that describes how the author exposes information to the observer. Among other things, these fingerprints give specific insights regarding each artist’s way of painting, showing where, on average, artists paint with more detail and give more emphasis, while also providing insights into each artist’s range of complexity. Using these matrices, we computed a distance matrix and utilized it to construct a phylogenetic tree. We discovered that these phylogenetic trees aggregated authors of the same style close to each other, as well as artists’ influence relationships, like Francis Bacon and Georges de la Tour, and George Braque and Hieronymus Bosch. Furthermore, we observed proximity between artists due to shared methods and techniques which are not correlated with the temporal era or artistic movement. An example of this occurrence is the proximity between Hans Holbein and Vermeer which don’t share styles, but both used optics to achieve precise positioning in their compositions. This evidence shows that artists’ fingerprints contain critical information into how the work is perceived, such as composition, unity, balance, movement, rhythm, focus, contrast, pattern, and proportion of the painting and space. Finally, we show that these measures improve current methodologies in the classification of artistic paintings and thus extract information which differs from non-handcrafted features. Furthermore, regional complexity provided the largest increase in accuracy on the classification tasks, showing its relevance as a descriptor of images of artistic paintings. 6 Conclusions In this paper, we introduce novel solutions to the field of computer analysis of artistic paintings and the problem of artist classification and authentication. Specifically, we assessed the viability of unsupervised measures that approximate the quantity of probabilistic and algorithmic information for performing these tasks. Our direct comparison between NC and BDM allowed us to understand the strengths and weaknesses of both measures. Although BDM has difficulty dealing with uniform pixel edition and full information quantification given the block representability, it serves as a useful tool for measure and indentification of data content having similarity to simple algorithms. On the other hand, the NC is more robust to data alterations (pixel edition and quantization) and is able to measure the quantity of information without underestimation. Regarding the application of information-based measures in artistic paintings, we studied and developed techniques that can be valuable for art authorship attribution and validation, art style categorization and organization, and art content explanation. Namely, the NC proved to be a robust measure that as a whole gives us some insight regarding the complexity of different styles showing hidden patterns and relationships present in artistic paintings that share the same range in complexity. Furthermore, it could be a stylistic descriptor when coupled with the roughness exponent α . On the other hand, fingerprints depict how each author perform typical content distribution on canvas. Thus, they can provide a suitable means of art content explanation, as well as being valuable for art authorship attribution and validation. Moreover, since they provided insights regarding the artists’ way of painting, they can be used as a means of relating authors, being therefore useful for depicting artists’ stylistic influences, and shared techniques. Additionally, using the distance between the artists’ regional complexity, we also find some interesting links between authors regarding the usage of space, technique, composition, rhythm, and proportion. Finally, we demonstrated that the regional complexity and the HDC function of the paintings could serve as useful auxiliary features capable of improving current methodologies in author and style classification of images of artistic paintings. Regarding future continuations to this study, there are many possible future lines of work that can be considered. For example, in this work we analysed the images of paintings by converting them to monochrome, and it would be interesting to separate the colour channels and to analyse them separately, therefore studying the influence of colour in the paintings in terms of complexity. Additionally, it would be interesting to explore how to separate different characteristics of the fingerprint, as well as detecting unknown repeated patterns that appear multiple times in a painting by creating and analysing their complexity surfaces [13]. Lastly, another interesting study would be to replicate the developed work in this article using a competitive compressor that would select the best compressor model for each painting or region. Website A support website to this site can be accessed at This site showcases among other things, the pipeline of this study, the author’s average NC and NBDM variation for different quantization levels, the results of combining the NC with the roughness exponent of HDC function ( α ), a complete catalogue of each author’s fingerprints as well as several examples of each author’s paintings, and the computed phylogenetic trees with a magnifier to allow a better observation of the results. CRediT authorship contribution statement Jorge Miguel Silva: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Diogo Pratas: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Rui Antunes: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Sérgio Matos: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Armando J. Pinho: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Declaration of Competing Interest The authors declare no competing interests. Acknowledgements This work was funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019 and the research grants SFRH/BD/141851/2018 and SFRH/BD/137000/2018 for J.M.S and R.A, respectively. D.P. is funded by national funds through FCT - Fundalo para a Ciłncia e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019. Supplementary material Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.patcog.2021.107864. Appendix A Supplementary materials Supplementary Data S1 Supplementary Raw Research Data. This is open data under the CC BY license Supplementary Data S1 References [1] R.W. Weisberg Creativity: Understanding Innovation in Problem Solving, Science, Invention, and the Arts 2006 John Wiley & Sons R. W. Weisberg, Creativity: Understanding innovation in problem solving, science, invention, and the arts, John Wiley & Sons, 2006. [2] A. Hertzmann Can computers create art? Arts vol. 7 2018 Multidisciplinary Digital Publishing Institute 18 A. Hertzmann, Can computers create art?, in: Arts, volume 7, Multidisciplinary Digital Publishing Institute, 2018, p. 18. [3] F.S. Khan S. Beigpour J. Van de Weijer M. Felsberg Painting-91: a large scale database for computational painting categorization Mach. Vis. Appl. 25 6 2014 1385 1397 F. S. Khan, S. Beigpour, J. Van de Weijer, M. Felsberg, Painting-91: a large scale database for computational painting categorization, Machine vision and applications 25(6) (2014) 1385–1397. [4] S. Lyu D. Rockmore H. Farid A digital technique for art authentication Proc. Natl. Acad. Sci. 101 49 2004 17006 17010 S. Lyu, D. Rockmore, H. Farid, A digital technique for art authentication, Proceedings of the National Academy of Sciences 101(49) (2004) 17006–17010. [5] D. Kim S.-W. Son H. Jeong Large-scale quantitative analysis of painting arts Sci. Rep. 4 2014 7370 D. Kim, S.-W. Son, H. Jeong, Large-scale quantitative analysis of painting arts, Scientific reports 4 (2014) 7370. [6] H. Zhang S. Sfarra K. Saluja J. Peeters J. Fleuret Y. Duan H. Fernandes N. Avdelidis C. Ibarra-Castanedo X. Maldague Non-destructive investigation of paintings on canvas by continuous wave terahertz imaging and flash thermography J. Nondestruct. Eval. 36 2 2017 34 H. Zhang, S. Sfarra, K. Saluja, J. Peeters, J. Fleuret, Y. Duan, H. Fernandes, N. Avdelidis, C. Ibarra-Castanedo, X. Maldague, Non-destructive investigation of paintings on canvas by continuous wave terahertz imaging and flash thermography, Journal of Nondestructive Evaluation 36(2) (2017) 34. [7] H. Zenil S. Hernández-Orozco N.A. Kiani F. Soler-Toscano A. Rueda-Toicen J. Tegnér A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity Entropy 20 8 2018 605 H. Zenil, S. Hernández-Orozco, N. A. Kiani, F. Soler-Toscano, A. Rueda-Toicen, J. Tegnér, A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity, Entropy 20(8) (2018) 605. [8] J.-P. Delahaye H. Zenil Numerical evaluation of algorithmic complexity for short strings: a glance into the innermost structure of randomness Applied Mathematics and Computation 219 1 2012 63 77 10.1016/j.amc.2011.10.006 J.-P. Delahaye, H. Zenil, Numerical evaluation of algorithmic complexity for short strings: A glance into the innermost structure of randomness, Applied Mathematics and Computation 219(1) (2012) 63–77. Towards a Computational Interpretation of Physical Theories. 10.1016/j.amc.2011.10.006 Towards a Computational Interpretation of Physical Theories. [9] F. Soler-Toscano H. Zenil J.-P. Delahaye N. Gauvrit Calculating Kolmogorov complexity from the output frequency distributions of small Turing machines PloS one 9 5 2014 F. Soler-Toscano, H. Zenil, J.-P. Delahaye, N. Gauvrit, Calculating Kolmogorov complexity from the output frequency distributions of small Turing machines, PloS one 9(5) (2014). [10] J. Smiers Arts Under Pressure: Protecting Cultural Diversity in the Age of Globalisation 2003 Zed Books J. Smiers, Arts under pressure: Protecting cultural diversity in the age of globalisation, Zed Books, 2003. [11] P.J. Ferreira A.J. Pinho A method to detect repeated unknown patterns in an image International Conference Image Analysis and Recognition 2014 Springer 12 19 P. J. Ferreira, A. J. Pinho, A method to detect repeated unknown patterns in an image, in: International Conference Image Analysis and Recognition, Springer, 2014, pp. 12–19. [12] A.J. Pinho P.J. Ferreira Finding unknown repeated patterns in images 2011 19th European Signal Processing Conference 2011 IEEE 584 588 A. J. Pinho, P. J. Ferreira, Finding unknown repeated patterns in images, in: 2011 19th European Signal Processing Conference, IEEE, 2011, pp. 584–588. [13] D. Pratas A.J. Pinho On the detection of unknown locally repeating patterns in images International Conference Image Analysis and Recognition 2012 Springer 158 165 D. Pratas, A. J. Pinho, On the detection of unknown locally repeating patterns in images, in: International Conference Image Analysis and Recognition, Springer, 2012, pp. 158–165. [14] A. Romashchenko A. Shen N. Vereshchagin Combinatorial interpretation of Kolmogorov complexity Theor. Comput. Sci. 271 1–2 2002 111 123 A. Romashchenko, A. Shen, N. Vereshchagin, Combinatorial interpretation of Kolmogorov complexity, Theoretical Computer Science 271(1-2) (2002) 111–123. [15] R.K. Niven Combinatorial entropies and statistics Eur. Phys. J. B 70 1 2009 49 63 R. K. Niven, Combinatorial entropies and statistics, The European Physical Journal B 70(1) (2009) 49–63. [16] S. Mantaci A. Restivo G. Rosone M. Sciortino A new combinatorial approach to sequence comparison Theory Comput. Syst. 42 3 2008 411 429 S. Mantaci, A. Restivo, G. Rosone, M. Sciortino, A new combinatorial approach to sequence comparison, Theory of Computing Systems 42(3) (2008) 411–429. [17] C.E. Shannon A mathematical theory of communication Bell Syst. Tech. J. 27 3 1948 379 423 C. E. Shannon, A mathematical theory of communication, Bell system technical journal 27(3) (1948) 379–423. [18] A.N. Kolmogorov Three approaches to the quantitative definition of information Probl. Inf. Transm. 1 1 1965 1 7 A. N. Kolmogorov, Three approaches to the quantitative definition of information’, Problems of information transmission 1(1) (1965) 1–7. [19] R.J. Solomonoff A formal theory of inductive inference. Part I Inf. Control 7 1 1964 1 22 R. J. Solomonoff, A formal theory of inductive inference. Part I, Information and control 7(1) (1964a) 1–22. [20] R.J. Solomonoff A formal theory of inductive inference. Part II Inf. Control 7 2 1964 224 254 R. J. Solomonoff, A formal theory of inductive inference. Part II, Information and control 7(2) (1964b) 224–254. [21] G.J. Chaitin On the length of programs for computing finite binary sequences J. ACM (JACM) 13 4 1966 547 569 G. J. Chaitin, On the length of programs for computing finite binary sequences, Journal of the ACM (JACM) 13(4) (1966) 547–569. [22] F. Soler-Toscano H. Zenil A computable measure of algorithmic probability by finite approximations with an application to integer sequences Complexity 2017 2017 F. Soler-Toscano, H. Zenil, A computable measure of algorithmic probability by finite approximations with an application to integer sequences, Complexity 2017 (2017). [23] N. Gauvrit H. Zenil F. Soler-Toscano J.-P. Delahaye P. Brugger Human behavioral complexity peaks at age 25 PLoS Comput. Biol. 13 4 2017 N. Gauvrit, H. Zenil, F. Soler-Toscano, J.-P. Delahaye, P. Brugger, Human behavioral complexity peaks at age 25, PLoS computational biology 13(4) (2017). [24] M. Li J.H. Badger X. Chen S. Kwong P. Kearney H. Zhang An information-based sequence distance and its application to whole mitochondrial genome phylogeny Bioinformatics 17 2 2001 149 154 M. Li, J. H. Badger, X. Chen, S. Kwong, P. Kearney, H. Zhang, An information-based sequence distance and its application to whole mitochondrial genome phylogeny, Bioinformatics 17(2) (2001) 149–154. [25] R. Cilibrasi P.M.B. Vitányi Clustering by compression IEEE Trans. Inf. Theory 51 4 2005 1523 1545 R. Cilibrasi, P. M. B. Vitányi, Clustering by compression, IEEE Transactions on Information theory 51(4) (2005) 1523–1545. [26] R. Cilibrasi P. Vitanyi Automatic extraction of meaning from the web 2006 IEEE International Symposium on Information Theory 2006 2309 2313 R. Cilibrasi, P. Vitanyi, Automatic extraction of meaning from the web, in: 2006 IEEE International Symposium on Information Theory, 2006, pp. 2309–2313. [27] M. Cebrián M. Alfonseca A. Ortega The normalized compression distance is resistant to noise IEEE Trans. Inf. Theory 53 5 2007 1895 1900 M. Cebrián, M. Alfonseca, A. Ortega, The normalized compression distance is resistant to noise, IEEE Transactions on Information Theory 53(5) (2007) 1895–1900. [28] A.R. Cohen P.M.B. Vitányi Normalized compression distance of multisets with applications IEEE Trans. Pattern Anal. Mach.Intell. 37 8 2014 1602 1614 A. R. Cohen, P. M. B. Vitányi, Normalized compression distance of multisets with applications, IEEE transactions on pattern analysis and machine intelligence 37(8) (2014) 1602–1614. [29] D. Pratas A.J. Pinho On the approximation of the Kolmogorov complexity for DNA sequences Iberian Conference on Pattern Recognition and Image Analysis 2017 Springer 259 266 D. Pratas, A. J. Pinho, On the approximation of the Kolmogorov complexity for DNA sequences, in: Iberian Conference on Pattern Recognition and Image Analysis, Springer, 2017, pp. 259–266. [30] S.S. Maniccam N. Bourbakis Lossless compression and information hiding in images Pattern Recognit. 37 3 2004 475 486 S. S. Maniccam, N. Bourbakis, Lossless compression and information hiding in images, Pattern Recognition 37(3) (2004) 475–486. [31] Z.-M. Lu S.-Z. Guo Lossless Information Hiding in Images 2016 Syngress Z.-M. Lu, S.-Z. Guo, Lossless information hiding in images, Syngress, 2016. [32] D. Pratas A.J. Pinho P.J. Ferreira Efficient compression of genomic sequences 2016 Data Compression Conference (DCC) 2016 IEEE 231 240 D. Pratas, A. J. Pinho, P. J. Ferreira, Efficient compression of genomic sequences, in: 2016 Data Compression Conference (DCC), IEEE, 2016, pp. 231–240. [33] D. Pratas M. Hosseini A.J. Pinho Substitutional tolerant Markov models for relative compression of DNA sequences International Conference on Practical Applications of Computational Biology & Bioinformatics 2017 Springer 265 272 D. Pratas, M. Hosseini, A. J. Pinho, Substitutional tolerant markov models for relative compression of DNA sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2017, pp. 265–272. [34] A.J. Pinho A.J.R. Neves P.J. Ferreira Inverted-repeats-aware finite-context models for DNA coding 2008 16th European Signal Processing Conference 2008 IEEE 1 5 A. J. Pinho, A. J. R. Neves, P. J. Ferreira, Inverted-repeats-aware finite-context models for DNA coding, in: 2008 16th European Signal Processing Conference, IEEE, 2008, pp. 1–5. [35] M. Mahoney, Data Compression Programs, 2020. accessed May 16, [36] J. Rissanen Modeling by shortest data description Automatica 14 5 1978 465 471 J. Rissanen, Modeling by shortest data description, Automatica 14(5) (1978) 465–471. [37] M. Li X. Chen X. Li B. Ma P.M.B. Vitányi The similarity metric IEEE Trans. Inf. Theory 50 12 2004 3250 3264 M. Li, X. Chen, X. Li, B. Ma, P. M. B. Vitányi, The similarity metric, IEEE transactions on Information Theory 50(12) (2004) 3250–3264. [38] M. Li P. Vitányi An Introduction to Kolmogorov Complexity and its Applications vol. 3 2008 Springer M. Li, P. Vitányi, et al., An introduction to Kolmogorov complexity and its applications, volume 3, Springer, 2008. [39] R.P. Taylor A.P. Micolich D. Jonas Fractal analysis of Pollock’s drip paintings Nature 399 6735 1999 R. P. Taylor, A. P. Micolich, D. Jonas, Fractal analysis of Pollock’s drip paintings, Nature 399(6735) (1999) 422–422. 422–422 [40] C.R. Johnson E. Hendriks I.J. Berezhnoy E. Brevdo S.M. Hughes I. Daubechies J. Li E. Postma J.Z. Wang Image processing for artist identification IEEE Signal Process. Mag. 25 4 2008 37 48 C. R. Johnson, E. Hendriks, I. J. Berezhnoy, E. Brevdo, S. M. Hughes, I. Daubechies, J. Li, E. Postma, J. Z. Wang, Image processing for artist identification, IEEE Signal Processing Magazine 25(4) (2008) 37–48. [41] J. Li J.Z. Wang Studying digital imagery of ancient paintings by mixtures of stochastic models IEEE Trans. Image Process. 13 3 2004 340 353 J. Li, J. Z. Wang, Studying digital imagery of ancient paintings by mixtures of stochastic models, IEEE Transactions on Image Processing 13(3) (2004) 340–353. [42] M. Bressan C. Cifarelli F. Perronnin An analysis of the relationship between painters based on their work 2008 15th IEEE International Conference on Image Processing 2008 IEEE 113 116 M. Bressan, C. Cifarelli, F. Perronnin, An analysis of the relationship between painters based on their work, in: 2008 15th IEEE International Conference on Image Processing, IEEE, 2008, pp. 113–116. [43] B.A. Olshausen M.R. DeWeese Applied mathematics: the statistics of style Nature 463 7284 2010 1027 B. A. Olshausen, M. R. DeWeese, Applied mathematics: The statistics of style, Nature 463(7284) (2010) 1027. [44] J.M. Hughes D.J. Graham D.N. Rockmore Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder Proc. Natl. Acad. Sci. 107 4 2010 1279 1283 J. M. Hughes, D. J. Graham, D. N. Rockmore, Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder, Proceedings of the National Academy of Sciences 107(4) (2010) 1279–1283. [45] D.G. Stork Y. Furuichi Image analysis of paintings by computer graphics synthesis: an investigation of the illumination in Georges de la Tour’s Christ in the carpenter’s studio Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100J D. G. Stork, Y. Furuichi, Image analysis of paintings by computer graphics synthesis: an investigation of the illumination in Georges de la Tour’s Christ in the carpenter’s studio, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100J. [46] M. Lettner R. Sablatnig Estimating the original drawing trace of painted strokes Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100C M. Lettner, R. Sablatnig, Estimating the original drawing trace of painted strokes, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100C. [47] M. Shahram D.G. Stork D. Donoho Recovering layers of brush strokes through statistical analysis of color and shape: an application to van Gogh’s self portrait with grey felt hat Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100D M. Shahram, D. G. Stork, D. Donoho, Recovering layers of brush strokes through statistical analysis of color and shape: an application to van Gogh’s\" self portrait with grey felt hat\", in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100D. [48] S.B. Hedges Image analysis of renaissance copperplate prints Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 681009 S. B. Hedges, Image analysis of renaissance copperplate prints, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 681009. [49] V.M. Petrov Entropy and stability in painting: an information approach to the mechanisms of artistic creativity Leonardo 35 2 2002 197 202 V. M. Petrov, Entropy and stability in painting: An information approach to the mechanisms of artistic creativity, Leonardo 35(2) (2002) 197–202. [50] J.T. Machado A.M. Lopes Artistic painting: a fractional calculus perspective Appl. Math. Modell. 65 2019 614 626 J. T. Machado, A. M. Lopes, Artistic painting: A fractional calculus perspective, Applied Mathematical Modelling 65 (2019) 614–626. [51] K.-C. Peng T. Chen Cross-layer features in convolutional neural networks for generic classification tasks 2015 IEEE International Conference on Image Processing (ICIP) 2015 IEEE 3057 3061 K.-C. Peng, T. Chen, Cross-layer features in convolutional neural networks for generic classification tasks, in: 2015 IEEE International Conference on Image Processing (ICIP), IEEE, 2015, pp. 3057–3061. [52] H. Mao M. Cheung J. She DeepArt: learning joint representations of visual arts Proceedings of the 25th ACM International conference on Multimedia 2017 1183 1191 H. Mao, M. Cheung, J. She, Deepart: Learning joint representations of visual arts, in: Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1183–1191. [53] W. Chu Y. Wu Image style classification based on learnt deep correlation features IEEE Trans. Multimed. 20 9 2018 2491 2502 W. Chu, Y. Wu, Image style classification based on learnt deep correlation features, IEEE Transactions on Multimedia 20(9) (2018) 2491–2502. [54] D. Hammer A. Romashchenko A. Shen N. Vereshchagin Inequalities for Shannon entropy and Kolmogorov complexity J. Comput. Syst. Sci. 60 2 2000 442 464 D. Hammer, A. Romashchenko, A. Shen, N. Vereshchagin, Inequalities for Shannon entropy and Kolmogorov complexity, Journal of Computer and System Sciences 60(2) (2000) 442–464. [55] T. Henriques H. Gonçalves L. Antunes M. Matias J.a. Bernardes C. Costa-Santos Entropy and compression: two measures of complexity J. Eval. Clin. Pract. 19 6 2013 1101 1106 T. Henriques, H. Gonçalves, L. Antunes, M. Matias, J. a. Bernardes, C. Costa-Santos, Entropy and compression: two measures of complexity, Journal of Evaluation in Clinical Practice 19(6) (2013) 1101–1106. [56] S. Terwijn L. Torenvliet P.M.B. Vitányi Nonapproximablity of the normalized information distance CoRR 2009 abs/0910.4353 S. Terwijn, L. Torenvliet, P. M. B. Vitányi, Nonapproximablity of the normalized information distance, CoRR abs/0910.4353 (2009). 0910.4353 [57] A. Rybalov On the strongly generic undecidability of the halting problem Theor. Comput. Sci. 377 1–3 2007 268 270 A. Rybalov, On the strongly generic undecidability of the halting problem, Theoretical Computer Science 377(1-3) (2007) 268–270. [58] P. Bloem F. Mota S. de Rooij L. Antunes P. Adriaans A safe approximation for Kolmogorov complexity International Conference on Algorithmic Learning Theory 2014 Springer 336 350 P. Bloem, F. Mota, S. de Rooij, L. Antunes, P. Adriaans, A safe approximation for Kolmogorov complexity, in: International Conference on Algorithmic Learning Theory, Springer, 2014, pp. 336–350. [59] R.R. Sokal A statistical method for evaluating systematic relationships Univ. Kansas, Sci. Bull. 38 1958 1409 1438 R. R. Sokal, A statistical method for evaluating systematic relationships, Univ. Kansas, Sci. Bull. 38 (1958) 1409–1438. [60] J.B. Kruskal On the shortest spanning subtree of a graph and the traveling salesman problem Proc. Am. Math. Soc. 7 1 1956 48 50 J. B. Kruskal, On the shortest spanning subtree of a graph and the traveling salesman problem, Proceedings of the American Mathematical society 7(1) (1956) 48–50. [61] S. Lloyd Least squares quantization in PCM IEEE Trans. Inf. Theory 28 2 1982 129 137 S. Lloyd, Least squares quantization in PCM, IEEE transactions on information theory 28(2) (1982) 129–137. [62] D.S. Taubman M.W. Marcellin JPEG2000: image compression fundamentals Stand. Pract. 11 2 2002 D. S. Taubman, M. W. Marcellin, JPEG2000: Image compression fundamentals, Standards and Practice 11(2) (2002). [63] J. Gailly, M. Adler, The gzip home page, 2020. accessed May 16, [64] bzip2, 2020. accessed May 16, [65] L. Collin, XZ Utils, 2020. accessed May 16, [66] I. Pavlov, 7-Zip, 2020. accessed May 16, [67] M. Hosseini D. Pratas A.J. Pinho AC: a compression tool for amino acid sequences Interdiscip. Sci. Comput. Life Sci. 11 1 2019 68 76 M. Hosseini, D. Pratas, A. J. Pinho, AC: A compression tool for amino acid sequences, Interdisciplinary Sciences: Computational Life Sciences 11(1) (2019) 68–76. [68] J. Cleary I. Witten Data compression using adaptive coding and partial string matching IEEE Trans. Commun. 32 4 1984 396 402 J. Cleary, I. Witten, Data compression using adaptive coding and partial string matching, IEEE transactions on Communications 32(4) (1984) 396–402. [69] A.J. Buchner, PAQ, 2020. accessed May 16, [70] M.V. Mahoney Adaptive weighing of context models for lossless data compression Technical Report 2005 Florida Institute of Technology CS Department of the W University Blvd M. V. Mahoney, Adaptive weighing of context models for lossless data compression, Technical Report, Florida Institute of Technology CS Department of the W University Blvd, 2005. [71] J. Rissanen G.G. Langdon Arithmetic coding IBM J. Res. Dev. 23 2 1979 149 162 J. Rissanen, G. G. Langdon, Arithmetic coding, IBM Journal of research and development 23(2) (1979) 149–162. [72] A. Moffat R.M. Neal I.H. Witten Arithmetic coding revisited ACM Trans. Inf. Syst. (TOIS) 16 3 1998 256 294 A. Moffat, R. M. Neal, I. H. Witten, Arithmetic coding revisited, ACM Transactions on Information Systems (TOIS) 16(3) (1998) 256–294. [73] B. Knoll N. de Freitas A machine learning perspective on predictive coding with PAQ8 2012 Data Compression Conference 2012 IEEE 377 386 B. Knoll, N. de Freitas, A machine learning perspective on predictive coding with PAQ8, in: 2012 Data Compression Conference, IEEE, 2012, pp. 377–386. [74] Best Artworks of All Time, 2020. accessed May 18, [75] Diabetic Retinopathy Detection, 2020. accessed May 18, [76] X. Wang Y. Peng L. Lu Z. Lu M. Bagheri R.M. Summers ChestX-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases IEEE CVPR 2017 3462 3471 X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, in: IEEE CVPR, 2017, pp. 3462–3471. [77] COCO - Common Objects in Context, 2020. accessed May 18, [78] C. Shapiro Abstract expressionism: the politics of apolitical painting Prospects 3 1978 175 214 C. Shapiro, et al., Abstract Expressionism: The politics of apolitical painting, Prospects 3 (1978) 175–214. [79] H. Rosenberg The Tradition of The New 1994 Hachette Books H. Rosenberg, The Tradition Of The New, Hachette Books, 1994. [80] L. Garrard Colourfield Painting: Minimal, Cool, Hard Edge, Serial and Post-Painterly Abstract Art from the Sixties to the Present 2007 Crescent Moon Publishing L. Garrard, Colourfield painting: Minimal, Cool, Hard Edge, Serial and Post-painterly Abstract Art from the Sixties to the present, Crescent Moon Publishing, 2007. [81] D. Hockney Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters 2006 Viking Studio D. Hockney, Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters, Viking Studio, 2006. [82] S. Yang G. Cheung P. Le Callet J. Liu Z. Guo Computational modeling of artistic intention: quantify lighting surprise for painting analysis 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX) 2016 IEEE 1 6 S. Yang, G. Cheung, P. Le Callet, J. Liu, Z. Guo, Computational modeling of artistic intention: Quantify lighting surprise for painting analysis, in: 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), IEEE, 2016, pp. 1–6. [83] L. Fichner-Rathus Foundations of Art and Design: An Enhanced Media Edition 2011 Cengage Learning L. Fichner-Rathus, Foundations of art and design: An enhanced media edition, Cengage Learning, 2011. [84] T. Chen C. Guestrin XGBoost: a scalable tree boosting system Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD ’16 2016 ACM New York, NY, USA 785 794 10.1145/2939672.2939785 T. Chen, C. Guestrin, XGBoost: A scalable tree boosting system, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, ACM, New York, NY, USA, 2016, pp. 785–794. 10.1145/2939672.2939785 [85] L. Nanni S. Ghidoni S. Brahnam Handcrafted vs. non-handcrafted features for computer vision classification Pattern Recognit. 71 2017 158 172 10.1016/j.patcog.2017.05.025 L. Nanni, S. Ghidoni, S. Brahnam, Handcrafted vs. non-handcrafted features for computer vision classification, Pattern Recognition 71 (2017) 158–172. 10.1016/j.patcog.2017.05.025 Jorge Miguel Silva is a Ph.D. Student and Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), a Computer Science and Engineering research unit. He holds a Master’s degree in Bioengineering from the Faculty of Engineering at the University of Porto. His research interests are Information Theory, Artificial Intelligence and Compression. Diogo Pratas is a computer scientist at IEETA/DETI at the University of Aveiro. Diogo’s research interests include computational biology, data compression, and information theory. Diogo earned his Ph.D. in computer science from the University of Aveiro and holds a B.S. degree in information and communications technologies also from the University of Aveiro, with a segment carried at the Pontifical University of Salamanca. He joined IEETA/DETI in 2009, after being working some years in the private sector. Diogo actively collaborates with the Department of Virology at the University of Helsinki, namely in the identification and reconstruction of viral genomes, and the respective human-hosts, at multi-organ level. Diogo is a member of the Super Dimension Fortress, the International Society for Computational Biology, and the Portuguese Association for Pattern Recognition. Rui Antunes is a Ph.D. Student and Research Fellow at IEETA, a Computer Science and Engineering research unit. He holds a Master’s degree in Electronic and Telecommunications Engineering from University of Aveiro. His research interests include artificial intelligence, machine learning, natural language processing, and information extraction. Sergio Matos graduated in Systems Engineering and Computing by the University of Algarve (Portugal) in 1999 and received his Ph.D. in computer engineering in 2007 by the University of Leicester (UK), where he developed a system, based on speech processing and recognition technologies, for monitoring patients with acute or chronic cough. Sergio Matos is an integrated member of the Biomedical Informatics and Technologies group at IEETA, University of Aveiro, since 2008 and Assistant Professor at the Department of Electronics, Telecommunications and Informatics since 2018. His research interests are information retrieval, text mining, NLP and machine-learning. Armando J. Pinho received the Electronics and Telecommunications Engineering degree from the University of Aveiro, Portugal, in 1988, the Master’s degree in electrical and computers engineering from IST, Technical University of Lisbon, Portugal, in 1991 and the Ph.D. in electrical engineering from the University of Aveiro, in 1996. Currently, Armando J. Pinho is an Associate Professor with habilitation at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, and a researcher at the Signal Processing Laboratory of the Institute of Electronics and Telematics Engineering of Aveiro - IEETA. His main research activity is in the area of image coding, data compression, and computational biology. He also has interests in other areas of research, such as bioinformatics and image and video analysis. "
    },
    {
        "doc_title": "Universal passage weighting mecanism (UPWM) in BioASQ 9b",
        "doc_scopus_id": "85113566576",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85113566576",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical informatics",
            "Document Retrieval",
            "Interaction model",
            "Point wise",
            "Ranking model",
            "Relevance score"
        ],
        "doc_abstract": "© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).This paper presents the participation of the University of Aveiro Biomedical Informatics and Techologies group (BIT) in the ninth edition of the BioASQ challenge for document and snippet retrieval. Our proposed systems follow a two-stage retrieval pipeline, similar to our BioASQ 8B submissions. However, we completely rebuilt our neural ranking model, maintaining the key ideas of its inception while improving its computational efficiency as well as adding interoperability with the transformer architecture. This resulted in a novel universal passage weighting mechanism (UPWM), which offers a more powerful way to derive a document relevance score from the combination of its sentences. More concretely, we have built two variants that use our passage mechanism, the lightning UPWM and the transformer UPWM. The first uses a shallow interaction model and the second uses a BERT model. Additionally, we also propose an effective pairwise joint training mechanism that combines document retrieval with snippet retrieval. Our systems achieved competitive results scoring at the top and close to the top for all the batches, with MAP values ranging from 0.3573 to 0.4236 in the document retrieval task. Although we only submitted for the snippet retrieval task in the last two batches, our system scored at top position in the last batch by using rank reciprocal fusion of pointwise and pairwise joint training approaches. Code to reproduce our submissions are available on https://github.com/bioinformatics-ua/BioASQ9b.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioASQ Synergy: A strong and simple baseline rooted in relevance feedback",
        "doc_scopus_id": "85113426348",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85113426348",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical informatics",
            "Query expansion",
            "Re-ranking",
            "Relevance feedback"
        ],
        "doc_abstract": "© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).This paper presents the participation of the University of Aveiro Biomedical Informatics and Techologies (BIT) group in the Synergy task of the ninth edition of the BioASQ challenge. Given availability of feedback data between rounds, we explored a traditional relevance feedback approach. More precisely, we performed query expansion by selecting the highest tf-idf terms from snippets judged as relevant by experts. Then, the revised query is processed by our BioASQ-8b pipeline consisting of BM25 followed by a lightweight neural reranking model. Our system achieved results above the median, which given its simplicity can be considered satisfactory. Furthermore, in two batches our best results were only second to the runs submitted by the top performing team. Code to reproduce our submissions are available on https://github.com/bioinformatics-ua/BioASQ9-Synergy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Benchmarking a transformer-FREE model for ad-hoc retrieval",
        "doc_scopus_id": "85107311005",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85107311005",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [
            "Ad Hoc retrieval",
            "Computational resources",
            "Free model",
            "Real-world",
            "Reproducibilities",
            "Retrieval performance"
        ],
        "doc_abstract": "© 2021 Association for Computational LinguisticsTransformer-based “behemoths” have grown in popularity, as well as structurally, shattering multiple NLP benchmarks along the way. However, their real-world usability remains a question. In this work, we empirically assess the feasibility of applying transformer-based models in real-world ad-hoc retrieval applications by comparison to a “greener and more sustainable” alternative, comprising only 620 trainable parameters. We present an analysis of their efficacy and efficiency and show that considering limited computational resources, the lighter model running on the CPU achieves a 3 to 20 times speedup in training and 7 to 47 times in inference while maintaining a comparable retrieval performance. Code to reproduce the efficiency experiments is available on https://github.com/bioinformatics-ua/ EACL2021-reproducibility/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Leveraging Clinical Notes for Enhancing Decision-Making Systems with Relevant Patient Information",
        "doc_scopus_id": "85107294095",
        "doc_doi": "10.1007/978-3-030-72379-8_26",
        "doc_eid": "2-s2.0-85107294095",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Automatic extraction",
            "Clinical decision support systems",
            "Clinical treatments",
            "Decision-making systems",
            "Electronic health record systems",
            "Patient information",
            "Patient trajectories",
            "Real time evaluation"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Personalised treatment is usually needed for hospitalised patients afflicted by secondary illnesses that demand daily medication. Even though clinical guidelines were designed to consider those circumstances exist, current decision-support features fail to assimilate detailed relevant patient information. This creates opportunities for the development of systems capable of performing a real-time evaluation of such data against existing knowledge and providing recommendations during clinical treatments. Herein, we describe a proposal for a new feature to be integrated with electronic health record (EHR) systems which can enrich the health treatment process through the automatic extraction of information from patient medical notes and the aggregation of this novel information in clinical protocols. The purpose of this work is to exploit the historical component of the patient trajectory to improve the performance of clinical decision support systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bilingual emotion analysis on social media throughout the COVID-19 pandemic in Portugal",
        "doc_scopus_id": "85103846666",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85103846666",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Basic emotions",
            "Coronaviruses",
            "Critical periods",
            "Emotion analysis",
            "General population",
            "Proof of concept",
            "Social media",
            "Well being"
        ],
        "doc_abstract": "Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reservedThis paper presents preliminary work on the topic of emotion analysis on Twitter, in the context of the coronavirus pandemic in Portugal. We collected, curated and analyzed covid-related tweets of users in Portugal in order to understand the evolution of the six basic emotions reflected in these tweets. We analyzed tweets written in both English and Portuguese. In this first step of our work we correlate this information with key events of the evolution of the pandemic in Portugal during March, which was the most critical period in Portugal. We do so in an attempt to estimate the online manifestation of the psychological toll that this pandemic has on the overall well-being status of the general population. Our findings show that the sentiment analysis of covid-related tweets is consistent with our hypothesis that negative emotions would intensify as the pandemic progressed. The preliminary results obtained stand as proof of concept that the analysis of real-time tweets or other social media messages through sentiment analysis can be an important tool for behavioural and well-being tracking.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extraction of family history information from clinical notes: Deep learning and heuristics approach",
        "doc_scopus_id": "85098599299",
        "doc_doi": "10.2196/22898",
        "doc_eid": "2-s2.0-85098599299",
        "doc_date": "2020-12-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© João Figueira Silva, João Rafael Almeida, Sérgio Matos. Originally published in JMIR Medical Informatics (http://medinform.jmir.org), 29.12.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on http://medinform.jmir.org/, as well as this copyright and license information must be included.Background: Electronic health records store large amounts of patient clinical data. Despite efforts to structure patient data, clinical notes containing rich patient information remain stored as free text, greatly limiting its exploitation. This includes family history, which is highly relevant for applications such as diagnosis and prognosis. Objective: This study aims to develop automatic strategies for annotating family history information in clinical notes, focusing not only on the extraction of relevant entities such as family members and disease mentions but also on the extraction of relations between the identified entities. Methods: This study extends a previous contribution for the 2019 track on family history extraction from national natural language processing clinical challenges by improving a previously developed rule-based engine, using deep learning (DL) approaches for the extraction of entities from clinical notes, and combining both approaches in a hybrid end-to-end system capable of successfully extracting family member and observation entities and the relations between those entities. Furthermore, this study analyzes the impact of factors such as the use of external resources and different types of embeddings in the performance of DL models. Results: The approaches developed were evaluated in a first task regarding entity extraction and in a second task concerning relation extraction. The proposed DL approach improved observation extraction, obtaining F1 scores of 0.8688 and 0.7907 in the training and test sets, respectively. However, DL approaches have limitations in the extraction of family members. The rule-based engine was adjusted to have higher generalizing capability and achieved family member extraction F1 scores of 0.8823 and 0.8092 in the training and test sets, respectively. The resulting hybrid system obtained F1 scores of 0.8743 and 0.7979 in the training and test sets, respectively. For the second task, the original evaluator was adjusted to perform a more exact evaluation than the original one, and the hybrid system obtained F1 scores of 0.6480 and 0.5082 in the training and test sets, respectively. Conclusions: We evaluated the impact of several factors on the performance of DL models, and we present an end-to-end system for extracting family history information from clinical notes, which can help in the structuring and reuse of this type of information. The final hybrid solution is provided in a publicly available code repository.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clinical concept normalization on medical records using word embeddings and heuristics",
        "doc_scopus_id": "85086886483",
        "doc_doi": "10.3233/SHTI200129",
        "doc_eid": "2-s2.0-85086886483",
        "doc_date": "2020-06-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Clinical history",
            "Electronic health record",
            "Free texts",
            "Medical record",
            "NAtural language processing",
            "Electronic Health Records",
            "Heuristics",
            "Humans",
            "Narration",
            "Natural Language Processing"
        ],
        "doc_abstract": "© 2020 European Federation for Medical Informatics (EFMI) and IOS Press.Electronic health records contain valuable information on patients'clinical history in the form of free text. Manually analyzing millions of these documents is unfeasible and automatic natural language processing methods are essential for efficiently exploiting these data. Within this, normalization of clinical entities, where the aim is to link entity mentions to reference vocabularies, is of utmost importance to successfully extract knowledge from clinical narratives. In this paper we present sieve-based models combined with heuristics and word embeddings and present results of our participation in the 2019 n2c2 (National NLP Clinical Challenges) shared-task on clinical concept normalization.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rule-based extraction of family history information from clinical notes",
        "doc_scopus_id": "85083034192",
        "doc_doi": "10.1145/3341105.3374000",
        "doc_eid": "2-s2.0-85083034192",
        "doc_date": "2020-03-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Clinical data",
            "Clinical notes",
            "Diagnosis and prognosis",
            "Electronic health record",
            "History informations",
            "Large amounts",
            "NAtural language processing",
            "Rule-based techniques"
        ],
        "doc_abstract": "© 2020 ACM.One of the features of Electronic Health Records (EHR) is to store the patient clinical data. Despite the efforts to structure all this data, clinical reports and notes containing essential information about each patient are still stored in free text. Some of this information refers to the family's health history and may be highly relevance for diagnosis and prognosis. We proposed two methodologies to unify this knowledge and extract family history information from clinical notes using rule-based techniques in natural language processing (NLP). With these methods, we intend to collect the family members mentioned in the text as well as associations to diseases and living status. The proposed methods were evaluated into two stages, demonstrating F-scores of 0.72 and 0.74 for the discovery of family members and observations, and 0.62 and 0.52 for the detection of the family relations with the observations, and their living status. Our methodologies raised new strategies to automatically annotate large amounts of EHRs, facilitating the detection of comorbidities within family relations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluating semantic textual similarity in clinical sentences using deep learning and sentence embeddings",
        "doc_scopus_id": "85083026511",
        "doc_doi": "10.1145/3341105.3373987",
        "doc_eid": "2-s2.0-85083026511",
        "doc_date": "2020-03-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Electronic health record (EHRs)",
            "Healthcare quality",
            "Medical decision making",
            "Medical information",
            "Semantic similarity",
            "Text preprocessing",
            "Textual similarities",
            "Understandability"
        ],
        "doc_abstract": "© 2020 ACM.The wide adoption of electronic health records (EHRs) has fostered an improvement in healthcare quality, with EHRs currently representing a major source of medical information. Nevertheless, this process has also brought new challenges to the medical environment since the facilitated replication of information (e.g. using copy-paste) has resulted in less concise and sometimes incorrect information, which hinders the understandability of this data and can compromise the quality of medical decisions drawn from it. Due to the high volume and redundancy in medical data, it is imperative to develop solutions that can condense information whilst retaining its value, with a possible methodology involving the assessment of the semantic similarity between clinical text excerpts. In this paper we present an approach that explores neural networks and different types of text preprocessing pipelines, and that evaluates the impact of using word embeddings or sentence embeddings. We present the results following our participation in the n2c2 shared-task on clinical semantic textual similarity, perform an error analysis and discuss obtained results along with possible future improvements.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BIT.UA at BioASQ 8: Lightweight neural document ranking with zero-shot snippet retrieval",
        "doc_scopus_id": "85121805169",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85121805169",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical informatics",
            "Document ranking",
            "Document Retrieval"
        ],
        "doc_abstract": "Copyright © 2020 for this paper by its authors.This paper presents the participation of the University of Aveiro Biomedical Informatics and Techologies (BIT) group in the eighth edition of the BioASQ challenge for the document and snippet retrieval tasks. Our system follows a two-stage retrieval pipeline, where a group of candidate documents is retrieved based on BM25 and reranked by a lightweight interaction-based model that uses the context of exact matches to refine the ranking. Additionally, we also show a zero-shot setup for snippet retrieval based on the architecture of our interaction based model. Our system achieved competitive results scoring at the top or close to the top for all the batches, with MAP values ranging from 33.98% to 48.42% in the document retrieval task, although being less effective on snippet retrieval.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Calling attention to passages for biomedical question answering",
        "doc_scopus_id": "85084187974",
        "doc_doi": "10.1007/978-3-030-45442-5_9",
        "doc_eid": "2-s2.0-85084187974",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "A-weighting",
            "Biomedical question answering",
            "Natural languages",
            "Network modeling",
            "Passage retrieval",
            "Question Answering",
            "Retrieval performance",
            "Retrieval systems"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Question answering can be described as retrieving relevant information for questions expressed in natural language, possibly also generating a natural language answer. This paper presents a pipeline for document and passage retrieval for biomedical question answering built around a new variant of the DeepRank network model in which the recursive layer is replaced by a self-attention layer combined with a weighting mechanism. This adaptation halves the total number of parameters and makes the network more suited for identifying the relevant passages in each document. The overall retrieval system was evaluated on the BioASQ tasks 6 and 7, achieving similar retrieval performance when compared to more complex network architectures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Understanding depression from psycholinguistic patterns in social media texts",
        "doc_scopus_id": "85084177429",
        "doc_doi": "10.1007/978-3-030-45442-5_50",
        "doc_eid": "2-s2.0-85084177429",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic analysis",
            "Mental health",
            "Mental illness",
            "Public forums",
            "Rule based",
            "Social media",
            "Standard machines",
            "World Health Organization"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.The World Health Organization reports that half of all mental illnesses begin by the age of 14. Most of these cases go undetected and untreated. The expanding use of social media has the potential to leverage the early identification of mental health diseases. As data gathered via social media are already digital, they have the ability to power up faster automatic analysis. In this article we evaluate the impact that psycholinguistic patterns can have on a standard machine learning approach for classifying depressed users based on their writings in an online public forum. We combine psycholinguistic features in a rule-based estimator and we evaluate their impact on this classification problem, along with three other standard classifiers. Our results on the Reddit Self-reported Depression Diagnosis dataset outperform some previously reported works on the same dataset. They stand for the importance of extracting psychologically motivated features when processing social media texts with the purpose of studying mental health.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancing decision-making systems with relevant patient information by leveraging clinical notes",
        "doc_scopus_id": "85083738156",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85083738156",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Clinical decision support systems",
            "Clinical treatments",
            "Decision-making systems",
            "Electronic health record systems",
            "Extracting information",
            "Patient information",
            "Patient trajectories",
            "Real time evaluation"
        ],
        "doc_abstract": "© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Hospitalised patients suffering from secondary illnesses that require daily medication typically need personalised treatment. Although clinical guidelines were designed considering those circumstances, existing decision-support features fail in assimilating detailed relevant patient information, which opens up opportunities for systems capable of performing a real-time evaluation of such data against existing knowledge and providing recommendations during clinical treatments. In this paper, we present a proposal for a new feature to integrate with electronic health record (EHR) systems that enriches the health treatment process by automatically extracting information from patient medical notes and aggregating it in clinical protocols. Our goal is to leverage the historical component of the patient trajectory to improve clinical decision support systems performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Statistical complexity analysis of turing machine tapes with fixed algorithmic complexity using the best-order markov model",
        "doc_scopus_id": "85078516231",
        "doc_doi": "10.3390/e22010105",
        "doc_eid": "2-s2.0-85078516231",
        "doc_date": "2020-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 by the authors.Sources that generate symbolic sequences with algorithmic nature may differ in statistical complexity because they create structures that follow algorithmic schemes, rather than generating symbols from a probabilistic function assuming independence. In the case of Turing machines, this means that machines with the same algorithmic complexity can create tapes with different statistical complexity. In this paper, we use a compression-based approach to measure global and local statistical complexity of specific Turing machine tapes with the same number of states and alphabet. Both measures are estimated using the best-order Markov model. For the global measure, we use the Normalized Compression (NC), while, for the local measures, we define and use normal and dynamic complexity profiles to quantify and localize lower and higher regions of statistical complexity. We assessed the validity of our methodology on synthetic and real genomic data showing that it is tolerant to increasing rates of editions and block permutations. Regarding the analysis of the tapes, we localize patterns of higher statistical complexity in two regions, for a different number of machine states. We show that these patterns are generated by a decrease of the tape's amplitude, given the setting of small rule cycles. Additionally, we performed a comparison with a measure that uses both algorithmic and statistical approaches (BDM) for analysis of the tapes. Naturally, BDM is efficient given the algorithmic nature of the tapes. However, for a higher number of states, BDM is progressively approximated by our methodology. Finally, we provide a simple algorithm to increase the statistical complexity of a Turing machine tape while retaining the same algorithmic complexity. We supply a publicly available implementation of the algorithm in C++ language under the GPLv3 license. All results can be reproduced in full with scripts provided at the repository.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Overview of the BioCreative VI Precision Medicine Track: Mining protein interactions and mutations for precision medicine",
        "doc_scopus_id": "85060617421",
        "doc_doi": "10.1093/database/bay147",
        "doc_eid": "2-s2.0-85060617421",
        "doc_date": "2019-01-28",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Computational Biology",
            "Data Mining",
            "Databases, Protein",
            "Humans",
            "Mutation",
            "Precision Medicine",
            "Protein Interaction Mapping",
            "Protein Interaction Maps",
            "Software"
        ],
        "doc_abstract": "© Oxford University Press 2019.The Precision Medicine Initiative is a multicenter effort aiming at formulating personalized treatments leveraging on individual patient data (clinical, genome sequence and functional genomic data) together with the information in large knowledge bases (KBs) that integrate genome annotation, disease association studies, electronic health records and other data types. The biomedical literature provides a rich foundation for populating these KBs, reporting genetic and molecular interactions that provide the scaffold for the cellular regulatory systems and detailing the influence of genetic variants in these interactions. The goal of BioCreative VI Precision Medicine Track was to extract this particular type of information and was organized in two tasks: (i) document triage task, focused on identifying scientific literature containing experimentally verified protein- protein interactions (PPIs) affected by genetic mutations and (ii) relation extraction task, focused on extracting the affected interactions (protein pairs). To assist system developers and task participants, a large-scale corpus of PubMed documents was manually annotated for this task. Ten teams worldwide contributed 22 distinct textmining models for the document triage task, and six teams worldwide contributed 14 different text-mining systems for the relation extraction task. When comparing the textmining system predictions with human annotations, for the triage task, the best F-score was 69.06%, the best precision was 62.89%, the best recall was 98.0% and the best average precision was 72.5%. For the relation extraction task, when taking homologous genes into account, the best F-score was 37.73%, the best precision was 46.5% and the best recall was 54.1%. Submitted systems explored a wide range of methods, from traditional rule-based, statistical and machine learning systems to state-of-the-art deep learning methods. Given the level of participation and the individual team results we find the precision medicine track to be successful in engaging the text-mining research community. In the meantime, the track produced a manually annotated corpus of 5509 PubMed documents developed by BioGRID curators and relevant for precision medicine. The data set is freely available to the community, and the specific interactions have been integrated into the BioGRID data set. In addition, this challenge provided the first results of automatically identifying PubMed articles that describe PPI affected by mutations, as well as extracting the affected relations from those articles. Still, much progress is needed for computer-assisted precision medicine text mining to become mainstream. Future work should focus on addressing the remaining technical challenges and incorporating the practical benefits of text-mining tools into real-world precision medicine informationrelated curation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extraction of chemical-protein interactions from the literature using neural networks and narrow instance representation",
        "doc_scopus_id": "85073496600",
        "doc_doi": "10.1093/database/baz095",
        "doc_eid": "2-s2.0-85073496600",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Databases, Bibliographic",
            "Deep Learning",
            "Protein Interaction Maps",
            "Proteins"
        ],
        "doc_abstract": "© 2019 The Author(s) 2019. Published by Oxford University Press.The scientific literature contains large amounts of information on genes, proteins, chemicals and their interactions. Extraction and integration of this information in curated knowledge bases help researchers support their experimental results, leading to new hypotheses and discoveries. This is especially relevant for precision medicine, which aims to understand the individual variability across patient groups in order to select the most appropriate treatments. Methods for improved retrieval and automatic relation extraction from biomedical literature are therefore required for collecting structured information from the growing number of published works. In this paper, we follow a deep learning approach for extracting mentions of chemical-protein interactions from biomedical articles, based on various enhancements over our participation in the BioCreative VI CHEMPROT task. A significant aspect of our best method is the use of a simple deep learning model together with a very narrow representation of the relation instances, using only up to 10 words from the shortest dependency path and the respective dependency edges. Bidirectional long short-term memory recurrent networks or convolutional neural networks are used to build the deep learning models. We report the results of several experiments and show that our best model is competitive with more complex sentence representations or network structures, achieving an F1-score of 0.6306 on the test set. The source code of our work, along with detailed statistics, is publicly available.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rule-based and machine learning hybrid system for patient cohort selection",
        "doc_scopus_id": "85064711607",
        "doc_doi": "10.5220/0007349300590067",
        "doc_eid": "2-s2.0-85064711607",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Electronic health record",
            "Hybrid approach",
            "Medical researchers",
            "NAtural language processing",
            "Patient Cohort Selection",
            "Rule based",
            "Selection criteria",
            "Time-consuming tasks"
        ],
        "doc_abstract": "© 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Clinical trials play a critical role in medical studies. However, identifying and selecting cohorts for such trials can be a troublesome task since patients must match a set of complex pre-determined criteria. Patient selection requires a manual analysis of clinical narratives in patients' records, which is a time-consuming task for medical researchers. In this work, natural language processing (NLP) techniques were used to perform automatic patient cohort selection. The approach herein presented was developed and tested on the 2018 n2c2 Track 1 Shared-Task dataset where each patient record is annotated with 13 selection criteria. The resulting hybrid approach is based on heuristics and machine learning and attained a micro-average and macro-average F1-score of 0.8844 and 0.7271, respectively, in the n2c2 test set. Part of the source code resultant from this work is available at https://github.com/ruiantunes/2018-n2c2-track-1/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Configurable web-services for biomedical document annotation",
        "doc_scopus_id": "85058990846",
        "doc_doi": "10.1186/s13321-018-0317-4",
        "doc_eid": "2-s2.0-85058990846",
        "doc_date": "2018-12-21",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Physical and Theoretical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1606"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 The Author(s).The need to efficiently find and extract information from the continuously growing biomedical literature has led to the development of various annotation tools aimed at identifying mentions of entities and relations. Many of these tools have been integrated in user-friendly applications facilitating their use by non-expert text miners and database curators. In this paper we describe the latest version of Neji, a web-services ready text processing and annotation framework. The modular and flexible architecture facilitates adaptation to different annotation requirements, while the built-in web services allow its integration in external tools and text mining pipelines. The evaluation of the web annotation server on the technical interoperability and performance of annotation servers track of BioCreative V.5 further illustrates the flexibility and applicability of this framework.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SCREEN-DR: Collaborative platform for diabetic retinopathy",
        "doc_scopus_id": "85055449567",
        "doc_doi": "10.1016/j.ijmedinf.2018.10.005",
        "doc_eid": "2-s2.0-85055449567",
        "doc_date": "2018-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Artificial intelligence algorithms",
            "Classification models",
            "Collaborative platform",
            "Computer aided methods",
            "Diabetic retinopathy",
            "Diabetic retinopathy screening",
            "Production environments",
            "Role-based Access Control",
            "Algorithms",
            "Artificial Intelligence",
            "Diabetic Retinopathy",
            "Humans",
            "Image Interpretation, Computer-Assisted",
            "Machine Learning",
            "Mass Screening",
            "Software"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.Background and objective: Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss. Screening programs, based on retinal imaging techniques, are fundamental to detect the disease since the initial stages are asymptomatic. Most of these examinations reflect negative cases and many have poor image quality, representing an important inefficiency factor. The SCREEN-DR project aims to tackle this limitation, by researching and developing computer-aided methods for diabetic retinopathy detection. This article presents a multidisciplinary collaborative platform that was created to meet the needs of physicians and researchers, aiming at the creation of machine learning algorithms to facilitate the screening process. Methods: Our proposal is a collaborative platform for textual and visual annotation of image datasets. The architecture and layout were optimized for annotating DR images by gathering feedback from several physicians during the design and conceptualization of the platform. It allows the aggregation and indexing of imagiology studies from diverse sources, and supports the creation and annotation of phenotype-specific datasets to feed artificial intelligence algorithms. The platform makes use of an anonymization pipeline and role-based access control for securing personal data. Results: The SCREEN-DR platform has been deployed in the production environment of the SCREEN-DR project at http://demo.dicoogle.com/screen-dr, and the source code of the project is publicly available. We provide a description of the platform's interface and use cases it supports. At the time of publication, four physicians have created a total of 1826 annotations for 701 distinct images, and the annotated data has been used for training classification models.",
        "available": true,
        "clean_text": "serial JL 271161 291210 291773 291870 291901 291919 31 International Journal of Medical Informatics INTERNATIONALJOURNALMEDICALINFORMATICS 2018-10-18 2018-10-18 2018-10-26 2018-10-26 2018-11-05T07:54:53 S1386-5056(18)30483-0 S1386505618304830 10.1016/j.ijmedinf.2018.10.005 S300 S300.1 FULL-TEXT 2018-12-15T18:47:24.199866Z 0 0 20181201 20181231 2018 2018-10-18T19:21:25.150617Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor highlightsabst primabst ref specialabst 1386-5056 13865056 true 120 120 C Volume 120 15 137 146 137 146 201812 December 2018 2018-12-01 2018-12-31 2018 Regular Papers article fla © 2018 Elsevier B.V. All rights reserved. SCREENDRCOLLABORATIVEPLATFORMFORDIABETICRETINOPATHY PEDROSA M 1 Introduction 2 Background 2.1 Digital medical imaging laboratories 2.2 DR screening platforms 3 Methods 3.1 Platform requirements 3.2 Architecture 3.3 RTS 3.4 Annotation service 3.5 Search, transfer and dataset creation 3.6 Upload and anonymization 3.7 Access control 3.8 User interface features 3.8.1 Searching 3.8.2 Dataset management 3.8.3 The upload tab 3.8.4 Textual annotation 3.8.5 Lesion annotation 3.8.6 PACScenter integration 4 Results 5 Discussion 6 Conclusions Author's contribution Acknowledgements References 2008 DIABETICRETINOPATHY WORLDHEALTHORGANIZATION 2010 GLOBALDATAVISUALIMPAIRMENTS2010TECHREP YAU 2012 556 564 J PRENTASIC 2016 281 292 P HUANG 2009 H PACSIMAGINGINFORMATICS DREYER 2006 K PACSAGUIDEDIGITALREVOLUTION HU 2009 788 794 J JORRITSMA 2014 27 36 W PIANYKH 2012 O DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOM VALENTE 2013 e61888 F COSTA 2009 71 77 C GODINHO 2016 367 375 T RIBEIRO 2014 1404 1412 L VALENTE 2012 356 364 F MOORE 1999 245 254 M BASTIAOSILVA 2018 33 42 L HAMDI 2014 100 112 O CHIU 2006 369 383 Y SUZUKI 2013 772 783 K ACHARYA 2012 2011 2020 U ROYCHOWDHURY 2014 1717 1728 S EGE 2000 165 175 B KAUPPI 2013 368514 T TRUCCO 2013 3546 E GULSHAN 2016 2402 V RAHIM 2015 69 79 S ENGINEERINGAPPLICATIONSNEURALNETWORKS AUTOMATICDETECTIONMICROANEURYSMSFORDIABETICRETINOPATHYSCREENINGUSINGFUZZYIMAGEPROCESSING RUBIN 2008 626 630 D PAPADOPOULOS 2017 D EXTREMECLICKINGFOREFFICIENTOBJECTANNOTATIONVOL8 BINNS 2018 377 R VALENTE 2016 284 296 F KULKARNI 2011 402 417 P PEDROSA 2018 330 337 M HURSCH 1995 W SEPARATIONCONCERNS SILVA 2018 81 90 J MONTEIRO 2017 89 E NEMA 2017 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMSUPPLEMENT163STOREOVERWEBBYREPRESENTATIONSSTATETRANSFERRESTSERVICESSTOWRS YAN 2012 1350 1357 Y ACTIVELEARNINGMULTIPLEKNOWLEDGESOURCES GRISAN 2008 310 319 E TRIBUNA 2017 672 680 L MEYER 2017 507 515 M ADEEPNEURALNETWORKFORVESSELSEGMENTATIONSCANNINGLASEROPHTHALMOSCOPYIMAGESVOL7 COSTA 2017 516 523 P ADVERSARIALSYNTHESISRETINALIMAGESVESSELTREESVOL7 REMESEIRO 2017 4520 4527 B ARAUJO 2017 101341K T INTERNATIONALSOCIETYFOROPTICSPHOTONICSVOL3 ESTIMATIONRETINALVESSELCALIBERUSINGMODELFITTINGRANDOMFORESTS MENDONCA 2017 101341L A INTERNATIONALSOCIETYFOROPTICSPHOTONICSVOL3 AUTOMATICSEMIAUTOMATICAPPROACHESFORARTERIOLARTOVENULARCOMPUTATIONINRETINALPHOTOGRAPHS COSTA 2017 10 P PEDROSAX2018X137 PEDROSAX2018X137X146 PEDROSAX2018X137XM PEDROSAX2018X137X146XM 2019-10-26T00:00:00.000Z 2019-10-26T00:00:00.000Z © 2018 Elsevier B.V. All rights reserved. item S1386-5056(18)30483-0 S1386505618304830 10.1016/j.ijmedinf.2018.10.005 271161 2018-12-15T18:47:24.199866Z 2018-12-01 2018-12-31 true 1567951 MAIN 10 53299 849 656 IMAGE-WEB-PDF 1 ga1 true 7188 164 165 gr1 4100 164 153 gr2 3847 164 114 gr3 7502 133 219 gr4 7117 164 164 gr5 7096 164 213 gr6 4974 164 186 gr7 5158 84 219 gr8 12154 164 145 gr9 10958 110 219 ga1 true 6661 200 201 gr1 14500 363 339 gr2 24087 487 339 gr3 35059 342 565 gr4 23080 339 339 gr5 18819 261 339 gr6 12037 298 339 gr7 33474 217 565 gr8 67036 637 565 gr9 40820 339 678 ga1 true 57304 886 892 gr1 110496 1604 1500 gr2 197797 2153 1500 gr3 338052 1514 2500 gr4 188171 1500 1500 gr5 142752 1153 1500 gr6 90935 1319 1500 gr7 286527 958 2500 gr8 767468 2819 2500 gr9 379866 1500 3000 am 3343358 IJB 3763 S1386-5056(18)30483-0 10.1016/j.ijmedinf.2018.10.005 Elsevier B.V. Fig. 1 Use-cases that the SCREEN-DR platform must contemplate. Fig. 1 Fig. 2 SCREEN-DR architecture. Fig. 2 Fig. 3 R-PACS semi-structured data model, comprising the DICOM model and the annotation extensions. Fig. 3 Fig. 4 RTS overall architecture. Fig. 4 Fig. 5 Textual annotation pipeline for the SCREEN-DR platform. Fig. 5 Fig. 6 SCREEN-DR authentication process. Fig. 6 Fig. 7 View of the SCREEN-DR platform search tab. Fig. 7 Fig. 8 Views of the SCREEN-DR platform annotation and lesion tabs. Annotation and contrast tools are shown at the top left. Fig. 8 Fig. 9 View of the SCREEN-DR platform PACSCenter tab. Fig. 9 Table 1 Number of annotations performed in each stage. Table 1 Annotated Image quality 1826 Bad 116 Partial 685 Good 1025 Diagnosis 1710 Table 2 The average time of an image evaluation on the quality stage, discriminated per readability. Table 2 Quality Time Actions TpA Bad 13.8 2.6 5.4 Partial 18.4 6.4 2.9 Good 15.7 6.2 2.5 Table 3 The average time taken to diagnose an image, discriminated per characteristic. Table 3 Quality &diagnosis Time Actions TpA R0 auto M0,P0 16.4 4.3 3.8 Retinopathy R0 22.5 4.6 4.8 R1 32.3 5.4 6.0 R2-M 32.3 6.8 4.8 R2-S 42.6 7.2 5.9 R3 67.0 7.7 8.7 RX 49.2 8.6 5.7 Maculopathy M0 41.6 6.3 6.6 M1 34.5 6.7 5.1 MX 36.2 6.3 5.8 Photocoagulation P0 32.7 6.6 5.0 P1 62.2 8.6 7.2 P2 46.9 7.0 6.7 PX 33.4 6.7 5.0 SCREEN-DR: Collaborative platform for diabetic retinopathy Micael Pedrosa Jorge Miguel Silva ⁎ João Figueira Silva Sérgio Matos Carlos Costa DETI/IEETA, University of Aveiro, Portugal DETI/IEETA, University of Aveiro Portugal ⁎ Corresponding author. Graphical abstract Background and objective Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss. Screening programs, based on retinal imaging techniques, are fundamental to detect the disease since the initial stages are asymptomatic. Most of these examinations reflect negative cases and many have poor image quality, representing an important inefficiency factor. The SCREEN-DR project aims to tackle this limitation, by researching and developing computer-aided methods for diabetic retinopathy detection. This article presents a multidisciplinary collaborative platform that was created to meet the needs of physicians and researchers, aiming at the creation of machine learning algorithms to facilitate the screening process. Methods Our proposal is a collaborative platform for textual and visual annotation of image datasets. The architecture and layout were optimized for annotating DR images by gathering feedback from several physicians during the design and conceptualization of the platform. It allows the aggregation and indexing of imagiology studies from diverse sources, and supports the creation and annotation of phenotype-specific datasets to feed artificial intelligence algorithms. The platform makes use of an anonymization pipeline and role-based access control for securing personal data. Results The SCREEN-DR platform has been deployed in the production environment of the SCREEN-DR project at and the source code of the project is publicly available. We provide a description of the platform's interface and use cases it supports. At the time of publication, four physicians have created a total of 1826 annotations for 701 distinct images, and the annotated data has been used for training classification models. Keywords Diabetic retinopathy screening Collaborative PACS Telemedicine Computer-aided diagnosis Image annotation 1 Introduction Diabetic retinopathy (DR) is the most prevalent microvascular complication of diabetes mellitus and can lead to irreversible visual loss [1], being a leading cause of blindness worldwide. According to the World Health Organization (WHO), 1% of all blindness cases in 2010 were attributed to this disease [2]. Moreover, it has a significant economic impact, particularly in industrialized countries. The disease has distinct progression phases, and an accurate diagnosis of the disease stage is required for a prompt and appropriate treatment to prevent visual loss. However, the disease is asymptomatic during early stages which makes its detection difficult. Therefore, many communities promote regular screening programs in the diabetic population to ensure a timely treatment [1,3]. Following these guidelines, the Portuguese North Health Administration (ARSN 1 1 ) is implementing a mass DR screening to reach around 75% of a population of 250,000 diabetic patients in the north of Portugal. The screening process consists of taking pictures of the patients’ eye fundus using mobile retinographers, which are sent to a centralized repository and posteriorly analyzed by experts. Healthy cases and examinations with bad image quality are discarded from the next screening stages. Cases identified with relevant DR are forwarded for complementary evaluation and treatment in a center of reference. Other pathologies are not considered in this screening process and the images will follow the current clinical protocol. In such mass screenings, diagnosis is a daunting task due to the large number of cases that must be analyzed. Moreover, in about 75% of cases, the patient does not show any DR manifestations, which means that experts spend the majority of time viewing non-relevant cases. The SCREEN-DR project was created to address this limitation in the DR screening process. It aims to research and develop an image analysis and machine learning (ML) platform for innovation in DR screening, based on the hypothesis that computer-aided DR detection methods [4] could be used to identify pathologic cases and indicate them to ophthalmologists. The project faces three main challenges: automatically discarding low quality images, which are not usable for diagnosing DR; automatically detecting cases without This article presents the SCREEN-DR collaborative platform, created to address the project's specific requirements, which cannot be satisfied by conventional Information and Communications Technology (ICT) platforms available in radiology departments, and to promote collaboration between physicians and researchers in the scope of a regional DR screening program. Within the project, the role of the researchers is to create classification algorithms to evaluate image quality, discard cases without DR, locate possible lesions and grade DR severity, while physicians are responsible for annotating datasets, including visual delineation of lesions. The collaborative platform collects the studies, indexes image metadata, and supports the creation and annotation of datasets. Result from an advanced search mechanism, which supports multi-source queries over the annotated data, can be exported or used to create new training datasets. 2 Background 2.1 Digital medical imaging laboratories Digital medical imaging environments are grounded on the Picture Archiving and Communication System (PACS) concept, referring to the set of software and hardware units responsible for the acquisition, storage, distribution and visualization of medical imaging studies [5–7]. The core component is the storage server (i.e. the archive), which receives images from different imaging modalities, stores them in a structured way, and supports content discovery and retrieval. These digital medical imaging environments are fundamental to ensure a fully digital and distributed workflow where the stakeholders can be anywhere, including the departmental intranet or at home [8]. The interoperability between equipment and information in digital medical imaging laboratories is ensured by the Digital Imaging and Communications in Medicine (DICOM) standard [9]. It defines the reference information model, that is, how data is encoded and transmitted. Data is agglutinated in DICOM object structures containing metadata related to procedure, patient, acquisition modality and institution, besides pixel data. In a traditional PACS environment, the archive serves a single organizational domain and authorized users have access to the whole repository, making accounting mechanisms unnecessary. However, more recent usage scenarios brought new requirements such as the coexistence of distinct ownership domains in the same server instance. For instance, research and collaborative environments deployed at the Web need to support policies for controlling access to resources. In the context of the SCREEN-DR project, a researcher may have a private dataset and share part of the resources with one or more physicians. Moreover, production medical imaging repositories are often looked at as “inert bags“ of DICOM objects. However, there are already some research-oriented PACS, such as the Dicoogle open source platform [10], a new PACS archive concept with an agile indexing and retrieval mechanism [11] that can be used in DICOM data mining tasks. Following the recent trends of Internet market, healthcare institutions are outsourcing their repositories to the Cloud and providing solutions as a service [12,13]. In such context, developing and managing DICOM based applications with special communication, decoding and visualization requirements is often a problematic barrier. Zero-Footprint Viewers (ZFV) are the state-of-the-art clients that use standard browsers and plugins to provide access to medical imaging repositories [14]. The integration of those DICOM-Web tools is fundamental in the context of a modern collaborative platform that aims to support medical imaging workflows. 2.2 DR screening platforms Healthcare is ever-evolving from the initial telemedicine software for videoconferencing, telemonitoring and teleradiology [15] to the integration of computer-aided diagnosis in telemedicine solutions. In the last few years, a large number of medical research projects have emerged around the world that make use of communication systems to achieve data integration [16], facilitate the patient access to their diagnosis [17] and create community-based screening platforms [18]. Several screening programs are currently in operation, for example in the UK, 2 2 UK Screening Programs: Ireland, 3 3 Ireland Screening Programs: Netherlands, 4 4 Netherlands Screening Programs: or in several states in the U.S. as in NC. 5 5 U.S.-NC Screening Programs: There are also evidences that automatic screening should be introduced in DR screening programs [19]. Regarding the DR automatic analysis, although there are positive results in the literature, the majority of research is highly optimized for small datasets [20–22], compromising generalization and the required standards for mass screening applications [20]. Furthermore, there is a lack of comprehensive studies applying novel image processing and big data analysis techniques to fundus images. Validation of automated DR screening methods is a key issue and has been reviewed by several authors [23,24]. These works describe several publicly available datasets of different detail and complexity that can be good starting points for training image processing pipelines. An ICT platform is a major need for the management of large amounts of image data, such as the retinal images produced in a screening program. Moreover, projects with large datasets require the extensive participation of ophthalmologists in the annotation phase [25], and an online platform for managing the process. Methods such as inserting annotations in Excel files [26] do not enable the collection of ROIs nor lesion pinpointing. Plugins for image viewers such as OsiriX-iPad [27] have the ability to annotate Regions of Interest (ROI), yet those are patient-centric platforms and do not offer an optimized pipeline to annotate massive numbers of images following a blind annotation process. There are already several attempts to construct such platforms, such as CrowdFlower 6 6 or RectLabel. 7 7 Yet, in some cases, there is the need to build customized annotation protocols that are highly optimized for working with thousands of images [28]. Google has created an annotation tool [25] for DR automatic diagnosis, but it was designed for annotating a single dataset containing 128,175 macula-centered images. Furthermore, it does not allow annotating lesions, and therefore it is not possible to validate if their algorithms can identify the regions where lesions are present on the image. Moreover, the interpretability of the model is an important step in making ML techniques accepted on computer-aided diagnosis (CAD) [29]. This importance is enlarged on GDPR Article 13, where it states that the subject has the right to “meaningful information about the logic involved“. Annotations of lesions can provide the required input to improve interpretability, hence, the importance of our work. 3 Methods 3.1 Platform requirements Fig. 1 shows the main actors and use-cases that must be supported by the SCREEN-DR platform. The platform needs to provide means for the creation and management of large textually and visually annotated phenotype-specific repositories to support training distinct ML classifiers. This requires a quick and user-friendly way for physicians to perform textual annotations on a dataset, as well as the necessary tools to perform detailed and lesion-specific visual annotations on the images. The platform must also accept multiple annotations from different physicians on the same image, allowing the creation of a consensus by combining different annotators and algorithms. Machine learning researchers also require the means to create and manipulate the datasets that will feed their classification models. As such, the platform must offer a way for researchers to browse, query and filter the annotated repository, using multi-source information from both textual and visual annotations, together with the metadata provided in the DICOM files. In addition, the platform must also encompass a way for researchers to easily and intuitively export the filtered datasets. The platform must also include an authentication module with single-sign-on support to allow simplified controlled access for each specific-role actor (physician, researcher, data provider), an anonymization pipeline to ensure patient confidentiality and, finally, provide a Web based interface to ensure an online collaborative environment. 3.2 Architecture Fig. 2 presents the overall architecture of the SCREEN-DR platform. It was developed on top of the Dicoogle open source PACS 8 8 dicoogle: and follows a modular approach, exploring Dicoogle's SDK and plugin-based structure for creating new functionalities [10,30]. Specifically, Dicoogle allowed us to develop a Retinopathy-PACS (R-PACS) responsible for storing and indexing the SCREEN-DR DICOM files. All storage, indexing and retrieval capabilities are delegated to independently deployed components, enabling the support of a multitude of use cases. The R-PACS has a semi-structured model, stored in a relational database and indexed with Elasticsearch 9 9 Elasticsearch: , a distributed index and analytics engine exposing RESTful services. Its data model extends the standard DICOM Information Model (DIM), a hierarchical organization that contemplates four levels (Patient, Study, Series and Image/Instance), to include a generic annotation model capable of accommodating distinct research requirements, as shown in Fig. 3 . The semi-structured model was implemented with PostgreSQL 10 10 PostgreSQL: tables and binary JSON for the flexible annotation schema. All annotations performed by an annotator are linked to the image, the smallest unit of the DICOM model. Annotation sections are provided by node types (e.g. image quality assessment or diagnosis), with new sections being easily created in the database at any time. Annotation data is inserted in the node fields, without having to obey any rigid structure. Lastly, the dataset and pointers provide a tracking sequence for annotated images in a dataset. Despite being primarily designed for the SCREEN-DR project, R-PACS can be used through small customizations in any medical imaging context where a set of annotations is applied to DICOM images. It also has a Command Line Interface (CLI) to drop and rebuild the Elasticsearch indexes. The R-PACS plug-in is available at the github repository On top of the R-PACS plug-in, a layer of diabetic retinopathy services were implemented, responsible for the dataset manipulation and encompassing the following services: • Annotation ⇒ to perform textual and visual annotations on the dataset. It allows to fetch, create and modify annotations, based on the image reference ID parameter; • Transfer ⇒ responsible for downloading / uploading DICOM files; • Index ⇒ uses selections of previously uploaded files and indexes them in the R-PACS. It also allows indexing image annotations; • Search ⇒ uses Lucene like queries 11 11 over DICOM metadata or annotation fields to retrieve images from the index; • Dataset Management ⇒ allows the creation of datasets from selected image unique identifiers (UIDs), provides subscription methods to datasets, and allows to define the default dataset to use in the annotation protocol. DR services were developed in the JAVA language and exposed by an in-house service stack denominated Reactive Through Services (RTS). This framework, which implements the web service endpoints, fills the gap between the reactive front-end and back-end services, maintaining the straightforward way of development that REST and JSON-RPC provides. Furthermore, it offers support to new protocols and models that extend REST and JSON-RPC. The role-based access control (RBAC), a very common feature of many online applications for sensitive information [31], was addressed through an authentication and authorization mechanism using Keycloak. 12 12 Keycloak: A Single Sign-On (SSO) mechanism was also added to grant JSON Web Tokens (JWT) 13 13 for the RBAC, which were intercepted and validated in the RTS pipeline. 3.3 RTS Reactive through services is publicly available at From the client point of view, RTS can be considered an extension of REST services, since it adds the push model to the existing pull network style of REST [32]. However, its architecture is much more complex, as depicted in Fig. 4 . The architecture was designed to be compatible with asynchronous server applications and message-driven architectures such as Vertx. 14 14 Vertx:vertx.io. On the other hand, its network endpoints and protocols were detached from service endpoints, such that with slight configurations it allows the same compatible service to be accessed from a REST endpoint or from an RTS endpoint. Moreover, RTS improves separation of concerns [33], providing clean service definitions and attaching points for other protocols, as long as they are compatible with the same communication model. Requests and replies from endpoints are transformed into a message based processing scheme, which after passing through the network endpoint enter a processing pipeline. Here, it can be forwarded to the next interceptor, delivered to the service or rejected. At the end of the pipeline, message delivery depends on a decision process based on some of the message fields; for instance, the message type decides if the message is to be delivered to a request or reply service handler. These message fields reflect the used communication model, thus avoiding the need for service descriptors to build client proxy implementations. Finally, the client-side has also the Pipeline and MessageBus components, and it is possible to have the same services on the client Pipeline and invert the request/reply flux from the server to the client. However, to avoid network issues, the network connection is always made in the client-server direction. 3.4 Annotation service The annotation service is responsible for fetching, creating and modifying annotations and it does so by making use of the image reference identifier. It contemplates two types of annotations: textual, in which physicians grade images, and visual, in which physicians delineate the lesions on the image. Both annotation types can be performed by several physicians. The textual annotation is itself divided in two gradings that can be performed either simultaneously or separately: quality and diagnosis. The overall pipeline of the textual annotation is depicted in Fig. 5 . In the quality stage, the physician decides if the image quality is acceptable (good/partial) or insufficient to allow an accurate diagnosis (bad). When a image is marked as of bad quality, the diagnosis stage is blocked. Otherwise, diagnosis can be performed, with the physician grading the image in terms of degree of retinopathy, maculopathy, photocoagulation and other suspected comorbidities. Images of healthy cases have a retinopathy preset at the R0 state and maculopathy at M0. Furthermore, if the physician is unable to classify the degree of a disease, the X option can be selected. It is worth mentioning that textual annotations take place before visual annotations, therefore lesion annotation can only be performed on images containing quality and diagnosis grading. Visual annotation uses images selected from the pool of textual annotated images that contain signs of DR (retinopathy states from R1 to R3). In this annotation process, physicians delineate lesions associated with retinopathy: microaneurysms, hemorrhages, hard exudates, soft exudates and neovascularizations. The platform provides several types of shapes and colors to delineate lesions, each specific for a type of lesion. 3.5 Search, transfer and dataset creation The DICOM standard does not possess sufficient flexibility to provide advanced search mechanisms to fulfill the SCREEN-DR requirements. As such, we incorporated search capabilities that allow researchers to access multiple sources in the archive, including DICOM metadata, image features and retinopathy annotations (e.g. image quality and pathology). The proposed search mechanism allows us to search over any attribute contained in R-PACS DICOM images metadata, as well as to combine attributes from distinct data sources, including labeled data from annotated datasets. This search service mechanism uses Lucene like queries 15 15 that make use of DICOM or annotation fields to retrieve the images from the index. Query results can be downloaded, including images and the associated annotations, or used to create new datasets. When downloading, a zip file is generated, containing the raw DICOM files and their respective annotations. The dataset service uses the unique identifiers (UIDs) of the selected images to create the datasets. This service also provides subscription methods to datasets, as well as the selection of the default dataset to be used in the annotation protocol. 3.6 Upload and anonymization The platform can receive and export retinopathy images and associated data using the DICOM standard interface or REST services. To address the upload of medical imaging studies, an upload module was created and integrated in the platform. This module allows users to upload examinations by dragging study folders or image files to the upload area. Compressed zip files containing several studies can also be uploaded. After validating the DICOM structure, they are indexed and stored in the R-PACS. The uploaded data may contain images with Protected Health Information (PHI) burned in the pixel data. Moreover, sensitive information is also included in DICOM headers [7]. To anonymize this information, uploaded studies are processed by a dedicated anonymization engine described in [34]. This pipeline is composed of two main parts: a visual anonymizer for removal of burned PHI, consisting of an improved version of the visual anonymizer described in [35], and a reversible de-identifier of medical imaging data that retains search capabilities over the original DICOM data index, used to anonymize sensitive data in DICOM headers. The generated de-identified patient reference is stored in the name field of the patient model (Fig. 3). The original name of the patient is never user. 3.7 Access control As expressed, SCREEN-DR is a collaborative platform that requires a role-based access control mechanism to support three main actors (Fig. 6 ): physicians, researchers and data providers. To satisfy this fundamental requirement, an authentication module was created with SSO support. It was integrated with Keycloak, an open source identity and access management solution aimed at modern applications and services. Instead of using a customized solution, users perform authentication with Keycloak, meaning the SCREEN-DR platform does not have to deal with login forms, nor with authenticating and storing users. Depending on the user, different endpoints of the platform are accessible. Physicians have access to the textual and visual annotation endpoints of the application, as well as to the PACS viewer endpoint. Researchers have access to the search and dataset creation endpoints. Finally, data providers have access to the platform's file upload endpoint. OpenID Connect protocol was used for the authentication process, as it will allow seamless integration with other external applications. Once logged-in to Keycloak, users do not have to login again to access a different application. Keycloak will also let users use credentials from other identity providers (e.g. Google) to access the SCREEN-DR platform, without further development. 3.8 User interface features The interface code and web services of the platform are publicly available at The platform graphical interface has six functional areas, accessible in the top menu of the main page, in accordance with the use cases presented in figure 1 : Search ⇒ typically used by researchers to query the repository, download files and create and manage datasets; Dataset Manager ⇒ exclusively used by researchers to associate physicians to datasets; Upload ⇒ mainly used by data providers to upload studies; Annotation ⇒ exclusively used by physicians to perform textual annotations on a given dataset; Lesion ⇒ exclusively used by physicians to perform visual annotations on a given dataset; Viewer ⇒ allows an integrated visualization of all images belonging to a study of a specific patient. 3.8.1 Searching The search area allows users to perform queries over the R-PACS repository. Fig. 7 presents an example of a query result. The query language is case insensitive, uses either free text or simple boolean logic, and supports queries over DICOM metadata, annotations or both. The designed query language has four field types: • Text ⇒ free text field • Numeric ⇒ natural numbers • Date ⇒ dates, with format YYYY-MM-DD • Option ⇒ a set of pre-defined text options Predicates can be enclosed in parentheses and four boolean operators are supported: • OR ⇒ joins 2 clauses indicating that the presence of one condition is a valid result; • AND ⇒ joins 2 clauses indicating that the presence of both conditions is required; • + ⇒ applied to a single clause indicating that this term must be present; • - ⇒ applied to a single clause indicating that this term must not be present. The following range operators can be specified for date, numeric or string fields: • TO ⇒ defines a range interval, ex: BirthDate:[1936-07-22 TO 1936-07-24] • <, >, <=, >= ⇒ defines range limits, ex: Columns:>3000 Finally, it is also possible to perform wildcard searches on individual terms, using the symbol ? to replace a single character, and * to replace zero or more characters. The returned results are image centric, the smallest entity in DIM. Each result includes a thumbnail for preview reasons, as well as information regarding the patient, station and annotations performed on the image. These results can be selected and used to generate a dataset or simply downloaded in a zip file format. 3.8.2 Dataset management The dataset management area allows the creation and assignment of multiple datasets to distinct users (e.g. physician). All images available in the R-PACS can be used to create a dataset in accordance with distinct selection criteria (patient gender, equipment, study date, etc). The search module can be used to select images from the R-PACS, allowing inclusively the use of another dataset already annotated as a query data source. For instance, the dataset for visual annotation of lesions is usually created as a sub-dataset of the first stage datasets, according to labeled data criteria. 3.8.3 The upload tab The upload tab is a simple frame that allows manual drag and drop of studies to the R-PACS. Uploaded files are stored and indexed using the Transfer service. Nevertheless, the platform also provides DICOM standard services, including the recent DICOM STOW-RS [36] that can be used by third-party systems to feed the platform with medical imaging studies. In both import options, cases are automatically anonymized when received. 3.8.4 Textual annotation Fig. 8a shows the annotation form. Here, physicians can grade image quality and diagnosis in accordance with the established protocol (Fig. 5). The two levels of annotations can be performed by the physician together or separately. All images are allocated to the physician's dataset through a blind process, so that physicians have no knowledge of which patients the images belong to. To help physicians with the grading process, the toolset provides a magnifier, a zoom and a contrast tool that allows them to see the image with more detail. Furthermore, there is a navigation bar allowing physicians to browse through the annotation dataset and correct possible grading mistakes as well as knowing how many annotations are required from them. 3.8.5 Lesion annotation The legion annotation page, depicted in Fig. 8b, is where physicians can delineate the lesions defined in the annotation protocol. It is worth mentioning that all images used to create the lesion annotation dataset are a subset of images that belong to a textually annotated dataset, from which only images with DR were selected. Besides having tools for lesion specific delineation, it also has tools for deleting and for moving markings around the image. To help with lesion delineation, physicians can zoom in and out on the image or use the magnifier tool. 3.8.6 PACScenter integration The SCREEN-DR platform was integrated with PACScenter 16 16 PACScenter: to satisfy the need to analyze all cases from a specific patient study (Fig. 9 ). BMD Software, 17 17 BMD: a project partner, provided and supported the integration of this commercial solution within the SCREEN-DR platform. PACScenter is a web based DICOM viewer entirely developed with web technologies, namely HTML5 and JavaScript, being accessible through a common web browser. A demo version is available at This multi-platform solution was designed to integrate with any external DICOM compliant archive, R-PACS in the context of SCREEN-DR. It allows accessing, downloading, visualizing, reviewing, reporting, and printing medical multi-modality image data in DICOM format. PACScenter shares the repository with the SCREEN-DR platform through Dicoogle, thus it is possible to search, view and manipulate retinopathy studies using a validated set of tools. While the visualization for annotation purposes is patient blind and image centric, the navigation through PACScenter is patient centric, being possible to see all studies from a patient and all images from a study, making it an important solution to handle cases where all images of a study are required in order to perform a correct diagnosis. 4 Results The first stable version of SCREEN-DR platform is accessible through the following web address The system is already in production and currently stores 1655 studies imported from the ARSN repository. The annotation process started in November 2017, and the platform currently has 4 physician annotators that have performed textual annotations on the data (Table 1 ) and will continue to do so as well as perform lesion annotations until June 2019. Currently, images have been annotated in terms of image quality and diagnosis. Furthermore, there are 4 researchers using the platform to navigate through annotated data and retrieve data together with the corresponding annotations to feed their machine learning algorithms. Annotations were performed on 701 distinct images with around 450 images per annotator, reflecting some degree of overlapping. Having multiple annotations on the same image may be relevant for machine learning techniques using multiple knowledge sources [37], and also for establishing a discrepancy baseline between different results. The software gathers the actions and timestamps for each annotated image. From these we compiled a quantitative analysis of the time taken on each stage. The summaries are presented in Tables 2 and 3 for image quality and diagnosis. Time refers to the average time (in seconds) taken to annotate an image when a certain characteristic is present, and Actions is the average number of actions such as clicking buttons, zooming, panning and saving. TpA is the average time-per-action (in seconds per action) calculated by the formula Time/Actions. At first glance, the T values seemed too large for a medical decision about the image. However, detailed analysis on the data showed that some physicians were about 2 times faster than others, influencing the average and reducing the T and TpA significance. This also indicates that A is probably the best measurement of effort. On the quality stage, the results show a minor effort when identifying a bad quality image, since this requires less actions, usually just a select and save. It is also expected that an image with partial readability would take more time to analyze than a good one. Since physicians prefer to perform quality and diagnosis at the same time, the diagnosis stage includes the quality grading process. Results show that the number of actions required to grade retinopathy level increases with the severity level. This is expected since a more careful analysis is required on more difficult cases. Also, the R0 and M0,P0 row concerning the auto-selection of M0 and P0 when R0 is selected, has lower values, suggesting that the feature is indeed useful in accelerating the diagnosis process. The minimum number of actions to complete the diagnose without the auto-selection feature is 6. Since the highest average is 8.6 with the possibility of inserting “suspected comorbidities“, this is in general in accordance with the expected results. 5 Discussion Regarding possible limitations, our system is optimized for annotating images, with the UI layout having been designed to present one image at a time with a set of tools for the annotations. However, adding spatial or time relations between images is not part of this research scope and can be treated as a limitation. Methods for automated annotations, such as for retinal vessel tortuosity [38], were not considered because we intended to obtain the physician's exact delineations rather than the results of automated tools. Moreover, it was our decision to export annotations in JSON format instead of DICOM-SR [39]. Despite not being DICOM compliant, JSON is an ubiquitous format from a developer perspective and ML experts do not always know the DICOM-SR format. Qualitative analyses of the software usability were not performed since the annotation module will not be used in a production environment. The specification and development was supervised by end-users (physicians and researchers) and should reflect the requirements imposed by them. The available annotation user interfaces and tools meet these requirements. In this work, bad quality images are discarded since these are not useful for training the machine learning algorithms considered in the project. The identification of reasons for bad quality, such as bad alignment, over exposure or other, could be of interest for training photographers, for example, but this aim is outside the scope of the current work and was left as possible future development. 6 Conclusions This article presents the collaborative platform of the SCREEN-DR project that promotes collaboration between physicians and researchers in the scope of a regional DR screening program. The role of researchers is to create classification algorithms to evaluate image quality, discard non-pathological cases, locate possible lesions and grade DR severity. Regarding that, significant output has already been made in the ML domain under the scope of the SCREEN-DR project [40–45]. Physicians are responsible for the annotation of datasets, including visual delineation of lesions. The collaborative platform collects the studies, indexes the images metadata, and manages the creation of datasets and the respective annotation process. An advanced searching mechanism supports multi-source queries over annotated datasets and exporting of results for feeding artificial intelligence algorithms. The described scenario could not be satisfied by traditional ICT platforms currently used in radiology departments and, as such, a novel modular architecture was designed that sits on top of an open source framework. The platform core module is the R-PACS repository that stores uploaded files, following a DICOM standard data model. Moreover, it was extended with a complementary indexing engine for supporting information retrieval over image metadata and physician annotations. A layer of web services was developed for dataset manipulation, which is exposed through endpoints using a dedicated architecture denominated as RTS. Concerning platform security, an intelligent anonymization framework was developed, and a role-based access control with SSO support was integrated. The system is already in production and currently stores 1655 studies imported from a repository of a regional public health entity. Author's contribution All authors participated in solution development, results analysis and drafting of all sections of the manuscript. Micael Pedrosa and Jorge Silva were the main developers of the created solution. Sergio Matos and Carlos Costa were the main responsibles for the project supervision. All authors have read and agreed to the paper being submitted as it is. Acknowledgements This work is financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalization – COMPETE 2020 Programme, and by National Funds through the FCT Fundação para a Ciência e a Tecnologia (Portuguese Foundation for Science and Technology) within project CMUP-ERI/TIC/0028/2014. Sérgio Matos is supported by a FCT Investigator grant. References [1] E.J. Duh Diabetic Retinopathy 2008 Humana Press Totowa, NJ 10.1007/978-1-59745-563-3 [Online] [2] World Health Organization Global Data on Visual Impairments 2010, Tech. Rep. 2010 [Online]. Available: [3] J.W.Y. Yau S.L. Rogers R. Kawasaki E.L. Lamoureux J.W. Kowalski T. Bek S.-J. Chen J.M. Dekker A. Fletcher J. Grauslund S. Haffner R.F. Hamman M.K. Ikram T. Kayama B.E.K. Klein R. Klein S. Krishnaiah K. Mayurasakorn J.P. O’Hare T.J. Orchard M. Porta M. Rema M.S. Roy T. Sharma J. Shaw H. Taylor J.M. Tielsch R. Varma J.J. Wang N. Wang S. West L. Xu M. Yasuda X. Zhang P. Mitchell T.Y. Wong Meta-Analysis for Eye Disease (META-EYE) Study Group Global prevalence and major risk factors of diabetic retinopathy Diabetes Care 35 3 2012 556 564 [Online]. Available: [4] P. Prentašić S. Lončarić Detection of exudates in fundus photographs using deep neural networks and anatomical landmark detection fusion Comput. Methods Prog. Biomed. 137 2016 281 292 [Online]. Available: [5] H.K. Huang PACS and Imaging Informatics 2009 John Wiley & Sons, Inc. Hoboken, NJ, USA 10.1002/9780470560525 [Online] [6] K.J. Dreyer PACS: A Guide to the Digital Revolution 2006 Springer [7] J. Hu F. Han A pixel-based scrambling scheme for digital medical images protection J. Netw. Comput. Appl. 32 4 2009 788 794 [Online]. Available: [8] W. Jorritsma F. Cnossen P.M. van Ooijen Merits of usability testing for PACS selection Int. J. Med. Inf. 83 1 2014 27 36 [Online]. Available: [9] O.S. Pianykh Digital Imaging and Communications in Medicine (DICOM) 2012 Springer Berlin Heidelberg Berlin, Heidelberg 10.1007/978-3-642-10850-1 [Online] [10] F. Valente C. Costa A. Silva Dicoogle, a PACS featuring profiled content based image retrieval PLoS ONE 8 5 2013 e61888 [Online]. Available: [11] C. Costa F. Freitas M. Pereira A. Silva J.L. Oliveira Indexing and retrieving DICOM data in disperse and unstructured archives Int. J. Comput. Assist. Radiol. Surg. 4 1 2009 71 77 10.1007/s11548-008-0269-7 [Online] [12] T.M. Godinho C. Viana-Ferreira L.A. Bastiao Silva C. Costa A routing mechanism for cloud outsourcing of medical imaging repositories IEEE J. Biomed. Health Inf. 20 1 2016 367 375 [Online]. Available: [13] L.S. Ribeiro C. Viana-Ferreira J.L. Oliveira C. Costa XDS-I outsourcing proxy: ensuring confidentiality while preserving interoperability IEEE J. Biomed. Health Inf. 18 4 2014 1404 1412 [Online]. Available: [14] F. Valente C. Viana-Ferreira C. Costa J.L. Oliveira A RESTful image gateway for multiple medical image repositories IEEE Trans. Inf. Technol. Biomed. 16 3 2012 356 364 [Online]. Available: [15] M. Moore The evolution of telemedicine Future Gener. Comput. Syst. 15 2 1999 245 254 [Online]. Available: [16] L. Bastião Silva A. Trifan J. Luís Oliveira MONTRA: An agile architecture for data publishing and discovery Comput. Methods Prog. Biomed. 160 2018 33 42 [Online]. Available: [17] O. Hamdi M.A. Chalouf D. Ouattara F. Krief eHealth: Survey on research projects, comparative study of telemonitoring architectures and main issues J. Netw. Comput. Appl. 46 2014 100 112 [Online]. Available: [18] Y.-H. Chiu L.-S. Chen C.-C. Chan D.-M. Liou S.-C. Wu H.-S. Kuo H.-J. Chang T.H.-H. Chen Health information system for community-based multiple screening in Keelung, Taiwan (Keelung Community-based Integrated Screening No. 3) Int. J. Med. Inf. 75 5 2006 369 383 [Online]. Available: [19] K. Suzuki Machine learning in computer-aided diagnosis of the thorax and colon in CT: a survey IEICE Tran. Inf. Syst., vol. E96-D 4 2013 772 783 [Online]. Available: [20] U.R. Acharya E.Y.K. Ng J.-H. Tan S.V. Sree K.-H. Ng An integrated index for the identification of diabetic retinopathy stages using texture parameters J. Med. Syst. 36 3 2012 2011 2020 10.1007/s10916-011-9663-8 [Online]. Available: [21] S. Roychowdhury D.D. Koozekanani K.K. Parhi DREAM: diabetic retinopathy analysis using machine learning IEEE J. Biomed. Health Inf. 18 5 2014 1717 1728 [Online]. Available: [22] B.M. Ege O.K. Hejlesen O.V. Larsen K. Møller B. Jennings D. Kerr D.A. Cavan Screening for diabetic retinopathy using computer based image analysis and statistical classification Comput. Methods Prog. Biomed. 62 3 2000 165 175 [Online]. Available: [23] T. Kauppi J.-K. Kämärä inen L. Lensu V. Kalesnykiene I. Sorri H. Uusitalo H. Kälviä inen Constructing benchmark databases and protocols for medical image analysis: diabetic retinopathy Comput. Math. Methods Med. 2013 2013 368514 [Online]. Available: [24] E. Trucco A. Ruggeri T. Karnowski L. Giancardo E. Chaum J.P. Hubschman B. al Diri C.Y. Cheung D. Wong M. Abràmoff G. Lim D. Kumar)Burlina N.M. Bressler H.F. Jelinek F. Meriaudeau G. Quellec T. MacGillivray B. Dhillon Validating retinal fundus image analysis algorithms: issues and a proposal Investig. Opthalmol. Vis. Sci. 54 5 2013 3546 [Online]. Available: [25] V. Gulshan L. Peng M. Coram M.C. Stumpe D. Wu A. Narayanaswamy S. Venugopalan K. Widner T. Madams J. Cuadros R. Kim R. Raman P.C. Nelson J.L. Mega D.R. Webster Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs JAMA 316 22 2016 2402 10.1001/jama.2016.17216 [Online] [26] S.S. Rahim V. Palade J. Shuttleworth C. Jayne R.N.R. Omar Automatic detection of microaneurysms for diabetic retinopathy screening using fuzzy image processing Engineering Applications of Neural Networks 2015 Springer 69 79 [27] D.L. Rubin C. Rodriguez)Shah C. Beaulieu iPad: Semantic annotation and markup of radiological images.” AMIA Annual Symposium Proceedings. AMIA Symposium, vol. 2008 2008 626 630 [Online]. Available: [28] D.P. Papadopoulos J.R.R. Uijlings F. Keller V. Ferrari Extreme Clicking for Efficient Object Annotation, vol. 8 2017 [Online]. Available: [29] R. Binns M. Van Kleek M. Veale U. Lyngs J. Zhao N. Shadbolt It's reducing a human being to a percentage’: perceptions of justice in algorithmic decisions Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM 2018 377 [30] F. Valente L.A.B. Silva T.M. Godinho C. Costa Anatomy of an extensible open source PACS J. Digit. Imaging 29 3 2016 284 296 10.1007/s10278-015-9834-0 [Online] [31] P. Kulkarni Y. Ozturk mPHASiS: Mobile patient healthcare and sensor information system J. Netw. Comput. Appl. 34 1 2011 402 417 [Online]. Available: [32] M. Pedrosa J. Miguel C. Costa Reactive through services - opinionated framework for developing reactive services Proceedings of the 8th International Conference on Cloud Computing and Services Science. SCITEPRESS – Science and Technology Publications, vol. 2018 2018 330 337 10.5220/0006661403300337 [Online] [33] W.L. Hürsch W.L. Hürsch C.V. Lopes Separation of Concerns 1995 [Online]. Available: [34] J.M. Silva E. Pinho E. Monteiro J.F. Silva C. Costa Controlled searching in reversibly de-identified medical imaging archives J. Biomed. Inf. 77 2018 81 90 [Online]. Available: [35] E. Monteiro C. Costa J.L. Oliveira A de-identification pipeline for ultrasound medical images in DICOM format J. Med. Syst. 41 5 2017 89 10.1007/s10916-017-0736-1 [Online] [36] NEMA Digital Imaging and Communications in Medicine (DICOM) Supplement 163: STore Over the Web by REpresentations State Transfer (REST) Services (STOW-RS) 2017 [37] Y. Yan R. Rosales G. Fung F. Farooq B. Rao J. Dy Active Learning from Multiple Knowledge Sources 2012 1350 1357 [Online]. Available: [38] E. Grisan M. Foracchia A. Ruggeri A novel method for the automatic grading of retinal vessel tortuosity IEEE Trans. Med. Imaging 27 3 2008 310 319 [Online]. Available: [39] L. Tribuna A. Silva P.S. Couto L. Bastião A study to understand the acceptance of DICOM Structured Reports on Breast Imaging Proc. Comput. Sci. 121 2017 672 680 [Online]. Available: [40] M.I. Meyer P. Costa A. Galdran A.M. Mendonça A. Campilho A Deep Neural Network for Vessel Segmentation of Scanning Laser Ophthalmoscopy Images, vol. 7 2017 Springer Cham 507 515 10.1007/978-3-319-59876-5_56 [Online] [41] P. Costa A. Galdran M.I. Meyer A.M. Mendonça A. Campilho Adversarial Synthesis of Retinal Images from Vessel Trees, vol. 7 2017 Springer Cham 516 523 10.1007/978-3-319-59876-5_57 [Online] [42] B. Remeseiro A.M. Mendonca A. Campilho Objective quality assessment of retinal images based on texture features 2017 International Joint Conference on Neural Networks (IJCNN). IEEE, vol. 5 2017 4520 4527 [Online]. Available: [43] T. Araújo A.M. Mendonça A. Campilho Estimation of retinal vessel caliber using model fitting and random forests S.G. Armato N.A. Petrick International Society for Optics and Photonics, vol. 3 2017 101341K 10.1117/12.2252025 [Online] [44] A.M. Mendonça B. Remeseiro B. Dashtbozorg A. Campilho Automatic and semi-automatic approaches for arteriolar-to-venular computation in retinal photographs S.G. Armato N.A. Petrick International Society for Optics and Photonics, vol. 3 2017 101341L 10.1117/12.2255096 [Online] [45] P. Costa A. Campilho Convolutional bag of words for diabetic retinopathy detection from eye fundus images IPSJ Trans. Comput. Vis. Appl. 9 1 2017 10 10.1186/s41074-017-0023-6 [Online] "
    },
    {
        "doc_title": "Recognition of genetic mutations in text using deep learning",
        "doc_scopus_id": "85058169058",
        "doc_doi": "10.1145/3279996.3280020",
        "doc_eid": "2-s2.0-85058169058",
        "doc_date": "2018-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Character-based models",
            "Curation",
            "F measure",
            "Genetic mutations",
            "Information extraction methods",
            "Knowledge resource",
            "Scientific publications",
            "Structured knowledge"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery. ACMKnowledge about genetic mutations and their impact on the organism is continuously being produced and communicated through scientific publications. This information is then collected by curated databases and integrated in structured knowledge resources, facilitating its discovery and reuse. To aid this work, information extraction methods are increasingly being integrated in the database curation pipelines. This work describes an information extraction method based on deep neural networks for the recognition of mutation mentions in literature abstracts. When applied to the tmVar dataset, the character based model reached an F-measure of 0.874. This result was achieved without use of knowledge resources or any handcrafted features.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automated ICD-9-CM medical coding of diabetic patient's clinical reports",
        "doc_scopus_id": "85058162044",
        "doc_doi": "10.1145/3279996.3280019",
        "doc_eid": "2-s2.0-85058162044",
        "doc_date": "2018-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Automated classification",
            "Convolutional Neural Networks (CNN)",
            "Diabetic patient",
            "Electronic health record",
            "Fully automated",
            "Independent variables",
            "Medical personnel",
            "Unsolved problems"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery. ACMThe assignment of ICD-9-CM codes to patient's clinical reports is a costly and wearing process manually done by medical personnel, estimated to cost about $25 billion per year in the United States. To develop a system that automates this process has been an ambition of researchers but is still an unsolved problem due to the inherent difficulties in processing unstructured clinical text. This problem is here formulated as a multi-label supervised learning one where the independent variable is the report's text and the dependent the several assigned ICD-9-CM labels. Different variations of two neural network based models, the Bag-of-Tricks and the Convolutional Neural Network (CNN) are investigated. The models are trained on the diabetic patient subset of the freely available MIMIC-III dataset. The results show that a CNN with three parallel convolutional layers achieves F1 scores of 44.51% for five digit codes and 51.73% for three digit, rolled up, codes. Although fully automated coding is not achievable, these results suggest that automated classification could be used to aid clinical staff by selecting the most probable codes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Daily cough frequency in tuberculosis and association with household infection",
        "doc_scopus_id": "85049750370",
        "doc_doi": "10.5588/ijtld.17.0652",
        "doc_eid": "2-s2.0-85049750370",
        "doc_date": "2018-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            },
            {
                "area_name": "Infectious Diseases",
                "area_abbreviation": "MEDI",
                "area_code": "2725"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Aerosols",
            "Contact Tracing",
            "Cough",
            "Cross-Sectional Studies",
            "Family Characteristics",
            "Female",
            "Humans",
            "Interferon-gamma Release Tests",
            "Logistic Models",
            "London",
            "Male",
            "Multivariate Analysis",
            "Mycobacterium tuberculosis",
            "Predictive Value of Tests",
            "Risk Factors",
            "Sputum",
            "Tuberculin Test",
            "Tuberculosis, Pulmonary"
        ],
        "doc_abstract": "© 2018 The Union.S E T T I N G: Although cough in tuberculosis (TB) is presumed to be important for transmission, there is little objective supporting evidence. O B J E C T I V E: To describe 24-h cough frequency in a group with TB, and investigate associations with household rates of infection. D E S I G N: Patients with a new diagnosis of pulmonary TB underwent 24-h cough frequency measurement at or just before initiation of anti-tuberculosis treatment. A group with latent Mycobacterium tuberculosis infection (LTBI) acted as controls. Rates of infection among household contacts of sputum smear-positive TB were measured using the interferon-gamma release assay and the tuberculin skin test, and compared with variables relating to the contacts themselves, and to the index case, including cough frequency. R E S U LT S: Daily cough frequency in TB patients (n ¼ 44) was variable (geometric mean [GM] 174, interquartile range [IQR] 68–475 coughs/24 h), higher than in LTBI (n ¼ 17; GM 19 coughs/24 h, IQR 8–53; P, 0.001), and higher during the day than overnight (GM 8.9 coughs/h, IQR 4.1–19.0 vs. GM 2.9 coughs/h, IQR 0.7–13.4; P, 0.0001). Also, 24-h cough frequency in TB was associated with sputum smear status (P ¼ 0.040), but not smoking (P ¼ 0.475). Multivariable logistic regression confirmed that infection in contacts was independently associated with index case sputum smear grade (P ¼ 0.014) and cough frequency (P ¼ 0.022). C O N C L U S I O N: Measurement of 24-h cough frequency in pulmonary TB helps predict infectiousness and transmission patterns.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ejection Fraction Classification in Transthoracic Echocardiography Using a Deep Learning Approach",
        "doc_scopus_id": "85050975427",
        "doc_doi": "10.1109/CBMS.2018.00029",
        "doc_eid": "2-s2.0-85050975427",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D CNN",
            "Cardio-vascular disease",
            "Classification methods",
            "Computer Aided Diagnosis(CAD)",
            "Convolutional neural network",
            "Ejection fraction",
            "Transthoracic echocardiography",
            "Ventricular ejection"
        ],
        "doc_abstract": "© 2018 IEEE.Cardiovascular diseases are the leading cause of death worldwide. These diseases are related with a broad range of factors but usually show high correlation with diminished left ventricle function, which can be evaluated by measuring the ventricular ejection fraction through transthoracic echocardiography (TTE), a cost-effective and highly portable first-line diagnosing technique. Ejection fraction (EF) is currently determined through a semi-automatic process that requires manual delineation of the left ventricle area both in a diastolic and systolic frame of the patient's exam. To remove this manual annotation step, which is both time-consuming and user dependent, automatic Computer-Aided Diagnosis (CAD) systems can be used. Herein, we propose the first steps for such a system that classifies ejection fraction in four classes, based on TTE exams, with the objective of automatically providing valuable information to physicians. Our classification method is based on a 3D-Convolutional Neural Network (3D-CNN) trained on a dataset constructed with exams from a cardiology reference center. The dataset creation consisted of three main steps: firstly, for each exam, cine-loops showing the apical 4 chambers view were manually selected; then, 30 sequential frames were extracted from each cine-loop; finally, each frame was pre-processed to mask burned-in metadata. The neural network was designed to explore concepts such as convolutions using asymmetric filters and residual learning blocks. The model was trained on a dataset with 4000 TTE exams and tested on a separate dataset containing 1600 TTE cases. We obtained an accuracy of 78% and a F1 score of 71.3% for unhealthy EF (below 45%), 63.3% for intermediate EF (45-55%), 72.3% for healthy EF (55-75%) and 54.6% for abnormally high EF (above 75%). These results are promising and show that convolutional neural networks can be applied to this domain. Furthermore, this work will serve as a foundation for future research where other relevant cardiac metrics will be determined.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SCREEN-DR: Software architecture for the diabetic retinopathy screening",
        "doc_scopus_id": "85046539905",
        "doc_doi": "10.3233/978-1-61499-852-5-396",
        "doc_eid": "2-s2.0-85046539905",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Artificial intelligence algorithms",
            "Classification algorithm",
            "Collaborative platform",
            "Diabetic retinopathy",
            "Diabetic retinopathy screening",
            "Multi-modal queries",
            "Screening programs",
            "Searching mechanism",
            "Algorithms",
            "Artificial Intelligence",
            "Diabetic Retinopathy",
            "Humans",
            "Mass Screening",
            "Software"
        ],
        "doc_abstract": "© 2018 European Federation for Medical Informatics (EFMI) and IOS Press.Diabetic Retinopathy (DR) is a common complication of diabetes that may lead to blindness if not treated. However, since DR evolves without any symptoms in the initial stages, early detection and treatment can only be achieved through routine checks. This article presents the collaborative platform of the SCREEN-DR project that promotes partnership between physicians and researchers in the scope of a regional DR screening program. The role of researchers is to create classification algorithms to evaluate image quality, discard non-pathological cases, locate possible lesions and grade DR severity. Physicians are responsible for annotating datasets, including the visual delineation of lesions. The collaborative platform collects the studies, indexes the images metadata, and manages the creation of datasets and the respective annotation process. An advanced searching mechanism supports multimodal queries over annotated datasets and exporting of results for feeding artificial intelligence algorithms.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Protein-Protein Interaction Article Classification Using a Convolutional Recurrent Neural Network with Pre-trained Word Embeddings",
        "doc_scopus_id": "85050303566",
        "doc_doi": "10.1515/jib-2017-0055",
        "doc_eid": "2-s2.0-85050303566",
        "doc_date": "2017-12-13",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Neural Networks (Computer)",
            "Protein Interaction Mapping",
            "Semantics"
        ],
        "doc_abstract": "Curation of protein interactions from scientific articles is an important task, since interaction networks are essential for the understanding of biological processes associated with disease or pharmacological action for example. However, the increase in the number of publications that potentially contain relevant information turns this into a very challenging and expensive task. In this work we used a convolutional recurrent neural network for identifying relevant articles for extracting information regarding protein interactions. Using the BioCreative III Article Classification Task dataset, we achieved an area under the precision-recall curve of 0.715 and a Matthew's correlation coefficient of 0.600, which represents an improvement over previous works.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Supervised Learning and Knowledge-Based Approaches Applied to Biomedical Word Sense Disambiguation",
        "doc_scopus_id": "85050298580",
        "doc_doi": "10.1515/jib-2017-0051",
        "doc_eid": "2-s2.0-85050298580",
        "doc_date": "2017-12-13",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Biomedical Research",
            "Databases as Topic",
            "Knowledge Bases",
            "Semantics"
        ],
        "doc_abstract": "Word sense disambiguation (WSD) is an important step in biomedical text mining, which is responsible for assigning an unequivocal concept to an ambiguous term, improving the accuracy of biomedical information extraction systems. In this work we followed supervised and knowledge-based disambiguation approaches, with the best results obtained by supervised means. In the supervised method we used bag-of-words as local features, and word embeddings as global features. In the knowledge-based method we combined word embeddings, concept textual definitions extracted from the UMLS database, and concept association values calculated from the MeSH co-occurrence counts from MEDLINE articles. Also, in the knowledge-based method, we tested different word embedding averaging functions to calculate the surrounding context vectors, with the goal to give more importance to closest words of the ambiguous term. The MSH WSD dataset, the most common dataset used for evaluating biomedical concept disambiguation, was used to evaluate our methods. We obtained a top accuracy of 95.6 % by supervised means, while the best knowledge-based accuracy was 87.4 %. Our results show that word embedding models improved the disambiguation accuracy, proving to be a powerful resource in the WSD task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Objective Assessment of Cough Frequency in Bronchiectasis",
        "doc_scopus_id": "85023773419",
        "doc_doi": "10.1007/s00408-017-0038-x",
        "doc_eid": "2-s2.0-85023773419",
        "doc_date": "2017-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Aged",
            "Bronchiectasis",
            "Carrier State",
            "Case-Control Studies",
            "Cough",
            "Disease Progression",
            "Female",
            "Forced Expiratory Volume",
            "Health Status",
            "Humans",
            "Male",
            "Middle Aged",
            "Pseudomonas aeruginosa",
            "Pseudomonas Infections",
            "Quality of Life",
            "Severity of Illness Index",
            "Sputum",
            "Surveys and Questionnaires",
            "Visual Analog Scale",
            "Vital Capacity"
        ],
        "doc_abstract": "© 2017, The Author(s).Introduction: Cough in bronchiectasis is associated with significant impairment in health status. This study aimed to quantify cough frequency objectively with a cough monitor and investigate its relationship with health status. A secondary aim was to identify clinical predictors of cough frequency. Methods: Fifty-four patients with bronchiectasis were compared with thirty-five healthy controls. Objective 24-h cough, health status (cough-specific: Leicester Cough Questionnaire LCQ and bronchiectasis specific: Bronchiectasis Health Questionnaire BHQ), cough severity and lung function were measured. The clinical predictors of cough frequency in bronchiectasis were determined in a multivariate analysis. Results: Objective cough frequency was significantly raised in patients with bronchiectasis compared to healthy controls [geometric mean (standard deviation)] 184.5 (4.0) vs. 20.6 (3.2) coughs/24-h; mean fold-difference (95% confidence interval) 8.9 (5.2, 15.2); p < 0.001 and they had impaired health status. There was a significant correlation between objective cough frequency and subjective measures; LCQ r = −0.52 and BHQ r = −0.62, both p < 0.001. Sputum production, exacerbations (between past 2 weeks to 12 months) and age were significantly associated with objective cough frequency in multivariate analysis, explaining 52% of the variance (p < 0.001). There was no statistically significant association between cough frequency and lung function. Conclusions: Cough is a common and significant symptom in patients with bronchiectasis. Sputum production, exacerbations and age, but not lung function, were independent predictors of cough frequency. Ambulatory objective cough monitoring provides novel insights and should be further investigated as an outcome measure in bronchiectasis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Intelligent Cloud Storage Gateway for Medical Imaging",
        "doc_scopus_id": "85027055714",
        "doc_doi": "10.1007/s10916-017-0790-8",
        "doc_eid": "2-s2.0-85027055714",
        "doc_date": "2017-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Cloud Computing",
            "Diagnostic Imaging",
            "Information Storage and Retrieval",
            "Internet",
            "Outsourced Services"
        ],
        "doc_abstract": "© 2017, Springer Science+Business Media, LLC.Historically, medical imaging repositories have been supported by indoor infrastructures. However, the amount of diagnostic imaging procedures has continuously increased over the last decades, imposing several challenges associated with the storage volume, data redundancy and availability. Cloud platforms are focused on delivering hardware and software services over the Internet, becoming an appealing solution for repository outsourcing. Although this option may bring financial and technological benefits, it also presents new challenges. In medical imaging scenarios, communication latency is a critical issue that still hinders the adoption of this paradigm. This paper proposes an intelligent Cloud storage gateway that optimizes data access times. This is achieved through a new cache architecture that combines static rules and pattern recognition for eviction and prefetching. The evaluation results, obtained from experiments over a real-world dataset, show that cache hit ratios can reach around 80%, leading to reductions of image retrieval times by over 60%. The combined use of eviction and prefetching policies proposed can significantly reduce communication latency, even when using a small cache in comparison to the total size of the repository. Apart from the performance gains, the proposed system is capable of adjusting to specific workflows of different institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sound: A non-invasive measure of cough intensity",
        "doc_scopus_id": "85019456990",
        "doc_doi": "10.1136/bmjresp-2017-000178",
        "doc_eid": "2-s2.0-85019456990",
        "doc_date": "2017-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017. All rights reserved.Introduction Cough intensity is an important determinant of cough severity reported by patients. Cough sound analysis has been widely validated for the measurement of cough frequency but few studies have validated its use in the assessment of cough strength. We investigated the relationship between cough sound and physiological measures of cough strength. Methods 32 patients with chronic cough and controls underwent contemporaneous measurements of voluntary cough sound, flow and oesophageal pressure. Sound power, peak energy, rise-time, duration, peak-frequency, bandwidth and centroid-frequency were assessed and compared with physiological measures. The relationship between sound and subjective cough strength Visual Analogue Score (VAS), the repeatability of cough sounds and the effect of microphone position were also assessed. Results Sound power and energy correlated strongly with cough flow (median Spearman’s r=0.87–0.88) and oesophageal pressure (median Spearman’s r=0.89). Sound power and energy correlated strongly with cough strength VAS (median Spearman’s r=0.84–0.86) and were highly repeatable (intraclass correlation coefficient=0.93–0.94) but both were affected by change in microphone position. Conclusions Cough sound power and energy correlate strongly with physiological measures and subjective perception of cough strength. Power and energy are highly repeatable measures but the microphone position should be standardised. Our findings support the use of cough sound as an index of cough strength.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Biomedical word sense disambiguation with word embeddings",
        "doc_scopus_id": "85025169861",
        "doc_doi": "10.1007/978-3-319-60816-7_33",
        "doc_eid": "2-s2.0-85025169861",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic extraction",
            "Embeddings",
            "Knowledge-based algorithms",
            "Knowledge-based approach",
            "Knowledge-based methods",
            "Literature database",
            "Supervised machine learning",
            "Word Sense Disambiguation"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.There is a growing need for automatic extraction of information and knowledge from the increasing amount of biomedical and clinical data produced, namely in textual form. Natural language processing comes in this direction, helping in tasks such as information extraction and information retrieval. Word sense disambiguation is an important part of this process, being responsible for assigning the proper concept to an ambiguous term. In this paper, we present results from machine learning and knowledge-based algorithms applied to biomedical word sense disambiguation. For the supervised machine learning algorithms we used word embeddings, calculated from the full MEDLINE literature database, as global features and compare the results to the use of local unigram and bigram features. For the knowledge-based method we represented the textual definitions of biomedical concepts from the UMLS database as word embedding vectors, and combined this with concept associations derived from the MeSH term co-occurrences. Both the machine learning and the knowledge-based results indicate that word embeddings are informative and improve the biomedical word disambiguation accuracy. Applied to the reference MSH WSD data set, our knowledge-based approach achieves 85.1% disambiguation accuracy, which is higher than some previously proposed approaches that do not use machine-learning strategies.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving document prioritization for protein-protein interaction extraction using shallow linguistics and word embeddings",
        "doc_scopus_id": "85025128038",
        "doc_doi": "10.1007/978-3-319-60816-7_6",
        "doc_eid": "2-s2.0-85025128038",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Correlation coefficient",
            "Embeddings",
            "Literature retrieval",
            "Protein interaction networks",
            "Protein-protein interaction extractions",
            "Protein-protein interactions",
            "Scientific literature",
            "Supervised classification"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Understanding of biological processes, associated to disease or pharmacological action for example, requires the analysis of large amounts of interconnected information. Protein interaction networks form part of this puzzle, and extracting this information from the scientific literature is an important but challenging task. In this work, we present a supervised classification approach for identifying and ranking literature documents that contain information regarding protein interactions. We studied the use of word embedding together with simple chunking features, and show that the combination of these features with baseline bag-of-words can lead to similar or even improved results when compared to the use of features based on deep linguistic parsing. When applied to the BioCreative III Article Classification Task dataset, our approach achieves an area under the precision-recall curve of 0.70 and a Matthew’s correlation coefficient of 0.56.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pattern recognition for cache management in distributed medical imaging environments",
        "doc_scopus_id": "84957948635",
        "doc_doi": "10.1007/s11548-015-1272-4",
        "doc_eid": "2-s2.0-84957948635",
        "doc_date": "2016-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Diagnostic Imaging",
            "Humans",
            "Models, Theoretical",
            "Neural Networks (Computer)",
            "Pattern Recognition, Automated"
        ],
        "doc_abstract": "© 2015, CARS.Purpose: Traditionally, medical imaging repositories have been supported by indoor infrastructures with huge operational costs. This paradigm is changing thanks to cloud outsourcing which not only brings technological advantages but also facilitates inter-institutional workflows. However, communication latency is one main problem in this kind of approaches, since we are dealing with tremendous volumes of data. To minimize the impact of this issue, cache and prefetching are commonly used. The effectiveness of these mechanisms is highly dependent on their capability of accurately selecting the objects that will be needed soon. Methods: This paper describes a pattern recognition system based on artificial neural networks with incremental learning to evaluate, from a set of usage pattern, which one fits the user behavior at a given time. The accuracy of the pattern recognition model in distinct training conditions was also evaluated. Results: The solution was tested with a real-world dataset and a synthesized dataset, showing that incremental learning is advantageous. Even with very immature initial models, trained with just 1 week of data samples, the overall accuracy was very similar to the value obtained when using 75 % of the long-term data for training the models. Preliminary results demonstrate an effective reduction in communication latency when using the proposed solution to feed a prefetching mechanism. Conclusions: The proposed approach is very interesting for cache replacement and prefetching policies due to the good results obtained since the first deployment moments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioCreative V BioC track overview: collaborative biocurator assistant task for BioGRID",
        "doc_scopus_id": "85037580282",
        "doc_doi": "10.1093/database/baw121",
        "doc_eid": "2-s2.0-85037580282",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Automatic Data Processing",
            "Data Curation",
            "Data Mining",
            "Information Dissemination"
        ],
        "doc_abstract": "© 2016 Published by Oxford University Press 2016. This work is written by US Government employees and is in the public domain in the US.BioC is a simple XML format for text, annotations and relations, and was developed to achieve interoperability for biomedical text processing. Following the success of BioC in BioCreative IV, the BioCreative V BioC track addressed a collaborative task to build an assistant system for BioGRID curation. In this paper, we describe the framework of the collaborative BioC task and discuss our findings based on the user survey. This track consisted of eight subtasks including gene/protein/organism named entity recognition, protein-protein/genetic interaction passage identification and annotation visualization. Using BioC as their data-sharing and communication medium, nine teams, world-wide, participated and contributed either new methods or improvements of existing tools to address different subtasks of the BioC track. Results from different teams were shared in BioC and made available to other teams as they addressed different subtasks of the track. In the end, all submitted runs were merged using a machine learning classifier to produce an optimized output. The biocurator assistant system was evaluated by four BioGRID curators in terms of practical usability. The curators' feedback was overall positive and highlighted the user-friendly design and the convenient gene/protein curation tool based on text mining. Database URL: http://www.biocreative.org/tasks/biocreative-v/track-1-bioc/",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Overview of the interactive task in BioCreative V",
        "doc_scopus_id": "85009926974",
        "doc_doi": "10.1093/database/baw119",
        "doc_eid": "2-s2.0-85009926974",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Automatic Data Processing",
            "Data Curation",
            "Data Mining"
        ],
        "doc_abstract": "Fully automated text mining (TM) systems promote efficient literature searching, retrieval, and review but are not sufficient to produce ready-to-consume curated documents. These systems are not meant to replace biocurators, but instead to assist them in one or more literature curation steps. To do so, the user interface is an important aspect that needs to be considered for tool adoption. The BioCreative Interactive task (IAT) is a track designed for exploring user-system interactions, promoting development of useful TM tools, and providing a communication channel between the biocuration and the TM communities. In BioCreative V, the IAT track followed a format similar to previous interactive tracks, where the utility and usability of TM tools, as well as the generation of use cases, have been the focal points. The proposed curation tasks are user-centric and formally evaluated by biocurators. In BioCreative V IAT, seven TM systems and 43 biocurators participated. Two levels of user participation were offered to broaden curator involvement and obtain more feedback on usability aspects. The full level participation involved training on the system, curation of a set of documents with and without TM assistance, tracking of time-on-task, and completion of a user survey. The partial level participation was designed to focus on usability aspects of the interface and not the performance per se. In this case, biocurators navigated the system by performing predesigned tasks and then were asked whether they were able to achieve the task and the level of difficulty in completing the task. In this manuscript, we describe the development of the interactive task, from planning to execution and discuss major findings for the systems tested.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mining clinical attributes of genomic variants through assisted literature curation in Egas",
        "doc_scopus_id": "85009124421",
        "doc_doi": "10.1093/database/baw096",
        "doc_eid": "2-s2.0-85009124421",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Animals",
            "Data Curation",
            "Data Mining",
            "Humans",
            "Internet",
            "User-Computer Interface",
            "Web Browser"
        ],
        "doc_abstract": "The veritable deluge of biological data over recent years has led to the establishment of a considerable number of knowledge resources that compile curated information extracted from the literature and store it in structured form, facilitating its use and exploitation. In this article, we focus on the curation of inherited genetic variants and associated clinical attributes, such as zygosity, penetrance or inheritance mode, and describe the use of Egas for this task. Egas is a web-based platform for text-mining assisted literature curation that focuses on usability through modern design solutions and simple user interactions. Egas offers a flexible and customizable tool that allows defining the concept types and relations of interest for a given annotation task, as well as the ontologies used for normalizing each concept type. Further, annotations may be performed on raw documents or on the results of automated concept identification and relation extraction tools. Users can inspect, correct or remove automatic text-mining results, manually add new annotations, and export the results to standard formats. Egas is compatible with the most recent versions of Google Chrome, Mozilla Firefox, Internet Explorer and Safari and is available for use at https://demo.bmd-software.com/egas/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A curation pipeline and web-services for PDF documents",
        "doc_scopus_id": "84985920440",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84985920440",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical literature",
            "Extract informations",
            "Side by sides",
            "Text extraction",
            "Text-based documents",
            "Textual content",
            "User friendly",
            "Web services platform"
        ],
        "doc_abstract": "The continuous growth of the biomedical literature and the need to efficiently find and extract information from its content led to the development of various text mining tools. More recently, these tools started being integrated in user-friendly applications facilitating their use by expert database curators. However, these tools were mainly designed to extract information from text based documents, in XML and other formats, while today a considerable part of the biomedical literature is published and distributed in PDF format. To address this limitation, we extended the web-based literature curation tool Egas, adding support for direct document curation and annotation over PDF files, with side-by-side visualization of the original PDF document and of the extracted textual content. Egas' PDF document processing and text-mining features are supported by a newly developed web-services platform built over Neji, a highly efficient information extraction framework. These web services allow integrating PDF text extraction and annotation capabilities to other tools and text mining pipelines.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic knowledge base construction from radiology reports",
        "doc_scopus_id": "84969263661",
        "doc_doi": "10.5220/0005709503450352",
        "doc_eid": "2-s2.0-84969263661",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical reports",
            "Health care information system",
            "Healthcare institutions",
            "Natural languages",
            "Radiology reports",
            "Semantic knowledge",
            "Text mining"
        ],
        "doc_abstract": "Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.The tremendous quantity of data stored daily in healthcare institutions demands the development of new methods to summarize and reuse available information in clinical practice. In order to leverage modern healthcare information systems, new strategies must be developed that address challenges such as extraction of relevant information, data redundancy, and the lack of associations within the data. This article proposes a pipeline to overcome these challenges in the context of medical imaging reports, by automatically extracting and linking information, and summarizing natural language reports into an ontology model. Using data from the Physionet MIMIC II database, we created a semantic knowledge base with more than 6.5 millions of triples obtained from a collection of 16,000 radiology reports.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ann2RDF: Moving annotations to semantic web",
        "doc_scopus_id": "84967145648",
        "doc_doi": "10.1145/2837185.2837253",
        "doc_eid": "2-s2.0-84967145648",
        "doc_date": "2015-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Annotation",
            "Annotation tool",
            "Data transformation",
            "Representation model",
            "Semantic knowledge",
            "Text mining",
            "Transition process"
        ],
        "doc_abstract": "© 2015 ACM.The annotation of concepts and susceptible interactions has been assuming a key role in the extraction of relevant information from published documents. However, distinct annotation tools generate also different formats, creating a barrier to efficiently combine and exchange this information. The migration of curated information into semantic web format and services provides an additional value to share that knowledge, but data transformation represents here an additional challenge. In this manuscript, we present a unified layer between text-mining tools and semantic web services to reduce the effort of combining different formats. The Ann2RDF is focused on reusing existing curated data from external text-mining tools to improve their availability through an open representation model. This result in a more suitable transition process, in which desired annotations are enriched with the possibility to be shared, compared and reused across semantic Knowledge Bases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioinformaticsUA: Machine Learning and Rule-Based Recognition of Disorders and Clinical Attributes from Patient Notes",
        "doc_scopus_id": "85122039614",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85122039614",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [
            "Entity recognition",
            "F-score",
            "Large amounts",
            "Machine-learning",
            "Normalisation",
            "Rule-based method",
            "Rule-based recognition",
            "SNOMED-CT",
            "Subtask",
            "Text-analysis methods"
        ],
        "doc_abstract": "© 2015 Association for Computational LinguisticsNatural language processing and text analysis methods offer the potential of uncovering hidden associations from large amounts of unprocessed texts. The SemEval-2015 Analysis of Clinical Text task aimed at fostering research on the application of these methods in the clinical domain. The proposed task consisted of disorder identification with normalization to SNOMED-CT concepts, and disorder attribute identification, or template filling. We participated in both sub-tasks, using a combination of machine-learning and rules for recognizing and normalizing disease mentions, and rule-based methods for template filling. We achieved an F-score of 71.2% in the entity recognition and normalization task, and a slot weighted accuracy of 69.5% in the template filling task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Overview of Biomolecular Event Extraction from Scientific Documents",
        "doc_scopus_id": "84947447541",
        "doc_doi": "10.1155/2015/571381",
        "doc_eid": "2-s2.0-84947447541",
        "doc_date": "2015-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Automatic extraction",
            "Biological process",
            "Biomedical literature",
            "Future perspectives",
            "Linguistic phenomena",
            "NAtural language processing",
            "Scientific documents",
            "State-of-the-art approach",
            "Animals",
            "Computational Biology",
            "Data Mining",
            "Databases, Factual",
            "Humans",
            "Machine Learning",
            "Natural Language Processing",
            "Systems Biology"
        ],
        "doc_abstract": "© 2015 Jorge A. Vanegas et al.This paper presents a review of state-of-the-art approaches to automatic extraction of biomolecular events from scientific texts. Events involving biomolecules such as genes, transcription factors, or enzymes, for example, have a central role in biological processes and functions and provide valuable information for describing physiological and pathogenesis mechanisms. Event extraction from biomedical literature has a broad range of applications, including support for information retrieval, knowledge summarization, and information extraction and discovery. However, automatic event extraction is a challenging task due to the ambiguity and diversity of natural language and higher-level linguistic phenomena, such as speculations and negations, which occur in biological texts and can lead to misunderstanding or incorrect interpretation. Many strategies have been proposed in the last decade, originating from different research areas such as natural language processing, machine learning, and statistics. This review summarizes the most representative approaches in biomolecular event extraction and presents an analysis of the current state of the art and of commonly used methods, features, and tools. Finally, current research trends and future perspectives are also discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A document processing pipeline for annotating chemical entities in scientific documents",
        "doc_scopus_id": "84946128141",
        "doc_doi": "10.1186/1758-2946-7-S1-S7",
        "doc_eid": "2-s2.0-84946128141",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Physical and Theoretical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1606"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2015 Campos et al.Background: The recognition of drugs and chemical entities in text is a very important task within the field of biomedical information extraction, given the rapid growth in the amount of published texts (scientific papers, patents, patient records) and the relevance of these and other related concepts. If done effectively, this could allow exploiting such textual resources to automatically extract or infer relevant information, such as drug profiles, relations and similarities between drugs, or associations between drugs and potential drug targets. The objective of this work was to develop and validate a document processing and information extraction pipeline for the identification of chemical entity mentions in text. Results: We used the BioCreative IV CHEMDNER task data to train and evaluate a machine-learning based entity recognition system. Using a combination of two conditional random field models, a selected set of features, and a post-processing stage, we achieved F-measure results of 87.48% in the chemical entity mention recognition task and 87.75% in the chemical document indexing task. Conclusions: We present a machine learning-based solution for automatic recognition of chemical and drug names in scientific documents. The proposed approach applies a rich feature set, including linguistic, orthographic, morphological, dictionary matching and local context features. Post-processing modules are also integrated, performing parentheses correction, abbreviation resolution and filtering erroneous mentions using an exclusion list derived from the training data. The developed methods were implemented as a document annotation tool and web service, freely available at http://bioinformatics.ua.pt/becas-chemicals/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A semantic layer for unifying and exploring biomedical document curation results",
        "doc_scopus_id": "84944460806",
        "doc_doi": "10.1007/978-3-319-16483-0_2",
        "doc_eid": "2-s2.0-84944460806",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical documents",
            "Biomedical information",
            "Computerized solution",
            "Curation",
            "Multiple interfaces",
            "Scientific publications",
            "Scientific workflows",
            "Text mining"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Tackling the ever-growing amount of specialized literature in the life sciences domain is a paramount challenge. Various scientific workflows depend on using domain knowledge from resources that summarize, in structured form, validated information extracted from scientific publications. Manual curation of these data is a demanding task, and latest strategies use computerized solutions to aid in the analysis, extraction and storage of relevant concepts and their respective attributes and relationships. The outcome of these complex document curation workflows provides valuable insights into the overwhelming amount of biomedical information being produced. Yet, the majority of automated and interactive annotation tools are not open, limiting access to knowledge and reducing the potential scope of the manually curated information. In this manuscript, we propose an interoperable semantic layer to unify document curation results and enable their proper exploration through multiple interfaces geared towards bioinformatics developers and general life sciences researchers. This enables a unique scenario where results from computational annotation tools are harmonized and further integrated into rich semantic knowledge bases, providing a solid foundation for discovering knowledge.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Incremental learning versus batch learning for classification of user's behaviour in medical imaging",
        "doc_scopus_id": "84938871846",
        "doc_doi": "10.5220/0005219704310438",
        "doc_eid": "2-s2.0-84938871846",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Batch learning",
            "Cache mechanism",
            "Communication latency",
            "DICOM",
            "Incremental learning",
            "Outsourcing solution",
            "Prefetching",
            "Storage capacity"
        ],
        "doc_abstract": "Communication latency still hinders the adoption of Cloud computing paradigms in medical imaging environments where it could serve as a reliable technology to support repository outsourcing solutions or inter-institutional workflows, for instance. One way to overcome this is by implementing cache repositories and prefetching mechanisms. Nevertheless, such solutions are usually based on static rules that may inefficiently manage the cache storage capacity. For that reason, this paper compares a pattern recognition system using incremental learning versus batch learning, in order to assess which one could be more appropriately used in a medical imaging cache mechanism.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The CHEMDNER corpus of chemicals and drugs and its annotation principles",
        "doc_scopus_id": "84925647188",
        "doc_doi": "10.1186/1758-2946-7-S1-S2",
        "doc_eid": "2-s2.0-84925647188",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Physical and Theoretical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1606"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2015 Krallinger et al.The automatic extraction of chemical information from text requires the recognition of chemical entity mentions as one of its key steps. When developing supervised named entity recognition (NER) systems, the availability of a large, manually annotated text corpus is desirable. Furthermore, large corpora permit the robust evaluation and comparison of different approaches that detect chemicals in documents. We present the CHEMDNER corpus, a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators, following annotation guidelines specifically defined for this task. The abstracts of the CHEMDNER corpus were selected to be representative for all major chemical disciplines. Each of the chemical entity mentions was manually labeled according to its structure-associated chemical entity mention (SACEM) class: abbreviation, family, formula, identifier, multiple, systematic and trivial. The difficulty and consistency of tagging chemicals in text was measured using an agreement study between annotators, obtaining a percentage agreement of 91. For a subset of the CHEMDNER corpus (the test set of 3,000 abstracts) we provide not only the Gold Standard manual annotations, but also mentions automatically detected by the 26 teams that participated in the BioCreative IV CHEMDNER chemical mention recognition task. In addition, we release the CHEMDNER silver standard corpus of automatically extracted mentions from 17,000 randomly selected PubMed abstracts. A version of the CHEMDNER corpus in the BioC format has been generated as well. We propose a standard for required minimum information about entity annotations for the construction of domain specific corpora on chemical and drug entities. The CHEMDNER corpus and annotation guidelines are available at: http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analysing Twitter and web queries for flu trend prediction",
        "doc_scopus_id": "84900339672",
        "doc_doi": "10.1186/1742-4682-11-S1-S6",
        "doc_eid": "2-s2.0-84900339672",
        "doc_date": "2014-05-07",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Humans",
            "Influenza, Human",
            "Internet",
            "Linear Models",
            "Portugal",
            "ROC Curve",
            "Search Engine",
            "Statistics as Topic"
        ],
        "doc_abstract": "Background: Social media platforms encourage people to share diverse aspects of their daily life. Among these, shared health related information might be used to infer health status and incidence rates for specific conditions or symptoms. In this work, we present an infodemiology study that evaluates the use of Twitter messages and search engine query logs to estimate and predict the incidence rate of influenza like illness in Portugal. Results: Based on a manually classified dataset of 2704 tweets from Portugal, we selected a set of 650 textual features to train a Naïve Bayes classifier to identify tweets mentioning flu or flu-like illness or symptoms. We obtained a precision of 0.78 and an F-measure of 0.83, based on cross validation over the complete annotated set. Furthermore, we trained a multiple linear regression model to estimate the health-monitoring data from the Influenzanet project, using as predictors the relative frequencies obtained from the tweet classification results and from query logs, and achieved a correlation ratio of 0.89 (p < 0.001). These classification and regression models were also applied to estimate the flu incidence in the following flu season, achieving a correlation of 0.72. Conclusions: Previous studies addressing the estimation of disease incidence based on user-generated content have mostly focused on the english language. Our results further validate those studies and show that by changing the initial steps of data preprocessing and feature extraction and selection, the proposed approaches can be adapted to other languages. Additionally, we investigated whether the predictive model created can be applied to data from the subsequent flu season. In this case, although the prediction result was good, an initial phase to adapt the regression model could be necessary to achieve more robust results. © 2014 Santos and Matos; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computational prediction of the human-microbial oral interactome",
        "doc_scopus_id": "84897570043",
        "doc_doi": "10.1186/1752-0509-8-24",
        "doc_eid": "2-s2.0-84897570043",
        "doc_date": "2014-02-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: The oral cavity is a complex ecosystem where human chemical compounds coexist with a particular microbiota. However, shifts in the normal composition of this microbiota may result in the onset of oral ailments, such as periodontitis and dental caries. In addition, it is known that the microbial colonization of the oral cavity is mediated by protein-protein interactions (PPIs) between the host and microorganisms. Nevertheless, this kind of PPIs is still largely undisclosed. To elucidate these interactions, we have created a computational prediction method that allows us to obtain a first model of the Human-Microbial oral interactome.Results: We collected high-quality experimental PPIs from five major human databases. The obtained PPIs were used to create our positive dataset and, indirectly, our negative dataset. The positive and negative datasets were merged and used for training and validation of a naïve Bayes classifier. For the final prediction model, we used an ensemble methodology combining five distinct PPI prediction techniques, namely: literature mining, primary protein sequences, orthologous profiles, biological process similarity, and domain interactions. Performance evaluation of our method revealed an area under the ROC-curve (AUC) value greater than 0.926, supporting our primary hypothesis, as no single set of features reached an AUC greater than 0.877. After subjecting our dataset to the prediction model, the classified result was filtered for very high confidence PPIs (probability ≥ 1-10-7), leading to a set of 46,579 PPIs to be further explored.Conclusions: We believe this dataset holds not only important pathways involved in the onset of infectious oral diseases, but also potential drug-targets and biomarkers. The dataset used for training and validation, the predictions obtained and the network final network are available at http://bioinformatics.ua.pt/software/oralint. © 2014 Coelho et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Twitter: A good place to detect health conditions",
        "doc_scopus_id": "84900419026",
        "doc_doi": "10.1371/journal.pone.0086191",
        "doc_eid": "2-s2.0-84900419026",
        "doc_date": "2014-01-29",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "With the proliferation of social networks and blogs, the Internet is increasingly being used to disseminate personal health information rather than just as a source of information. In this paper we exploit the wealth of user-generated data, available through the micro-blogging service Twitter, to estimate and track the incidence of health conditions in society. The method is based on two stages: we start by extracting possibly relevant tweets using a set of specially crafted regular expressions, and then classify these initial messages using machine learning methods. Furthermore, we selected relevant features to improve the results and the execution times. To test the method, we considered four health states or conditions, namely flu, depression, pregnancy and eating disorders, and two locations, Portugal and Spain. We present the results obtained and demonstrate that the detection results and the performance of the method are improved after feature selection. The results are promising, with areas under the receiver operating characteristic curve between 0.7 and 0.9, and f-measure values around 0.8 and 0.9. This fact indicates that such approach provides a feasible solution for measuring and tracking the evolution of health states within the society. © 2014 Prieto et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TrigNER: Automatically optimized biomedical event trigger recognition on scientific documents",
        "doc_scopus_id": "84892661802",
        "doc_doi": "10.1186/1751-0473-9-1",
        "doc_eid": "2-s2.0-84892661802",
        "doc_date": "2014-01-08",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: Cellular events play a central role in the understanding of biological processes and functions, providing insight on both physiological and pathogenesis mechanisms. Automatic extraction of mentions of such events from the literature represents an important contribution to the progress of the biomedical domain, allowing faster updating of existing knowledge. The identification of trigger words indicating an event is a very important step in the event extraction pipeline, since the following task(s) rely on its output. This step presents various complex and unsolved challenges, namely the selection of informative features, the representation of the textual context, and the selection of a specific event type for a trigger word given this context. Results: We propose TrigNER, a machine learning-based solution for biomedical event trigger recognition, which takes advantage of Conditional Random Fields (CRFs) with a high-end feature set, including linguistic-based, orthographic, morphological, local context and dependency parsing features. Additionally, a completely configurable algorithm is used to automatically optimize the feature set and training parameters for each event type. Thus, it automatically selects the features that have a positive contribution and automatically optimizes the CRF model order, n-grams sizes, vertex information and maximum hops for dependency parsing features. The final output consists of various CRF models, each one optimized to the linguistic characteristics of each event type. Conclusions: TrigNER was tested in the BioNLP 2009 shared task corpus, achieving a total F-measure of 62.7 and outperforming existing solutions on various event trigger types, namely gene expression, transcription, protein catabolism, phosphorylation and binding. The proposed solution allows researchers to easily apply complex and optimized techniques in the recognition of biomedical event triggers, making its application a simple routine task. We believe this work is an important contribution to the biomedical text mining community, contributing to improved and faster event recognition on scientific articles, and consequent hypothesis generation and knowledge discovery. This solution is freely available as open source at http://bioinformatics.ua.pt/trigner. © 2014 Campos et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioinformaticsUA: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework",
        "doc_scopus_id": "85122043120",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85122043120",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [
            "'current",
            "Amount of information",
            "Concept recognition",
            "Discharge summary",
            "Discharge tests",
            "Modular architectures",
            "Modulars",
            "Normalisation",
            "Test reports",
            "Text-processing"
        ],
        "doc_abstract": "© 8th International Workshop on Semantic Evaluation, SemEval 2014 - co-located with the 25th International Conference on Computational Linguistics, COLING 2014, Proceedings. All rights reserved.Clinical texts, such as discharge summaries or test reports, contain a valuable amount of information that, if efficiently and effectively mined, could be used to infer new knowledge, possibly leading to better diagnosis and therapeutics. With this in mind, the SemEval-2014 Analysis of Clinical Text task aimed at assessing and improving current methods for identification and normalization of concepts occurring in clinical narrative. This paper describes our approach in this task, which was based on a fully modular architecture for text mining. We followed a pure dictionary-based approach, after performing error analysis to refine our dictionaries. We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision over all submitted runs (81.3%), with above average recall (60.5%). In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Current methodologies for biomedical named entity recognition",
        "doc_scopus_id": "85101461012",
        "doc_doi": "10.1002/9781118617151.ch37",
        "doc_eid": "2-s2.0-85101461012",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 John Wiley & Sons, Inc.The primary goal of text mining is to retrieve knowledge that is hidden in text and to present it in a concise and simple form to the final users. To achieve this objective, two main directions of research can be defined: Information Extraction (IE) and Information Retrieval (IR). Named Entity Recognition (NER) is one of the most important tasks, since the IE steps will be performed using the names provided by it. The chapter presents the steps necessary to implement solutions using dictionaries and machine learning and hybrid approaches, respectively. Some practical examples for each approach are provided. It also describes the existing approaches to develop NER and normalization solutions, presenting and explaining the core techniques accompanied with examples of some existent systems. The chapter presents one practical example for each approach in order to demonstrate how the several steps could be implemented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Egas: A collaborative and interactive document curation platform",
        "doc_scopus_id": "84987673679",
        "doc_doi": "10.1093/database/bau048",
        "doc_eid": "2-s2.0-84987673679",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© The Author(s) 2014.With the overwhelming amount of biomedical textual information being produced, several manual curation efforts have been set up to extract and store concepts and their relationships into structured resources. As manual annotation is a demanding and expensive task, computerized solutions were developed to perform such tasks automatically. However, high-end information extraction techniques are still not widely used by biomedical research communities, mainly because of the lack of standards and limitations in usability. Interactive annotation tools intend to fill this gap, taking advantage of automatic techniques and existing knowledge bases to assist expert curators in their daily tasks. This article presents Egas, a web-based platform for biomedical text mining and assisted curation with highly usable interfaces for manual and automatic in-line annotation of concepts and relations. A comprehensive set of de facto standard knowledge bases are integrated and indexed to provide straightforward concept normalization features. Real-time collaboration and conversation functionalities allow discussing details of the annotation task as well as providing instant feedback of curator's interactions. Egas also provides interfaces for on-demand management of the annotation task settings and guidelines, and supports standard formats and literature services to import and export documents. By taking advantage of Egas, we participated in the BioCreative IV interactive annotation task, targeting the assisted identification of protein-protein interactions described in PubMed abstracts related to neuropathological disorders. When evaluated by expert curators, it obtained positive scores in terms of usability, reliability and performance. These results, together with the provided innovative features, place Egas as a state-of-the-art solution for fast and accurate curation of information, facilitating the task of creating and updating knowledge bases and annotated resources.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extracting sentences describing biomolecular events from the biomedical literature",
        "doc_scopus_id": "84906048690",
        "doc_doi": "10.1007/978-3-319-07593-8_48",
        "doc_eid": "2-s2.0-84906048690",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bio-molecular",
            "Biomedical literature",
            "Scientific articles",
            "Scientific literature",
            "Sentence-based",
            "Sources of informations",
            "Specific information",
            "Supervised machine learning"
        ],
        "doc_abstract": "The scientific literature is one of the main sources of information for researchers. However, due to the rapid increase of the number of scientific articles, satisfying a specific information need has become a very demanding task, and researchers often have to scan through a large number of publications in search of a specific nugget of information. In this work we propose the use of supervised machine learning techniques to retrieve and rank sentences describing different types of biomolecular events. The objective is to classify and rank sentences that match any general query according to the likelihood of mentioning events involving one or more biomolecular entities. These ranked results should provide a condensed, or summarized, view of the knowledge present in the literature and related to the user's information need. © Springer International Publishing Switzerland 2014.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A modular framework for biomedical concept recognition",
        "doc_scopus_id": "84884469671",
        "doc_doi": "10.1186/1471-2105-14-281",
        "doc_eid": "2-s2.0-84884469671",
        "doc_date": "2013-09-24",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Appropriate techniques",
            "Biomedical information extractions",
            "Command-line interfaces",
            "Named entity recognition",
            "NAtural language processing",
            "Normalization methods",
            "Open source frameworks",
            "Part of speech tagging"
        ],
        "doc_abstract": "Background: Concept recognition is an essential task in biomedical information extraction, presenting several complex and unsolved challenges. The development of such solutions is typically performed in an ad-hoc manner or using general information extraction frameworks, which are not optimized for the biomedical domain and normally require the integration of complex external libraries and/or the development of custom tools.Results: This article presents Neji, an open source framework optimized for biomedical concept recognition built around four key characteristics: modularity, scalability, speed, and usability. It integrates modules for biomedical natural language processing, such as sentence splitting, tokenization, lemmatization, part-of-speech tagging, chunking and dependency parsing. Concept recognition is provided through dictionary matching and machine learning with normalization methods. Neji also integrates an innovative concept tree implementation, supporting overlapped concept names and respective disambiguation techniques. The most popular input and output formats, namely Pubmed XML, IeXML, CoNLL and A1, are also supported. On top of the built-in functionalities, developers and researchers can implement new processing modules or pipelines, or use the provided command-line interface tool to build their own solutions, applying the most appropriate techniques to identify heterogeneous biomedical concepts. Neji was evaluated against three gold standard corpora with heterogeneous biomedical concepts (CRAFT, AnEM and NCBI disease corpus), achieving high performance results on named entity recognition (F1-measure for overlap matching: species 95%, cell 92%, cellular components 83%, gene and proteins 76%, chemicals 65%, biological processes and molecular functions 63%, disorders 85%, and anatomical entities 82%) and on entity normalization (F1-measure for overlap name matching and correct identifier included in the returned list of identifiers: species 88%, cell 71%, cellular components 72%, gene and proteins 64%, chemicals 53%, and biological processes and molecular functions 40%). Neji provides fast and multi-threaded data processing, annotating up to 1200 sentences/second when using dictionary-based concept identification.Conclusions: Considering the provided features and underlying characteristics, we believe that Neji is an important contribution to the biomedical community, streamlining the development of complex concept recognition solutions. Neji is freely available at http://bioinformatics.ua.pt/neji. © 2013 Campos et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BeCAS: Biomedical concept recognition services and visualization",
        "doc_scopus_id": "84880552535",
        "doc_doi": "10.1093/bioinformatics/btt317",
        "doc_eid": "2-s2.0-84880552535",
        "doc_date": "2013-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Databases, Factual",
            "Internet",
            "MEDLINE",
            "Software"
        ],
        "doc_abstract": "The continuous growth of the biomedical scientific literature has been motivating the development of text-mining tools able to efficiently process all this information. Although numerous domain-specific solutions are available, there is no web-based concept-recognition system that combines the ability to select multiple concept types to annotate, to reference external databases and to automatically annotate nested and intercepted concepts. BeCAS, the Biomedical Concept Annotation System, is an API for biomedical concept identification and a web-based tool that addresses these limitations. MEDLINE abstracts or free text can be annotated directly in the web interface, where identified concepts are enriched with links to reference databases. Using its customizable widget, it can also be used to augment external web pages with concept highlighting features. Furthermore, all text-processing and annotation features are made available through an HTTP REST API, allowing integration in any text-processing pipeline.Availability: BeCAS is freely available for non-commercial use at http://bioinformatics.ua.pt/becas.Contacts: or jlo@ua.pt © 2013 The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A longitudinal assessment of acute cough",
        "doc_scopus_id": "84877111516",
        "doc_doi": "10.1164/rccm.201209-1686OC",
        "doc_eid": "2-s2.0-84877111516",
        "doc_date": "2013-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            },
            {
                "area_name": "Critical Care and Intensive Care Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Rationale: Cough can be assessed with visual analog scales (VAS), health status measures, and 24-hour cough frequency monitors (CF24). Evidence for their measurement properties in acute cough caused by upper respiratory tract infection (URTI) and longitudinal data is limited. Objectives: To assess cough long itudinally in URTI with subjective and objective outcome measures and determine sample size for future studies. Methods: Thirty-three previously healthy subjects with URTI completed cough VAS, Leicester Cough Questionnaire (LCQ-acute), and CF24 monitoring (Leicester Cough Monitor) on three occasions, 4 days apart. Changes in subjects' condition were assessed with a global rating of change questionnaire. The potential for baseline first-hour cough frequency (CF1), VAS, and LCQ to identify low CF24 was assessed. Measurements and Main Results: Mean ±D duration of cough at visit 1 was 4.1 ± 2.5 days. Geometric mean ± log SD baseline CF24 and median (interquartile range) cough bouts were high (14.9 ± 0.4 coughs/h and 85 [39-195] bouts/24 h). Health status was severely impaired. There was a significant reduction in CF24 and VAS, and improvement in LCQ, from visits 1-3. At visit 3, CF24 remained above normal limits in 52% of subjects. The smallest changes in CF 24, LCQ, and VAS that subjects perceived important were 54%,2-and 17-mm change from baseline, respectively. The sample sizes required for parallel group studies to detect these changes are 27, 51, and 25 subjects per group, respectively. CF1 (<20.5 coughs/h) was predictive of low CF 24. Conclusions: CF24, VAS, and LCQ are responsive outcome tools for the assessment of acute cough. The smallest change in cough frequency perceived important by subjects is 54%. The sample sizes required for future studies are modest and achievable. Copyright © 2013 by the American Thoracic Society.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gimli: Open source and high-performance biomedical name recognition",
        "doc_scopus_id": "84873744177",
        "doc_doi": "10.1186/1471-2105-14-54",
        "doc_eid": "2-s2.0-84873744177",
        "doc_date": "2013-02-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Automatic recognition",
            "Biomedical information extractions",
            "Configuration files",
            "Name recognition",
            "Named-entity recognition",
            "Optimized models",
            "System characteristics",
            "Underlying systems"
        ],
        "doc_abstract": "Background: Automatic recognition of biomedical names is an essential task in biomedical information extraction, presenting several complex and unsolved challenges. In recent years, various solutions have been implemented to tackle this problem. However, limitations regarding system characteristics, customization and usability still hinder their wider application outside text mining research.Results: We present Gimli, an open-source, state-of-the-art tool for automatic recognition of biomedical names. Gimli includes an extended set of implemented and user-selectable features, such as orthographic, morphological, linguistic-based, conjunctions and dictionary-based. A simple and fast method to combine different trained models is also provided. Gimli achieves an F-measure of 87.17% on GENETAG and 72.23% on JNLPBA corpus, significantly outperforming existing open-source solutions.Conclusions: Gimli is an off-the-shelf, ready to use tool for named-entity recognition, providing trained and optimized models for recognition of biomedical entities from scientific text. It can be used as a command line tool, offering full functionality, including training of new models and customization of the feature set and model parameters through a configuration file. Advanced users can integrate Gimli in their text mining workflows through the provided library, and extend or adapt its functionalities. Based on the underlying system characteristics and functionality, both for final users and developers, and on the reported performance results, we believe that Gimli is a state-of-the-art solution for biomedical NER, contributing to faster and better research in the field. Gimli is freely available at http://bioinformatics.ua.pt/gimli. © 2013 Campos et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Structuring and exploring the biomedical literature using latent semantics",
        "doc_scopus_id": "84891135595",
        "doc_doi": "10.1007/978-3-319-00551-5_72",
        "doc_eid": "2-s2.0-84891135595",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical fields",
            "Biomedical literature",
            "Knowledge exploration",
            "Latent Semantic Analysis",
            "Latent semantics",
            "Literature search",
            "Visualization technique",
            "Wealth of information"
        ],
        "doc_abstract": "The fast increasing amount of articles published in the biomedical field is creating difficulties in the way this wealth of information can be efficiently exploited by researchers. As a way of overcoming these limitations and potentiating a more efficient use of the literature, we propose an approach for structuring the results of a literature search based on the latent semantic information extracted from a corpus. Moreover, we show how the results of the Latent Semantic Analysis method can be adapted so as to evidence differences between results of different searches. We also propose different visualization techniques that can be applied to explore these results. Used in combination, these techniques could empower users with tools for literature guided knowledge exploration and discovery. © Springer International Publishing Switzerland 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analysing Relevant Diseases from Iberian Tweets",
        "doc_scopus_id": "84880375780",
        "doc_doi": "10.1007/978-3-319-00578-2_10",
        "doc_eid": "2-s2.0-84880375780",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Binary classification",
            "Eating disorders",
            "Feasible solution",
            "Health condition",
            "Information sources",
            "Micro-blogging services",
            "Personal health informations",
            "Social media"
        ],
        "doc_abstract": "The Internet constitutes a huge source of information that can be exploited by individuals in many different ways. With the increasing use of social networks and blogs, the Internet is now used not only as an information source but also to disseminate personal health information. In this paper we exploit the wealth of user-generated data, available through the micro-blogging service Twitter, to estimate and track the incidence of health conditions in society, specifically in Portugal and Spain. We present results for the acquisition of relevant tweets for a set of four different conditions (flu, depression, pregnancy and eating disorders) and for the binary classification of these tweets as relevant or not for each case. The results obtained, ranging in AUC from 0.7 to 0.87, are very promising and indicate that such approach provides a feasible solution for measuring and tracking the evolution of many health related aspects within the society. © Springer International Publishing Switzerland 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cough frequency in health and disease",
        "doc_scopus_id": "84872173633",
        "doc_doi": "10.1183/09031936.00089312",
        "doc_eid": "2-s2.0-84872173633",
        "doc_date": "2013-01-01",
        "doc_type": "Letter",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Harmonization of gene/protein annotations: Towards a gold standard medline",
        "doc_scopus_id": "84860473173",
        "doc_doi": "10.1093/bioinformatics/bts125",
        "doc_eid": "2-s2.0-84860473173",
        "doc_date": "2012-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Motivation: The recognition of named entities (NER) is an elementary task in biomedical text mining. A number of NER solutions have been proposed in recent years, taking advantage of available annotated corpora, terminological resources and machine-learning techniques. Currently, the best performing solutions combine the outputs from selected annotation solutions measured against a single corpus. However, little effort has been spent on a systematic analysis of methods harmonizing the annotation results and measuring against a combination of Gold Standard Corpora (GSCs). Results: We present Totum, a machine learning solution that harmonizes gene/protein annotations provided by heterogeneous NER solutions. It has been optimized and measured against a combination of manually curated GSCs. The performed experiments show that our approach improves the F-measure of state-of-the-art solutions by up to 10% (achieving ≈70%) in exact alignment and 22% (achieving ≈82%) in nested alignment. We demonstrate that our solution delivers reliable annotation results across the GSCs and it is an important contribution towards a homogeneous annotation of MEDLINE abstracts. © The Author 2012. Published by Oxford University Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Four-hour cough frequency monitoring in chronic cough",
        "doc_scopus_id": "84868611639",
        "doc_doi": "10.1378/chest.11-3309",
        "doc_eid": "2-s2.0-84868611639",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            },
            {
                "area_name": "Critical Care and Intensive Care Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2706"
            },
            {
                "area_name": "Cardiology and Cardiovascular Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2705"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: The recent development of automated cough monitors has enabled objective assessment of cough frequency. A study was undertaken to determine whether short-duration recordings (<6 h) accurately reflect 24-h cough frequency and to investigate their responsiveness. Methods: One hundred adults with chronic cough underwent 24-h cough frequency monitoring with the Leicester Cough Monitor and completed cough visual analog scales (VASs) and the Leicester Cough Questionnaire (LCQ). Cough recordings were analyzed using customized software to derive cough frequencies from 1 to 6 h and 24-h recordings. Responsiveness was assessed with repeat assessments following therapeutic trials. Results: The median (interquartile range) 24-h cough frequency was 11.5 (5.8-26.6) coughs/h. Four hours was considered the shortest recording duration that represented 24-h cough frequency (ρ= 0.9, P ≤.001). Median 4-h cough frequency was 16.6 (7.3-36.8) coughs/h. Both 4-h and 24-h cough frequency correlated moderately with cough VAS (ρ= 0.49, P ≤.01 and ρ= 0.44, P ≤.01) and LCQ (ρ= -0.48, P ≤.01;ρ= -0.50, P ≤.01). Four-hour cough frequency was responsive to improvements in cough severity following trials of therapy. Conclusions: Four-hour cough frequency correlates highly with 24-h cough frequency recordings and relates equally well with subjective measures in chronic cough. Short-duration cough monitoring could be a practical tool to validate the presence of cough and assess response to trials of therapy in the clinic setting. © 2012 American College of Chest Physicians.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Protein-Protein Interaction tasks of BioCreative III: Classification/ranking of articles and linking bio-ontology concepts to full text",
        "doc_scopus_id": "80053423937",
        "doc_doi": "10.1186/1471-2105-12-S8-S3",
        "doc_eid": "2-s2.0-80053423937",
        "doc_date": "2011-10-03",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Biomedical text minings",
            "Classification tasks",
            "Correlation coefficient",
            "Different granularities",
            "Interaction detection",
            "Machine learning methods",
            "Measuring performance",
            "Protein-protein interactions"
        ],
        "doc_abstract": "Background: Determining usefulness of biomedical text mining systems requires realistic task definition and data selection criteria without artificial constraints, measuring performance aspects that go beyond traditional metrics. The BioCreative III Protein-Protein Interaction (PPI) tasks were motivated by such considerations, trying to address aspects including how the end user would oversee the generated output, for instance by providing ranked results, textual evidence for human interpretation or measuring time savings by using automated systems. Detecting articles describing complex biological events like PPIs was addressed in the Article Classification Task (ACT), where participants were asked to implement tools for detecting PPI-describing abstracts. Therefore the BCIII-ACT corpus was provided, which includes a training, development and test set of over 12,000 PPI relevant and non-relevant PubMed abstracts labeled manually by domain experts and recording also the human classification times. The Interaction Method Task (IMT) went beyond abstracts and required mining for associations between more than 3,500 full text articles and interaction detection method ontology concepts that had been applied to detect the PPIs reported in them.Results: A total of 11 teams participated in at least one of the two PPI tasks (10 in ACT and 8 in the IMT) and a total of 62 persons were involved either as participants or in preparing data sets/evaluating these tasks. Per task, each team was allowed to submit five runs offline and another five online via the BioCreative Meta-Server. From the 52 runs submitted for the ACT, the highest Matthew's Correlation Coefficient (MCC) score measured was 0.55 at an accuracy of 89% and the best AUC iP/R was 68%. Most ACT teams explored machine learning methods, some of them also used lexical resources like MeSH terms, PSI-MI concepts or particular lists of verbs and nouns, some integrated NER approaches. For the IMT, a total of 42 runs were evaluated by comparing systems against manually generated annotations done by curators from the BioGRID and MINT databases. The highest AUC iP/R achieved by any run was 53%, the best MCC score 0.55. In case of competitive systems with an acceptable recall (above 35%) the macro-averaged precision ranged between 50% and 80%, with a maximum F-Score of 55%.Conclusions: The results of the ACT task of BioCreative III indicate that classification of large unbalanced article collections reflecting the real class imbalance is still challenging. Nevertheless, text-mining tools that report ranked lists of relevant articles for manual selection can potentially reduce the time needed to identify half of the relevant articles to less than 1/4 of the time when compared to unranked results. Detecting associations between full text articles and interaction detection method PSI-MI terms (IMT) is more difficult than might be anticipated. This is due to the variability of method term mentions, errors resulting from pre-processing of articles provided as PDF files, and the heterogeneity and different granularity of method term concepts encountered in the ontology. However, combining the sophisticated techniques developed by the participants with supporting evidence strings derived from the articles for human interpretation could result in practical modules for biological annotation workflows. © 2011 Krallinger et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The gene normalization task in BioCreative III",
        "doc_scopus_id": "80052774027",
        "doc_doi": "10.1186/1471-2105-12-S8-S2",
        "doc_eid": "2-s2.0-80052774027",
        "doc_date": "2011-10-03",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Comparative performance",
            "Expectation-maximization algorithms",
            "Gold standards",
            "Human annotations",
            "Manual annotation",
            "Participating teams",
            "Past and present",
            "Team performance"
        ],
        "doc_abstract": "Background: We report the Gene Normalization (GN) challenge in BioCreative III where participating teams were asked to return a ranked list of identifiers of the genes detected in full-text articles. For training, 32 fully and 500 partially annotated articles were prepared. A total of 507 articles were selected as the test set. Due to the high annotation cost, it was not feasible to obtain gold-standard human annotations for all test articles. Instead, we developed an Expectation Maximization (EM) algorithm approach for choosing a small number of test articles for manual annotation that were most capable of differentiating team performance. Moreover, the same algorithm was subsequently used for inferring ground truth based solely on team submissions. We report team performance on both gold standard and inferred ground truth using a newly proposed metric called Threshold Average Precision (TAP-k).Results: We received a total of 37 runs from 14 different teams for the task. When evaluated using the gold-standard annotations of the 50 articles, the highest TAP-k scores were 0.3297 (k=5), 0.3538 (k=10), and 0.3535 (k=20), respectively. Higher TAP-k scores of 0.4916 (k=5, 10, 20) were observed when evaluated using the inferred ground truth over the full test set. When combining team results using machine learning, the best composite system achieved TAP-k scores of 0.3707 (k=5), 0.4311 (k=10), and 0.4477 (k=20) on the gold standard, representing improvements of 12.4%, 21.8%, and 26.6% over the best team results, respectively.Conclusions: By using full text and being species non-specific, the GN task in BioCreative III has moved closer to a real literature curation task than similar tasks in the past and presents additional challenges for the text mining community, as revealed in the overall team results. By evaluating teams using the gold standard, we show that the EM algorithm allows team submissions to be differentiated while keeping the manual annotation effort feasible. Using the inferred ground truth we show measures of comparative performance between teams. Finally, by comparing team rankings on gold standard vs. inferred ground truth, we further demonstrate that the inferred ground truth is as effective as the gold standard for detecting good team performance. © 2011 Lu et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Prioritizing literature search results using a training set of classified documents",
        "doc_scopus_id": "80052949641",
        "doc_doi": "10.1007/978-3-642-19914-1_49",
        "doc_eid": "2-s2.0-80052949641",
        "doc_date": "2011-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Area of interest",
            "Biomedical fields",
            "Biomedical literature",
            "Classified documents",
            "Literature search",
            "Protein-protein Interactions",
            "Ranking strategy",
            "Rapid expansion",
            "Scientific literature",
            "Test sets",
            "Training sets"
        ],
        "doc_abstract": "Finding relevant articles is rapidly becoming a demanding task for researchers in the biomedical field, due to the rapid expansion of the scientific literature. We investigate the use of ranking strategies for prioritizing literature search results given an initial topic of interest. Focusing on the topic of protein-protein interactions, we compared ranking strategies based on different classifiers and features. The best result obtained on the BioCreative III PPI test set was an area under the interpolated precision-recall curve of 0,629. We then analyze the use of this method for ranking the result of PubMed queries. The results shown indicate that this strategy can be used by database curators to prioritize articles for extraction of protein-protein interactions, and also by general researchers looking for publications describing protein-protein interactions within a particular area of interest. © 2011 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Classification methods for finding articles describing protein-protein interactions in PubMed.",
        "doc_scopus_id": "84855546021",
        "doc_doi": "10.1515/jib-2011-178",
        "doc_eid": "2-s2.0-84855546021",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "With the rapid expansion in the number of published papers in the biomedical field, finding relevant articles has become a demanding task for researchers. This has led to increasing interest in the use of text mining tools that help search the literature and identify the most relevant documents or information. One specific topic of interest is related to the identification of articles that might be used for extracting protein-protein interactions. Using the BioCreative III Article Classification Task dataset, composed of PubMed abstracts classified as relevant or non-relevant for describing protein-protein interactions, we compare different classification methods with different sets of features. The best results--area under the interpolated precision-recall curve of 0.654--indicate that the proposed classification strategy could be incorporated in the database curation workflows in order to prioritize articles for extraction of protein-protein interactions. Furthermore, we also analysed the use of this method for ranking documents resulting from general PubMed queries, and propose that this approach could be useful for general researchers looking for publications describing protein-protein interactions within a particular topic of interest. Copyright 2011 The Author(s). Published by Journal of Integrative Bioinformatics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Recognition of gene/protein names using conditional random fields",
        "doc_scopus_id": "78651435591",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78651435591",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Gene/Protein names",
            "Machine-learning",
            "Named entity recognition",
            "NAtural language processing",
            "Text mining"
        ],
        "doc_abstract": "With the overwhelming amount of publicly available data in the biomedical field, traditional tasks performed by expert database annotators rapidly became hard and very expensive. This situation led to the development of computerized systems to extract information in a structured manner. The first step of such systems requires the identification of named entities (e.g. gene/protein names), a task called Named Entity Recognition (NER). Much of the current research to tackle this problem is based on Machine Learning (ML) techniques, which demand careful and sensitive definition of the several used methods. This article presents a NER system using Conditional Random Fields (CRFs) as the machine learning technique, combining the best techniques recently described in the literature. The proposed system uses biomedical knowledge and a large set of orthographic and morphological features. An F-measure of 0,7936 was obtained on the BioCreative II Gene Mention corpus, achieving a significantly better performance than similar baseline systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concept-based query expansion for retrieving gene related publications from MEDLINE",
        "doc_scopus_id": "77951569837",
        "doc_doi": "10.1186/1471-2105-11-212",
        "doc_eid": "2-s2.0-77951569837",
        "doc_date": "2010-04-28",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Alternative solutions",
            "Biological resources",
            "Exponential increase",
            "High-throughput method",
            "Relevant documents",
            "Retrieved documents",
            "Scientific literature",
            "Scientific publications"
        ],
        "doc_abstract": "Background: Advances in biotechnology and in high-throughput methods for gene analysis have contributed to an exponential increase in the number of scientific publications in these fields of study. While much of the data and results described in these articles are entered and annotated in the various existing biomedical databases, the scientific literature is still the major source of information. There is, therefore, a growing need for text mining and information retrieval tools to help researchers find the relevant articles for their study. To tackle this, several tools have been proposed to provide alternative solutions for specific user requests.Results: This paper presents QuExT, a new PubMed-based document retrieval and prioritization tool that, from a given list of genes, searches for the most relevant results from the literature. QuExT follows a concept-oriented query expansion methodology to find documents containing concepts related to the genes in the user input, such as protein and pathway names. The retrieved documents are ranked according to user-definable weights assigned to each concept class. By changing these weights, users can modify the ranking of the results in order to focus on documents dealing with a specific concept. The method's performance was evaluated using data from the 2004 TREC genomics track, producing a mean average precision of 0.425, with an average of 4.8 and 31.3 relevant documents within the top 10 and 100 retrieved abstracts, respectively.Conclusions: QuExT implements a concept-based query expansion scheme that leverages gene-related information available on a variety of biological resources. The main advantage of the system is to give the user control over the ranking of the results by means of a simple weighting scheme. Using this approach, researchers can effortlessly explore the literature regarding a group of genes and focus on the different aspects relating to these genes. © 2010 Matos et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Long-term low-dose erythromycin in patients with unexplained chronic cough: A double-blind placebo controlled trial",
        "doc_scopus_id": "78649636793",
        "doc_doi": "10.1136/thx.2010.142711",
        "doc_eid": "2-s2.0-78649636793",
        "doc_date": "2010-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Aims: Unexplained chronic cough is a common condition with no satisfactory treatments. Previous work has suggested that cough may be linked to neutrophilic airway inflammation. This study tested the hypothesis that long-term low-dose erythromycin reduces the induced sputum neutrophil count and 24 h cough frequency in patients with unexplained chronic cough. Methods: 30 patients with an unexplained chronic cough lasting more than 8 weeks were randomly assigned to take 250 mg erythromycin once daily (n=15) or placebo (n=15) for 12 weeks in a double-blind parallel group study. Cough frequency, cough reflex sensitivity and cough severity were assessed at baseline, 6, 12 and 24 weeks. The primary outcome measure was change in 24 h cough frequency at 12 weeks. Results: There was no difference in the change in cough frequency between the erythromycin and placebo groups at 12 weeks (mean difference in fold change 1.1; 95% CI 0.7 to 1.5; p=0.585) or at other times. There was a statistically significant between-treatment difference in the change in sputum neutrophils at 12 weeks (-10.2% vs +6.6% with erythromycin and placebo; mean difference 16.8%; 95% CI 1.6 to 32.1; p=0.03) but not at other times. There was no difference in the change in other measures of cough between treatments. Conclusions: Treatment with low-dose erythromycin for 12 weeks reduces the induced sputum neutrophil count but not cough frequency or severity in patients with unexplained chronic cough.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Syntactic parsing for bio-molecular event detection from scientific literature",
        "doc_scopus_id": "71049149316",
        "doc_doi": "10.1007/978-3-642-04686-5_7",
        "doc_eid": "2-s2.0-71049149316",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bio-molecular events",
            "Biomedical literature",
            "Information extraction",
            "Semantic properties",
            "Syntactic parsing"
        ],
        "doc_abstract": "Rapid advances in science and in laboratorial and computing methods are generating vast amounts of data and scientific literature. In order to keep up-to-date with the expanding knowledge in their field of study, researchers are facing an increasing need for tools that help manage this information. In the genomics field, various databases have been created to save information in a formalized and easily accessible form. However, human curators are not capable of updating these databases at the same rate new studies are published. Advanced and robust text mining tools that automatically extract newly published information from scientific articles are required. This paper presents a methodology, based on syntactic parsing, for identification of gene events from the scientific literature. Evaluation of the proposed approach, based on the BioNLP shared task on event extraction, produced an average F-score of 47.1, for six event types. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Leicester cough monitor: A semi-automated, semi-validated cough detection system? (European Respiratory Journal (2008) 32, (530-531))",
        "doc_scopus_id": "58849144267",
        "doc_doi": "10.1183/09031936.50060808",
        "doc_eid": "2-s2.0-58849144267",
        "doc_date": "2009-01-01",
        "doc_type": "Erratum",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From the authors",
        "doc_scopus_id": "58849131973",
        "doc_doi": "10.1183/09031936.00060808",
        "doc_eid": "2-s2.0-58849131973",
        "doc_date": "2008-08-01",
        "doc_type": "Letter",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Leicester Cough Monitor: Preliminary validation of an automated cough detection system in chronic cough",
        "doc_scopus_id": "47049099230",
        "doc_doi": "10.1183/09031936.00057407",
        "doc_eid": "2-s2.0-47049099230",
        "doc_date": "2008-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Chronic cough is a common condition that presents to both primary and secondary care. Assessment and management are hampered by the absence of well-validated outcome measures. The present study comprises the validation of the Leicester Cough Monitor (LCM), an automated sound-based ambulatory cough monitor. Cough frequency was measured with the LCM and compared with coughs and other sounds counted manually over 2 h of a 6-h recording by two observers in nine patients with chronic cough in order to determine the sensitivity and specificity of the LCM. Automated cough frequency was also compared with manual counts from one observer in 15 patients with chronic cough and eight healthy subjects. All subjects underwent 6-h recordings. A subgroup consisting of six control and five patients with stable chronic cough underwent repeat automated measurements ≥3 months apart. A further 50 patients with chronic cough underwent 24-h automated cough monitoring. The LCM had a sensitivity and specificity of 91 and 99%, respectively, for detecting cough and a false-positive rate of 2.5 events·h-1. Mean±SEM automated cough counts·patient·h-1 was 48±9 in patients with chronic cough and 2±1 in the control group (mean difference 46 counts·patient·h-1; 95% confidence interval (CI) 20-71). The automated cough counts were repeatable (intra-subject SD 11.4 coughs·patient·h-1; intra-class correlation coefficient 0.9). The cough frequency in patients undergoing 24-h automated monitoring was 19 coughs·patient·h-1; daytime (08:00-22:00 h) cough frequency was significantly greater than overnight cough frequency (25 versus 10 coughs·patient·h-1; mean difference 15 coughs·patient·h-1, 95% CI 8-22). The Leicester Cough Monitor is a valid and reliable tool that can be used to assess 24-h cough frequency in patients with cough. It should be a useful tool to assess patients with cough in clinical trials and longitudinal studies. Copyright©ERS Journals Ltd 2008.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Obstructive sleep apnoea: A cause of chronic cough",
        "doc_scopus_id": "34547183940",
        "doc_doi": "10.1186/1745-9974-3-7",
        "doc_eid": "2-s2.0-34547183940",
        "doc_date": "2007-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Otorhinolaryngology",
                "area_abbreviation": "MEDI",
                "area_code": "2733"
            },
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Chronic cough is a common reason for presentation to both general practice and respiratory clinics. In up to 25% of cases, the cause remains unclear after extensive investigations. We report 4 patients presenting with an isolated chronic cough who were subsequently found to have obstructive sleep apnoea. The cough improved rapidly with nocturnal continuous positive airway pressure therapy. Further studies are required to investigate the prevalence of coexistence of these common conditions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An automated system for 24-h monitoring of cough frequency: The Leicester cough monitor",
        "doc_scopus_id": "34447509649",
        "doc_doi": "10.1109/TBME.2007.900811",
        "doc_eid": "2-s2.0-34447509649",
        "doc_date": "2007-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Automatic analysis",
            "Biomedical monitoring",
            "Biomedical signal detection",
            "Cough",
            "Statistical models"
        ],
        "doc_abstract": "The objective monitoring of cough for extended periods of time has long been recognized as an important step towards a better understanding of this symptom, and a better management of chronic cough patients. In this paper, we present a system for the automatic analysis of 24-h, continuous, ambulatory recordings of cough. The system uses audio recordings from a miniature microphone and the detection algorithm is based on statistical models of the time-spectral characteristics of cough sounds. We validated the system against manual counts obtained by a trained observer on 40 ambulatory recordings and our results show a median sensitivity value of 85.7%, median positive predictive value of 94.7% and median false positive rate of 0.8 events/h. An analysis application was developed, with a graphical user interface, allowing the use of the system in clinical settings by technical or medical staff. The result of the analysis of a recording session is presented as a concise, graphical-based report. The modular nature of the system interface facilitates its enhancement with the integration of further modules. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of cough signals in continuous audio recordings using hidden Markov models",
        "doc_scopus_id": "33744988731",
        "doc_doi": "10.1109/TBME.2006.873548",
        "doc_eid": "2-s2.0-33744988731",
        "doc_date": "2006-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Automatic detection",
            "Cough counts",
            "Cough monitor",
            "Hidden Markov models"
        ],
        "doc_abstract": "Cough is a common symptom of many respiratory diseases. The evaluation of its intensity and frequency of occurrence could provide valuable clinical information in the assessment of patients with chronic cough. In this paper we propose the use of hidden Markov models (HMMs) to automatically detect cough sounds from continuous ambulatory recordings. The recording system consists of a digital sound recorder and a microphone attached to the patient's chest. The recognition algorithm follows a keyword-spotting approach, with cough sounds representing the keywords. It was trained on 821 min selected from 10 ambulatory recordings, including 2473 manually labeled cough events, and tested on a database of nine recordings from separate patients with a total recording time of 3060 min and comprising 2155 cough events. The average detection rate was 82% at a false alarm rate of seven events/h, when considering only events above an energy threshold relative to each recording's average energy. These results suggest that HMMs can be applied to the detection of cough sounds from ambulatory patients. A postprocessing stage to perform a more detailed analysis on the detected events is under development, and could allow the rejection of some of the incorrectly detected events. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cough frequency, cough sensitivity and health status in patients with chronic cough",
        "doc_scopus_id": "33646029390",
        "doc_doi": "10.1016/j.rmed.2005.09.023",
        "doc_eid": "2-s2.0-33646029390",
        "doc_date": "2006-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pulmonary and Respiratory Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2740"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: Little is known about the frequency of cough in health and in patients with chronic cough. Methods: We measured cough frequency and its relationship with other markers of cough severity in 20 patients with chronic cough and 9 healthy subjects using the Leicester Cough Monitor (LCM), which is an automated ambulatory digital cough monitor that records sound only. All subjects had a 6-h recording and recordings were manually counted. A subgroup of 6 normals and 6 patients with a stable chronic cough had repeat measurements up to 6 months apart. Results: Mean (sem) cough counts/hour were 43(8) in patients with chronic cough and 2(1) in normals (mean difference 41; 95% confidence interval 24-59; P < 0.001). The cough counts were repeatable (within subject standard deviation: 23 coughs/hour; intraclass correlation coefficient 0.8). Cough counts correlated significantly with physical ( r = - 0.6, P = 0.03), social ( r = - 0.7, P = 0.01) and total Leicester Cough Questionnaire (LCQ) health status scores ( r = - 0.6, P = 0.03) and cough sensitivity (concentration of capsaicin causing 5 coughs: r = 0.9, P = 0.008). Conclusion: We have shown that there are marked differences in cough frequency between patients with chronic cough and healthy subjects, that these measurements are repeatable, and that they correlate with cough-specific health status. © 2005 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272432 291210 291911 31 80 Respiratory Medicine RESPIRATORYMEDICINE 2005-11-02 2005-11-02 2010-03-29T17:02:51 S0954-6111(05)00378-1 S0954611105003781 10.1016/j.rmed.2005.09.023 S300 S300.1 FULL-TEXT 2015-05-15T03:38:37.732967-04:00 0 0 20060601 20060630 2006 2005-11-02T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav absattachment articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 0954-6111 09546111 100 100 6 6 Volume 100, Issue 6 22 1105 1109 1105 1109 200606 June 2006 2006-06-01 2006-06-30 2006 Original Articles article fla Copyright © 2005 Elsevier Ltd. All rights reserved. COUGHFREQUENCYCOUGHSENSITIVITYHEALTHSTATUSINPATIENTSCHRONICCOUGH BIRRING S Introduction Methods Subjects Cough monitor Protocol and clinical measurement Analysis Results Discussion Acknowledgements References IRWIN 1981 413 417 R BIRRING 2004 199 201 S BIRRING 2003 339 343 S FRENCH 2002 1123 1131 C PRUDON 2005 550 557 B HSU 1994 1246 1253 J LOUDON 1967 1137 1143 R CHUNG 2002 267 272 K BRIGHTLING 1999 406 410 C IRWIN 1998 133S 181S R MUNYARD 1996 531 534 P CHANG 1997 1637 1639 A PAVESI 2001 1121 1128 L ZHENG 1997 864 865 S BIRRINGX2006X1105 BIRRINGX2006X1105X1109 BIRRINGX2006X1105XS BIRRINGX2006X1105X1109XS 2013-07-18T13:44:46Z OA-Window ElsevierBranded Full item S0954-6111(05)00378-1 S0954611105003781 10.1016/j.rmed.2005.09.023 272432 2010-10-08T05:28:57.813452-04:00 2006-06-01 2006-06-30 true 155094 MAIN 5 59308 849 656 IMAGE-WEB-PDF 1 si37 341 13 63 si36 296 13 54 si35 248 13 49 si34 308 13 56 si33 248 13 48 si32 271 13 54 si31 232 13 49 si30 252 13 54 si29 232 13 49 si28 307 13 61 si27 247 13 62 si26 297 13 61 si25 263 13 62 si24 284 13 60 si23 241 13 62 si22 297 13 61 si21 263 13 62 si20 343 13 70 si19 322 13 70 si18 332 13 62 si17 247 16 49 si16 247 16 49 si15 257 16 49 si14 257 16 49 si13 268 16 49 si12 268 16 49 si11 268 16 49 si10 255 16 49 si9 true 340 13 69 si8 true 248 13 49 si7 true 297 13 61 si6 true 263 13 62 si5 true 284 13 60 si4 true 241 13 62 si3 true 297 13 61 si2 true 263 13 62 si1 true 332 13 62 gr1 15132 282 351 gr1 1755 93 116 gr3 32449 651 350 gr3 1472 93 50 gr2 19200 140 351 gr2 2226 50 125 YRMED 2162 S0954-6111(05)00378-1 10.1016/j.rmed.2005.09.023 Elsevier Ltd Figure 1 Cough counts/hour in healthy controls and patients with chronic cough. Mean (sem). Figure 2 Mean cough counts for each hour of recording. Figure 3 Relationship between cough counts per hour and cough reflex sensitivity. C 2 and C 5 (μmol/L): concentration of capsaicin that causes 2 and 5 coughs, respectively. Table 1 Subject characteristics. Normal Chronic cough P values Number (male) 9 (0) 20 (5) — Age (years) 48 (3) 53 (3) 0.1 Cough duration (years) — 6 (2) — FEV1%predicted 92 (4) 92 (4) 0.9 FEV1/FVC (%) 80 (1) 76 (2) 0.3 Data expressed as mean (sem); FEV1: forced expiratory volume in 1s; FVC: forced vital capacity. Table 2 Relationship between cough frequency and other parameters of cough severity in subjects with chronic cough. Cough frequency C 2 LCQ Total score Cough frequency — 0.8* 0.6* C 2 — −0.1 LCQ total score — Data expressed as Pearson's correlation coefficient (r). C 2: concentration of capsaicin causing 2 coughs; LCQ: Leicester Cough Questionnaire. * P < 0.05 . ☆ This study was funded by University Hospitals of Leicester NHS Trust. Surinder Birring is a British Lung Foundation Clinical Research Fellow. Cough frequency, cough sensitivity and health status in patients with chronic cough Surinder S. Birring a ⁎ Sergio Matos b Ronnak B. Patel a Benjamin Prudon a David H. Evans b Ian D. Pavord a a Institute for Lung Health, Department of Respiratory Medicine, Glenfield Hospital, Leicester, LE3 9QP, UK b Department of Medical Physics, Leicester Royal Infirmary, Leicester, UK ⁎ Corresponding author. Tel.: +441162871471; fax: +441162367768. Background Little is known about the frequency of cough in health and in patients with chronic cough. Methods We measured cough frequency and its relationship with other markers of cough severity in 20 patients with chronic cough and 9 healthy subjects using the Leicester Cough Monitor (LCM), which is an automated ambulatory digital cough monitor that records sound only. All subjects had a 6-h recording and recordings were manually counted. A subgroup of 6 normals and 6 patients with a stable chronic cough had repeat measurements up to 6 months apart. Results Mean (sem) cough counts/hour were 43(8) in patients with chronic cough and 2(1) in normals (mean difference 41; 95% confidence interval 24–59; P < 0.001 ). The cough counts were repeatable (within subject standard deviation: 23 coughs/hour; intraclass correlation coefficient 0.8). Cough counts correlated significantly with physical ( r = - 0.6 , P = 0.03 ), social ( r = - 0.7 , P = 0.01 ) and total Leicester Cough Questionnaire (LCQ) health status scores ( r = - 0.6 , P = 0.03 ) and cough sensitivity (concentration of capsaicin causing 5 coughs: r = 0.9 , P = 0.008 ). Conclusion We have shown that there are marked differences in cough frequency between patients with chronic cough and healthy subjects, that these measurements are repeatable, and that they correlate with cough-specific health status. Keywords Chronic cough Cough monitor Cough counts Cough sensitivity Quality of life Health status Introduction Cough is one of the commonest causes of presentation to general practice. Most cases are acute and self-limiting although a significant minority are referred for a specialist opinion with an isolated persistent chronic cough. The assessment of patients with chronic cough is commonly based on the anatomical, diagnostic protocol 1 which is a systematic evaluation based on the understanding that most cases are due to disease of the upper respiratory tract where cough receptors are most plentiful. Treatment trials form an important part of the assessment of patients with chronic cough. However, there are few well-validated outcome measures to assess treatment efficacy. Cough visual analogue scores, 2 cough specific health status questionnaires, 3,4 cough reflex sensitivity measurement 5 and cough monitors 6 have been proposed as potential tools to assess cough. The subjective nature of symptom scores and health status questionnaires and the poor specificity of cough reflex sensitivity measurement to identify patients with chronic cough 5 has led to a renewed interest in the development of automated ambulatory cough monitors. 7,8 Current cough monitors are limited by expense and size, reliance on combined sound and electromyographic signals and are poorly validated in the chronic cough setting. As a result, little is known about cough frequency in healthy adults and patients with chronic cough and there is no information on repeatability, and the relationship between cough frequency to other parameters of cough severity such as cough sensitivity and health status. These factors are important determinants of the clinical usefulness of cough frequency measurement and the pursual of automated cough monitors. We have utilised recent advances in digital recording technology to develop the Leicester Cough Monitor (LCM®), an ambulatory cough monitor that records sound only. The aim of this preliminary validation study was to assess cough frequency and its repeatability in healthy adults and patients with chronic cough and assess the relationship between cough frequency, cough sensitivity and cough specific health status in patients with chronic cough. Methods Subjects Twenty consecutive patients with an isolated chronic cough (>3 weeks duration) were recruited from a specialised cough clinic. The causes of cough in patients with chronic cough were: cough variant asthma ( n = 5 ) , eosinophilic bronchitis ( n = 3 ) , gastro-oesophageal reflux ( n = 3 ) , idiopathic ( n = 3 ) , post-viral ( n = 2 ) , bronchiectasis ( n = 2 ) , chronic bronchitis ( n = 1 ) and chronic obstructive pulmonary disease ( n = 1 ) . The clinic receives referrals from primary and secondary care largely confined to a population of 970,000 within Leicestershire. Nine normal controls were recruited from healthy volunteers responding to local advertising. Investigations were carried out according to a standardised algorithm. 9 The protocol for investigation and treatment, and criteria for accepting diagnosis were as previously described. 9,10 Normal subjects were asymptomatic, non-smokers and had normal spirometry and a methacholine PC20 FEV1>16mg/mL. No subjects had received corticosteroids or other specific treatment for the condition causing cough for at least six weeks prior to the study. Six healthy subjects and 6 patients with a stable chronic cough (3 with cough variant asthma, 1 with gastro-oesophageal reflux associated cough, 1 with bronchiectasis and 1 with idiopathic chronic cough) participated in cough frequency repeatability studies. A randomly selected subgroup of 8 healthy subjects and 7 patients with chronic cough also had cough reflex sensitivity measurement. All subjects gave full informed consent to participate. The protocol for this study was approved by the Leicestershire Research Ethics Committee. Cough monitor The LCM® is a digital ambulatory cough monitor (personal stereo size) that records sound from a free field microphone attached to the anterior chest wall. Data stored on the recorder is downloaded onto a computer when the recording is complete where it is analysed by a cough detection algorithm. The current data was generated by manual counting since the cough detection algorithm is currently under development and is not fully validated. For each subject, the entire recording was analysed by an experienced observer and each cough was identified separately whether occurring singly or in a cluster or ‘epoch’ of coughs. Protocol and clinical measurement The cough monitor was attached at 9am in all subjects and returned 6h later. Subjects were told that the LCM was a new investigative tool being developed to assess the nature of the cough and were encouraged to resume their normal activity in their usual environment. Cough sensitivity was assessed after the cough recording with capsaicin cough challenge test using a dosimeter method standardised to limit inspiratory flow to 0.5L/s. 5 Cough specific health status was assessed in patients with chronic cough with the Leicester Cough Questionnaire (LCQ) 3 which is a 19 item, self-completed, well validated cough-specific health status questionnaire that has 3 domains (physical, psychological and social). The range for total LCQ score is 3–21 where a higher score indicates a better health status and the range for domains scores is 1–7. To assess repeatability, subjects with a stable chronic cough had a second cough frequency measurement three to six months after the first, at the same time of day in order to avoid possible bias from diurnal variations. Analysis Subject characteristics were described using descriptive statistics and expressed as means (standard error). The concentration of capsaicin that causes 2 and 5 coughs (C 2 and C 5 μmol/L) were calculated by linear interpolation of the log-dose–response curves and described as geometric mean (log-sem). Cough frequency was expressed as individual coughs per hour for the duration of the recording. Comparisons of cough frequency, health status and cough sensitivity were undertaken using unpaired t-tests. Correlations between variables were analysed using Pearson's correlation coefficient (r). Repeatability data was assessed as the intraclass correlation coefficients. Results The subject characteristics are as shown (Table 1 ). Mean (sem) cough counts/hour were 43(8) in patients with chronic cough and 2(1) in normals (mean difference 41; 95% confidence interval of difference 24–59; P < 0.001 ; Fig. 1 ). There were no significant differences in cough frequency between diagnostic groups or gender in patients with chronic cough. Cough frequency decreased with increasing time of recording in patients with chronic cough but not normals (Fig. 2 ). The mean (sem) LCQ cough specific health status scores in patients with chronic cough were total: 13.1 (1.0), physical: 4.3 (0.3), psychological: 4.5 (0.4) and social: 4.3 (0.3). Patients with a chronic cough had heightened cough reflex sensitivity compared with the control group for both geometric mean C 2 (3.3 vs. 12.5μmol/L; mean difference 1.9 doubling doses (DD); 95% confidence interval of difference 0.4–3.4 DD; P = 0.017 ) and C 5 (33.7 vs. 266.7μmol/L; mean difference 3.0 DD; 95% CI of difference 0.3–5.7 DD; P = 0.035 ). Cough counts per hour in patients with chronic cough correlated significantly with physical ( r = - 0.6 , P = 0.03 ), social ( r = - 0.7 , P = 0.01 ) and total LCQ scores ( r = - 0.6 , P = 0.03 ) but not psychological scores ( r = - 0.5 , P = 0.08 ; Table 2 ). There was no correlation between cough counts per hour in normal subjects and cough sensitivity (C 2: r = 0.2 , P = 0.7 and C 5: r = 0.2 , P = 0.6 ). There was a significant correlation between cough frequency and cough sensitivity in patients with chronic cough (C 2: r = 0.8 , P < 0.05 ; C 5: r = 0.9 , P < 0.01 ; Fig. 3 ). There were no significant correlations between LCQ scores and cough sensitivity in patients with chronic cough. The cough counts were repeatable in the 12 subjects that underwent repeatability testing (within subject sd: 23 coughs/hour; intraclass correlation coefficient 0.8) and the tendency for cough frequency to decrease with time was also evident in the second recording in patients with chronic cough but not healthy subjects. Discussion This is the first study to investigate the relationship between cough frequency, cough reflex sensitivity and cough specific health status in adult patients with chronic cough. We found large differences in cough frequency between patients with chronic cough and healthy controls and we have shown that this measure was repeatable. Cough frequency was related to cough reflex sensitivity and cough specific health status in patients with chronic cough but not in healthy controls. Our findings suggest that daytime cough frequency measurement is potentially useful in the assessment of patients with chronic cough. We found that cough was present in healthy subjects but that it was significantly more frequent in patients with chronic cough, consistent with cough frequency data from combined electromyographic and sound signals reported by others. 6,11,12 Pavesi et al. 13 have developed a computerised cough monitor that has been evaluated in subjects with acute cough in a confined setting but has not been assessed or validated in patients with chronic cough. We used manual cough counts from the entire recording to measure cough frequency since they are considered the gold standard and are free from the false positives seen with automated recordings due to incorrect categorisation of other sounds such as sneezing, throat clearing and speech. The manual counting process is very time consuming and not practical for clinical practice, so there is a need for automated cough detection algorithms. The data from this study provides a strong basis to pursue the development of automated cough monitors. There was a tendency for cough counts to decrease with time in patients with chronic cough suggesting that there is diurnal variation in cough frequency as reported by others. 6,14 The alternative possibility that cough counts decreased as subjects adjusted to the cough monitor seems unlikely as the same pattern was seen in repeat recordings of patients with chronic cough. Furthermore, healthy subjects did not have a reduction in cough frequency with time. Patients with chronic cough, cough very little at night 6 so we doubt that confining our recordings to daytime affected the validity of our measure. However, when advances in battery life allow, further work is required to investigate the relationship between 6 and 24-h recordings. We did not find differences in cough frequency between different diagnostic groups or gender in patients with chronic cough. The numbers involved in this study were small and it was not our aim to study disease specific cough frequency so it is possible that differences could have been missed due to lack of power. This was a preliminary study to investigate the range and repeatability of cough frequency measurement and assess its relationship with other markers of chronic cough. The results of this study indicate that cough frequency measurement shows promise as a method of validating the presence of chronic cough and monitoring the response to treatment and should encourage the further development of this technique. Unexpectedly, our findings suggest a positive relationship between cough frequency and cough reflex sensitivity. This may have been a chance finding and larger studies are required. However, the relationship between these parameters is likely to be complex since a heightened cough reflex sensitivity is not always associated with chronic cough and many patients with chronic cough have normal cough sensitivity. 5 More work is required since the cough frequency in healthy subjects with heightened cough reflex sensitivity is not known. There are several potential uses for cough monitors such as the LCM. It can be used to validate the presence of cough, assess its frequency and identify patients with an altered perception of cough rather than increased frequency. It can be used to assess the response to treatment trials which form an integral part of the anatomical, diagnostic protocol widely used to investigate patients. Since cough can be a prominent feature of airway diseases such as asthma and COPD, cough monitors may be used to assess a wider range of disorders. Finally, cough monitors may have a role in monitoring paediatric asthma where other objective recordings such as peak expiratory flow may not be possible. In conclusion, we have shown large differences in cough frequency between patients with chronic cough and healthy subjects and that daytime cough frequency relates to health status. Our preliminary findings should stimulate the development of more practical, automated detection systems and the use of cough monitoring in validation and monitoring of chronic cough in clinical practice. Acknowledgements We would like to thank the subjects who participated in the study, and Debbie Parker, Natalie Neale, Dhiraj D. Vara and the Department of Respiratory Physiology for assistance in the clinical characterisation of some of the patients. References 1 R.S. Irwin W.M. Corrao M.R. Pratter Chronic persistent cough in the adult: the spectrum and frequency of causes and successful outcome of specific therapy Am Rev Respir Dis 123 4 Part 1 1981 413 417 2 S.S. Birring C. Passant R.B. Patel B. Prudon G.E. Murty I.D. Pavord Chronic tonsillar enlargement and cough: preliminary evidence of a novel and treatable cause of chronic cough Eur Respir J 23 2 2004 199 201 3 S.S. Birring B. Prudon A.J. Carr S.J. Singh M.D. Morgan I.D. Pavord Development of a symptom specific health status measure for patients with chronic cough: Leicester cough questionnaire (LCQ) Thorax 58 4 2003 339 343 4 C.T. French R.S. Irwin K.E. Fletcher T.M. Adams Evaluation of a cough-specific quality-of-life questionnaire Chest 121 4 2002 1123 1131 5 B. Prudon S.S. Birring D.D. Vara A.P. Hall J.P. Thompson I.D. Pavord Cough and glottic stop reflex sensitivity in health and disease Chest 127 2005 550 557 6 J.Y. Hsu R.A. Stone R.B. Logan-Sinclair M. Worsdell C.M. Busst K.F. Chung Coughing frequency in patients with persistent cough: assessment using a 24h ambulatory recorder Eur Respir J 7 7 1994 1246 1253 7 R.G. Loudon L.C. Brown Cough frequency in patients with respiratory disease Am Rev Respir Dis 96 6 1967 1137 1143 8 K.F. Chung Assessment and measurement of cough: the value of new tools Pulm Pharmacol Ther 15 3 2002 267 272 9 C.E. Brightling R. Ward K.L. Goh A.J. Wardlaw I.D. Pavord Eosinophilic bronchitis is an important cause of chronic cough Am J Respir Crit Care Med 160 2 1999 406 410 10 R.S. Irwin L.P. Boulet M.M. Cloutier R. Fuller P.M. Gold V. Hoffstein Managing cough as a defense mechanism and as a symptom. A consensus panel report of the American college of chest physicians Chest 114 2 Suppl Managing 1998 133S 181S 11 P. Munyard A. Bush How much coughing is normal? Arch Dis Child 74 6 1996 531 534 12 A.B. Chang R.G. Newman P.D. Phelan C.F. Robertson A new use for an old Holter monitor: an ambulatory cough meter Eur Respir J 10 7 1997 1637 1639 13 L. Pavesi S. Subburaj K. Porter-Shaw Application and validation of a computerized cough acquisition system for objective monitoring of acute cough: a meta-analysis Chest 120 4 2001 1121 1128 14 S. Zheng M. Yanai T. Matsui K. Sekizawa H. Sasaki Nocturnal cough in patients with sputum production Lancet 350 9081 1997 864 865 "
    },
    {
        "doc_title": "Training neural networks and neuro-fuzzy systems: A unified view",
        "doc_scopus_id": "80054744269",
        "doc_doi": "10.3182/20020721-6-es-1901.00722",
        "doc_eid": "2-s2.0-80054744269",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Different structure",
            "Neurofuzzy system",
            "Non-linear modelling",
            "Non-linear parameters",
            "Nonlinear mappings",
            "Radial basis functions",
            "Supervised training algorithm",
            "Wavelet neural networks"
        ],
        "doc_abstract": "Copyright © 2002 IFAC.Neural and neuro-fuzzy models are powerful nonlinear modelling tools. Different structures, with different properties, are widely used to capture static or dynamical nonlinear mappings. Static (non-recurrent) models share a common structure: a nonlinear stage, followed by a linear mapping. In this paper, the separability of linear and nonlinear parameters is exploited for completely supervised training algorithms. Examples of this unified view are presented, involving multilayer perceptrons, radial basis functions, wavelet networks, B-splines, Mamdani and TSK fuzzy systems.",
        "available": true,
        "clean_text": "serial JL 314898 291210 291718 291882 291883 31 IFAC Proceedings Volumes IFACPROCEEDINGSVOLUMES 2016-04-25 2016-04-25 2016-04-25 2016-04-25 2008-07-06 2016-06-29T09:13:13 S1474-6670(15)39143-6 S1474667015391436 10.3182/20020721-6-ES-1901.00722 S350 S350.2 HEAD-AND-TAIL 2022-06-08T09:31:00.10094Z 0 0 20020101 20021231 2002 2016-04-25T18:21:31.664063Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate isbn isbns isbnnorm isbnsnorm issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1474-6670 14746670 978-3-902661-74-6 9783902661746 false 35 35 1 1 Volume 35, Issue 1 722 415 420 415 420 2002 2002 2002-01-01 2002-12-31 2002 15th IFAC World Congress article fla Copyright © 2002 IFAC. Published by Elsevier Ltd. All rights reserved. TRAININGNEURALNETWORKSNEUROFUZZYSYSTEMSAUNIFIEDVIEW RUANO A GILL 1981 P PRACTICALOPTIMIZATION GOLUB 1973 413 432 G KAUFMAN 1975 49 57 L MARQUARDT 1963 431 441 D RUMELHART 1986 D PARALLELDISTRIBUTEDPROCESSING SJOBERG 1995 1691 1724 J RUANOX2002X415 RUANOX2002X415X420 RUANOX2002X415XA RUANOX2002X415X420XA item S1474-6670(15)39143-6 S1474667015391436 10.3182/20020721-6-ES-1901.00722 314898 2016-06-29T05:05:45.512863-04:00 2002-01-01 2002-12-31 true 285050 MAIN 6 45525 849 656 IMAGE-WEB-PDF 1 Copyright © 2002 IFAC 15th Triennial World Congress, Barcelona, Spain www.elsevier.com/locate/ifac TRAINING NEURAL NETWORKS AND NEURO-FUZZY SYSTEMS: A UNIFIED VIEW António E. Ruano, Pedro M. Ferreira, C. Cabrita, S. Matos ADEEC and Institute of Systems & Robotics, Faculty of Science & Technology, University of Algarve, 8000 Faro Portugal Abstract: Neural and neuro-fuzzy models are powerful nonlinear modelling tools. Different structures, with different properties, are widely used to capture static or dynamical nonlinear mappings. Static (non-recurrent) models share a common structure: a nonlinear stage, followed by a linear mapping. In this paper, the separability of linear and nonlinear parameters is exploited for completely supervised training algorithms. Examples of this unified view are presented, involving multilayer perceptrons, radial basis functions, wavelet networks, B-splines, Mamdani and TSK fuzzy systems. Copyright © 2002 IFAC Keywords: Neural Networks; Neuro-Fuzzy Systems; Multilayer Perceptrons; Radial Basis Functions; Wavelet Neural Networks 1. INTRODUCTION During the recent years, feedforward neural networks and neuro-fuzzy systems have been increasingly recognized as powerful nonlinear black-box modelling tools, whether for static or dynamic systems. Several authors (see, for instance, Sjoberg et al., 1995) recognized that although different neural networks or neuro-fuzzy systems are employed, they share a common structure: they can be envisaged as a two-stages model, the first performing a nonlinear mapping from an input space to an intermediate space (it will be denoted here as a basis function space), usually of greater dimensionality, and a latter stage, consisting of a linear mapping between the basis function space and the output space. What is not common to find in the literature is the explicit exploitation, for training purposes, of this common nonlinear-linear topology. This is highlighted in the present paper, where different types of neural and neuro-fuzzy structures will be considered. The next section will introduce a reformulated criterion, different supervised training methods, and how to compute the needed derivatives for each model type. Multilayer perceptrons, radial basis functions, wavelet networks, B-splines, Mamdani and TSK fuzzy systems are considered here. Section 3 shows examples of the application of this reformulated criterion for different examples, and for each model type. Conclusions are drawn in Section 4. 2. A NEW TRAINING CRITERION The aim of training the different models is to find the values of their parameters that minimize the sum of the square of the errors between the target and the actual output: 2 1T 1 E = -- e e = -- e [ i ] 2 2i = 1 N , (1) where e [ i ] = o ( w ) [ i ] ­ t [ i ] , o being the model output, t the desired output and w the model parameters. Without lack of generality we shall consider in this paper only one output, i.e., we are considering a MISO model. 415 As the models considered have a layered structure (q being the number of layers), let us partition the parameter vector as: w(q ­ 1) w= w( q ­ 2 ) ... w( 1 ) =u v where the dependence of A on v has been omitted. In (6) P A is the orthogonal projection matrix to the complementary space spanned by the columns of A. This new criterion (6) depends only on the nonlinear weights, and although different from the standard criterion (4), their minima are the same. We can therefore, instead of determining the optimum ^ of (4), first minimize (6), and then, using v in eq. (5), ^ obtain the complete optimal weight vector w . This new criterion reformulates the training problem in its correct formulation: an iterative algorithm should look for the best nonlinear mapping that produces the best basis functions for the particular problem. Besides reducing the dimensionality of the problem, the main advantage of using this new criterion is a faster convergence of the training algorithm, a property which seems to be independent of the actual training method employed. This reduction in the number of iterations needed to find a local minimum may be explained by two factors: A better convergence rate is normally observed with this criterion, compared with the standard one; The initial value of the criterion (6) is usually much smaller than the initial value of (4), for the same initial value of the nonlinear parameters. This is because at each iteration, including the first, the value of the linear parameters is the optimal, conditioned by the value taken by the nonlinear parameters. As the criterion is very sensitive to the value of the linear parameters, a large reduction is obtained, in comparison with the situation where random numbers are used as starting values for these weights. (2) where w denotes the matrix of weights connecting layer i to layer i+1, in vectorial form, u denotes the (linear) weights that connect to the output neuron, and v denotes all the other (nonlinear) weights. Using this convention, and as in all models the output layer is linear, the output vector can be given as: o( q ) = O( q ­ 1 ) 1 u (i) (3) In (3), a bias is assumed to be employed in the output neuron. If this is not the case, then the column of ones in (3) should be omitted. When (3) is substituted in (1), the resulting criterion is: t ­ O( q ­ 1 ) 1 u 2 ( u, v ) = -----------------------------------------------2 2 (4) The dependence of this criterion on v is nonlinear and appears only through O ( q ­ 1 ) ; on the other hand, is linear in the variables u , i.e. the weights in criterion (4) can be separated into two classes: nonlinear ( v ) and linear ( u ) weights. For any value of v, the minimum of (4) w.r.t. u can be found using a standard least squares solution. The optimum value of the linear parameters is therefore conditional upon the value taken by the nonlinear variables. + ^ u( v ) = O( q ­ 1 ) ( v ) 1 t 2.1Training algorithms Different training schemes can be employed for the minimization of criteria (4) or (6). Here, only gradientbased completed supervised training algorithms will be considered. The most used method to minimize (4) is the ErrorBack-Propagation (BP) (Werbos, 1974) algorithm, which is a steepest descent algorithm. Each iteration of the BP algorithm is: w [ k ] = w [ k ­ 1 ] ­ g [ k ] (5) In (5), a pseudo-inverse is used for the sake of simplicity; however, it should be noticed that O( q ­ 1 ) 1 is assumed to be full-column rank. Denoting this matrix by A, for simplicity, the last equation can then be replaced into (4), creating therefore a new criterion: PA t 2 2 t ­ AA + t 2 2 ( v ) = ---------------------------- = ------------------- , 2 2 (7) where g stands for the gradient vector of (4): g[k] = w [ k ] T (6) (w[k]) (8) 416 Notice that if (6) is used, (7) should be reformulated in terms of v [ k ] , and (8) in terms of and v [ k ] . The BP algorithm is a first-order method as it only uses derivatives of the first order. If no line-search is used, then it has no guarantee of convergence and the convergence rate obtained is usually very slow. If a second-order method is to be employed, the best algorithm (Gill et. al., 1981) to use is the LevenbergMarquardt (LM) algorithm (Marquardt, 1963). Denoting as J the Jacobian matrix: J[k] = w [ k] T where g ^ u=u , J and e denote respectively the gradient, the partition of the Jacobian matrix associated with the weights v and the error vector of obtained when the values of linear weights are their conditional optimal values, i.e.: J = ( A )v A+ t e = PA t (13) (14) The simplest way to compute g or J is to propagate o (q) (w[ k] ) (9) the input patterns through the nonlinear layer(s), ^ compute the optimal values of u using (5), and then compute g or J using standard algorithms. Notice also that, due to the special structure of ( A )v , we only i the LM update, s [ k ] = [ k ] ­ [ k ­ 1 ], is given as the solution of: ( J [ k ]J [ k ] + I )s [ k ] = ­ g [ k ] = ­ J [ k ]e [ k ] T T need to compute (10) (q ­ 1) O., i vi , where the subscript ., i denotes the ith column of the matrix and v i denotes the In (10), is a regularization parameter, which controls both the search direction and the magnitude of the update. The good results presented by the LM method (compared with other second-order methods such as the quasi-Newton and conjugate gradient methods) are due to its explicit exploitation of the underlying characteristics of the optimization problem, a sum-of-square of errors. Please notice that, if (6) is used, (9) and (10) should be updated as described before. In order to perform the training, the derivatives of (6) must be obtained. For this purpose the derivative of A w.r.t. v must be introduced. This is a three-dimensional quantity and will be denoted as: A v partition of the nonlinear parameters vector related with the ith column of the matrix. The computation of g has the additional burden of ^ determining u . However, since the dimensionality of the problem has been reduced, there is also some gain at each iteration that must be taken into account. Several algorithms can be used to compute these optimal values, but QR or SVD factorization are advisable, because of possible ill-conditioning of A. Although there is an additional cost in complexity per iteration to pay using this approach, a large reduction in the number of iterations needed for convergence is obtained if (6) is used as the training criterion. If the Levenberg-Marquardt method is employed with this new formulation, then the Jacobian of (6) must be obtained. Three different Jacobian matrices have been proposed for this problem: the first was introduced by (Golub and Pereyra, 1973), who were also the first to introduce the reformulated criterion. To reduce the computational complexity of this approach, in (Kaufman, 1975) a simpler Jacobian matrix is proposed. Ruano et al. (Ruano et al., 1991) suggested to employ the Jacobian matrix J , introduced in (12), which further reduces the computational complexity of each training iteration. Notice that the computation of this Jacobian matrix follows the same lines as the computation of g , discussed earlier. In terms of computational costs, the computation of J does not involve more costs than the computation of J. The use of this Jacobian matrix in the LM algorithm with the ( A )v = (11) Notice that, for each nonlinear parameter v i , ( A ) v is i a matrix with only one non-zero column. Considering first the use of the error back-propagation algorithm with this new criterion, the gradient of must be obtained. It can be proved (see Ruano, 1992) that the gradient vector of can be given as: ( o( q ­ 1 ) )T e 1 T J 0 = g =­ g ^ u=u (12) 417 reformulated criterion makes each iteration actually less complex than if the standard criterion is used. o (3) =O (2) u= cos ( 2Zj, i )e Z j, i ­ ------2 2 u, (19) To employ these techniques for the different models considered, we must explicitly compute the corresponding derivatives. Only the Jacobian computation will be indicated here, as it is known that the gradient vector can be obtained as: g = ­J T e where Z j, i = ( o j ti (1) ­ ti )d i , (20) (15) and d i being the translation and dilation Multilayer Perceptrons. In this type of networks, the computation of the gradient is sufficiently described in the literature as the BP algorithm (see, for instance, Rumelhart et al., 1986), and for this reason, will be omitted. The Jacobian can be obtained by backpropagating 1, and not the error, and by not performing the accumulation through all patterns in each weight. Radial Basis Functions. The output of an RBF, with Gaussian activation function for the hidden layers, can be expressed as: ci ­ x [ j ] 2 ­ -------------------------2 2 i 2 parameters, respectively. The derivatives are: O j, i ti (2) = ­ di O j, i Z j, i (2) (21) O j, i ti (2) Oj, i = ­d i , Z j, i (2) (22) where (2) O j, i Z j, i u = ­ Z j, i O j, i ­ 2 sin ( 2Z j, i )e (2) Z j, i ­ ------2 2 (23) o (3) [ j] = O (2) u= e (16) where i runs through all neurons and j through all patterns. An output bias is sometimes considered. (2) O j, i B-Splines Neural Networks. B-spline neural networks belong to the class of networks termed lattice-based associative memories networks (AMN). They are composed of three layers: a normalized input space layer, a basis functions layer and a linear weight layer. As the complexity of these networks rises exponentially with the number of inputs, it is beneficial to employ, instead of one single multivariate B-spline model covering all the n-dimensional input space, a linear combination of sub-models, each one with lower input dimension. The nonlinear parameters, in this network, are the knots employed to form a grid in the input space. Due to lack of space, a detailed description of this network and the computation of its derivatives is omitted. It can be found in (Ruano et. al. 2001). Mamdani Fuzzy Models. It can be proved (see Ruano et. al. 2001) that, provided the following assumptions are verified: The input value x is a crisp numeric quantity; The linguistic terms of the rule antecedents and consequents are modelled by B-splines of order 2, i.e., triangular membership functions; For implementing logic connectives such as the conjunction and implication, the t-norm used is the product; To compute vi , as the nonlinear parameters are the centre vector ( c i ) and the spread ( i ) for all neurons, the following equations are used: O j, i ci (2) (1) ( 2 ) ( O j, . ­ c i ) = O j, i ------------------------2 i (17) O j, i i (2) 2 (1) ( 2 ) O j, . ­ c i 2 ---------------------------= O j, i 3 i (18) Wavelet Networks. Wavelet decomposition is a typical example of the use of local basis functions. Loosely speaking, the \"mother basis function\" (usually referred as mother wavelet) is dilated and translated to form a wavelet basis. Considering, for instance, a unidimensional Mortlet mother wavelet, the network output can be given as: 418 The membership function of the output fuzzy set is computed pointwisely by taking the sum over all the rule's output fuzzy sets; ·The linguistic-to-numeric conversion (the socalled \"defuzzification\" process) is obtained by the centre of gravity method, a Mamdani fuzzy model is equivalent to a B-spline network with one multivariate sub-model, where splines of order 2 are used for every dimension. The network basis functions are equivalent to the linguistic terms of the rule antecedents, and the network output linear parameters are related with the linguistic terms of the rule consequents and their confidence values by a linear relationship. Therefore, the training algorithms described are also applicable to Mamdani fuzzy models that satisfy those assumptions. Takagi-Kang-Sugeno Fuzzy Models. The TakagiKang-Sugeno (or simply Sugeno) model is a fuzzy model where the consequents of the rules are realvalued functions; polynomials being the most common, and widely used. For the SISO case, a typical Sugeno model has a set of rules, such as the following: R (i) This can be easily generalized for the multi-input case. In terms of derivatives, and denoting by O (2) the BS derivatives of the basis functions of a B-spline/ Mamdani sub-model, the derivatives of a TSK model, can be given as: O (2 ) = O TSK × (2) BS O (2) ×x , BS (29) where the symbol O (2) implies that every column of is multiplied, element-by-element, by the BS vector x. 3. RESULTS Two examples will be employed to demonstrate the application of the reformulated criterion. The first example, an academical problem, illustrates an inverse kinematic transformation between Cartesian coordinates and one of the angles of a two-links manipulator. Considering first an MLP with 2 hidden layers, with 4 neurons in each layer, and a RBF with 10 hidden neurons, the following table illustrates the results obtained in terms of MSE, for the first 100 iterations of the BP and LM algorithms, minimizing both (4) and (6). The value in brackets in the BP cells denotes the learning rate used. Table 1: MSE after 100 iterations BP (4) BP (6) LM (4) LM (6) MLP (0.01) 14 10-3 (0.5) 1 10-3 6.6 10-9 2.7 10-9 RBF (0.001) 3 10-3 (0.1) 2 10-6 4 10-6 3.5 10-9 Comparing the 2 criteria, it can be concluded that the reformulated criterion produces better results than the standard one, independently of the training algorithm used. If the 2 algorithms are compared, the LM algorithm definitely performs better. Considering the same example, the following table illustrates the results obtained with a B-spline/ Mamdani model and a TSK model. Both used 2 interior knots, employing triangular splines. Here a termination criterion, related with the desired accuracy in the results was used. For this reason, the following table presents the results of the training algorithms in terms of the number of iterations needed to find a local minimum with the desired accuracy. A maximum value of 100 implies that training was terminated at ::=if ( x is X (i) ) then y (i) = w0 + w1 x (i) (i) (24) i = 1...r , r being the number of rules. Given a crisp input datum x, the output of this model is: (i) µ X( i ) ( x )y r -------------------------------------y = i=1 r (25) µX (i )( x ) i=1 When the linguistic terms of the rule-antecedents X(i) are modelled by B-splines, for all x in the input space, r i=1 µ X( i ) ( x ) = 1 . Therefore, (25) can be written as: (3) (2) o =O u, (26) where (2) O =µ X (1)( x ) ...µ X (r )( x ) µ X ( 1 ) ( x )x ...µ X ( r ) ( x )x (27) u = w ( 1 ) ... w( r ) w ( 1 ) ... w ( r ) 0 0 1 1 T (28) 419 that iteration, without convergence. The minimum MSEs for B-spline and TSK were 14 10-3 and 9 10-4, respectively. Analysing this table, the same conclusions as above can be taken. Table 2: Number of iterations BP (4) BP (6) LM (4) LM (6) B-Splines (.005) 100 (0.01) 21 12 9 TSK (.005) 100 (0.1) 100 20 13 To illustrate the application of these methods for wavelet networks, an example taken from (Matos et al., 2001) is used. Here, wavelet networks are used to approximate transcranial Doppler ultrasound blood flow signals, which will be, in a later stage, classified as normal, or embolic (gaseous or solid) signals. Employing a wavelet network, with 10 Mortlet neurons, Table 3 illustrates the results obtained, for one particular signal. Here, a termination criterion was also employed, and both the final MSE obtained and the final iteration are reported. As before, a limit of 100 training iterations was considered. Table 3: Number of iterations and final MSE -3 algorithms, should be used to derive \"good\" initial nonlinear model parameters, in order to increase the chances of determining the global minimum. These algorithms can also be applied to determine a \"good\" structure (number of neurons, number of sub-models, etc.) for the neural or neuro-fuzzy model at hand. These topics are the subject of the author's current research. ACKNOWLEDGEMENTS The authors acknowledge the support of Praxis. REFERENCES Gill, P., W. Murray and M. Wright (1981). Practical optimization. Academic Press Golub, G. and V. Pereyra (1973). The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate. SIAM JNA, 10, 2, 413-432 Kaufman, L. (1975). A variable projection method for solving separable nonlinear least squares problems. BIT, 15, 49-57 Marquardt, D. (1963). An algorithm for least-squares estimation of nonlinear parameters. SIAM J. Appl. Math., 11, 431-441 Matos, S., M.G. Ruano, A.E. Ruano and D.H. Evans (2001). Neural networks calssification of cerebral embolic signals, accepted to 23rd Annual Int. Conf. of the IEEE Engineering in Medicine and Biology Society, Istanbul, Turkey, 25-28 October Ruano, A. E., D. Jones, D. and Fleming P. J. (1991). A new formulation of the learning problem for a neural network controller. 30th IEEE Conference on Decision and Control, Brighton, England, 1, 865-866 Ruano, A.E. (1992). Applications of Neural Networks to Control Systems. PhD. Thesis, UCNW, UK. Ruano, A. E., C. Cabrita, C., J. V. Oliveira and L.T. Koczy (2001). Completely supervised training algrothms for B-spline and neuro-fuzzy systems, accepted to IFAC Conference on New Technologies for Computer Control (NTCC'2001), Hong-Kong, 19-22 November Rumelhart, D., McClelland, J. and the PDP Research Group (1986). Parallel distributed processing Vol. 1, MIT Press Sjoberg, J., Q. Zhang, L. Ljung, A. Benveniste, BDelyon, P. Glorennec, H. Hjalmarsson and A. Juditsky (1995). Nonlinear Black-box Modelling in Systems Identification: a Unified Overview. Automatica, 31, 12, 1691-1724 Werbos, P. (1974). Beyond regression: New tools for prediction and analysis in the behavioral sciences. Doctoral Dissertation, Appl. Math, Harvard University, U.S.A. BP (4) - 5 10 Numb. Iter. Final MSE 90 8.9 100 100 25 5.7 6.9 1.9 BP (6)- 5 10-4 LM (4) LM (6) 4. CONCLUSIONS In this paper it was made explicit that, in training neural networks and neuro-fuzzy systems, the aim is really to find the basis functions in the last hidden layer. As a consequence of this view, the training criterion should be changed so that it is only dependent on the parameters related with the hidden layers. This approach can be used with the whole class of gradient-based training algorithms, presenting, usually, better convergence properties. It has been demonstrated with multilayer perceptrons, radial basis functions, wavelet networks, B-splines, Mamdani and TSK fuzzy models, although it is applicable to any kind of structure whose output is a linear combination of nonlinearly parameterized basis functions. Although, in the authors' experience, the LevenbergMarquardt algorithm minimizing the reformulated criterion, introduced here, performs always better than any training algorithm specifically introduced for the model considered, it should be stressed that it only guarantees (a fast) convergence to a local minimum. Other types of algorithms, namely evolutionary 420 t networks, an example taken from (Matos et al., 2001) is used. Here, wavelet networks are used to approximate transcranial Doppler ultrasound blood flow signals, which will be, in a later stage, classified as normal, or embolic (gaseous or solid) signals. Employing a wavelet network, with 10 Mortlet neurons, Table 3 illustrates the results obtained, for one particular signal. Here, a termination criterion was also employed, and both the final MSE obtained and the final iteration are reported. As before, a limit of 100 training iterations was considered. Table 3: Number of iterations and final MSE -3 algorithms, should be used to derive \"good\" initial nonlinear model parameters, in order to increase the chances of determining the global minimum. These algorithms can also be applied to determine a \"good\" structure (number of neurons, number of sub-models, etc.) for the neural or neuro-fuzzy model at hand. These topics are the subject of the author's current research. ACKNOWLEDGEMENTS The authors acknowledge the support of Praxis. REFERENCES Gill, P., W. Murray and M. Wright (1981). Practical optimization. Academic Press Golub, G. and V. Pereyra (1973). The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate. SIAM JNA, 10, 2, 413-432 Kaufman, L. (1975). A variable projection method for solving separable nonlinear least squares problems. BIT, 15, 49-57 Marquardt, D. (1963). An algorithm for least-squares estimation of nonlinear parameters. SIAM J. Appl. Math., 11, 431-441 Matos, S., M.G. Ruano, A.E. Ruano and D.H. Evans (2001). Neural networks calssification of cerebral embolic signals, accepted to 23rd Annual Int. Conf. of the IEEE Engineering in Medicine and Biology Society, Istanbul, Turkey, 25-28 October Ruano, A. E., D. Jones, D. and Fleming P. J. (1991). A new IPV 39143 S1474-6670(15)39143-6 10.3182/20020721-6-ES-1901.00722 IFAC TRAINING NEURAL NETWORKS AND NEURO-FUZZY SYSTEMS: A UNIFIED VIEW António E. Ruano Pedro M. Ferreira C. Cabrita S. Matos ADEEC and Institute of Systems & Robotics, Faculty of Science & Technology, University of Algarve, 8000 Faro Portugal ADEEC and Institute of Systems & Robotics Faculty of Science & Technology University of Algarve Faro 8000 Portugal Neural and neuro-fuzzy models are powerful nonlinear modelling tools. Different structures, with different properties, are widely used to capture static or dynamical nonlinear mappings. Static (non-recurrent) models share a common structure: a nonlinear stage, followed by a linear mapping. In this paper, the separability of linear and nonlinear parameters is exploited for completely supervised training algorithms. Examples of this unified view are presented, involving multilayer perceptrons, radial basis functions, wavelet networks, B-splines, Mamdani and TSK fuzzy systems. Keywords Neural Networks Neuro-Fuzzy Systems Multilayer Perceptrons Radial Basis Functions Wavelet Neural Networks References Gill et al., 1981 P. Gill W. Murray M. Wright Practical optimization 1981 Academic Press Golub and Pereyra, 1973 G. Golub V. Pereyra The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate SIAM JNA 10 2 1973 413 432 Kaufman, 1975 L. Kaufman A variable projection method for solving separable nonlinear least squares problems BIT 15 1975 49 57 Marquardt, 1963 D. Marquardt An algorithm for least-squares estimation of nonlinear parameters SIAM J. Appl. Math. 11 1963 431 441 Matos et al., 2001 Matos, S., M. G. Ruano, A. E. Ruano and D. H. Evans (2001). Neural networks calssification of cerebral embolic signals, accepted to 23rd Annual Int. Conf. of the IEEE Engineering in Medicine and Biology Society, Istanbul, Turkey, 25-28 October. Ruano, 1991 Ruano, A. E., D. Jones, D. and Fleming P. J. (1991). A new formulation of the learning problem for a neural network controller. 30th IEEE Conference on Decision and Control, Brighton, England, 1, 865-866. Ruano, 1992 Ruano, A. E. (1992). Applications of Neural Networks to Control Systems. PhD. Thesis, UCNW, UK. Ruano et al., 2001 Ruano, A. E., C. Cabrita, C., J. V. Oliveira and L. T. Koczy (2001). Completely supervised training algrothms for B-spline and neuro-fuzzy systems, accepted to IFAC Conference on New Technologies for Computer Control (NTCC'2001), Hong-Kong, 19-22 November. Rumelhart et al., 1986 D. Rumelhart J. McClelland PDP Research Group Parallel distributed processing 1 1986 MIT Press Sjoberg et al., 1995 J. Sjoberg Q. Zhang L. Ljung A. Benveniste BDelyon, P. Glorennec H. Hjalmarsson A. Juditsky Nonlinear Black-box Modelling in Systems Identification: a Unified Overview Automatica 31 12 1995 1691 1724 Werbos, 1974 Werbos, P. (1974). Beyond regression: New tools for prediction and analysis in the behavioral sciences. Doctoral Dissertation, Appl. Math, Harvard University, U.S.A. "
    },
    {
        "doc_title": "Neural network classification of cerebral embolic signals",
        "doc_scopus_id": "61549098434",
        "doc_doi": "10.1109/IEMBS.2001.1020560",
        "doc_eid": "2-s2.0-61549098434",
        "doc_date": "2001-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The presence of circulating cerebral emboli represents an increased risk of stroke. The detection of such emboli is possible with the use of a transcranial Doppler ultrasound (TCD) system. When a gaseous or particulate embolus passes through the TCD sample volume, it produces high intensity transient signals that are normally relatively easily detected. However, because most current TCD systems rely on human experts for the detection and classification of candidate events, this technique is not widely used. The appearance of a reliable automatic system, able to detect these signals and to classify them as originating from either a gaseous or solid source, would encourage the widespread utilization of this technique. This paper reports the application of new signal processing techniques to the analysis and classification of embolic signals. We applied a Wavelet Neural Network algorithm to approximate the embolic signals, with the parameters of the wavelet nodes being used to train a Neural Network to classify these signals as resulting from normal flow, or from gaseous or solid emboli.",
        "available": false,
        "clean_text": ""
    }
]