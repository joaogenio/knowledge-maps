[
    {
        "doc_title": "Automatic Quality Improvement of Data on the Evolution of 2D Regions",
        "doc_scopus_id": "85125401126",
        "doc_doi": "10.1007/978-3-030-95408-6_22",
        "doc_eid": "2-s2.0-85125401126",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "2-D space",
            "Data cleaning",
            "Data quality",
            "Prophet",
            "Quad trees",
            "Quality data",
            "Quality improvement",
            "Real-world",
            "Spatio-temporal data",
            "Times series"
        ],
        "doc_abstract": "© 2022, Springer Nature Switzerland AG.This work deals with data cleaning and quality improvement when representing the evolution of 2D regions extracted from real-world observations. It presents a method that combines quadtrees and time series to identify inconsistencies and poor-quality data in a sequence of 2D regions. Our algorithm splits a 2D space recursively into buckets and creates time series using spatial functions on bucket-delimited subregions. Smaller buckets represent the subregions with a higher number of inconsistencies over time. Then, it uses time series outlier detection methods and consistency metrics to identify polygons that are poor-quality representations and remove them from the original sequence. The proposed method identifies errors and inaccuracies even if they occur in several consecutive observations. We evaluated our strategy using a dataset extracted from real-world videos and compared it with another method from the literature. The results prove the effectiveness of this proposal.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Survey on Data-driven Performance Tuning for Big Data Analytics Platforms",
        "doc_scopus_id": "85100773006",
        "doc_doi": "10.1016/j.bdr.2021.100206",
        "doc_eid": "2-s2.0-85100773006",
        "doc_date": "2021-07-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021 Elsevier Inc.Many research works deal with big data platforms looking forward to data science and analytics. These are complex and usually distributed environments, composed of several systems and tools. As expected, there is a need for a closer look at performance issues. In this work, we review performance tuning strategies in the big data environment. We focus on data-driven tuning techniques, discussing the use of database inspired approaches. Concerning big data and NoSQL stores, performance tuning issues are quite different from the so-called conventional systems. Many existing solutions are mostly ad-hoc activities that do not fit for multiple situations. But there are some categories of data-driven solutions that can be taken as guidelines and incorporated into general-purpose auto-tuning modules for big data systems. We examine typical performance tuning actions, discussing available solutions to support some of the tuning process's primary activities. We also discuss recent implementations of data-driven performance tuning solutions for big data platforms. We propose an initial classification based on the domain state-of-the-art and present selected tuning actions for large-scale data processing systems. Finally, we organized existing works towards self-tuning big data systems based on this classification and presented general and system-specific tuning recommendations. We found that most of the literature pieces evaluate the use of tuning actions at the physical design perspective, and there is a lack of self-tuning machine-learning-based solutions for big data systems.",
        "available": true,
        "clean_text": "serial JL 305627 291210 291867 291870 31 Big Data Research BIGDATARESEARCH 2021-01-27 2021-01-27 2021-02-09 2021-02-09 2021-12-27T15:56:00 S2214-5796(21)00023-X S221457962100023X 10.1016/j.bdr.2021.100206 S300 S300.3 FULL-TEXT 2021-12-27T16:17:35.389936Z 0 0 20210715 2021 2021-01-27T17:07:51.678393Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid orcid primabst ref 2214-5796 22145796 true 25 25 C Volume 25 6 100206 100206 100206 20210715 15 July 2021 2021-07-15 2021 article fla © 2021 Elsevier Inc. All rights reserved. ASURVEYDATADRIVENPERFORMANCETUNINGFORBIGDATAANALYTICSPLATFORMS COSTA R 1 Introduction 2 Big Data and analytics: concepts, uses and benchmarks 2.1 Big Data and Big Data analytics 2.2 Architectures for big data analytics 2.2.1 System types 2.2.2 Workload types 2.2.3 Architectures 2.3 Applications 2.4 Benchmarking 3 Database systems tuning overview 3.1 Performance tuning in database systems 3.2 Tuning approaches 3.2.1 Logical and physical design 3.2.2 Knobs and system parameters 3.2.3 Machine-learning techniques 3.3 NoSQL stores 4 Tuning in large-scale data processing systems 4.1 Storage layout and data placement 4.2 Data replication, data transfer, fault tolerance and performance 4.3 Caching and memory tuning 4.4 Big data warehousing and logical organization 4.5 Monitoring, advising and self-tuning 4.5.1 Monitors and advisors 4.5.2 Self-tuning systems 5 Recommendations and opportunities 5.1 Tuning recommendations 5.2 Research challenges and opportunities 6 Conclusions Acknowledgements References ABADI 2020 44 53 D MOORTHY 2015 74 96 J SIVARAJAH 2017 263 286 U WANG 2019 512 523 Y ADVANCESINDATABASETECHNOLOGY22NDINTERNATIONALCONFERENCEEXTENDINGDATABASETECHNOLOGY MODELINGBUILDINGIOTDATAPLATFORMSACTORORIENTEDDATABASES ARVANITIS 2019 A CIDR20199THBIENNIALCONFERENCEINNOVATIVEDATASYSTEMSRESEARCH AUTOMATEDPERFORMANCEMANAGEMENTFORBIGDATASTACK NAVAZ 2018 137 154 A RASMUSSEN 2012 1 14 A PROCEEDINGSTHIRDACMSYMPOSIUMCLOUDCOMPUTINGSOCC THEMISIOEFFICIENTMAPREDUCE ZHANG 2018 1 15 H PROCEEDINGSTHIRTEENTHEUROSYSCONFERENCE RIFFLEOPTIMIZEDSHUFFLESERVICEFORLARGESCALEDATA LU 2018 1970 1973 J HERODOTOU 2011 261 272 H CIDR2011FIFTHBIENNIALCONFERENCEINNOVATIVEDATASYSTEMSRESEARCH STARFISHASELFTUNINGSYSTEMFORBIGDATAANALYTICS CHEN 2012 1802 1813 Y SHAH 2015 351 367 T RIAHI 2018 524 528 Y ULARU 2012 3 14 E JIN 2015 59 64 X OZCAN 2017 1771 1775 F PROCEEDINGSACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATAPARTF1277 HYBRIDTRANSACTIONALANALYTICALPROCESSINGASURVEY ABADI 2016 92 99 D THEAPACHESOFTWAREFOUNDATION THUSOO 2009 1626 1629 A KORNACKER 2015 1406 1415 M CIDR2015SEVENTHBIENNIALCONFERENCEINNOVATIVEDATASYSTEMSRESEARCHONLINEPROCEEDINGS IMPALAAMODERNOPENSOURCESQLENGINEFORHADOOP ARMBRUST 2015 1383 1394 M PROCEEDINGS2015ACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA SPARKSQLRELATIONALDATAPROCESSINGINSPARK CORBELLINI 2017 1 23 A CATTELL 2010 12 27 R TUDORICA 2011 B PROCROEDUNETIEEEINTCONF ACOMPARISONBETWEENSEVERALNOSQLDATABASESCOMMENTSNOTES HECHT 2011 336 341 R 2011INTERNATIONALCONFERENCECLOUDSERVICECOMPUTING NOSQLEVALUATIONAUSECASEORIENTEDSURVEY STEFANI 2018 86 92 E KABAKUS 2017 520 525 A LI 2018 95 108 C ARULRAJ 2017 1753 1758 J PROCEEDINGS2017ACMINTERNATIONALCONFERENCEMANAGEMENTDATASIGMODCONFERENCE HOWBUILDANONVOLATILEMEMORYDATABASEMANAGEMENTSYSTEM PETROV 2018 1 8 I ENCYCLOPEDIABIGDATATECHNOLOGIES HARDWAREASSISTEDTRANSACTIONPROCESSINGNVM KIM 2019 464 467 D PROCEEDINGS34THACMSIGAPPSYMPOSIUMAPPLIEDCOMPUTING ASCALABLEPERSISTENTKEYVALUESTOREUSINGNONVOLATILEMEMORY TOMMASINI 2019 199 202 R PROCEEDINGS13THACMINTERNATIONALCONFERENCEDISTRIBUTEDEVENTBASEDSYSTEMS OUTLOOKDECLARATIVELANGUAGESFORBIGSTEAMINGDATA ALDINUCCI 2020 102694 M CHENG 2019 169 183 Y BARBAGONZALEZ 2020 538 550 C BERGAMASCHI 2017 695 702 S HIRAMAN 2018 2018 2020 B 2018INTCONFINFORMATIONCOMMUNENGTECHNOLICICET2018 ASTUDYAPACHEKAFKAINBIGDATASTREAMPROCESSING KHIATI 2018 467 471 R IEEEINTLCONFNETWORKINFRASTRUCTUREDIGITALCONTENT STREAMPROCESSINGENGINESFORSMARTHEALTHCARESYSTEMS PERSICO 2018 98 109 V PSOMAKELIS 2020 531 539 E KIRAN 2015 2785 2792 M 2015IEEEINTERNATIONALCONFERENCEBIGDATABIGDATA LAMBDAARCHITECTUREFORCOSTEFFECTIVEBATCHSPEEDBIGDATAPROCESSING PERSICO 2018 98 109 V SHAH 2017 3476 3481 P PROC2017IEEEINTCONFBIGDATABIGDATA20172018JANUA TOWARDSDEVELOPMENTSPARKBASEDAGRICULTURALINFORMATIONSYSTEMINCLUDINGGEOSPATIALDATA WOLFERT 2017 69 80 S ATLURI 2018 G YANG 2019 1 14 C SUBBU 2017 33 43 K WANG 2019 160 172 S CHAUHAN 2016 1 6 T INTLCONFRESEARCHENTREPRENEURSHIP USINGBIGDATAANALYTICSFORDEVELOPINGCRIMEPREDICTIVEMODEL ULLAH 2019 81 118 F LI 2019 129 142 W LNENICKA 2019 124 141 M ZHANG 2017 626 641 Y FAHMIDEH 2019 948 963 M PFEIFFER 2015 213 220 D SPANGENBERG 2017 310 317 N MANOGARAN 2018 375 387 G SAKR 2016 44 58 S GHANI 2019 417 428 N GUO 2018 1085 1096 C PROCIEEE34THINTCONFDATAENGICDE2018 LEARNINGROUTESPARSETRAJECTORYSETS SNOWDON 2018 1 6 J 5THINTACMSIGMODWORKMANAGMINENRICHEDGEOSPATIALDATAGEORICH2018CONJUNCTIONSIGMOD2018 SPATIOTEMPORALTRAFFICVOLUMEESTIMATIONMODELBASEDGPSSAMPLES NEILSON 2019 35 44 A BALDUINI 2019 66 84 M SILVA 2020 975 987 B RORIZJUNIOR 2019 59 73 M GHAZAL 2013 1197 1208 A PROCEEDINGSACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA BIGBENCHTOWARDSINDUSTRYSTANDARDBENCHMARKFORBIGDATAANALYTICS WANG 2014 488 499 L 2014IEEE20THINTERNATIONALSYMPOSIUMHIGHPERFORMANCECOMPUTERARCHITECTURE BIGDATABENCHABIGDATABENCHMARKSUITEINTERNETSERVICES MING 2014 138 154 Z LECTURENOTESINCOMPUTERSCIENCEINCLUDINGSUBSERIESLECTURENOTESINARTIFICIALINTELLIGENCELECTURENOTESINBIOINFORMATICS8585 BDGSASCALABLEBIGDATAGENERATORSUITEINBIGDATABENCHMARKING HUANG 2010 41 51 S 2010IEEE26THINTERNATIONALCONFERENCEDATAENGINEERINGWORKSHOPS HIBENCHBENCHMARKSUITECHARACTERIZATIONMAPREDUCEBASEDDATAANALYSIS AHMAD 2012 F PUMAPURDUEMAPREDUCEBENCHMARKSSUITE COOPER 2010 143 154 B PROCEEDINGS1STACMSYMPOSIUMCLOUDCOMPUTING BENCHMARKINGCLOUDSERVINGSYSTEMSYCSB LI 2017 2575 2589 M LI 2015 M PROCEEDINGS12THACMINTERNATIONALCONFERENCECOMPUTINGFRONTIERS SPARKBENCHACOMPREHENSIVEBENCHMARKINGSUITEFORINMEMORYDATAANALYTICPLATFORMSPARK LU 2014 69 78 R 2014IEEEACM7THINTERNATIONALCONFERENCEUTILITYCLOUDCOMPUTING STREAMBENCHTOWARDSBENCHMARKINGMODERNDISTRIBUTEDSTREAMCOMPUTINGFRAMEWORKS HAN 2018 580 597 R PAGLIARI 2019 3711 3716 A PROCEEDINGS2019IEEEINTERNATIONALCONFERENCEBIGDATA TOWARDSAHIGHLEVELDESCRIPTIONFORGENERATINGSTREAMPROCESSINGBENCHMARKAPPLICATIONS CEESAY 2017 2821 2828 S 2017IEEEINTERNATIONALCONFERENCEBIGDATABIGDATAVOL2018JANUA PLUGPLAYBENCHSIMPLIFYINGBIGDATABENCHMARKINGUSINGCONTAINERS ZAHARIA 2016 56 65 M SANTOS 2017 242 252 M PROCEEDINGS21STINTERNATIONALDATABASEENGINEERINGAPPLICATIONSSYMPOSIUMIDEAS2017 EVALUATINGSQLONHADOOPFORBIGDATAWAREHOUSINGNOTSOGOODHARDWARE SETHI 2019 1802 1813 R 35THIEEEINTERNATIONALCONFERENCEDATAENGINEERING PRESTOSQLEVERYTHING HAUSENBLAS 2013 100 104 M COSTA 2019 34 E ONEIL 2009 P STARSCHEMABENCHMARKSSB MEHTA 2017 1226 1237 P BROWN 2010 963 968 P PROCEEDINGS2010ACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA OVERVIEWSCIDBLARGESCALEARRAYSTORAGEPROCESSINGANALYSIS HALPERIN 2014 881 884 D PROCEEDINGS2014ACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA DEMONSTRATIONMYRIABIGDATAMANAGEMENTSERVICE ABADI M CHAUDHURI 2005 964 965 S PROCEEDINGSACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA FOUNDATIONSAUTOMATEDDATABASETUNING ABOUZEID 2009 922 933 A CHAUDHURI 2007 3 14 S PROCEEDINGS33RDINTERNATIONALCONFERENCELARGEDATABASES SELFTUNINGDATABASESYSTEMSADECADEPROGRESS CHAUDHURI 2006 1265 S PROCEEDINGS32NDINTERNATIONALCONFERENCELARGEDATA FOUNDATIONSAUTOMATEDDATABASETUNING ALMEIDA 2019 240 254 A CONCEPTUALMODELING38THINTERNATIONALCONFERENCE ONTOLOGICALPERSPECTIVEFORDATABASETUNINGHEURISTICS NOON 2016 47 53 N AMERI 2016 201 205 P 32NDIEEEINTERNATIONALCONFERENCEDATAENGINEERINGWORKSHOPSICDEWORKSHOPS ASELFTUNINGINDEXRECOMMENDATIONAPPROACHFORDATABASES CURINO 2010 48 57 C ZHAO 2012 127 134 L WORKSHOPSPROCEEDINGSIEEE28THINTERNATIONALCONFERENCEDATAENGINEERING APPLICATIONMANAGEDDATABASEREPLICATIONVIRTUALIZEDCLOUDENVIRONMENTS BOROVICAGAJIC 2016 1029 1040 R SANDERS 2001 G PROCEEDINGS34THANNUALHAWAIIINTERNATIONALCONFERENCESYSTEMSCIENCES DENORMALIZATIONEFFECTSPERFORMANCERDBMS CHAUDHURI 1997 65 74 S RANGEL 2006 514 523 R COMPUTERSCIENCETHEORYAPPLICATIONSFIRSTINTERNATIONALCOMPUTERSCIENCESYMPOSIUMINRUSSIA LEASTLIKELYUSEANEWPAGEREPLACEMENTSTRATEGYFORIMPROVINGDATABASEMANAGEMENTSYSTEMRESPONSETIME THAKARE 2019 1237 1271 A LU 2019 1970 1973 J LI 2019 2118 2130 G ZHENG 2014 1 12 C PROCEEDINGS10THINTERNATIONALCONFERENCEINTELLIGENTCOMPUTINGTHEORYICIC SELFTUNINGPERFORMANCEDATABASESYSTEMSNEURALNETWORK AKEN 2017 1009 1024 D PROCEEDINGSACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA AUTOMATICDATABASEMANAGEMENTSYSTEMTUNINGTHROUGHLARGESCALEMACHINELEARNING ZHANG 2019 415 432 J PROCEEDINGS2019INTERNATIONALCONFERENCEMANAGEMENTDATASIGMOD19 ENDTOENDAUTOMATICCLOUDDATABASETUNINGSYSTEMUSINGDEEPREINFORCEMENTLEARNING DAVOUDIAN 2018 40 A GUZMAN 2019 26 37 A HYBRIDARTIFICIALINTELLIGENTSYSTEMS14THINTERNATIONALCONFERENCE CREATIONADISTRIBUTEDNOSQLDATABASEDISTRIBUTEDHASHTABLES BLOOM 1970 422 426 B CHEVALIER 2016 142 149 M PROCEEDINGS18THINTERNATIONALCONFERENCEENTERPRISEINFORMATIONSYSTEMSVOL1 DOCUMENTORIENTEDMODELSFORDATAWAREHOUSESNOSQLDOCUMENTORIENTEDFORDATAWAREHOUSES BANSAL 2014 1 6 G WORKSHOPSMARTERPLANETBIGDATAANALYTICS AFRAMEWORKFORPERFORMANCEANALYSISTUNINGINHADOOPBASEDCLUSTERS LEE 2008 1075 1086 S PROCEEDINGS2008ACMSIGMODINTERNATIONALCONFERENCEMANAGEMENTDATA ACASEFORFLASHMEMORYSSDINENTERPRISEDATABASEAPPLICATIONS BAKRATSAS 2018 1 10 M MOON 2015 3525 3548 S KRISH 2014 207 212 K 2014IEEEINTERNATIONALCONFERENCEBIGDATABIGDATA VENUORCHESTRATINGSSDSINHADOOPSTORAGE WU 2013 125 130 D 2013INTERNATIONALCONFERENCEADVANCEDCLOUDBIGDATA UNDERSTANDINGIMPACTSSOLIDSTATESTORAGEHADOOPPERFORMANCE REN 2018 22 26 D PROCSINTLCONFCOMPUTINGDATAENGINEERING FILESYSTEMPERFORMANCETUNINGFORSTANDARDBIGDATABENCHMARKS TORABZADEHKASHI 2019 M HAAS 2016 1 6 S 201653NDACMEDACIEEEDESIGNAUTOMATIONCONFERENCE MPSOCFORENERGYEFFICIENTDATABASEQUERYPROCESSING BALKESEN 2018 1407 1419 C PROCEEDINGS2018INTERNATIONALCONFERENCEMANAGEMENTDATASIGMOD18 RAPIDINMEMORYANALYTICALQUERYPROCESSINGENGINEEXTREMEPERFORMANCEPERWATT RAO 2012 1 14 S PROCEEDINGSTHIRDACMSYMPOSIUMCLOUDCOMPUTINGSOCC12 SAILFISHAFRAMEWORKFORLARGESCALEDATAPROCESSING KUMAR 2016 1 6 A 20161STINDIAINTERNATIONALCONFERENCEINFORMATIONPROCESSING PERFORMANCEANALYSISMYSQLPARTITIONHIVEPARTITIONBUCKETINGAPACHEPIG KOLIOPOULOS 2016 353 356 A 2016IEEEINTERNATIONALCONGRESSBIGDATABIGDATACONGRESS TOWARDSAUTOMATICMEMORYTUNINGFORINMEMORYBIGDATAANALYTICSINCLUSTERS AZIZ 2019 78 K GOUNARIS 2018 22 32 A PTICEK 2017 6 10 M PROCEEDINGS2017INTERNATIONALCONFERENCECLOUDBIGDATACOMPUTINGICCBDC2017 BIGDATANEWDATAWAREHOUSINGAPPROACHES ZDRAVEVSKI 2019 3754 3763 E PROCEEDINGS2019IEEEINTERNATIONALCONFERENCEBIGDATA CLUSTERSIZEOPTIMIZATIONWITHINACLOUDBASEDETLFRAMEWORKFORBIGDATA COSTA 2018 459 473 C EVALUATINGSEVERALDESIGNPATTERNSTRENDSINBIGDATAWAREHOUSINGSYSTEMS DECARVALHOCOSTA 2006 207 217 R 8THINTERNATIONALCONFERENCEDATAWAREHOUSINGKNOWLEDGEDISCOVERY DATAWAREHOUSESINGRIDSHIGHQOS FURTADO 2009 658 677 P DATABASETECHNOLOGIES EFFICIENTROBUSTNODEPARTITIONEDDATAWAREHOUSES WU 2013 89 98 D 20THANNUALINTERNATIONALCONFERENCEHIGHPERFORMANCECOMPUTING ASELFTUNINGSYSTEMBASEDAPPLICATIONPROFILINGPERFORMANCEANALYSISFOROPTIMIZINGHADOOPMAPREDUCECLUSTERCONFIGURATION ALIPOURFARD 2017 O CHERRYPICKADAPTIVELYUNEARTHINGBESTCLOUDCONFIGURATIONSFORBIGDATAANALYTICS ZHU 2017 338 350 Y PROCEEDINGS2017SYMPOSIUMCLOUDCOMPUTING BESTCONFIGTAPPINGPERFORMANCEPOTENTIALSYSTEMSVIAAUTOMATICCONFIGURATIONTUNING BAO 2018 181 190 L 2018IEEEINTERNATIONALCONFERENCEBIGDATABIGDATA LEARNINGBASEDAUTOMATICPARAMETERTUNINGFORBIGDATAANALYTICSFRAMEWORKS BERRAL 2015 1701 1710 J PROCEEDINGS21THACMSIGKDDINTERNATIONALCONFERENCEKNOWLEDGEDISCOVERYDATAMININGKDD15 ALOJAMLAFRAMEWORKFORAUTOMATINGCHARACTERIZATIONKNOWLEDGEDISCOVERYINHADOOPDEPLOYMENTS TARIQ 2019 93 100 H PROCEEDINGS12THIEEEACMINTERNATIONALCONFERENCEUTILITYCLOUDCOMPUTING MODELLINGPREDICTIONRESOURCEUTILIZATIONHADOOPCLUSTERS WANG 2016 586 593 G 2016IEEE18THINTERNATIONALCONFERENCEHIGHPERFORMANCECOMPUTINGCOMMUNICATIONSIEEE14THINTERNATIONALCONFERENCESMARTCITYIEEE2NDINTERNATIONALCONFERENCEDATASCIENCESYSTEMSHPCCSMARTCITYDSS ANOVELMETHODFORTUNINGCONFIGURATIONPARAMETERSSPARKBASEDMACHINELEARNING COSTAX2021X100206 COSTAX2021X100206XR 2023-02-09T00:00:00.000Z 2023-02-09T00:00:00.000Z © 2021 Elsevier Inc. All rights reserved. 2021-06-05T01:47:02.775Z FCT UIDB/00127/2020 UIDB/04524/2020 FCT Fundação para a Ciência e a Tecnologia CAPES CAPES Coordenação de Aperfeiçoamento de Pessoal de Nível Superior CNPq CNPq Conselho Nacional de Desenvolvimento Científico e Tecnológico Fundo Europeu de Desenvolvimento Regional POCI-01-0145-FEDER-032636 POCI-01-0247-FEDER-024541 ERDF European Regional Development Fund Programa Operacional Competitividade e Internacionalização This work is partially funded by National Funds through the FCT (Foundation for Science and Technology) in the context of the projects UIDB/04524/2020 and UIDB/00127/2020 , and by Fundo Europeu de Desenvolvimento Regional (FEDER), Programa Operacional Competitividade e Internacionalização in the context of the projects POCI-01-0145-FEDER-032636 and Produtech II SIF – POCI-01-0247-FEDER-024541 . Some of the authors are partially supported by grants from CNPq and CAPES , Brazilian public funding agencies and research institutes. 0 item S2214-5796(21)00023-X S221457962100023X 10.1016/j.bdr.2021.100206 305627 2021-12-27T16:17:35.389936Z 2021-07-15 true 1055671 MAIN 17 57343 849 656 IMAGE-WEB-PDF 1 gr001 18924 189 505 gr002 22824 203 490 gr003 10079 96 466 gr004 12493 200 254 gr006 36518 347 539 gr007 49620 391 589 gr005 9841 109 512 gr005 3401 47 219 gr001 4696 82 219 gr002 5097 91 219 gr003 3257 45 219 gr004 5965 164 208 gr006 6355 141 219 gr007 7296 145 219 gr001 70945 502 1343 gr002 81667 541 1303 gr003 38984 255 1239 gr004 42361 532 675 gr006 284545 1536 2388 gr007 379703 1733 2610 gr005 43599 968 4534 am false 472528 BDR 100206 100206 S2214-5796(21)00023-X 10.1016/j.bdr.2021.100206 Elsevier Inc. Fig. 1 Components of a general purpose Big Data architecture. Fig. 1 Fig. 2 The Lambda architecture (adapted from [43]). Fig. 2 Fig. 3 The Kappa architecture (adapted from [43]). Fig. 3 Fig. 4 Distributed queries on heterogeneous databases. Fig. 4 Fig. 5 Database tuning activities. Fig. 5 Fig. 6 Classification of main data-oriented tuning approaches. Fig. 6 Fig. 7 Classification of works on data-driven tuning of large-scale processing systems. Fig. 7 Table 1 Big Data Systems Applications. Table 1 Application area Analytics Machine Learning and Data Mining Image, video and audio analysis, spatial and spatio-temporal IoT and sensor data Agriculture [44,45] [46] [44] [45] Climate / Environmental Science [47–49] [47,46,48] [47–49] [47–49] Crimes [46,50] [46,50] Cybersecurity [51] Energy efficiency [49,52] [49,52] [49,6] Government Enterprises [53] Human mobility prediction [48] [48] [48] Manufacturing Enterprises [54,55] [55] Neuroscience [46] [46] Public Health [47,56,48,57] [47,46,48,57] [47,46,56] [47,56,48] and Epidemiology [6,58,59] [6,58,59] Recommendation systems [48] [48] [48] Smart buildings / homes [48] [48] [48] Social media [60] [46,60] [46] Traffic Dynamics [61] [46,61] [46,61,62] Transportation [47,48,63] [47,48,63] [47,48,63] [48,63] Urbanization / Smart Cities [47,64,49,65] [64] [47,64,49] [49,65,66] Table 2 Challenging skews for maintaining low I/O in MapReduce. Table 2 Record size skew Partitioning skew common in semi-structured and unstructured data systems leads to workload unbalancing between nodes Computational skew Performance heterogeneity some tasks take much longer than others even between identical cluster nodes Table 3 Selected data-driven recommendations for large-scale big data processing systems. Table 3 General recommendations • Consider data access and processing patterns when replacing magnetic storage by SSD [118,116] • Map workload characteristics into the file system attributes to allocate data efficiently. [120] • The number and types of devices should be considered when configuring storage devices [120] MapReduce-based Systems • Use SSD to store intermediate data [117] (i.e., map output temporarily stored locally) • Compress intermediate data [120] • When using small block sizes, evaluate increasing the number of threads [114] • Avoid using great block sizes as they can impact on parallelism [114] • Partition data that results from the mapping and not the input data itself [7] • Avoiding transferring too much data between nodes [8] • In clusters where failures are uncommon, reduce intermediate data materialization to improve performance [7,124] • Use small map-resulting partitions to execute in-memory sorting [7] • Use sampling to estimate the approximate distribution of map-resulting data [7] • Consider if reducers outputs should be sorted to choose partition policy. [7] • Merge fragmented intermediate files into larger blocks (with large sequential I/O requests) to deal with skews in intermediate data [8,124] • Each task input size should fit the available memory to avoid disk spill [8] • Tune the number of tasks and the size of data consumed by each one to reduce disk spill [8] HDFS and Hive • Place caching data in SSD [118] • Multi-level partitioning should be used carefully to avoid excessive partitioning (which leads to multiple levels of folders with small files and less data per folder) [84] • Use Bucketing simultaneously in two tables and by the attribute used to join them. [84] Spark • Verify file's size, available cluster-wide main memory and distributed objects representation overheads to select caching strategy [126] • Avoid compression and serialization if cluster-wide main memory can cache the entire dataset [126] • Use compression when at least half of data can be cached [126] A Survey on Data-driven Performance Tuning for Big Data Analytics Platforms R.L.C. Rogério Luís de C. Costa a b ⁎ José Moreira b c Paulo Pintor c Veronica dos Santos d Sérgio Lifschitz d a Computer Science and Communication Research Centre (CIIC), Polytechnic of Leiria - Leiria - 2411-901, Portugal Computer Science and Communication Research Centre (CIIC) Polytechnic of Leiria Leiria 2411-901 Portugal Computer Science and Communication Research Centre (CIIC), Polytechnic of Leiria - Leiria - 2411-901, Portugal b Institute of Electronics and Informatics Engineering (IEETA), University of Aveiro - Aveiro - 3810-193, Portugal Institute of Electronics and Informatics Engineering (IEETA) University of Aveiro Aveiro 3810-193 Portugal Institute of Electronics and Informatics Engineering (IEETA), University of Aveiro - Aveiro - 3810-193, Portugal c Department of Eletronics, Telecommunications and Informatics (DETI), University of Aveiro - Aveiro - 3810-193, Portugal Department of Eletronics Telecommunications and Informatics (DETI) University of Aveiro Aveiro 3810-193 Portugal Department of Eletronics, Telecommunications and Informatics (DETI), University of Aveiro - Aveiro - 3810-193, Portugal d Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio), RJ - 22451-900, Brazil Departamento de Informática Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio) RJ 22451-900 Brazil Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio), RJ 22451-900, Brazil ⁎ Corresponding author. Many research works deal with big data platforms looking forward to data science and analytics. These are complex and usually distributed environments, composed of several systems and tools. As expected, there is a need for a closer look at performance issues. In this work, we review performance tuning strategies in the big data environment. We focus on data-driven tuning techniques, discussing the use of database inspired approaches. Concerning big data and NoSQL stores, performance tuning issues are quite different from the so-called conventional systems. Many existing solutions are mostly ad-hoc activities that do not fit for multiple situations. But there are some categories of data-driven solutions that can be taken as guidelines and incorporated into general-purpose auto-tuning modules for big data systems. We examine typical performance tuning actions, discussing available solutions to support some of the tuning process's primary activities. We also discuss recent implementations of data-driven performance tuning solutions for big data platforms. We propose an initial classification based on the domain state-of-the-art and present selected tuning actions for large-scale data processing systems. Finally, we organized existing works towards self-tuning big data systems based on this classification and presented general and system-specific tuning recommendations. We found that most of the literature pieces evaluate the use of tuning actions at the physical design perspective, and there is a lack of self-tuning machine-learning-based solutions for big data systems. Keywords Big data systems Big data platforms Performance tuning Database systems 1 Introduction A wide range of devices, including mobile phones, GPS devices, social networks, sensors, and IoT devices [1–4] is generating a large volume of distributed and heterogeneous data. Transforming such massive amount of data into valuable information while revealing its underlying meaning is a crucial function of big data analytics [5,6]. New requirements in terms of analytics (e.g., real-time data management) contribute to a big data analytics environment composed of a myriad of solutions and distributed systems, commonly known as the big data stack [5]. Many of these solutions involve distributed processing of large volumes of data, replicated or moved over networks on the fly. Failures are somewhat common, and runtime auto-recovery is required and commonly achieved based on re-processing of tasks and through the use of intermediate data materialization [7,8]. In such an environment, I/O costs and data movement may impose relevant costs and overheads. Capacity planning may turn out to be a problem and lead to increasing costs, especially in applications that must meet some type of Service Level Agreement (SLA) specified in terms of performance metrics [5]. Also, there is no optimizer to build the most efficient execution plan to execute each user's job, which increases the importance of carefully tuning the system performance. Both automatic tuning and real-time analytics tuning are some of the main open challenges for big data systems [9]. In big data analytics environments, the main tuning opportunities include the reorganization of data layouts (e.g., applying partitioning) and the efficient use of materialization, mainly to persist intermediate results [10]. These opportunities are somewhat related to some of the most common performance tuning actions usually taken in traditional database systems, which are data-driven decisions, mostly oriented to reduce I/O operations and improve memory consumption. Some of the aspects that impact traditional database systems' performance also affect large-scale big data processing systems. For instance, query (job) size, data skew, and temporal locality in data access impact both database systems and MapReduce-based systems performance [11]. In this paper, we review the use of performance tuning techniques in the big data stack. We focus on data-driven performance tuning, discussing works related to I/O optimization, data partitioning and materialization, file placement and data transfer, caching, and memory consumption. We describe traditional tuning activities and present how it has evolved into the NoSQL solutions. We describe proposals on data-driven performance tuning for big data systems and warehouses, mostly in solutions based on the MapReduce paradigm and distributed systems. We present selected tuning actions that would benefit data analysts. We also present big data benchmarking tools, self-tuning solutions, and performance monitoring tools and advisors, and discuss some future research opportunities. For the best of our knowledge, there is no prior work that presents an organized review of tuning techniques and works in the context of big data systems and analytics with such a data-driven perspective. In the following section, we describe some background on the big data systems. We also discuss some related big data benchmarks and studies on performance comparison. Section 3 describes the main performance tuning actions, discussing available solutions used to support some of the tuning process's main activities in relational and NoSQL stores, based on our data-driven performance tuning classification. Section 4 reviews data-driven performance (self)tuning in the large-scale processing systems. In Section 5, we classify existing works, extract general and system-specific tuning recommendations, and discuss research opportunities. Finally, section 6 concludes the paper and presents future work. 2 Big Data and analytics: concepts, uses and benchmarks In this section, we review some concepts and background on big data and big data analytics, including common systems and workload types, application areas, benchmark tools and performance comparison studies. 2.1 Big Data and Big Data analytics The term Big Data has been widely used in the last ten years due to the increasing use of smart devices, social networks, GPS devices among others, and more recently, with the emergence of the Internet-of-things (IoT) [1–4]. These devices and platforms have been generating vast amounts of data, in a variety of formats, e.g., text, audio and video, among other. Big data is usually associated with the so-called key-features (3 V's, 4 V', etc.). Its three fundamental features are volume, concerning the size of the data generated by the devices and platforms that need to be stored and processed, variety, concerning the diversity of the data sources and models, e.g., structured, semi-structured and unstructured data, and velocity, denoting how fast the data are produced and how fast they should be processed to generate responses in near real-time, if required [12,13]. Other important features concern the authenticity of the data (veracity) [14], the insights that can be inferred from the data and the added value for companies (value) [15] and the meaning of the data and its variants on different contexts, e.g., spatial and temporal contexts (variability) [2,3]. The focus of data analytics is on examining datasets to get insights and draw conclusions about the information they contain using automated processes and specialized systems. This is a broad subject that encompasses different types of data analysis, e.g., using statistical methods (descriptive analytics), explaining why something happened (diagnostic analytics) or what is likely to happen in the future (predictive analytics) [13,3]. Traditional approaches like data warehousing and OLAP are based on monolithic and centralized systems with a well-defined organization. Data is usually created and consumed by the same entity and they are pre-processed before loading into the data warehouse. Data and query results are assumed to be correct and complete. On the other hand, in big data analytics there is an demand for near real time processing, data is often semi-structured or unstructured, and raw data, distributed in several platforms, may be used with no trust guarantee to generate complete results. Storing, organizing and taking insights from very large, heterogeneous and highly dynamic datasets raise several processing and management challenges [3]. The volume of data produced increases every day and there is a demand for efficient distributed management systems that are capable to store and retrieve very large datasets, but also to consume, process and filter out such big data on the fly. This includes efficient methods for cleaning (noise reduction, outlier detection, etc.), transforming and integrating data in near real-time. However, running analytical queries over the very recent big data leads to novel optimizations and design choices [16]. Also, with the rise of NoSQL, the trend became to use the most suitable technology for each use case, which means that modern systems must also be able to execute cross database queries on database engines that are possibly heterogeneous in terms of data models, query languages, indexing methods and optimization strategies. The increasing use of distributed systems, potentially geographically distant from each other, has renewed the interest on handling distributed transactions and queries, and the trade-offs between high throughput, low latency, bandwidth, consistency, isolation level and scalability [1]. Many data driven systems moved to public clouds that provide flexible and elastic services on-demand [17]. Then, computing resources and data are in the cloud, but computing resources are managed independently of storage resources. This separation is a novelty that influences the design and optimization of cloud-based data management and analysis systems. Developments in hardware are also influencing the architecture of data management systems. For instance, GPU accelerated databases have the potential to decompose computationally intensive queries like sorting and aggregations, into subqueries that can run in parallel. However, taking full advantage of GPUs for such operations requires specific programming and this depends largely on the underlying organization of the data in the databases (e.g., row or column storage). The developments in memory and storage, e.g., HDD (Hard Disk Drive), SSD (Solid-State Drive) and in-memory based systems, are also changing the trade-offs between storage, computing and communication costs, and consequently the best design choices and tuning must also be investigated. Data mining and machine learning are also closely related to the retrieval and processing of large datasets and therefore, it would also be interesting to investigate how to use specialized query optimization techniques for the typical workloads in these areas. On the other hand, machine learning techniques can be used to find the best execution plans for complex queries and performance tuning. 2.2 Architectures for big data analytics 2.2.1 System types Over the last 15 years, several big data special purpose solutions were proposed. One of the first was Hadoop [18], a framework for the distributed processing of large datasets across clusters. Hadoop has three main components: a file system (Hadoop Distributed File System - HDFS), a MapReduce instantiation and “Yet Another Resource Negotiator” (YARN). HDFS is a fault-tolerant distributed file system designed to be deployed on commodity hardware. By default, it uses large blocks (128 MB), which are replicated and distributed among the participating nodes. MapReduce is a programming paradigm for writing and executing parallel programs (with the Map and Reduce functions) in a cluster. YARN is the resource manager and is responsible for managing the job submissions and allocating resources to the running applications. Currently, there are several SQL-on-Hadoop offerings built on the top of Hadoop, such as, Hive [19], Impala [20] and Spark SQL [21]. They have been designed for OLAP queries over large datasets and they use columnar storage formats, e.g., Parquet and ORC. They lack transaction support, but recent solutions such as Kudu [22] are offering basic support for updating and deleting data [1,2]. There also are several categories of NoSQL databases providing many different options for storage and retrieval of large datasets of structured, semi-structured or unstructured data. These include key-value stores (e.g., Hazelcast, Redis, Membase/Couchbase, Riak, Voldemort and Infinispan), document stores (e.g., CouchDB, MongoDB, Terrastore, RavenDB, SimpleDB) and graph stores (e.g., Neo4J, InfoGrid, HypergraphDB, AllegroGraph and BigData) [23–26], as well as triple stores (e.g., Virtuoso, Apache Jena and Sesame) [27]), among others. Most of these platforms store data in disks and the access speed depends on the characteristics of the disks. However, memory is much cheaper than before and the need for fast data access made appear the called in-memory storage. In-memory databases can be much faster than in-disk databases and they are easier to implement [28]. There are special-purpose in-memory databases, like Apache Ignite, Gemfire and FluteDB [29], but commercial databases like Oracle, Microsoft and SAP are also offering solutions for in-memory data management. Main memory is volatile and data can be lost, but the introduction of non-volatile random-access memories (NVM) can help to overcome this problem [30]. These are almost as fast as Dynamic Random-Access Memories (DRAM). However, there are important architectural changes in both cases, which potentially invalidate some of the traditional design choices in data management systems [31,32]. There also are solutions to query continuous data streams on the fly. Data streaming processors such as Storm, Kafka or Flink, are able to collect the input data, manage queues and forward the data to workers (distributed nodes), collect the results and handle failures. Several streaming SQL languages are arising [33]. Big data systems are mainly focused on data intensive applications that need high I/O throughput. Typically, they are implemented on a Cloud environment, the nodes communicate via network and they are optimized for cost effectiveness. Data nodes and compute nodes are in the same physical machine to maximize data locality. In contrast, High Performance Computing (HPC) is specialized in computationally intensive tasks like modeling and simulation in climate and environmental sciences. Data nodes and compute nodes are physically separated, hence high-speed connections are needed for communication and data transfer between nodes. Dedicated parallel file systems are often used. However, it is increasingly difficult to establish a boundary between these two fields because Big data applications are increasingly computationally intensive and HPC applications are becoming more data intensive [34]. Deep Learning and near-real-time data stream processing are notable examples. Deep Learning applications need to process very large datasets to train the models and run computationally intensive algorithms using dense linear algebra operations, which can be efficiently implemented using special-purpose hardware like multi-GPU accelerators. Data stream processing applications need to continuously ingest and analyze unbounded data streams generated by an increasing number of data sources with very different characteristics, and produce results almost in real-time [35]. The problem is that the architectures and computing models in Big Data and High Performance Computing are quite different and novel data structures, algorithms and optimization strategies are needed, to improve the performance and scalability of these applications. 2.2.2 Workload types There are two main types of workload in big data analytics systems: batch and stream processing. Batch processing consists in processing large amounts of data all at once. The data are offline and they are in a variety of formats. The jobs consist of running several tasks like data extraction, transformation and loading executed in non-stop and sequential order. The resulting data is also stored offline. This approach is efficient to process periodically large amounts of data that are collected over predefined periods of time. Stream processing consists in processing and analyzing data arriving from multiple sources at high rates, to yield results in near real time. This requires the continuous computation of static queries over a sliding window of live input data. During this process, the raw input streams can be sorted, filtered merged, normalized or enriched with existing metadata. The resulting streams are ready to be used as input in subsequent processes or to be stored. In this scenario, the output rate of the system must be higher than the input data stream rate to avoid memory and storage issues. Currently, there are several platforms for data streaming. Apache Kafka and Apache Flink [36,37] are notable examples. These platforms are distributed processing engines that allow to execute chains of data processing tasks using a publisher/subscriber pattern. For instance, Twitter and LinkedIn are using Apache kafka to implement use cases such as activity tracking, message exchange, ad serving or metric gathering [38]. In theory, these platforms designed to process data streams (unbounded data) can also be used for batch processing (bounded data) [39]. There also is an increasing number of big data applications that need to process batch and streaming workloads at the same time, to enable high-performance mixed querying over historical and fresh data [40]. In this scenario, it is possible to run interactive queries (batch processing) and online queries (stream processing) in parallel and to use the results of these processes as inputs in subsequent processes. For instance, the results of a streaming process can be the input of another streaming process or stored in a database and used for later batch analytics, e.g., OLAP and data mining. The output of a batch process can be used as metadata for subsequent streaming processes. In either case, it is important that queries over offline and online data can see a consistent view of the data. As an example, [41] presents an AIS-based vessel trajectory prediction system. The batch processing tasks include storing historical data in a database and model training. Stream processing tasks include GPS data (timestamp and location) ingestion, data cleaning, and prediction of the vessels' location in the near future. Similar approaches are also used in online social networks like Twitter and AWS [42]. 2.2.3 Architectures The first data analytics systems worked only with offline data. The source data (operational data) were typically stored in relational databases and documents (e.g., spreadsheets), and went through a process of extraction, transformation and loading (ETL) into a data warehouse (analytical data). These data are ready to be analyzed using OLAP tools, dashboards or reports. These systems have greatly evolved over the years. Databases have evolved from the relational model to NoSQL and nowadays there are solutions specialized for data warehousing and OLAP applications, e.g., time series databases and in-memory databases. Moreover, the organizations are migrating their data from local warehouses or data lakes to the cloud. Meanwhile, there also was a great evolution in the areas of data mining and machine learning, which also need to process large volumes of data stored in databases and data warehouses. In parallel, there is an increasing number of applications targeting the processing of data streams. The volume of data that need to be processed is increasing fast and there are new requirements on latency and throughput. Offline and online data processing systems are being integrated gradually and nowadays, many big data management and processing systems are implementing both pipelines in parallel (Fig. 1 ). In this context, two data processing architectures are emerging. The Lambda architecture (Fig. 2 ) proposes using two pipelines called Cold path and Hot path [43]. The streaming data are sent to both paths. The Hot path is characterized by high throughput, low latency, negligible errors and near real-time analytics. The data streams are processed in the speed layer and served as real time-views. This layer only takes into account the new data. The Cold path merges the incoming data with historical data and produces enriched data and details. The Cold path has two layers. The first, the Batch layer, combines streaming data with historical data, and the second, the Serving layer, serves the data generated in the previous one (indexed batch views). The data are not available as quickly as in the Hot path, but the data is cleaned and prepared to be used at the outside. The answers to queries are built on the real-time views and indexed batch views. The Kappa architecture (Fig. 3 ) has just one path, i.e., stream and batch processing take place in the same path [43]. This path also has two layers. The streaming layer processes the input data and the serving layer makes the data prepared in the previous step available for querying. This data can also be fed into auxiliary stored for serving analytical clients. These two architectures have the same purpose which is to handle real-time and historical analytics in a single environment. However, the Kappa architecture processes both types of workload using a single technology stack, while the Lambda architecture uses a technology stack for batch processing, e.g., Apache Hadoop and Hive, and stream processing is performed using a different technology, e.g., Apache Flink. Since Kappa combines the data from both two processes (stream and batch) in a messaging system (e.g. Apache Kafka) - where it is possible to read the data, transform it and enrich it - when the output needs to be changed, it is only necessary to update the code and then run it again over stored data (streaming data previously stored in disk) in a batch manner. However, the Kappa architecture is less tolerant to failures and is not as efficient as the Lambda architecture when processing historical data. For many years we had separate OLTP and OLAP systems. Consequently, there is a lag between what the OLTP system can see and what the OLAP system can query. This approach fulfills the requirements of many applications, but today there is also a lot of interest in integrating OLTP and OLAP systems into a single system. This means shifting from a traditional approach based on data warehouses and ETL, or more recently event streams, to an architecture based on data lakes, where data from all domains (transactional, analytical and streaming) are accessible using connectors (mediators) and distributed query engines (Figure-4 ). This means that a query engine can retrieve data from the data lake, transform it and store the results back to the data lake. 2.3 Applications The ability to collect almost all kinds of data on any event or subject has sparked an unprecedented interest on data analytics. From the users' point of view, big data analytics does not have well-defined boundaries, since they are mainly interested in obtaining the best possible insights from the data and use those insights to create value for their organizations. Thus, the applications in this field are also closely related to topics such as data mining, machine learning or IoT. Each of these topics is present in a wide range of application areas. Table 1 presents a selection of papers proposed in the computer science literature organized by dominant topic (some papers cover more than one topic) and application area. 2.4 Benchmarking Benchmarking comprises the evaluation of performance metrics on the execution of standardized workloads using distinct systems over the same dataset. Big data systems include a variety of data representation models, processing architectures, and distributed schemes due to significant data velocity and variety characteristics. It makes benchmarking big data systems somewhat more challenging than benchmarking traditional (relational) database systems. There are a few generators, systems, and suites that generate or contains standardized data or workloads used for big data benchmarking, including BigBench [67], BigDataBench [68], Big Data Generator Suite [69], HiBench [70], PUMA (Purdue MapReduce Benchmark Suite) [71], YCSB (Yahoo! Cloud Serving Benchmark) [72], SparkBench [73,74], Stream Bench [75], SWIM [11] and some of the traditional TPC benchmarks (e.g. TPC-H, TPC-DS, TPC-ID, TPCx-BB) [76]. Such benchmarks have distinct features, including in terms of operations (i) SQL operators; (ii) algorithms for graph traversal and analysis; (iii) compress and count emails; and (iv) clustering and classification, concerning the data generation process (a) pure synthetic generators, (b) real-world based data generators, and (c) hybrid data generators. As in the context of big data systems, we may group the commonly available metrics in categories like (i) performance metrics, (ii) price-performance metrics and (iii) energy consumption metrics [77], such benchmark systems also have distinct features and objectives in terms performance metrics evaluation (e.g., throughput, the ratio of system available time and processor computation intensity). The use of standardized data and workload generators is an attempt to provide a fair comparison. Still, in many situations, it is not enough, and specific properties of resources, workload, and data may influence different systems in distinct ways. Also, some of the existing workload generators are context-specific [78]. A full description and comparison of standardized big data and workload generators and benchmark suites are out of the scope of this paper and can be found at [77]. Ceesay et al. [79] propose using dock containers to simplify the configuration, deployment, and usage of a generator and benchmarking tools and suites. They argue that plug-and-play components can be used to automate the setup, data generation, and task execution phases. Some work in the literature provides a performance comparison of high-level architectures or general-purpose solutions in specific contexts. In [43], authors compare the Lambda and Kappa big data architectures in terms of task execution time, deployment costs, and outcome quality when used for On-line Social Networks (OSNs) data analysis. The authors used Apache Spark [80] and Apache Storm to implement the Lambda and Kappa architectures, respectively. They used an influence analysis algorithm that automates the real-time analysis of multimedia streams as an analysis task, the Microsoft Azure public cloud platform as an execution environment, and several instances (of distinct sizes) of the Yahoo Flickr Creative Commons 100 Million (YFCC100M) as the dataset. Authors conclude that both architectures provide proper support for the used application and have good scalability concerning the size of the dataset (which is a significant influence on performance). Still, the Lambda architecture outperforms Kappa in the studied context. Santos et al. [81] compare the performance of some large SQL systems: Spark [80], Presto [82], Drill [83], and Hive [19] on Tez. Authors use TPC-H [76] queries and a denormalized version of the dataset stored on Hive Tables over HDFS. Presto achieved the best performance. Hive's performance was the worst for small datasets, but it was better than Spark and Drill in large datasets. Spark and Drill led to similar query execution times in most of the executed tests. Presto also outperforms Hive in a more recent work ([84]), which uses the star schema benchmark (SSB) [85] and evaluates the use of partitioning and bucketing in Hive tables. In [86], Mehta et al. evaluate (qualitatively and quantitatively) the use of parallel data processing systems and libraries (namely SciDB [87], Myria [88], Spark [80], Dask [89], and TensorFlow [90]) to support image analytics pipelines from astronomy and neuroscience. The qualitative evaluation is based on the system's ease of use – expressed in terms of pipeline implementation complexity and required lines of code. Authors made the required changes in Python implementations of the pipelines to make them run using the evaluated systems and libraries (for SciDB and TensorFlow, they had to re-implement the full code). Only Dask, Myria, and Spark were capable of running both pipelines fully. Dask achieved the best performance on small clusters (16-nodes), and Myria achieved the best performance on the high biggest used cluster (64-nodes). Authors also claim that Dask has the worst performance when processing small datasets, as it has the biggest startup overhead among the evaluated systems. Mehta et al. also present the need to perform fine-tuning to achieve high performance. They highlight the need to tune the degree of parallelism, and aspects related to memory management - with which they deal using intermediate result materialization - and data caching. 3 Database systems tuning overview Performance is crucial for database systems. It is the main optimization goal of the tuning activity, and it is affected by several aspects, e.g., the amount of data or replication and consistency requirements [91]. Database performance tuning is the process of changes and adjustments at the database level that would improve database applications' performance. The process of database tuning can be done continuously in a feedback loop style (Fig. 5 ). It is an activity where we need to: • monitor the system's behavior; • make plans for possible tuning actions; • evaluate and execute them on the database system and, • monitors again the database system to observe the impact on the workload. Although most Web data is unstructured, companies still consider transactional data stored in RDBMs in their business decisions. Systems like HadoopDB [92] enable MapReduce over distributed relational database management systems (e.g., PostgreSQL and MySQL). Furthermore, tuning and self-tuning techniques have been studied in the relational database area for decades. Considering both affirmations, we will present in the next sections, some tuning approaches and actions for relational database systems found in the literature. 3.1 Performance tuning in database systems Database tuning is a group of interrelated activities responsible for optimizing database systems and applications by the efficient use of existing resources. Its goals are to reduce transaction execution time and improve systems' throughput without changing physical components. The overall process includes activities like: • monitoring the system looking for contentions and bottlenecks; • configuring memory-related (e.g., shared buffers, checkpoint segments) and transaction-related parameters (e.g., isolation levels); • physical schema partitioning; and • query rewriting and schema design (de)normalization. Database administrators (DBA) are mainly responsible for tuning operations. Tuning may be supported by monitoring and alternatives enumeration and evaluation. The alternatives enumeration may be based on DBA previous experience or the use of advisor tools, which are commonly spotted on the what-if analysis or based on machine learning techniques. These tools can be of general scope or specialized in different types of tuning actions, most of them offered by database manufacturers. Most RDBMSs support well known physical access structures that may enhance workload performance, such as indexes and materialized views. Clustered indexes, hypothetical indexes, and horizontal and vertical partitioning are also interesting optional features since they can provide query optimization with little or no additional storage and update overhead. There is a need to decide which one to use since we can choose only one clustering index or partition criteria for a relation. Another concern of the DBA involves maintaining database statistics updated since the query optimizer relies on selecting the best execution plan. Configuration parameters are often adjusted empirically, and there are intricate relationships among them that can negatively impact the performance improvement goal. Database experts must decide, based on their background knowledge, the most suitable strategy since some actions can cause conflict or be ignored by the optimizer or harms specific SQL commands [93]. It is difficult for the DBA to test and evaluate all possible tuning actions and trade-offs for a given workload. The complexity of tuning tasks and the costs associated with hiring tuning experts for database management motivated the development of self-tuning tools. Self-tuning tools automate this process by delegating steps usually performed by a DBA to autonomic tools that can follow algorithms and heuristics to monitor and execute tuning actions. Several reasons motivate database self-tuning systems, including the increasing complexity of multi-tenant monolithic applications and services, the growing complexity of RDBMS' administration, and tuning, which provides hundreds of tuning knobs. Moreover, the high cost of ownership for RDBMS-based solutions, where hiring experts in system tuning and management, is dominant [91]. Different paradigms, isolated or combined, can address Self-tuning to solve specific tuning problems [94]. Such models are implemented by existing RDBMS such as Oracle through its Oracle Automatic Memory Management feature - online optimization by memory governance - and the Automatic Database Diagnostic Monitor (ADDM), a feedback loop control by workload monitoring and diagnosing root causes for low performances. Heuristics can express tuning strategies presented in the literature and acquired in practice by specialized DBA. We may model these heuristics to turn such knowledge precisely defined and be used by automatic or semi-automatic database tuning tools. Outer-Tuning framework [95] proposed an ontological perspective for database tuning heuristics execution. The advantage of such an approach is to make the decision rationale with its considered alternatives known besides representing its concepts and terminology in a standardized and vendor-independent manner. The framework has two independent and complementary (sub)ontologies (DB tuning concepts and DB tuning heuristic), which can be extended with new or revised heuristics, just adding new rules or updating the existing ones. 3.2 Tuning approaches We may organize the main performance tuning activities into two axes: first, the observation level, which is a fundamental step that should not be harmful for system availability and execution. Then, the tuning actions per se, ranging from common physical disturbances like index and materialized view creations, up to recent and new machine-learning-based approaches. Fig. 6 presents a classification of the main data-oriented tuning actions. In what follows, we will briefly discuss some previous and current works on each of these categories applied in relational database systems, to give an idea of the current state-of-the-art. 3.2.1 Logical and physical design Caching techniques involve creating persistent storage structures like materialized views and dynamically allocating available memory based on its purpose (data buffer cache, library cache, etc.). Materialized views store precomputed joins and filtered, aggregated, and sorted data; it also restricts relational tables vertically, caching on disk only the columns needed by a query. A materialized view can speed up database response time, reducing database queries cost consuming less CPU, memory, and I/O resources [96]. However, we must consider the disk space usage. Indexes are auxiliary structures that provide fast access to data based on one or more (key) values. We may define the Index Selection Problem (ISP) as the situation where we have to identify and create appropriate indexes for a given workload that minimizes the response time while respecting the total index storage space constraints. It is an NP-hard problem considering all different types of indexes (B-tree, bitmap, text, hash, partial, clustered, geospatial) and the variety of tables accessed by SQL commands in the workload. Another concern is to achieve a trade-off in the read and write operations. Since having more indexes on attributes frequently used in write operations can increase system load because for each update, we must update the index structure too [97]. Index recommendation approaches, implemented by RDBMS, use the database optimizer and collected statistics to estimate the costs of queries based on hypothetical indexes. These are virtual index structures, with no physical space consumption, created only in the database catalog, simulating how query execution plans would benefit them. Indexing can also be treated as a caching technique since indexes restrict relational tables horizontally and can be used by the optimizer to fetch only the rows that satisfy range or lookup filter conditions avoiding full scan [96]. According to specific criteria, the physically partitioning of data means splitting the same logically structured data. On the one hand, a partition may reduce the number of I/O operations required to execute a specific task. Also, in distributed systems, partitioning may significantly increase the degree of parallelism. On the other hand, a wrong choice in terms of partitioning partition placement criteria may lead to several types of skew that would reduce performance gains. Horizontal partitioning split tables into tuples groups based on a schema partitioning criterion, and we can store each group on the same storage nodes (aka site) or different nodes. Using the last alternative queries can be sent to each node to scan its partitions in parallel, and one node should be responsible for grouping the subsets. Besides scalability, partitioning can also improve availability if, in case of partition failure, the remaining partitions are still able to respond to some application requests. The most widely used approaches are round-robin (send each tuple to partition nodes in a circular queue), range (divide tuples based on range values of table columns), and hash (assign tuples based on hash function result). Range approaches are suitable for OLAP workloads, but OLTP workloads with small transactions (few records) don't benefit from them. When the transaction requires access to multiple sites, distributed transactions can even reduce performance compared to running transactions on a single node system [98]. Data replication distributes physical copies of logical data items of a database onto different storage nodes. A replica is a physical copy of a particular data item, and it is possible to have more than one replica of each item. The main reason to use replication on a database system is to increase availability providing fault tolerance since the data still can be accessed if at least one replica is available. It can also boost scalability and load balancing by distributing access requests across the replicas and adding new storage nodes (and copies) when the incoming load increases, leveraging a higher throughput [99]. The main issue in such scenarios is to resolve update conflicts among replicas, which can diverge occasionally but are expected to converge eventually. Distinct types of secondary storage may have its characteristics in terms of latency and speed. Also, user data (and additional data, like systems' logs) may have its usage characteristics (e.g., sequential or random access). Choose the best storage system for each data that may significantly impact performance. Enterprise databases have been using storage tiering techniques to allocate data to storage types based on their usage frequency. Three-tier storage hierarchy is a common approach divided into SSD/DRAM for low-latency performance (online tier); SATA HDD for high-density capacity tier (nearline tier); tape libraries for low-cost archival (offline tier). Algorithms for hot and cold data classification have been developed in main-memory databases or multitiered databases. They are used by Hierarchical Storage Managers (HSM) to improve database performance by caching hot data in low-latency storage devices. Such an approach can also reduce TCO when storing large amounts of data, but tape-based archival access is slower than HDD-based access in high magnitude. This drawback could make the archival tier unsuitable for cold data storage since run batch analytics over cold data is unfeasible. In [100], the authors experimented with evaluating the use of new devices named Cold Storage Devices (CSD). The average time for TPC-H queries on a 50GB TPC-H dataset of PostgreSQL instance services using CSD was measured and compared with HDD-based nearline tier and tape-based offline tier. Operational databases are physically designed to support known online transaction processing (OLTP) applications and their workloads. Database design normalization techniques are the golden standard in OLTP environments since they minimize update anomalies and maximize data accessibility. On the other hand, denormalization can speed up data retrieval by reducing the amount of joins operations. Database designers may denormalize at table level: collapsing tables involved in one-to-one or many-to-many relationships and splitting a table (horizontal or vertical). It also can be done at the column level: adding redundant columns and adding derived columns (summary, total). An effect of denormalization by adding redundant information is storage consumption increase. One of the most useful areas for applying denormalization techniques is in Data Warehouse (DW) [101]. Online analytical processing (OLAP) queries executed against operational databases usually runs with unacceptable performance. A well-established enterprise approach is to build a distinct environment with a DW targeted for decision support workload. Inmon defined DW in 1992 as “subject-oriented, integrated, time-varying, non-volatile collection of data that is used primarily in organizational decision making.” Compared with operational databases, DW tends to be orders of magnitude larger since it contains consolidated and historical data from many sources. A popular conceptual model for DW is the multidimensional model composed of numeric measures that are the objects of analysis. Multidimensional data model and OLAP operations demand specific data organization and auxiliary access structures like materialized views and Bitmap indexes. Star schema is the most common logical model used to represent the multidimensional data model. Using a RDBMS as data storage, the physical model comprises a single fact table that stores the numeric measures and pointers to its dimension and only one table for each dimension. Snowflake schemas, a refinement of star schemas, enable dimensional hierarchy through normalization of dimension tables. Here, the normalization facilitates dimension tables maintenance, although denormalized dimensional star schemas were considered more appropriate for OLAP queries since it requires less joins operations [102]. The practice showed no rigid rule, and database designers have to choose the best normalization level for each scenario. In many read-intensive systems, the use of denormalization and the creation of summaries can substantially increase performance. On the other hand, in write-intensive systems, normalized logical design usually leads to better results. 3.2.2 Knobs and system parameters One of the first major concerns of a DBA is how to configure system resources in terms of the amount of memory, cache sizes, disk types, and operational system's parameters for resource limitation. In terms of RDBMS memory-specific parameters, a well-done configuration of buffer size and its policies can leverage both disk IO and memory efficiency. The buffer pool holds in-memory images of database on-disk pages currently in use and retains those supposed to be used again soon. The buffer manager is the database management system component responsible for page replacement in response to buffer faults and retention policies of the buffer pool. Its goal is to minimize hard-disk access and provide optimal use of main memory [103] with an increase of the hit ratio. Many works in page-level tuning focus on page replacement strategies. They have been carried out to optimize hard disk-oriented buffer management classic algorithms, like LRU, MRU, or MFU. Novel approaches have been proposed using probabilistic adaptive algorithms in conjunction with newly emerged flash memory-based disks [104]. However, to adjust the buffer areas' size, choose the buffer replacement policy to be used or even adjust the system's specific parameters, like the number of allowed concurrent processes, may be insufficient. Configuration parameter tuning is a challenging activity since most database systems offer many tuning knobs, and some of them might affect, in different ways, the performance of different types of workloads (OLTP, OLAP, hybrid). In a distributed environment, the complexity scales up since system administrators also have to tune configuration parameters from storage nodes (CPU, storage, memory) and its communication network. Different approaches for knobs and parameter tuning are classified (see [105]) into six main categories: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. This classification allows a comparison of the approaches based on the strengths and weaknesses of each one. For instance, there are simulation-based tuning approaches oriented to solve one or more specific problems. To mention an example, predicting the overall impact on database performance under hypothetical resources or parameter changes, creating what-if scenarios. These approaches can help a DBA predict the cache hit ratio, the page-access response time, and other memory-based performance indicators for the given cache size. In the next section, we will explore in-depth machine-learning based techniques since it is a promising field of research. 3.2.3 Machine-learning techniques Studies on automatic database tuning supported by learning-based methods apply machine learning (supervised, unsupervised, and reinforcement learning), neural networks, and deep learning techniques to tune database knobs. Specifically, knob tuning, which is an NP-hard problem, should benefit from these approaches since the knobs are mostly numeric values (categorical, discrete, or continuous). They are used to control the amount of cache memory concerning how often data is written to storage and represented as features. These are useful in the model training phase and to predict the estimated values of the tuning parameters [106]. Tuning knowledge, automatically learned, can be reused to reduce the amount of time and resources applied in tuning a DBMS for a specific workload or even a new application. The proposals of [107] [108] [8] use these techniques to support coarse-grained tuning (e.g., workload-level tuning). In [107], an architecture is presented that constantly collects, in a non-intrusive way, data from the workload, current parameters, and performance indicators of the DBMS and trains the neural network with these data. In this way, it is possible to find relationships between the parameters candidates to be adjusted and the performance indicators and submit them to the self-tuning algorithm that can tune multi-parameters simultaneously. OtterTuning [108] [8] is a tool that uses machine learning, supervised and unsupervised methods, to train the model by reusing training data gathered from previous tuning sessions to select the most impactful knobs and recommend the most suitable knob settings in different workloads. It also uses linear regression, a statistical method, to determine the strength of the relationship between one or more dependent variables and each of the independent variables. Li et al. [106] proposed a query-aware database tuning system named QTune to tune the database configurations, with three tuning granularity: query-level (fine-grained tuning), workload-level, and cluster-level, using a deep reinforcement learning (DRL) model. Reinforcement learning can be defined as a framework that enables an agent to act in a specific scenario and learn, through trial and error, from the environment's interactions. It works in discrete time steps to maximize an objective function (reward) and does not need a large volume of training data than supervised learning. SQL queries characteristics, including query type, tables, and query cost, are represented as features (Query2Vector) and fed the query vectors into the DRL model. In this way, the model can dynamically choose optimized settings. The authors recommend Query-level tuning for workloads, including transactional and analytical queries, to optimize the latency, and workload-level tuning is recommended to throughput improvement. They also proposed a query clustering method using a deep learning model to enable cluster-level tuning for analytical scenarios to balance the throughput and latency. The system was tested using SQL (MySQL, PostgreSQL) and NoSQL (MongoDB) databases and JOB, TPC-H, and Sysbench benchmarks. CDBTune [109] is another proposal of applying DRL for holistic knob tuning. It is mainly designed for cloud databases (CDB) since performance in cloud environments is affected by memory size, disk capacity, workload, CPU model, database type, and other factors. Due to its complexity, it is hard to reproduce all conditions and obtain large-high-quality training samples in such scenarios. Unlike OtterTune [108] [8], CDBTune adopts an end-to-end approach and accumulates experience via the trial-and-error method, and makes the model generate a few diverse samples. CDBTune utilizes the deep deterministic policy gradient (DDPG), a policy-based way, to find the best knob settings in high-dimensional continuous space compared with simple regression OtterTune used. The DDPG algorithm makes it feasible for deep neural networks (DNN) to process high-dimensional states, specifically, internal metrics and knob configurations, and generate continuous actions. CDBTune also depends on an active reward function, designed to simulate the DBA's empirical judgment in a real environment, to accelerate model convergence speed. The experiments, conducted under six different workloads on actual cloud databases and three kinds of benchmark tools (Sysbench, MySQL-TPCH, and TPC-MySQL), showed that CDBTune achieves higher throughput and low latency than OtterTune. 3.3 NoSQL stores Column-oriented databases, key-value stores, and document-oriented stores are the types of NoSQL stores mostly commonly used to handle big data [77]. In distributed database environments using NoSQL Stores, the horizontal scalability is intrinsically related to the data partitioning strategy. Partitioning implies dividing the database into smaller and disjoint datasets, and it is possible to use different approaches in partitioning. We generally assign each subset to a storage node to distribute the system load. Static Horizontal partitioning, also called Sharding, separates data into sets of key-value pairs according to the key (range, simple hash, consistent Hash and hyperspace Hash) or group data share interrelated (transversal) values. Vertical partitioning separates data into a set of columns that are accessed together. Unlike horizontal partitioning that applies to all models, it refers exclusively to the NoSQL Stores Wide-column data model [110]. Some NoSQL stores and libraries such as MongoDB and Berkeley DB use B+-tree, one of the popular index structures used to make read operations faster in RDBMS. However, write-intensive workloads may suffer from costly tree maintenance. Most Key-Value prefers key-based search structures such as DHT (Distributed hash table) and LSM (Log-Structured Merge-trees) [110]. We may define DHT as a decentralized storage system that provides insertion, search, and removal functions similar to a hash table to map data in the form of key-value pairs on its storage nodes [111]. The most used approach to model data distribution is using Consistent Hashing and its variants. Consistent Hashing is a distributed hash scheme that operates regardless of the number of storage nodes, assigning each storage node and each key entry to a position in an abstract circle or hash ring. Wide column-stores also use an LSM-tree data structure to implement their storage. An LSM tree structure includes an in-memory tree (or buffer), and some stored on-disk trees with immutable buffer copies. This solution efficiently stores and sorts (key, value) pairs of data in-memory. Data are flushed to disk in append-only batches only when the buffer exceeds a threshold, providing excellent response time for writing operations. LSM-trees also operate well with random read operations since read operations access the buffer before the disk files. It uses the Bloom filter index to avoid I/O operations [110]. Bloom filter index is a probabilistic and straightforward data structure proposed in 1970 [112], which prevents looking up the storage file for a key that is not there. It is composed of a bit vector, initialized with 0, and fulfilled based on one or more hash functions applied to key data. It is possible to perform association tests on the search key since to find out if an element exists, use the same hash functions on key, and check the respective result positions in the vector. If just one position is filled with 0, the key is definitely not in the dataset, but the key can be if all are filled as 1. Traditional RDBMS like PostgreSQL also offers such type of index as an extension. Normalization and denormalization concerns are also present when modeling and implementing NoSQL data models for OLTP or OLAP purposes. In [113] the authors experimented with analyzing how normalization and denormalization impact data load, storage size, and query response time in document-oriented NoSQL for DW. They generated data for the experiment using an extended version of the Star Schema Benchmark (SSB). 4 Tuning in large-scale data processing systems Large-scale data processing systems have hundreds of configurable parameters, and several of them may affect performance [9]. Having a reasonable set of parameters that can help tune the application execution to a particular context may seem desirable, but configuring too many settings to achieve the best configuration in terms of throughput often turns out to be a challenging and time-consuming task [114]. Herodotoua et al. [10] define three categories of optimization opportunities for big data analytic workloads: (i) data-flow sharing (i.e., a single job performs computations for different logical nodes), (ii) efficient use of materialization of intermediate results, and (iii) automatic reorganization of intermediate data storing, with the help of new data layouts (e.g., employing partitioning) and storage engines of models (e.g., column-based) that may fit better for the considered workflow. Hence, we need to review the actual notion of (database) system tuning. Acquiring new hardware or upgrading software is still out of focus. However, there is no notion of an optimizer that can react to new data structures and access methods, as we have seen in the previous section. In this section, we focus on current techniques for performance tuning within big data systems. We will first cover physical design, considering new and ubiquitous hardware, like SSD disks and data allocation. Then, we discuss distributed data management and its consequences for better performances on those big data systems, including large data warehouses. We review monitoring aspects and automatic, or self-managed, big data systems. 4.1 Storage layout and data placement In traditional database systems, I/O operations are commonly the most expensive ones. Reducing the need for I/O operations or improving I/O performance can substantially impact the overall system's performance. In that context, SSD disks have been used for several years, showing a considerable performance improvement when used instead of a magnetic disk for files with sequential access, like transaction logs and rollback segments [115]. Some works consider using SSD disks to support big data systems (e.g., [116–119]). Special attention has been given to the impact of SSD disks on Hadoop's performance. Although solid-state storage has superior performance over traditional magnetic storage, data access and processing patterns should be considered when replacing magnetic storage with SSD disks [118,116]. In real-world high-performance scenarios, HDDs are used for permanent data storage as they are more cost-effective than SSD [8]. In [117], MapReduce-based system performance is improved through SSD used to store intermediate data (i.e., map output temporarily stored locally). Compression of intermediate data can also enhance MapReduce-based systems I/O performance, but it does not significantly influence HDFS's performance [120]. In [118], the authors propose a data management system that predicts file access patterns and uses SSD to store cached data in HDFS. Authors argue that the overall I/O throughput can be increased when using SSD to cache some data instead of storing all data. Ren and Xia [120] map workload characteristics into the file system, attributes, and use the mapping to allocate data efficiently. Authors experimentally evaluate some benchmarks (HiBench K-means micro-benchmark [70] and TPCx-HS, TPCx-BB, and TPC-H [76]) using two file systems: Btrfs and Ext4. Ext4 performed better for the TPC-H benchmark, while Btrfs achieved better performance for the other benchmarks. In [114], authors use the PUMA Benchmark [71] and Hadoop to evaluate the effect of a data block and memory size on MapReduce performance. They show that increasing the number of threads can improve the performance for specific (small) block sizes but does not reduce execution time by similar factors when block size increases. Also, if block size increases over a particular threshold value, the system loses on the parallelism performance metrics. Authors also present specific parameters to be tuned in terms of Map spill and in the reduce phase. Torabzadehkashi et al. [121] present Catalina, a storage platform equipped with computational storage devices (CSDs) that can be deployed in clusters. Authors argue that using CSD can reduce data movement and increase performance and power efficiency while executing applications in a cluster. Although there exist some works on the use of specialized hardware for database operations and analytic workloads (e.g., [122,123]), current big data analytical environments commonly use commodity hardware. 4.2 Data replication, data transfer, fault tolerance and performance In distributed database systems, we may use data replication to improve availability and fault tolerance. Together with partitioning, data replication also plays a crucial role in distributed systems' performance. Table 2 present some of the challenges related to maintaining reduced I/O [7]. Although it is often possible for users to specify custom partitioning functions, it may be somewhat complicated to know the data's distribution. We need to partition data that results from the mapping and not the input data itself [7]. Besides that, data transfer between nodes can significantly impact overall performance [8]. In large scale clusters, failures are common [7,8], and fault tolerance is required to avoid the cost of fully re-executing a high number of long tasks. In MapReduce implementations, task-level failure recovery costs are commonly reduced through the use of persisted intermediate files. In some early proposals [7,124], such intermediate data materialization mechanisms were reduced/changed to improve performance. In [7] the authors claim that the average size of real-world clusters would not be too large. They describe Themis, a system that aims to reduce the number of executed I/O operations by avoiding materializing intermediate files to disks. To guarantee in-memory sorting, map-resulting partitions should be small. Sampling estimates the approximate distribution of data resulting from mapping to reduce skews and guarantee that intermediate data fits available memory. A coordinator defines partition keys. Range partitioning is used if we must sort reducers outputs; otherwise, the coordinator uses a hash function to define data partitions. Partition and bucketing in Hive-based systems are studied in some works [125,84]. Hive is a warehouse tool [19] built on top of Hadoop that provides a SQL interface and stores data using the HDFS system. Hive's structures include tables, buckets, and partitions. At the low level, tables are mapped to HDFS directories; partitions are assigned to (table's) sub-directories, and buckets define file segments. Hence, Hive partitions are a way to partition tables horizontally. Hive uses a function on the value of the bucketing attribute (one per table) to define to which file segment a row belongs. To evaluate partition and bucketing in big data warehousing, Costa et al. [84] test Hive and Presto using queries and data of the star schema benchmark (SSB) [85]. Data is stored with the Optimized Row Columnar (ORC) format and ZLIB for compression. Authors argue that single-level and multi-level partitioning data reduce query execution time even when the SQL where clause does not use the partitioning attribute directly. However, multi-level partitioning should be used carefully to avoid excessive partitioning. A large number of sub-directories (which are created by partitioning) would lead to excessive overhead and reduced performance improvements. After testing bucketing in several scenarios, the authors conclude that bucketing improved performance only when applied simultaneously in two tables and by the attribute used to join them. In [124], authors state that only about 5% of daily jobs in cluster workloads at Yahoo! use over 90% of available resources. The authors present Sailfish, a system that uses the aggregation of MapReduce intermediate data and data-driven auto-tuning functionalities to achieve high performance. To aggregate intermediate data, authors propose a specialized file type (intermediate data file) created through an extension to a distributed file system. The system automatically determines the number of reduced tasks and range-keys in jobs assigned to deal with data volume changes and skews in intermediate data. The authors' proposals can also be applied to other contexts than MapReduce that have to handle operations with materialized intermediate data, namely Group-by SQL commands and joins in parallel database systems. Zhang et al. [8] describe Riffle, a shuffle merge scheduler used on Spark clusters jobs at Facebook to merge fragmented intermediate shuffle files into larger blocks. A shuffle merge instance runs on worker nodes. A scheduler collects block sizes of intermediate files generated by tasks, communicates with merge instances, sends merger requests, and obtains results. Merges are done on compressed serialized files, on a block by block basis, and blocks are grouped considering reduce partitions. Merging policies are configurable and based on the existing number of map output files and the average block sizes. Authors claim that this is important to adapt the system to file systems with distinct I/O characteristics. Shuffle merging results in larger files with large sequential I/O requests, instead of random requests in small files. 4.3 Caching and memory tuning Main-memory caching can significantly improve data-related jobs. In [126], authors evaluate in-memory caching configurations and propose an automatic selection of caching strategy to be used in Spark [80] (but argue that their proposals may apply to other in-memory distributed platforms). The proposed approach considers the file's size in the file system, available cluster-wide main memory, and overhead factors (e.g., due to serialization and compression) related to caching strategies. Preliminary results show performance improvements over default settings. Spark memory-related parameter tuning is also discussed in [127,128]. In highly concurrent systems, each task gets just a small piece of working memory, and intermediate data and results may be spilled to disk. Such disk spill may happen in relational DBMS executing a too high number of concurrent SQLs with sorting operations (e.g., as part of joins and aggregations) and MapReduce-based analytics that performs too many tasks simultaneously [8]. Then, to avoid performance degradation due to the disk spill in MapReduce, each task input size should fit the available memory [8]. Note that reducing the number of tasks may increase the size of data consumed by each one, and there should be a threshold value (usually found by experimentation) that may lead to reduced disk spill. 4.4 Big data warehousing and logical organization Big data and traditional warehouses are not incompatible, and bringing them together can lead to relevant benefits [129]. Although most of the current works on big data warehousing focus on the use of MapReduce and NoSQL stores [129], there exist research works combine the use of already established analytical capabilities of traditional data warehouses with big data technologies (e.g., [84,81,130]). Star schema data organization is a common approach in traditional warehouses. Costa et al. [84] uses star schema in the big data context and present a performance evaluation on query execution over partitioned data. The authors compare the use of star schema with a fully denormalized table and indicate that the latter tends to outperform the former in terms of performance. In [131], authors discuss batch and streaming analytical workloads in the big data warehousing context. They describe the SSB+ Benchmark, based on the TPC-H [76]. It incorporates new data structures and generators to represent streaming data and workloads (i.e., simulating social networks data) and describing performance results using Cassandra (a NoSQL data store), Hive, and Presto with SSB+. Concerning the logical data layout, authors compare the use of flat tables and star schema in big data warehousing and conclude that the size of dimensions can influence in which organization should be used. They propose that practitioners should perform preliminary analysis using sample data to choose between star schema and flat tables. Notice that partitioning and replicating dimensions tables on warehousing for performance and availability has been studied in several works on the database literature (e.g., [132,133]). 4.5 Monitoring, advising and self-tuning Performance tuning is often performed based on the experience of the system administrator. But monitoring tools are beneficial to identify bottlenecks and possible configuration problems. Although we may find monitoring tools at various levels of an application infrastructure (e.g., disk, network, and file system/operational system), one of the main challenges on their usage is to put the available information together and identify which is actually causing a performance problem. 4.5.1 Monitors and advisors In [134], authors use the time a task waits for a resource to identify inter-query contention on Spark. Users may select one or more executing queries to analyze using a web interface. A multi-level DAG (directed acyclic graph) recognizes the degree of responsibility of a query on the contention of others. Resource usage metrics (related to network, memory, I/O, and CPU) are collected periodically by the system, which may also identify contentions by background and external processes. Metric collection interval may impact prediction accuracy, but collecting data in tiny intervals may cause significant overhead. Cherrypick [135] uses Bayesian Optimization to build a performance model for the considered application. It uses such a model to propose the best cloud configuration (e.g. the number of virtual machines, CPU count and speed per core, etc.) for the application in terms of usage cost and SLA (e.g., guarantee performance and overheads limit). BestConfig [136] is a general-purpose tuning system that considers user-specified performance optimization goals. The tuning process aims to maximize a performance metric, a weighted function defined over the user-defined performance goals and resource usage constraints. Divide and Diverge Sampling and Recursive Bound and Search are used together to propose parameter tuning, in a repeated process until the number of tests reaches a threshold limit or tested parameter configurations reaches parameter upper/lower bounds. The authors present evaluation results on using BestConfig over several systems, including Cassandra (NoSQL database), MySQL (RDBMS), and Hive (for data analytic). HiBench [70] and YCSB [72] are some of the used benchmark tools. Authors claim that BestConfig can lead to performance improvements even though using small samples. Still, that performance increases significantly in some systems (including in the evaluated big data analytic jobs) when increasing the sample size. AutoTune [137] generates a small-scale testbed environment based on production data and uses it to train a prediction model. The trained model proposes parameter configurations. Time constraints are used to limit several steps of the process. Other works on machine learning-based performance predictors and tuning advisors include [138–140]. 4.5.2 Self-tuning systems Self-tuning systems may automatically adjust one or more parameters are to improve the systems' performance. While the systems' complexity and the number of available parameters increase, the need for self-tuning systems becomes more critical. In an early work, Herodotoua et al. [10] present Starfish, a self-tuning tool for MapReduce whose architecture “is guided by work on self-tuning database systems”. Some of Starfish's components are the Data Manager, whose responsibilities include metadata and intermediate data management, the What-if Engine, that estimates workload performance on simulated configurations, and the Just-in-time Optimizer, that uses profiling and sampling statistics together with what-if results to select configuration parameter and job execution techniques. In [134], authors present PPABS, a framework to automate parameter tuning in MapReduce jobs. The system has two main components: Analyzer and Recognizer. The first is responsible for creating classifications of MapReduce jobs into categories and associating fine-tuned parameters' configurations to each category. Historical data (e.g., I/O time and CPU time in kernel and user mode) on executed jobs is used to build a model on performance patterns. Then, a clustering algorithm is used to create the categories. Simulated Annealing is used to choose the parameter configurations for each category. Only a small subset of parameters is considered, including buffer size for sorting, block size in the file system, and limit to do disk spill during the sort. The Recognizer is responsible for classifying incoming jobs into defined categories and for loading the chosen parameter configurations associated with the job's category. The Recognizer uses sampling, i.e., the system executes the task using just a part of its input data to classify the job into pre-defined categories. More recently, Koliopoulos et al. [126] proposed the automatic selection of caching strategy based on data, available cluster-wide main memory, and overhead factors (e.g., due to serialization and compression) related to caching strategies. 5 Recommendations and opportunities Performance tuning is a crucial activity, especially in large data-bound systems, where I/O operations and data movement impose relevant costs. In Section 4, we reviewed the state-of-the-art on data-driven performance tuning in large-scale processing systems. Such works can be categorized in the same way we did in Section 3 to classify tuning actions in traditional database systems, as represented in Fig. 7 . Most of the works in the literature evaluate the use of tuning actions at the physical design perspective. Still, there is no silver bullet that can be used to solve performance problems in every environment. In the following, we present some selected recommendations that can be considered for an initial evaluation when tuning the performance of data-driven large scale jobs. 5.1 Tuning recommendations The first step in performance tuning is usually to identify which are the components to tune and what are the possible changes to be made. Most common tuning interventions suggest some trade-off decisions. The exact impact of making a small change in a system's component or parameter would have on the whole system is not readily known in advance. For instance, the data block size choice is a trade-off decision between page replacement activity and parallelism degree. The use of data compression can be considered another trade-off decision, as it reduces I/O costs, but the use of highly compressed data increases CPU costs. Hence, candidate tuning actions must be carefully evaluated before going into production. Also, tuning actions may not have a straightforward implementation, like data partitioning in MapReduce-based systems. In this context, the input data partition should be done considering mapping results unknown in advance. A poorly designed data partition scheme may not only reduce partitioning effectiveness but lead to some processing overheads depending on the type of skew created. Tuning activities may also involve trade-offs in terms of system recovery costs and performance. For example, the materialization of intermediate results may improve failure recovery but also degrade job execution performance. Another trade-off concerns performance and monetary costs. High-performance storage shows a considerable performance improvement compared to magnetic disks, but storing all data in SSD may have prohibitive costs). Nevertheless, it is still possible to enumerate tuning actions that would likely become good practices to be evaluated when aiming to improve system performance. Table 3 presents such enumeration of selected data-driven performance tuning recommendations for big data systems, summarizing most of the recommendations of the works described in the previous sections. Chosen recommendations in Table 3 are organized according to the environment where they are most likely to be effective. However, we may still apply equivalent recommendations in other settings. 5.2 Research challenges and opportunities Tuning systems that have hundreds of configurable parameters or making design changes in environments that run large jobs on vast volumes of data may turn out to be a challenging task, even when considering the use of lists of recommendations like the one of Table 3. In real-world scenarios, tuning support tools, like monitors and advisors, help identify hot spots and possible parameter changes. Monitoring is a crucial aspect of successful performance tuning. But monitoring too many counters can lead to many useless data and may make it more challenging to find relevant information. Advisors may provide useful insights, but most focus only on certain aspects and do not evaluate the global impact of proposed modifications. Hence, the suggested improvements should be carefully considered before use. Therefore, there would be of great help to have the system to tune itself automatically. But for a self-tuning system to be effective, it should not do just what-if analysis in one or more parameters. In this context, machine learning techniques can make the system know characteristics related to input jobs, data access patterns, and the underlying infrastructure. Then, it should learn what would be the pros-and-cons of each tuning decision in a system-wide perspective and consider the performance trade-off of parameter and design changes before applying them. The use of artificial intelligence in general purpose auxiliary structures (i.e., indexes) to be used in the large-scale processing context is also a possible and challenging way. 6 Conclusions Big data has radically transformed the traditional analytics environment. The big data analytics environment is composed of several heterogeneous systems and commonly distributed solutions. Mixed workloads, produced by batch and stream processing, and near-real-time analytics, have emerged as a real necessity in many contexts. Such complex environments usually deal with a considerable volume of data. As failures are somewhat common, replication and materialization are typically used to allow fast recovery with reduced reprocessing. Also, data shuffling and movement are also frequent operations. Performance tuning is both essential and challenging in the complex environments of big data systems and big data analytics. Usually, there are many parameters to be tuned and decisions to be made. However, we can obtain relevant performance improvements through the use of data-driven tuning actions. Many of such data-driven tuning actions have been studied for several years in traditional database systems. In this work, we review performance tuning in big data systems and analytics. We describe the tuning process and work on some of its activities, like benchmarking, monitoring, and profiling, in the context of big data. We survey and describe data-driven tuning solutions for big data and present them in an organized and categorized way. We also describe tuning advisors and self-tuning proposals for big data environments, extract general and system-specific tuning recommendations and discuss research opportunities. As future work, we plan to take a more profound step in studying a self-tuning system that merges global and local tuning actions. We look forward to combining some of the distinct categories we present, including selective caching, intelligent physical placement, and flexible partitioning. We also aim to extend Tuning Ontology from [95] by instantiating the heuristic (sub)ontology with new rules definition, adding both concepts and relationships to model the common tuning aspects shared among big data systems. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work is partially funded by National Funds through the FCT (Foundation for Science and Technology) in the context of the projects UIDB/04524/2020 and UIDB/00127/2020, and by Fundo Europeu de Desenvolvimento Regional (FEDER), Programa Operacional Competitividade e Internacionalização in the context of the projects POCI-01-0145-FEDER-032636 and Produtech II SIF – POCI-01-0247-FEDER-024541. Some of the authors are partially supported by grants from CNPq and CAPES, Brazilian public funding agencies and research institutes. References [1] D. Abadi A. Ailamaki D. Andersen P. Bailis M. Balazinska P. Bernstein P. Boncz S. Chaudhuri A. Cheung A.H. Doan L. Dong M.J. Franklin J. Freire A. Halevy J.M. Hellerstein S. Idreos D. Kossmann T. Kraska S. Krishnamurthy V. Markl S. Melnik T. Milo C. Mohan T. Neumann B.C. Ooi F. Ozcan J. Patel A. Pavlo R. Popa R. Ramakrishnan C. Ré M. Stonebraker D. Suciu The Seattle report on database research SIGMOD Rec. 48 4 2020 44 53 D. Abadi, A. Ailamaki, D. Andersen, P. Bailis, M. Balazinska, P. Bernstein, P. Boncz, S. Chaudhuri, A. Cheung, A. H. Doan, L. Dong, M. J. Franklin, J. Freire, A. Halevy, J. M. Hellerstein, S. Idreos, D. Kossmann, T. Kraska, S. Krishnamurthy, V. Markl, S. Melnik, T. Milo, C. Mohan, T. Neumann, B. C. Ooi, F. Ozcan, J. Patel, A. Pavlo, R. Popa, R. Ramakrishnan, C. Ré, M. Stonebraker, D. Suciu, The Seattle Report on Database Research, SIGMOD Rec. 48 (4) (2020) 44–53. [2] J. Moorthy R. Lahiri N. Biswas D. Sanyal J. Ranjan K. Nanath P. Ghosh Big Data: prospects and challenges Vikalpa 40 1 2015 74 96 J. Moorthy, R. Lahiri, N. Biswas, D. Sanyal, J. Ranjan, K. Nanath, P. Ghosh, Big Data: Prospects and Challenges, Vikalpa 40 (1) (2015) 74–96. [3] U. Sivarajah M.M. Kamal Z. Irani V. Weerakkody Critical analysis of Big Data challenges and analytical methods J. Bus. Res. 70 2017 263 286 U. Sivarajah, M. M. Kamal, Z. Irani, V. Weerakkody, Critical analysis of Big Data challenges and analytical methods, J. Bus. Res. 70 (2017) 263–286. [4] Y. Wang J.C. dos Reis K.M. Borggren M.A.V. Salles C.B. Medeiros Y. Zhou Modeling and building iot data platforms with actor-oriented databases Advances in Database Technology - 22nd International Conference on Extending Database Technology EDBT 2019 512 523 Y. Wang, J. C. dos Reis, K. M. Borggren, M. A. V. Salles, C. B. Medeiros, Y. Zhou, Modeling and building iot data platforms with actor-oriented databases, in: Advances in Database Technology - 22nd International Conference on Extending Database Technology, EDBT, 2019, pp. 512–523. [5] A. Arvanitis S. Babu E. Chu A. Popescu A. Simitsis K. Wilkinson Automated performance management for the big data stack CIDR 2019 - 9th Biennial Conference on Innovative Data Systems Research 2019 A. Arvanitis, S. Babu, E. Chu, A. Popescu, A. Simitsis, K. Wilkinson, Automated performance management for the big data stack, CIDR 2019 - 9th Biennial Conference on Innovative Data Systems Research (2019). [6] A.N. Navaz M.A. Serhani N. Al-Qirim M. Gergely Towards an efficient and energy-aware mobile big health data architecture Comput. Methods Programs Biomed. 166 2018 137 154 10.1016/j.cmpb.2018.10.008 A. N. Navaz, M. A. Serhani, N. Al-Qirim, M. Gergely, Towards an efficient and Energy-Aware mobile big health data architecture, Comput. Methods Programs Biomed. 166 (2018) 137–154. doi:10.1016/j.cmpb.2018.10.008. [7] A. Rasmussen V.T. Lam M. Conley G. Porter R. Kapoor A. Vahdat Themis: an I/O-efficient MapReduce Proceedings of the Third ACM Symposium on Cloud Computing - SoCC 2012 ACM Press 1 14 A. Rasmussen, V. T. Lam, M. Conley, G. Porter, R. Kapoor, A. Vahdat, Themis: An I/O-Efficient MapReduce, in: Proceedings of the Third ACM Symposium on Cloud Computing - SoCC, ACM Press, 2012, pp. 1–14. [8] H. Zhang B. Cho E. Seyfe A. Ching M.J. Freedman Riffle: optimized Shuffle service for large-scale data Proceedings of the Thirteenth EuroSys Conference 2018 1 15 H. Zhang, B. Cho, E. Seyfe, A. Ching, M. J. Freedman, Riffle: Optimized Shuffle Service for Large-Scale Data, in: Proceedings of the Thirteenth EuroSys Conference, 2018, pp. 1–15. [9] J. Lu Y. Chen H. Herodotou S. Babu Speedup your analytics: automatic parameter tuning for databases and big data systems Proc. VLDB Endow. 12 12 2018 1970 1973 J. Lu, Y. Chen, H. Herodotou, S. Babu, Speedup your analytics: Automatic parameter tuning for databases and big data systems, Proceedings of the VLDB Endowment 12 (12) (2018) 1970–1973. [10] H. Herodotou H. Lim G. Luo N. Borisov L. Dong B. Cetin S. Babu Starfish: a self-tuning system for Big Data analytics CIDR 2011, Fifth Biennial Conference on Innovative Data Systems Research 2011 261 272 H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, B. Cetin, S. Babu, Starfish: A Self-tuning System for Big Data Analytics, in: CIDR 2011, Fifth Biennial Conference on Innovative Data Systems Research, 2011, pp. 261–272. [11] Y. Chen S. Alspaugh R. Katz Interactive analytical processing in big data systems Proc. VLDB Endow. 5 12 2012 1802 1813 Y. Chen, S. Alspaugh, R. Katz, Interactive analytical processing in big data systems, PVLDB 5 (12) (2012) 1802–1813. [12] T. Shah F. Rabhi P. Ray Investigating an ontology-based approach for Big Data analysis of inter-dependent medical and oral health conditions Clust. Comput. 18 1 2015 351 367 T. Shah, F. Rabhi, P. Ray, Investigating an ontology-based approach for Big Data analysis of inter-dependent medical and oral health conditions, Cluster Comput. 18 (1) (2015) 351–367. [13] Y. Riahi S. Riahi Big Data and Big Data analytics: concepts, types and technologies Int. J. Res. Eng. 5 9 2018 524 528 Y. Riahi, S. Riahi, Big Data and Big Data Analytics: concepts, types and technologies, Int. J. Res. Eng. 5 (9) (2018) 524–528. [14] E.G. Ularu F.C. Puican A. Apostu M. Velicanu Perspectives on Big Data and Big Data analytics Database Syst. J. 3 4 2012 3 14 E. G. ULARU, F. C. PUICAN, A. APOSTU, M. VELICANU, Perspectives on Big Data and Big Data Analytics, Database Syst. J. 3 (4) (2012) 3–14. [15] X. Jin B.W. Wah X. Cheng Y. Wang Significance and challenges of Big Data research Big Data Res. 2 2 2015 59 64 X. Jin, B. W. Wah, X. Cheng, Y. Wang, Significance and Challenges of Big Data Research, Big Data Res. 2 (2) (2015) 59–64. [16] F. Özcan Y. Tian P. Tözün Hybrid transactional/analytical processing: a survey Proceedings of the ACM SIGMOD International Conference on Management of Data Part F1277 2017 1771 1775 F. Özcan, Y. Tian, P. Tözün, Hybrid transactional/analytical processing: A survey, Proceedings of the ACM SIGMOD International Conference on Management of Data Part F1277 (2017) 1771–1775. [17] D. Abadi R. Agrawal A. Ailamaki M. Balazinska P.A. Bernstein M.J. Carey S. Chaudhuri J. Dean A. Doan M.J. Franklin J. Gehrke L.M. Haas A.Y. Halevy J.M. Hellerstein Y.E. Ioannidis H.V. Jagadish D. Kossmann S. Madden S. Mehrotra T. Milo J.F. Naughton R. Ramakrishnan V. Markl C. Olston B.C. Ooi C. Re D. Suciu M. Stonebraker T. Walter J. Widom Beckman report on database research Commun. ACM 59 2 2016 92 99 D. Abadi, R. Agrawal, A. Ailamaki, M. Balazinska, P. A. Bernstein, M. J. Carey, S. Chaudhuri, J. Dean, A. Doan, M. J. Franklin, J. Gehrke, L. M. Haas, A. Y. Halevy, J. M. Hellerstein, Y. E. Ioannidis, H. V. Jagadish, D. Kossmann, S. Madden, S. Mehrotra, T. Milo, J. F. Naughton, R. Ramakrishnan, V. Markl, C. Olston, B. C. Ooi, C. Re, D. Suciu, M. Stonebraker, T. Walter, J. Widom, Beckman Report on Database research, Communications of the ACM 59 (2) (2016) 92–99. [18] The Apache Software Foundation Apache Hadoop 2020 The Apache Software Foundation, Apache Hadoop, last accessed: 2020-06-06 (2020). URL [19] A. Thusoo J.S. Sarma N. Jain Z. Shao P. Chakka S. Anthony H. Liu P. Wyckoff R. Murthy Hive: a warehousing solution over a map-reduce framework Proc. VLDB Endow. 2 2 2009 1626 1629 A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu, P. Wyckoff, R. Murthy, Hive: A warehousing solution over a map-reduce framework, PVLDB 2 (2) (2009) 1626–1629. [20] M. Kornacker A. Behm V. Bittorf T. Bobrovytsky C. Ching A. Choi J. Erickson M. Grund D. Hecht M. Jacobs I. Joshi L. Kuff D. Kumar A. Leblang N. Li I. Pandis H. Robinson D. Rorke S. Rus J. Russell D. Tsirogiannis S. Wanderman-Milne M. Yoder Impala: a modern, open-source SQL engine for Hadoop CIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research, Online Proceedings Asilomar, CA, USA, January 4-7, 2015 2015 1406 1415 M. Kornacker, A. Behm, V. Bittorf, T. Bobrovytsky, C. Ching, A. Choi, J. Erickson, M. Grund, D. Hecht, M. Jacobs, I. Joshi, L. Kuff, D. Kumar, A. Leblang, N. Li, I. Pandis, H. Robinson, D. Rorke, S. Rus, J. Russell, D. Tsirogiannis, S. Wanderman-Milne, M. Yoder, Impala: A modern, open-source SQL engine for hadoop, in: CIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings, 2015, p. 1406–1415. [21] M. Armbrust R.S. Xin C. Lian Y. Huai D. Liu J.K. Bradley X. Meng T. Kaftan M.J. Franklin A. Ghodsi M. Zaharia Spark SQL: relational data processing in spark Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data SIGMOD '15 2015 1383 1394 M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng, T. Kaftan, M. J. Franklin, A. Ghodsi, M. Zaharia, Spark sql: Relational data processing in spark, in: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD '15, 2015, p. 1383–1394. [22] Apache Kudu - Fast Analytics on Fast Data 2020 Apache Kudu - Fast Analytics on Fast Data, last Accessed: 2020-06-06 (2020). [23] A. Corbellini C. Mateos A. Zunino D. Godoy S. Schiaffino Persisting big-data: the NoSQL landscape Inf. Syst. 63 2017 1 23 10.1016/j.is.2016.07.009 A. Corbellini, C. Mateos, A. Zunino, D. Godoy, S. Schiaffino, Persisting big-data: The NoSQL landscape, Inf. Syst. 63 (2017) 1–23. doi:10.1016/j.is.2016.07.009. [24] R. Cattell Scalable SQL and NoSQL data stores SIGMOD Rec. 39 4 2010 12 27 10.1145/1978915.1978919 R. Cattell, Scalable SQL and NoSQL data stores, SIGMOD Rec. 39 (4) (2010) 12–27. doi:10.1145/1978915.1978919. [25] B.G. Tudorica C. Bucur A comparison between several NoSQL databases with comments and notes Proc. - RoEduNet IEEE Int. Conf. 2011 10.1109/RoEduNet.2011.5993686 B. G. Tudorica, C. Bucur, A comparison between several NoSQL databases with comments and notes, Proc. - RoEduNet IEEE Int. Conf. (2011). doi:10.1109/RoEduNet.2011.5993686. [26] R. Hecht S. Jablonski Nosql evaluation: a use case oriented survey 2011 International Conference on Cloud and Service Computing CSC 2011 IEEE Computer Society 336 341 R. Hecht, S. Jablonski, Nosql evaluation: A use case oriented survey, in: 2011 International Conference on Cloud and Service Computing, CSC, IEEE Computer Society, 2011, pp. 336–341. [27] E. Stefani K. Hoxha Implementing triple-stores using NoSQL databases CEUR Workshop Proc. 2280 2018 86 92 E. Stefani, K. Hoxha, Implementing Triple-Stores using NoSQL Databases, CEUR Workshop Proc. 2280 (2018) 86–92. [28] A.T. Kabakus R. Kara A performance evaluation of in-memory databases J. King Saud Univ, Comput. Inf. Sci. 29 4 2017 520 525 10.1016/j.jksuci.2016.06.007 A. T. Kabakus, R. Kara, A performance evaluation of in-memory databases, Journal of King Saud University - Computer and Information Sciences 29 (4) (2017) 520 – 525. doi:10.1016/j.jksuci.2016.06.007. URL [29] C. Li B. Li M.Z.A. Bhuiyan L. Wang J. Si G. Wei J. Li Flutedb: an efficient and scalable in-memory time series database for sensor-cloud J. Parallel Distrib. Comput. 122 2018 95 108 10.1016/j.jpdc.2018.07.021 C. Li, B. Li, M. Z. A. Bhuiyan, L. Wang, J. Si, G. Wei, J. Li, Flutedb: An efficient and scalable in-memory time series database for sensor-cloud, Journal of Parallel and Distributed Computing 122 (2018) 95 – 108. doi:10.1016/j.jpdc.2018.07.021. URL [30] J. Arulraj A. Pavlo How to build a non-volatile memory database management system S. Salihoglu W. Zhou R. Chirkova J. Yang D. Suciu Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017 ACM 1753 1758 J. Arulraj, A. Pavlo, How to build a non-volatile memory database management system, in: S. Salihoglu, W. Zhou, R. Chirkova, J. Yang, D. Suciu (Eds.), Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference, ACM, 2017, pp. 1753–1758. [31] I. Petrov A. Koch T. Vinçon S. Hardock C. Riegger Hardware-assisted transaction processing: NVM S. Sakr A.Y. Zomaya Encyclopedia of Big Data Technologies 2018 Springer 1 8 I. Petrov, A. Koch, T. Vinçon, S. Hardock, C. Riegger, Hardware-assisted transaction processing: NVM, in: S. Sakr, A. Y. Zomaya (Eds.), Encyclopedia of Big Data Technologies, Springer, 2018, pp. 1–8. [32] D. Kim W.G. Choi H. Sung S. Park A scalable and persistent key-value store using non-volatile memory Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing SAC '19 2019 Association for Computing Machinery New York, NY, USA 464 467 10.1145/3297280.3298991 D. Kim, W. G. Choi, H. Sung, S. Park, A scalable and persistent key-value store using non-volatile memory, in: Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC '19, Association for Computing Machinery, New York, NY, USA, 2019, p. 464–467. doi:10.1145/3297280.3298991. URL [33] R. Tommasini S. Sakr M. Balduini E.D. Valle An outlook to declarative languages for big steaming data Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems DEBS '19 2019 Association for Computing Machinery New York, NY, USA 199 202 10.1145/3328905.3332462 R. Tommasini, S. Sakr, M. Balduini, E. D. Valle, An outlook to declarative languages for big steaming data, in: Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems, DEBS '19, Association for Computing Machinery, New York, NY, USA, 2019, p. 199–202. doi:10.1145/3328905.3332462. URL [34] M. Aldinucci V. Cardellini G. Mencagli M. Torquati Data stream processing in HPC systems: new frameworks and architectures for high-frequency streaming Parallel Comput. 98 2020 102694 10.1016/j.parco.2020.102694 M. Aldinucci, V. Cardellini, G. Mencagli, M. Torquati, Data stream processing in hpc systems: New frameworks and architectures for high-frequency streaming, Parallel Computing 98 (2020) 102694. doi:10.1016/j.parco.2020.102694. URL [35] Y. Cheng Z. Hao R. Cai Auto-scaling for real-time stream analytics on HPC cloud Serv. Oriented Comput. Appl. 13 2 2019 169 183 10.1007/s11761-019-00262-0 Y. Cheng, Z. Hao, R. Cai, Auto-scaling for real-time stream analytics on hpc cloud, Serv. Oriented Comput. Appl. 13 (2) (2019) 169–183. doi:10.1007/s11761-019-00262-0. URL [36] C. Barba-González A.J. Nebro A. Benítez-Hidalgo J. García-Nieto J.F. Aldana-Montes On the design of a framework integrating an optimization engine with streaming technologies Future Gener. Comput. Syst. 107 2020 538 550 10.1016/j.future.2020.02.020 C. Barba-González, A. J. Nebro, A. Benítez-Hidalgo, J. García-Nieto, J. F. Aldana-Montes, On the design of a framework integrating an optimization engine with streaming technologies, Future Generation Computer Systems 107 (2020) 538 – 550. doi:10.1016/j.future.2020.02.020. URL [37] S. Bergamaschi L. Gagliardelli G. Simonini S. Zhu Bigbench workload executed by using apache flink Proc. Manuf. 11 2017 695 702 27th International Conference on Flexible Automation and Intelligent Manufacturing FAIM2017, 27-30 June 2017, Modena, Italy 2017 10.1016/j.promfg.2017.07.169 S. Bergamaschi, L. Gagliardelli, G. Simonini, S. Zhu, Bigbench workload executed by using apache flink, Procedia Manufacturing 11 (2017) 695 – 702; in: 27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy. doi:10.1016/j.promfg.2017.07.169. URL [38] B.R. Hiraman M.C. Viresh C.K. Abhijeet A study of apache Kafka in Big Data stream processing 2018 Int. Conf. Information, Commun. Eng. Technol. ICICET 2018 2018 2018 2020 10.1109/ICICET.2018.8533771 B. R. Hiraman, M. C. Viresh, C. K. Abhijeet, A Study of Apache Kafka in Big Data Stream Processing, 2018 Int. Conf. Information, Commun. Eng. Technol. ICICET 2018 (2018) 2018–2020, doi:10.1109/ICICET.2018.8533771. [39] R. Khiati M. Hanif C. Lee Stream processing engines for smart healthcare systems IEEE Intl. Conf. on Network Infrastructure and Digital Content IC-NIDC 2018 IEEE 467 471 R. Khiati, M. Hanif, C. Lee, Stream Processing Engines for Smart Healthcare Systems, in: IEEE Intl. Conf. on Network Infrastructure and Digital Content (IC-NIDC), IEEE, 2018, pp. 467–471. [40] V. Persico A. Pescapé A. Picariello G. Sperlí Benchmarking big data architectures for social networks data processing using public cloud platforms Future Gener. Comput. Syst. 89 2018 98 109 10.1016/j.future.2018.05.068 V. Persico, A. Pescapé, A. Picariello, G. Sperlí, Benchmarking big data architectures for social networks data processing using public cloud platforms, Future Generation Computer Systems 89 (2018) 98 – 109. doi:10.1016/j.future.2018.05.068. URL [41] E. Psomakelis K. Tserpes D. Zissis D. Anagnostopoulos T. Varvarigou Context agnostic trajectory prediction based on λ-architecture Future Gener. Comput. Syst. 110 2020 531 539 10.1016/j.future.2019.09.046 E. Psomakelis, K. Tserpes, D. Zissis, D. Anagnostopoulos, T. Varvarigou, Context agnostic trajectory prediction based on λ-architecture, Future Generation Computer Systems 110 (2020) 531 – 539. doi:10.1016/j.future.2019.09.046. URL [42] M. Kiran P. Murphy I. Monga J. Dugan S.S. Baveja Lambda architecture for cost-effective batch and speed big data processing 2015 IEEE International Conference on Big Data (Big Data) 2015 2785 2792 M. Kiran, P. Murphy, I. Monga, J. Dugan, S. S. Baveja, Lambda architecture for cost-effective batch and speed big data processing, in: 2015 IEEE International Conference on Big Data (Big Data), 2015, pp. 2785–2792. [43] V. Persico A. Pescapé A. Picariello G. Sperlí Benchmarking big data architectures for social networks data processing using public cloud platforms Future Gener. Comput. Syst. 89 2018 98 109 V. Persico, A. Pescapé, A. Picariello, G. Sperlí, Benchmarking big data architectures for social networks data processing using public cloud platforms, Future Generation Computer Systems 89 (2018) 98–109. [44] P. Shah D. Hiremath S. Chaudhary Towards development of spark based agricultural information system including geo-spatial data Proc. - 2017 IEEE Int. Conf. Big Data, Big Data 2017 2018-Janua 2017 3476 3481 P. Shah, D. Hiremath, S. Chaudhary, Towards development of spark based agricultural information system including geo-spatial data, Proc. - 2017 IEEE Int. Conf. Big Data, Big Data 2017 2018-Janua (2017) 3476–3481. [45] S. Wolfert L. Ge C. Verdouw M.J. Bogaardt Big Data in smart farming – a review Agric. Syst. 153 2017 69 80 S. Wolfert, L. Ge, C. Verdouw, M. J. Bogaardt, Big Data in Smart Farming – A review, Agric. Syst. 153 (2017) 69–80. [46] G. Atluri A. Karpatne V. Kumar Spatio-temporal data mining: a survey of problems and methods ACM Comput. Surv. 51 4 2018 G. Atluri, A. Karpatne, V. Kumar, Spatio-temporal data mining: A survey of problems and methods, ACM Comput. Surv. 51 (4) (2018). [47] C. Yang K. Clarke S. Shekhar C.V. Tao Big spatiotemporal data analytics: a research and innovation frontier Int. J. Geogr. Inf. Sci. 2019 1 14 10.1080/13658816.2019.1698743 C. Yang, K. Clarke, S. Shekhar, C. V. Tao, Big Spatiotemporal Data Analytics: a research and innovation frontier, Int. J. Geogr. Inf. Sci. (2019) 1–14doi:10.1080/13658816.2019.1698743. [48] K.P. Subbu A.V. Vasilakos Big Data for context aware computing – perspectives and challenges Big Data Res. 10 2017 33 43 K. P. Subbu, A. V. Vasilakos, Big Data for Context Aware Computing – Perspectives and Challenges, Big Data Res. 10 (2017) 33–43. [49] S. Wang Y. Zhong E. Wang An integrated GIS platform architecture for spatiotemporal big data Future Gener. Comput. Syst. 94 2019 160 172 S. Wang, Y. Zhong, E. Wang, An integrated GIS platform architecture for spatiotemporal big data, Futur. Gener. Comput. Syst. 94 (2019) 160–172. [50] T. Chauhan R. Aluvalu Using big data analytics for developing crime predictive model Intl. Conf. on Research and Entrepreneurship ICRE 2016 1 6 T. Chauhan, R. Aluvalu, Using big data analytics for developing crime predictive model, in: Intl. Conf. on Research and Entrepreneurship (ICRE), 2016, pp. 1–6. [51] F. Ullah M. Ali Babar Architectural tactics for Big Data cybersecurity analytics systems: a review J. Syst. Softw. 151 2019 81 118 F. Ullah, M. Ali Babar, Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review, J. Syst. Softw. 151 (2019) 81–118. [52] W. Li X. Ye D. Wang H. Zhang Z. Tang D. Fan N. Sun PIM-WEAVER: a high energy-efficient, general-purpose acceleration architecture for string operations in Big Data processing Sustain. Comput. Inf. Sci. 21 2019 129 142 W. Li, X. Ye, D. Wang, H. Zhang, Z. Tang, D. Fan, N. Sun, PIM-WEAVER: A High Energy-efficient, General-purpose Acceleration Architecture for String Operations in Big Data Processing, Sustain. Comput. Informatics Syst. 21 (2019) 129–142. [53] M. Lnenicka J. Komarkova Developing a government enterprise architecture framework to support the requirements of big and open linked data with the use of cloud computing Int. J. Inf. Manag. 46 December 2018 2019 124 141 M. Lnenicka, J. Komarkova, Developing a government enterprise architecture framework to support the requirements of big and open linked data with the use of cloud computing, Int. J. Inf. Manage. 46 (December 2018) (2019) 124–141. [54] Y. Zhang S. Ren Y. Liu S. Si A big data analytics architecture for cleaner manufacturing and maintenance processes of complex products J. Clean. Prod. 142 2017 626 641 10.1016/j.jclepro.2016.07.123 Y. Zhang, S. Ren, Y. Liu, S. Si, A big data analytics architecture for cleaner manufacturing and maintenance processes of complex products, J. Clean. Prod. 142 (2017) 626–641. doi:10.1016/j.jclepro.2016.07.123. [55] M. Fahmideh G. Beydoun Big data analytics architecture design—an application in manufacturing systems Comput. Ind. Eng. 128 August 2018 2019 948 963 M. Fahmideh, G. Beydoun, Big data analytics architecture design—An application in manufacturing systems, Comput. Ind. Eng. 128 (August 2018) (2019) 948–963. [56] D.U. Pfeiffer K.B. Stevens Spatial and temporal epidemiological analysis in the Big Data era Prev. Vet. Med. 122 1–2 2015 213 220 D. U. Pfeiffer, K. B. Stevens, Spatial and temporal epidemiological analysis in the Big Data era, Prev. Vet. Med. 122 (1-2) (2015) 213–220. [57] N. Spangenberg M. Wilke B. Franczyk A Big Data architecture for intra-surgical remaining time predictions Proc. Comput. Sci. 113 2017 310 317 10.1016/j.procs.2017.08.332 N. Spangenberg, M. Wilke, B. Franczyk, A Big Data architecture for intra-surgical remaining time predictions, Procedia Comput. Sci. 113 (2017) 310–317. doi:10.1016/j.procs.2017.08.332. [58] G. Manogaran R. Varatharajan D. Lopez P.M. Kumar R. Sundarasekar C. Thota A new architecture of Internet of things and big data ecosystem for secured smart healthcare monitoring and alerting system Future Gener. Comput. Syst. 82 2018 375 387 G. Manogaran, R. Varatharajan, D. Lopez, P. M. Kumar, R. Sundarasekar, C. Thota, A new architecture of Internet of Things and big data ecosystem for secured smart healthcare monitoring and alerting system, Futur. Gener. Comput. Syst. 82 (2018) 375–387. [59] S. Sakr A. Elgammal Towards a comprehensive data analytics framework for smart healthcare services Big Data Res. 4 2016 44 58 10.1016/j.bdr.2016.05.002 S. Sakr, A. Elgammal, Towards a Comprehensive Data Analytics Framework for Smart Healthcare Services, Big Data Res. 4 (2016) 44–58. doi:10.1016/j.bdr.2016.05.002. [60] N.A. Ghani S. Hamid I.A. Targio Hashem E. Ahmed Social media big data analytics: a survey Comput. Hum. Behav. 101 August 2018 2019 417 428 N. A. Ghani, S. Hamid, I. A. Targio Hashem, E. Ahmed, Social media big data analytics: A survey, Comput. Human Behav. 101 (August 2018) (2019) 417–428. [61] C. Guo B. Yang J. Hu C. Jensen Learning to route with sparse trajectory sets Proc. - IEEE 34th Int. Conf. Data Eng. ICDE 2018 2018 1085 1096 C. Guo, B. Yang, J. Hu, C. Jensen, Learning to route with sparse trajectory sets, Proc. - IEEE 34th Int. Conf. Data Eng. ICDE 2018 (2018) 1085–1096. [62] J. Snowdon O. Gkountouna A. Züfle D. Pfoser Spatiotemporal traffic volume estimation model based on GPS samples 5th Int. ACM SIGMOD Work. Manag. Min. Enriched Geo-Spatial Data, GeoRich 2018 - Conjunction with SIGMOD 2018 2018 1 6 J. Snowdon, O. Gkountouna, A. Züfle, D. Pfoser, Spatiotemporal traffic volume estimation model based on GPS samples, 5th Int. ACM SIGMOD Work. Manag. Min. Enriched Geo-Spatial Data, GeoRich 2018 - Conjunction with SIGMOD 2018 (2018) 1–6. [63] A. Neilson Indratmo B. Daniel S. Tjandra Systematic review of the literature on Big Data in the transportation domain: concepts and applications Big Data Res. 17 2019 35 44 A. Neilson, Indratmo, B. Daniel, S. Tjandra, Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications, Big Data Res. 17 (2019) 35–44. [64] M. Balduini M. Brambilla E. Della Valle C. Marazzi T. Arabghalizi B. Rahdari M. Vescovi Models and practices in Urban data science at scale Big Data Res. 17 2019 66 84 M. Balduini, M. Brambilla, E. Della Valle, C. Marazzi, T. Arabghalizi, B. Rahdari, M. Vescovi, Models and Practices in Urban Data Science at Scale, Big Data Res. 17 (2019) 66–84. [65] B.N. Silva M. Khan K. Han Integration of Big Data analytics embedded smart city architecture with RESTful web of things for efficient service provision and energy management Future Gener. Comput. Syst. 107 2020 975 987 B. N. Silva, M. Khan, K. Han, Integration of Big Data analytics embedded smart city architecture with RESTful web of things for efficient service provision and energy management, Future Generation Computer Systems 107 (2020) 975–987. [66] M. Roriz Junior R.P. de Oliveira F. Carvalho S. Lifschitz M. Endler Mensageria: a smart city framework for real-time analysis of traffic data streams, big social data and Urban computing (BiDU@VLDB2018 workshop) extended version Commun. Comput. Inf. Sci. 926 2019 59 73 M. Roriz Junior, R. P. de Oliveira, F. Carvalho, S. Lifschitz, M. Endler, Mensageria: A smart city framework for real-time analysis of traffic data streams, Big Social Data and Urban Computing (BiDU@VLDB2018 workshop) extended version - Communications in Computer and Information Science 926 (2019) 59–73. [67] A. Ghazal T. Rabl M. Hu F. Raab M. Poess A. Crolotte H.A. Jacobsen BigBench: towards an industry standard benchmark for big data analytics Proceedings of the ACM SIGMOD International Conference on Management of Data 2013 1197 1208 A. Ghazal, T. Rabl, M. Hu, F. Raab, M. Poess, A. Crolotte, H. A. Jacobsen, BigBench: Towards an industry standard benchmark for big data analytics, Proceedings of the ACM SIGMOD International Conference on Management of Data (2013) 1197–1208. [68] L. Wang J. Zhan C. Luo Y. Zhu Q. Yang Y. He W. Gao Z. Jia Y. Shi S. Zhang C. Zheng G. Lu K. Zhan X. Li B. Qiu BigDataBench: a big data benchmark suite from Internet services 2014 IEEE 20th International Symposium on High Performance Computer Architecture HPCA 2014 IEEE 488 499 L. Wang, J. Zhan, C. Luo, Y. Zhu, Q. Yang, Y. He, W. Gao, Z. Jia, Y. Shi, S. Zhang, C. Zheng, G. Lu, K. Zhan, X. Li, B. Qiu, BigDataBench: A big data benchmark suite from internet services, in: 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA), IEEE, 2014, pp. 488–499. [69] Z. Ming C. Luo W. Gao R. Han Q. Yang L. Wang J. Zhan BDGS: a scalable big data generator suite in big data benchmarking Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8585 2014 138 154 Z. Ming, C. Luo, W. Gao, R. Han, Q. Yang, L. Wang, J. Zhan, BDGS: A scalable big data generator suite in big data benchmarking, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8585 (2014) 138–154. [70] S. Huang J. Huang J. Dai T. Xie B. Huang The HiBench benchmark suite: characterization of the MapReduce-based data analysis 2010 IEEE 26th International Conference on Data Engineering Workshops ICDEW 2010 2010 41 51 S. Huang, J. Huang, J. Dai, T. Xie, B. Huang, The hibench benchmark suite: Characterization of the mapreduce-based data analysis, in: 2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010), 2010, pp. 41–51. [71] F. Ahmad S. Lee M. Thottethodi T.N. Vijaykumar Puma: Purdue MapReduce benchmarks suite Tech. Rep. Tech Report TR-ECE-12-11, Technical Report, Purdue ECE 2012 F. Ahmad, S. Lee, M. Thottethodi, T. N. Vijaykumar, Puma: Purdue mapreduce benchmarks suite, Tech. Rep. Tech Report TR-ECE-12-11, Technical Report, Purdue ECE (2012). [72] B.F. Cooper A. Silberstein E. Tam R. Ramakrishnan R. Sears Benchmarking cloud serving systems with YCSB Proceedings of the 1st ACM Symposium on Cloud Computing 2010 143 154 B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, R. Sears, Benchmarking cloud serving systems with ycsb, in: Proceedings of the 1st ACM Symposium on Cloud Computing, 2010, p. 143–154. [73] M. Li J. Tan Y. Wang L. Zhang V. Salapura Sparkbench: a spark benchmarking suite characterizing large-scale in-memory data analytics Clust. Comput. 20 3 2017 2575 2589 M. Li, J. Tan, Y. Wang, L. Zhang, V. Salapura, Sparkbench: a spark benchmarking suite characterizing large-scale in-memory data analytics, Cluster Computing 20 (3) (2017) 2575–2589. [74] M. Li J. Tan Y. Wang L. Zhang V. Salapura SPARKBENCH: a comprehensive benchmarking suite for in memory data analytic platform spark Proceedings of the 12th ACM International Conference on Computing Frontiers CF 2015 2015 M. Li, J. Tan, Y. Wang, L. Zhang, V. Salapura, SPARKBENCH: A comprehensive benchmarking suite for in memory data analytic platform spark, Proceedings of the 12th ACM International Conference on Computing Frontiers, CF 2015 (2015). [75] R. Lu G. Wu B. Xie J. Hu Stream bench: towards benchmarking modern distributed stream computing frameworks 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing 2014 69 78 R. Lu, G. Wu, B. Xie, J. Hu, Stream bench: Towards benchmarking modern distributed stream computing frameworks, in: 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing, 2014, pp. 69–78. [76] TPC Homepage 2020 TPC Homepage, last Accessed: 2020-06-06 (2020). [77] R. Han L.K. John J. Zhan Benchmarking Big Data systems: a review IEEE Trans. Serv. Comput. 11 3 2018 580 597 R. Han, L. K. John, J. Zhan, Benchmarking Big Data Systems: A Review, IEEE Transactions on Services Computing 11 (3) (2018) 580–597. [78] A. Pagliari F. Huet G. Urvoy-Keller Towards a high-level description for generating stream processing benchmark applications Proceedings - 2019 IEEE International Conference on Big Data 2019 3711 3716 A. Pagliari, F. Huet, G. Urvoy-Keller, Towards a High-Level Description for Generating Stream Processing Benchmark Applications, Proceedings - 2019 IEEE International Conference on Big Data (2019) 3711–3716. [79] S. Ceesay A. Barker B. Varghese Plug and play bench: simplifying big data benchmarking using containers 2017 IEEE International Conference on Big Data (Big Data), Vol. 2018-Janua 2017 IEEE 2821 2828 S. Ceesay, A. Barker, B. Varghese, Plug and play bench: Simplifying big data benchmarking using containers, in: 2017 IEEE International Conference on Big Data (Big Data), Vol. 2018-Janua, IEEE, 2017, pp. 2821–2828. [80] M. Zaharia R.S. Xin P. Wendell T. Das M. Armbrust A. Dave X. Meng J. Rosen S. Venkataraman M.J. Franklin A. Ghodsi J. Gonzalez S. Shenker I. Stoica Apache spark: a unified engine for Big Data processing Commun. ACM 59 11 2016 56 65 M. Zaharia, R. S. Xin, P. Wendell, T. Das, M. Armbrust, A. Dave, X. Meng, J. Rosen, S. Venkataraman, M. J. Franklin, A. Ghodsi, J. Gonzalez, S. Shenker, I. Stoica, Apache Spark: A Unified Engine for Big Data Processing, Communications of the ACM 59 (11) (2016) 56–65. [81] M.Y. Santos C. Costa J. Galvão C. Andrade B.A. Martinho F.V. Lima E. Costa Evaluating SQL-on-Hadoop for Big Data warehousing on not-so-good hardware Proceedings of the 21st International Database Engineering & Applications Symposium on - IDEAS 2017 2017 242 252 M. Y. Santos, C. Costa, J. Galvão, C. Andrade, B. A. Martinho, F. V. Lima, E. Costa, Evaluating SQL-on-Hadoop for Big Data Warehousing on Not-So-Good Hardware, in: Proceedings of the 21st International Database Engineering & Applications Symposium on - IDEAS 2017, 2017, pp. 242–252. [82] R. Sethi M. Traverso D. Sundstrom D. Phillips W. Xie Y. Sun N. Yegitbasi H. Jin E. Hwang N. Shingte C. Berner Presto: SQL on everything 35th IEEE International Conference on Data Engineering ICDE 2019 1802 1813 R. Sethi, M. Traverso, D. Sundstrom, D. Phillips, W. Xie, Y. Sun, N. Yegitbasi, H. Jin, E. Hwang, N. Shingte, C. Berner, Presto: SQL on everything, in: 35th IEEE International Conference on Data Engineering, ICDE, 2019, pp. 1802–1813. [83] M. Hausenblas J. Nadeau Apache drill: interactive ad-hoc analysis at scale Big Data 1 2013 100 104 10.1089/big.2013.0011 M. Hausenblas, J. Nadeau, Apache drill: Interactive ad-hoc analysis at scale, Big Data 1 (2013) 100–104. doi:10.1089/big.2013.0011. [84] E. Costa C. Costa M.Y. Santos Evaluating partitioning and bucketing strategies for hive-based Big Data Warehousing systems J. Big Data 6 1 2019 34 E. Costa, C. Costa, M. Y. Santos, Evaluating partitioning and bucketing strategies for Hive-based Big Data Warehousing systems, Journal of Big Data 6 (1) (2019) 34. [85] P. O'neil B. O'neil X. Chen The Star Schema Benchmark (SSB) 2009 P. O'neil, B. O'neil, X. Chen, The star schema benchmark (SSB), 2009. [86] P. Mehta S. Dorkenwald D. Zhao T. Kaftan A. Cheung M. Balazinska A. Rokem A.J. Connolly J. VanderPlas Y. AlSayyad Comparative evaluation of big-data systems on scientific image analytics workloads Proc. VLDB Endow. 10 11 2017 1226 1237 10.14778/3137628.3137634 P. Mehta, S. Dorkenwald, D. Zhao, T. Kaftan, A. Cheung, M. Balazinska, A. Rokem, A. J. Connolly, J. VanderPlas, Y. AlSayyad, Comparative evaluation of big-data systems on scientific image analytics workloads, Proc. VLDB Endow. 10 (11) (2017) 1226–1237. doi:10.14778/3137628.3137634. URL [87] P.G. Brown Overview of sciDB: large scale array storage, processing and analysis Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data 2010 963 968 P. G. Brown, Overview of scidb: Large scale array storage, processing and analysis, in: Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data, 2010, p. 963–968. [88] D. Halperin V. Teixeira de Almeida L.L. Choo S. Chu P. Koutris D. Moritz J. Ortiz V. Ruamviboonsuk J. Wang A. Whitaker S. Xu M. Balazinska B. Howe D. Suciu Demonstration of the Myria big data management service Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data 2014 881 884 D. Halperin, V. Teixeira de Almeida, L. L. Choo, S. Chu, P. Koutris, D. Moritz, J. Ortiz, V. Ruamviboonsuk, J. Wang, A. Whitaker, S. Xu, M. Balazinska, B. Howe, D. Suciu, Demonstration of the myria big data management service, in: Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data, 2014, p. 881–884. [89] Dask Development Team Dask: library for dynamic task scheduling 2016 Dask Development Team, Dask: Library for dynamic task scheduling, last accessed: 2020-06-06 (2016). URL [90] M. Abadi A. Agarwal P. Barham E. Brevdo Z. Chen C. Citro G.S. Corrado A. Davis J. Dean M. Devin S. Ghemawat I. Goodfellow A. Harp G. Irving M. Isard Y. Jia R. Jozefowicz L. Kaiser M. Kudlur J. Levenberg D. Mané R. Monga S. Moore D. Murray C. Olah M. Schuster J. Shlens B. Steiner I. Sutskever K. Talwar P. Tucker V. Vanhoucke V. Vasudevan F. Viégas O. Vinyals P. Warden M. Wattenberg M. Wicke Y. Yu X. Zheng TensorFlow: large-scale machine learning on heterogeneous systems software available from tensorflow.org 2015 M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, X. Zheng, TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow.org - Last accessed: 2020-06-06 (2015). URL [91] S. Chaudhuri G. Weikum Foundations of automated database tuning Proceedings of the ACM SIGMOD International Conference on Management of Data Baltimore, Maryland, USA, June 14-16, 2005 2005 ACM Baltimore, Maryland, USA 964 965 S. Chaudhuri, G. Weikum, Foundations of automated database tuning, in: Proceedings of the ACM SIGMOD International Conference on Management of Data, Baltimore, Maryland, USA, June 14-16, 2005, ACM, 2005, pp. 964–965. [92] A. Abouzeid K. Bajda-Pawlikowski D. Abadi A. Silberschatz A. Rasin Hadoopdb: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads Proc. VLDB Endow. 2 1 2009 922 933 10.14778/1687627.1687731 A. Abouzeid, K. Bajda-Pawlikowski, D. Abadi, A. Silberschatz, A. Rasin, Hadoopdb: An architectural hybrid of mapreduce and dbms technologies for analytical workloads, Proc. VLDB Endow. 2 (1) (2009) 922–933. doi:10.14778/1687627.1687731. URL [93] S. Chaudhuri V.R. Narasayya Self-tuning database systems: a decade of progress Proceedings of the 33rd International Conference on Very Large Data Bases 2007 3 14 S. Chaudhuri, V. R. Narasayya, Self-tuning database systems: A decade of progress, in: Proceedings of the 33rd International Conference on Very Large Data Bases, 2007, pp. 3–14. [94] S. Chaudhuri G. Weikum Foundations of automated database tuning Proceedings of the 32nd International Conference on Very Large Data 2006 1265 S. Chaudhuri, G. Weikum, Foundations of automated database tuning, in: Proceedings of the 32nd International Conference on Very Large Data, 2006, p. 1265. [95] A.C. Almeida M.L.M. Campos F.A. Baião S. Lifschitz R.P. de Oliveira D. Schwabe An ontological perspective for database tuning heuristics Conceptual Modeling - 38th International Conference ER 2019 240 254 A. C. Almeida, M. L. M. Campos, F. A. Baião, S. Lifschitz, R. P. de Oliveira, D. Schwabe, An ontological perspective for database tuning heuristics, in: Conceptual Modeling - 38th International Conference, ER, 2019, pp. 240–254. [96] N. Noon J. Getta Automated performance tuning of data management systems with materializations and indices J. Comput. Commun. 04 2016 47 53 N. Noon, J. Getta, Automated performance tuning of data management systems with materializations and indices, Journal of Computer and Communications 04 (2016) 47–53. [97] P. Ameri On a self-tuning index recommendation approach for databases 32nd IEEE International Conference on Data Engineering Workshops, ICDE Workshops 2016 201 205 P. Ameri, On a self-tuning index recommendation approach for databases, in: 32nd IEEE International Conference on Data Engineering Workshops, ICDE Workshops, 2016, pp. 201–205. [98] C. Curino Y. Zhang E.P.C. Jones S. Madden Schism: a workload-driven approach to database replication and partitioning Proc. VLDB Endow. 3 1 2010 48 57 C. Curino, Y. Zhang, E. P. C. Jones, S. Madden, Schism: a workload-driven approach to database replication and partitioning, PVLDB 3 (1) (2010) 48–57. [99] L. Zhao S. Sakr A. Fekete H. Wada A. Liu Application-managed database replication on virtualized cloud environments Workshops Proceedings of the IEEE 28th International Conference on Data Engineering ICDE 2012 127 134 L. Zhao, S. Sakr, A. Fekete, H. Wada, A. Liu, Application-managed database replication on virtualized cloud environments, in: Workshops Proceedings of the IEEE 28th International Conference on Data Engineering, ICDE, 2012, pp. 127–134. [100] R. Borovica-Gajić R. Appuswamy A. Ailamaki Cheap data analytics using cold storage devices Proc. VLDB Endow. 9 12 2016 1029 1040 10.14778/2994509.2994521 R. Borovica-Gajić, R. Appuswamy, A. Ailamaki, Cheap data analytics using cold storage devices, Proc. VLDB Endow. 9 (12) (2016) 1029–1040. doi:10.14778/2994509.2994521. URL [101] G.L. Sanders Seungkyoon Shin Denormalization effects on performance of RDBMS Proceedings of the 34th Annual Hawaii International Conference on System Sciences 2001 9 pp G. L. Sanders, Seungkyoon Shin, Denormalization effects on performance of rdbms, in: Proceedings of the 34th Annual Hawaii International Conference on System Sciences, 2001, pp. 9 pp.–. [102] S. Chaudhuri U. Dayal An overview of data warehousing and OLAP technology SIGMOD Rec. 26 1 1997 65 74 10.1145/248603.248616 S. Chaudhuri, U. Dayal, An overview of data warehousing and olap technology, SIGMOD Rec. 26 (1) (1997) 65–74. doi:10.1145/248603.248616. URL [103] R.A.P. Rangel J.P. Ortega J.A. Martínez Flores J.J.G. Barbosa Mirna P. Ponce F. Least likely to use: a new page replacement strategy for improving database management system response time Computer Science - Theory and Applications, First International Computer Science Symposium in Russia CSR 2006 514 523 R. A. P. Rangel, J. P. Ortega, J. A. Martínez Flores, J. J. G. Barbosa, M. P. P. F., Least likely to use: A new page replacement strategy for improving database management system response time, in: Computer Science - Theory and Applications, First International Computer Science Symposium in Russia, CSR, 2006, pp. 514–523. [104] A.O. Thakare P.S. Deshpande Probabilistic page replacement policy in buffer cache management for flash-based cloud databases Comput. Inform. 38 6 2019 1237 1271 A. O. Thakare, P. S. Deshpande, Probabilistic page replacement policy in buffer cache management for flash-based cloud databases, Comput. Informatics 38 (6) (2019) 1237–1271. [105] J. Lu Y. Chen H. Herodotou S. Babu Speedup your analytics: automatic parameter tuning for databases and big data systems Proc. VLDB Endow. 12 12 2019 1970 1973 J. Lu, Y. Chen, H. Herodotou, S. Babu, Speedup your analytics: Automatic parameter tuning for databases and big data systems, Proc. VLDB Endow. 12 (12) (2019) 1970–1973. [106] G. Li X. Zhou S. Li B. Gao Qtune: a query-aware database tuning system with deep reinforcement learning Proc. VLDB Endow. 12 12 2019 2118 2130 G. Li, X. Zhou, S. Li, B. Gao, Qtune: A query-aware database tuning system with deep reinforcement learning, PVLDB 12 (12) (2019) 2118–2130. [107] C. Zheng Z. Ding J. Hu Self-tuning performance of database systems with neural network Proceedings of 10th International Conference Intelligent Computing Theory ICIC 2014 1 12 C. Zheng, Z. Ding, J. Hu, Self-tuning performance of database systems with neural network, in: Proceedings of 10th International Conference Intelligent Computing Theory ICIC, 2014, pp. 1–12. [108] D.V. Aken A. Pavlo G.J. Gordon B. Zhang Automatic database management system tuning through large-scale machine learning Proceedings of the ACM SIGMOD International Conference on Management of Data 2017 1009 1024 D. V. Aken, A. Pavlo, G. J. Gordon, B. Zhang, Automatic database management system tuning through large-scale machine learning, Proceedings of the ACM SIGMOD International Conference on Management of Data (2017) 1009–1024. [109] J. Zhang L. Liu M. Ran Z. Li Y. Liu K. Zhou G. Li Z. Xiao B. Cheng J. Xing Y. Wang T. Cheng An end-to-end automatic cloud database tuning system using deep reinforcement learning Proceedings of the 2019 International Conference on Management of Data - SIGMOD '19 2019 ACM Press 415 432 J. Zhang, L. Liu, M. Ran, Z. Li, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang, T. Cheng, An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning, in: Proceedings of the 2019 International Conference on Management of Data - SIGMOD '19, ACM Press, 2019, pp. 415–432. [110] A. Davoudian L. Chen M. Liu A survey on NoSQL stores ACM Comput. Surv. 51 2 2018 40 A. Davoudian, L. Chen, M. Liu, A Survey on NoSQL Stores, ACM Comput. Surv. 51 (2) (2018) 40:1–40:43. [111] A.S.R. Guzmán D. Valdeolmillos A. Rivas A.G. Arrieta P. Chamoso Creation of a distributed NoSQL database with distributed hash tables Hybrid Artificial Intelligent Systems - 14th International Conference HAIS 2019 26 37 A. S. R. Guzmán, D. Valdeolmillos, A. Rivas, A. G. Arrieta, P. Chamoso, Creation of a distributed nosql database with distributed hash tables, in: Hybrid Artificial Intelligent Systems - 14th International Conference, HAIS, 2019, pp. 26–37. [112] B.H. Bloom Space/time trade-offs in hash coding with allowable errors Commun. ACM 13 7 1970 422 426 B. H. Bloom, Space/time trade-offs in hash coding with allowable errors, Commun. ACM 13 (7) (1970) 422–426. [113] M. Chevalier M. El Malki A. Kopliku O. Teste R. Tournier Document-oriented models for data Warehouses - NoSQL document-oriented for data Warehouses Proceedings of the 18th International Conference on Enterprise Information Systems, vol. 1 2016 SCITEPRESS - Science and Technology Publications 142 149 10.5220/0005830801420149 M. Chevalier, M. El Malki, A. Kopliku, O. Teste, R. Tournier, Document-oriented Models for Data Warehouses - NoSQL Document-oriented for Data Warehouses, in: Proceedings of the 18th International Conference on Enterprise Information Systems, Vol. 1, SCITEPRESS - Science and and Technology Publications, 2016, pp. 142–149. doi:10.5220/0005830801420149. [114] G. Bansal A. Gupta U. Pyne M. Singhal S. Banerjee A framework for performance analysis and tuning in Hadoop based clusters Workshop on Smarter Planet and Big Data Analytics SPBDA 2014 1 6 G. Bansal, A. Gupta, U. Pyne, M. Singhal, S. Banerjee, A framework for performance analysis and tuning in hadoop based clusters, in: Workshop on Smarter Planet and Big Data Analytics (SPBDA), 2014, pp. 1–6. [115] S.-W. Lee B. Moon C. Park J.-M. Kim S.-W. Kim A case for flash memory SSD in enterprise database applications Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data 2008 1075 1086 S.-W. Lee, B. Moon, C. Park, J.-M. Kim, S.-W. Kim, A case for flash memory ssd in enterprise database applications, in: Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, 2008, p. 1075–1086. [116] M. Bakratsas P. Basaras D. Katsaros L. Tassiulas Hadoop MapReduce performance on SSDs for analyzing social networks Big Data Res. 11 2018 1 10 M. Bakratsas, P. Basaras, D. Katsaros, L. Tassiulas, Hadoop MapReduce Performance on SSDs for Analyzing Social Networks, Big Data Research 11 (2018) 1–10. [117] S. Moon J. Lee X. Sun Y. suk Kee Optimizing the Hadoop MapReduce framework with high-performance storage devices J. Supercomput. 71 9 2015 3525 3548 S. Moon, J. Lee, X. Sun, Y. suk Kee, Optimizing the Hadoop MapReduce Framework with high-performance storage devices, Journal of Supercomputing 71 (9) (2015) 3525–3548. [118] K.R. Krish M.S. Iqbal A.R. Butt Venu: orchestrating SSDs in Hadoop storage 2014 IEEE International Conference on Big Data (Big Data) 2014 207 212 K. R. Krish, M. S. Iqbal, A. R. Butt, Venu: Orchestrating ssds in hadoop storage, in: 2014 IEEE International Conference on Big Data (Big Data), 2014, pp. 207–212. [119] D. Wu W. Luo W. Xie X. Ji J. He D. Wu Understanding the impacts of solid-state storage on the Hadoop performance 2013 International Conference on Advanced Cloud and Big Data 2013 125 130 D. Wu, W. Luo, W. Xie, X. Ji, J. He, D. Wu, Understanding the impacts of solid-state storage on the hadoop performance, in: 2013 International Conference on Advanced Cloud and Big Data, 2013, pp. 125–130. [120] D.Q. Ren B. Xia File system performance tuning for standard Big Data benchmarks Procs. Intl. Conf. on Computing and Data Engineering ICCDE 2018 22 26 D. Q. Ren, B. Xia, File System Performance Tuning for Standard Big Data Benchmarks, in: Procs. Intl. Conf. on Computing and Data Engineering (ICCDE), 2018, pp. 22–26. [121] M. Torabzadehkashi S. Rezaei A. HeydariGorji H. Bobarshad V. Alves N. Bagherzadeh Computational storage: an efficient and scalable platform for big data and HPC applications J. Big Data 6 1 2019 M. Torabzadehkashi, S. Rezaei, A. HeydariGorji, H. Bobarshad, V. Alves, N. Bagherzadeh, Computational storage: an efficient and scalable platform for big data and HPC applications, Journal of Big Data 6 (1) (2019). [122] S. Haas O. Arnold B. Nöthen S. Scholze G. Ellguth A. Dixius S. Höppner S. Schiefer S. Hartmann S. Henker T. Hocker J. Schreiter H. Eisenreich J. Schlüßler D. Walter T. Seifert F. Pauls M. Hasler Y. Chen H. Hensel S. Moriam E. Matús C. Mayr R. Schüffny G.P. Fettweis An MPSoC for energy-efficient database query processing 2016 53nd ACM/EDAC/IEEE Design Automation Conference DAC 2016 1 6 S. Haas, O. Arnold, B. Nöthen, S. Scholze, G. Ellguth, A. Dixius, S. Höppner, S. Schiefer, S. Hartmann, S. Henker, T. Hocker, J. Schreiter, H. Eisenreich, J. Schlüßler, D. Walter, T. Seifert, F. Pauls, M. Hasler, Y. Chen, H. Hensel, S. Moriam, E. Matús, C. Mayr, R. Schüffny, G. P. Fettweis, An mpsoc for energy-efficient database query processing, in: 2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC), 2016, pp. 1–6. [123] C. Balkesen V. Varadarajan A. Viswanathan B. Chandrasekaran S. Idicula N. Agarwal E. Sedlar N. Kunal G. Giannikis P. Fender S. Sundara F. Schmidt J. Wen S. Agrawal A. Raghavan RAPID: in-memory analytical query processing engine with extreme performance perWatt Proceedings of the 2018 International Conference on Management of Data - SIGMOD '18 2018 ACM Press New York, New York, USA 1407 1419 C. Balkesen, V. Varadarajan, A. Viswanathan, B. Chandrasekaran, S. Idicula, N. Agarwal, E. Sedlar, N. Kunal, G. Giannikis, P. Fender, S. Sundara, F. Schmidt, J. Wen, S. Agrawal, A. Raghavan, RAPID: In-memory analytical query processing engine with extreme performance perWatt, in: Proceedings of the 2018 International Conference on Management of Data - SIGMOD '18, ACM Press, New York, New York, USA, 2018, pp. 1407–1419. [124] S. Rao R. Ramakrishnan A. Silberstein M. Ovsiannikov D. Reeves Sailfish: a framework for large scale data processing Proceedings of the Third ACM Symposium on Cloud Computing - SoCC '12 2012 ACM Press 1 14 S. Rao, R. Ramakrishnan, A. Silberstein, M. Ovsiannikov, D. Reeves, Sailfish: A Framework For Large Scale Data Processing, in: Proceedings of the Third ACM Symposium on Cloud Computing - SoCC '12, ACM Press, 2012, pp. 1–14. [125] A.S. Kumar Performance analysis of MySQL partition, hive partition-bucketing and apache pig 2016 1st India International Conference on Information Processing IICIP 2016 1 6 A. S. Kumar, Performance analysis of mysql partition, hive partition-bucketing and apache pig, in: 2016 1st India International Conference on Information Processing (IICIP), 2016, pp. 1–6. [126] A.-K. Koliopoulos P. Yiapanis F. Tekiner G. Nenadic J. Keane Towards automatic memory tuning for in-memory Big Data analytics in clusters 2016 IEEE International Congress on Big Data (BigData Congress) 2016 353 356 A.-K. Koliopoulos, P. Yiapanis, F. Tekiner, G. Nenadic, J. Keane, Towards Automatic Memory Tuning for In-Memory Big Data Analytics in Clusters, in: 2016 IEEE International Congress on Big Data (BigData Congress), 2016, pp. 353–356. [127] K. Aziz D. Zaidouni M. Bellafkih Leveraging resource management for efficient performance of Apache Spark J. Big Data 6 1 2019 78 K. Aziz, D. Zaidouni, M. Bellafkih, Leveraging resource management for efficient performance of Apache Spark, Journal of Big Data 6 (1) (2019) 78. [128] A. Gounaris J. Torres A methodology for Spark parameter tuning Big Data Res. 11 2018 22 32 A. Gounaris, J. Torres, A Methodology for Spark Parameter Tuning, Big Data Research 11 (2018) 22–32. [129] M. Ptiček B. Vrdoljak Big Data and new data Warehousing approaches Proceedings of the 2017 International Conference on Cloud and Big Data Computing - ICCBDC 2017 2017 ACM Press 6 10 M. Ptiček, B. Vrdoljak, Big Data and New Data Warehousing Approaches, in: Proceedings of the 2017 International Conference on Cloud and Big Data Computing - ICCBDC 2017, ACM Press, 2017, pp. 6–10. [130] E. Zdravevski P. Lameski A. Dimitrievski M. Grzegorowski C. Apanowicz Cluster-size optimization within a cloud-based ETL framework for Big Data Proceedings - 2019 IEEE International Conference on Big Data 2019 3754 3763 E. Zdravevski, P. Lameski, A. DImitrievski, M. Grzegorowski, C. Apanowicz, Cluster-size optimization within a cloud-based ETL framework for Big Data, Proceedings - 2019 IEEE International Conference on Big Data (2019) 3754–3763. [131] C. Costa M.Y. Santos Evaluating several design patterns and trends in Big Data Warehousing systems Advanced Information Systems Engineering 2018 Springer International Publishing 459 473 C. Costa, M. Y. Santos, Evaluating Several Design Patterns and Trends in Big Data Warehousing Systems, in: Advanced Information Systems Engineering, Springer International Publishing, 2018, pp. 459–473. [132] R.L. de Carvalho Costa P. Furtado Data warehouses in grids with high QoS 8th International Conference, Data Warehousing and Knowledge Discovery DaWaK Lecture Notes in Computer Science vol. 4081 2006 Springer 207 217 R. L. de Carvalho Costa, P. Furtado, Data warehouses in grids with high qos, in: Data Warehousing and Knowledge Discovery, 8th International Conference, DaWaK, Vol. 4081 of Lecture Notes in Computer Science, Springer, 2006, pp. 207–217. [133] P. Furtado Efficient and robust node-partitioned data Warehouses Database Technologies 2009 IGI Global 658 677 P. Furtado, Efficient and Robust Node-Partitioned Data Warehouses, in: Database Technologies, IGI Global, 2009, pp. 658–677. [134] D. Wu A. Gokhale A self-tuning system based on application profiling and performance analysis for optimizing Hadoop MapReduce cluster configuration 20th Annual International Conference on High Performance Computing 2013 IEEE 89 98 D. Wu, A. Gokhale, A self-tuning system based on application Profiling and Performance Analysis for optimizing Hadoop MapReduce cluster configuration, in: 20th Annual International Conference on High Performance Computing, IEEE, 2013, pp. 89–98. [135] O. Alipourfard H.H. Liu J. Chen S. Venkataraman M. Yu M. Zhang Cherrypick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics 2017 ACM O. Alipourfard, H. H. Liu, J. Chen, S. Venkataraman, M. Yu, M. Zhang, Cherrypick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics, ACM, 2017. [136] Y. Zhu J. Liu M. Guo Y. Bao W. Ma Z. Liu K. Song Y. Yang BestConfig: tapping the performance potential of systems via automatic configuration tuning Proceedings of the 2017 Symposium on Cloud Computing 2017 ACM 338 350 Y. Zhu, J. Liu, M. Guo, Y. Bao, W. Ma, Z. Liu, K. Song, Y. Yang, BestConfig: Tapping the Performance Potential of Systems via Automatic Configuration Tuning, in: Proceedings of the 2017 Symposium on Cloud Computing, ACM, 2017, pp. 338–350. [137] L. Bao X. Liu W. Chen Learning-based automatic parameter tuning for Big Data analytics frameworks 2018 IEEE International Conference on Big Data (Big Data) 2018 181 190 L. Bao, X. Liu, W. Chen, Learning-based Automatic Parameter Tuning for Big Data Analytics Frameworks, in: 2018 IEEE International Conference on Big Data (Big Data), 2018, pp. 181–190. [138] J.L. Berral N. Poggi D. Carrera A. Call R. Reinauer D. Green ALOJA-ML: a framework for automating characterization and knowledge discovery in Hadoop deployments Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15 2015 ACM Press 1701 1710 J. L. Berral, N. Poggi, D. Carrera, A. Call, R. Reinauer, D. Green, ALOJA-ML: A Framework for Automating Characterization and Knowledge Discovery in Hadoop Deployments, in: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15, ACM Press, 2015, pp. 1701–1710. [139] H. Tariq H. Al-Sahaf I. Welch Modelling and prediction of resource utilization of Hadoop clusters Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing 2019 93 100 H. Tariq, H. Al-Sahaf, I. Welch, Modelling and Prediction of Resource Utilization of Hadoop Clusters, in: Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing, 2019, pp. 93–100. [140] G. Wang J. Xu B. He A novel method for tuning configuration parameters of spark based on machine learning 2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS) 2016 586 593 G. Wang, J. Xu, B. He, A novel method for tuning configuration parameters of spark based on machine learning, in: 2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS), 2016, pp. 586–593. "
    },
    {
        "doc_title": "EasyBDI: Near real-time data analytics over heterogeneous data sources",
        "doc_scopus_id": "85113713822",
        "doc_doi": "10.5441/002/edbt.2021.88",
        "doc_eid": "2-s2.0-85113713822",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Data distribution",
            "Data warehouse systems",
            "Distributed data sources",
            "Heterogeneous data sources",
            "Logical abstraction",
            "Near real-time datum",
            "Photovoltaic energy",
            "Subject-oriented"
        ],
        "doc_abstract": "© 2021 Copyright held by the owner/author(s).The large volume of currently available data creates several opportunities for sciences and industry, especially with the application of data analytics. But also raises challenges that make unfeasible the use of batch-based ETL processes. Indeed, near real-time data analytics is a requirement in several domains as an alternative to traditional data warehouses. In the last years, big data platforms have been developed to enable query execution over distributed data sources. However, they do not deal with subject-oriented analysis, do not provide data distribution transparency, or do not assist with schema mapping and integration. In this demonstration, we present EasyBDI. It's a near real-time big data analytics prototype that enables users to run queries over heterogeneous data sources based on global logical abstractions created by the system and provides some usual concepts of data warehouse systems, like facts and dimensions. We use two motivating scenarios, one based on three years of real data on photovoltaic energy production and consumption, and the other based on the SSB+ benchmark. We will also present implementation challenges, issues, solutions, and insights.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experience: Quality Assessment and Improvement on a Forest Fire Dataset",
        "doc_scopus_id": "85100401157",
        "doc_doi": "10.1145/3428155",
        "doc_eid": "2-s2.0-85100401157",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Assessment and improvement",
            "Controlled fires",
            "Geometric operations",
            "Graph analysis",
            "Natural phenomena",
            "Quality assessment",
            "Segmentation techniques",
            "Video tracking"
        ],
        "doc_abstract": "© 2021 ACM.Spatiooral data can be used to study and simulate the movement and behavior of objects and natural phenomena. However, the use of real-world data raises several challenges related to its acquisition, representation, and quality. This article presents a data cleaning process, based on consistency rules and checks, that uses geometric operations to detect and remove outliers or inaccurate data in a spatiooral series. The proposal consists of selecting key frames and applying the process iteratively until the data have the desired quality. The case study consists of extracting and cleaning spatiooral data from a video tracking the propagation of a controlled fire captured using drones. The source data was generated using segmentation techniques to obtain the regions representing the burned area across time. The main issues concern noisy data (e.g., the height of flames is highly variable) and occlusion due to smoke. The results show that the quality assessment and improvement method proposed in this work can identify and remove inconsistencies from a dataset of more than 22,500 polygons in just a few iterations. The quality of the corrected dataset is verified using metrics and graph analysis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Near real time analytic processing of traffic data streams",
        "doc_scopus_id": "85097256608",
        "doc_doi": "10.1145/3423457.3429365",
        "doc_eid": "2-s2.0-85097256608",
        "doc_date": "2020-11-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Transportation",
                "area_abbreviation": "SOCI",
                "area_code": "3313"
            }
        ],
        "doc_keywords": [
            "Historical data",
            "Prediction methods",
            "Public transportation",
            "Real-time information",
            "Short-term traffic forecasting",
            "Traffic management",
            "Traffic volumes",
            "Transportation and urban planning"
        ],
        "doc_abstract": "© 2020 Owner/Author.Location data is vital for traffic management and for transportation and urban planning, but also benefits people in daily life, helping on decisions related to route planing and on the use of public transportation. Although historical data can provide insights on expected traffic volume at a certain region and time, predictions based solely on historical data fail to deal with events like street works and traffic-accidents. In this work, we use real time information together with historical data to predict traffic by road segment in the near future. The paper outlines the architecture of the system, the data model and the prediction method. Preliminary results using real world data on taxi positions show that using stochastic processes is a promising approach for short-term traffic forecasting.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards the automatic selection of moving regions representation methods",
        "doc_scopus_id": "85096826009",
        "doc_doi": "10.1145/3423335.3428170",
        "doc_eid": "2-s2.0-85096826009",
        "doc_date": "2020-11-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Abstract specifications",
            "Efficient implementation",
            "Interpolation function",
            "Interpolation method",
            "Optimization strategy",
            "Representation method",
            "Spatiotemporal behaviors",
            "Spatiotemporal evolution"
        ],
        "doc_abstract": "© 2020 Owner/Author.Moving region is an abstraction used to represent the spatio-temporal behavior of real-world phenomena in database systems. The most common approach to model moving regions uses geometries to represent their position and shape at different times (observations), and interpolation functions to generate the evolution of the geometries between observations. Several region interpolation methods have been proposed in the databases literature, but as there is no suitable method for all use cases, users must select the most adequate algorithm to represent each region by visual inspection. This can be infeasible when dealing with large datasets. This paper presents the first steps towards a system that suggests which methods (and configurations) can generate representations fitting the requirements of a particular application. It includes an abstract specification of user-defined rules on the spatio-temporal evolution of moving regions to assess the suitability of region interpolation functions, a discussion on optimization strategies for efficient implementation of the rules and illustrative examples using real-world data to show how to use this approach to select the best methods to represent a spatio-temporal phenomena.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sampling strategies to create moving regions from real world observations",
        "doc_scopus_id": "85083039742",
        "doc_doi": "10.1145/3341105.3374019",
        "doc_eid": "2-s2.0-85083039742",
        "doc_date": "2020-03-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Continuous modeling",
            "Interpolation algorithms",
            "Interpolation function",
            "Real-world objects",
            "Sampling strategies",
            "Slice decomposition",
            "Spatial informations",
            "Spatio-temporal data"
        ],
        "doc_abstract": "© 2020 ACM.Spatio-temporal data may be used to represent the evolution of real world objects and phenomena. Such data can be represented in discrete time, which associates spatial information (like position and shape) to time instants, or in continuous time, in which the representation of the evolution of the phenomena is decomposed into slices and interpolation functions are used to estimate the intermediate position and shape at any time. The use of a discrete model may seem more straightforward but a continuous representation provides potential gains in terms of data management, including in compression and spatio-temporal operations. In this work, we study the use of the continuous model to represent deformable moving regions captured at discrete snapshots. We propose strategies to select the observations that should be used to define the time slices of the continuous representation, thus transforming data acquired at discrete steps into a continuous model. We also study how the use of geometry simplification mechanisms may impact on moving regions interpolation quality. We evaluate our proposals using a dataset composed by thousands of aerial bush-fires images. After applying object simplification and slice decomposition, we use interpolation algorithms to generate in-between observations and compare them with real images. The results prove the effectiveness of our proposals and their importance in terms of interpolation accuracy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Matching-aware shape simplification",
        "doc_scopus_id": "85083571483",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85083571483",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Comparative studies",
            "Image representations",
            "Polygon simplification",
            "Real-world",
            "Relevant vertex",
            "Shape simplification",
            "Simplification algorithms",
            "Spatio-temporal data"
        ],
        "doc_abstract": "Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Current research has shown significant interest in spatio-temporal data. The acquisition of spatio-temporal data usually begins with the segmentation of the objects of interest from raw data, which are then simplified and represented as polygons (contours). However, the simplification is usually performed individually, i.e., one polygon at a time, without considering additional information that can be inferred by looking at the correspondences between the polygons obtained from consecutive snapshots. This can reduce the quality of polygon matching, as the simplification algorithm may choose to remove vertices that would be relevant for the matching and maintain other less relevant ones. This causes undesired situations like unmatched vertices and multiple matched vertices. This paper presents a new methodology for polygon simplification that operates on pairs of shapes. The aim is to reduce the occurrence of unmatched and multiple matched vertices, while maintaining relevant vertices for image representation. We evaluated our method on synthetic and real world data and performed an extensive comparative study with two well-known simplification algorithms. The results show that our method outperforms current simplification algorithms, as it reduces the amount of unmatched vertexes and of vertexes with multiple matches.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a qualitative analysis of interpolation methods for deformable moving regions",
        "doc_scopus_id": "85076959248",
        "doc_doi": "10.1145/3347146.3359368",
        "doc_eid": "2-s2.0-85076959248",
        "doc_date": "2019-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Earth-Surface Processes",
                "area_abbreviation": "EART",
                "area_code": "1904"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Interpolation problems",
            "Moving regions",
            "Quality metrics",
            "Spatio-temporal data",
            "Visual analysis"
        ],
        "doc_abstract": "© 2019 Association for Computing Machinery. All rights reserved.Spatio-temporal data on the evolution of real-world phenomena are normally acquired as snapshots in discrete time. The continuous evolution of a phenomenon between observations can be approximated using interpolation methods capable of generating deformable moving regions. Several region interpolation methods have been proposed in the spatio-temporal databases literature, each one with its own characteristics that can be more suited to represent the evolution of specific physical phenomena. In this work, we present SPT Data Lab for the qualitative and quantitative comparison of different region interpolation methods. SPT Data Lab allows users to visualize and refine 2D regions extracted from sequences of observations, execute several region interpolation methods and visualize and compare their results. SPT Data Lab also provides quality metrics that are collected during interpolation, allowing users to assess the quality of different methods. Demonstration participants will experience the application of methods on real-world data that exemplify the importance of using an appropriate method for each use case.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling and representing real-world spatio-temporal data in databases",
        "doc_scopus_id": "85072649872",
        "doc_doi": "10.4230/LIPIcs.COSIT.2019.6",
        "doc_eid": "2-s2.0-85072649872",
        "doc_date": "2019-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Benchmarking methodology",
            "Interpolation problems",
            "Morphing techniques",
            "Moving regions",
            "Natural representation",
            "Research questions",
            "Spatio-temporal data",
            "Spatio-temporal database"
        ],
        "doc_abstract": "© 2019 Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing. All rights reserved.Research in general-purpose spatio-temporal databases has focused mainly on the development of data models and query languages. However, since spatio-temporal data are captured as snapshots, an important research question is how to compute and represent the spatial evolution of the data between observations in databases. Current methods impose constraints to ensure data integrity, but, in some cases, these constraints do not allow the methods to obtain a natural representation of the evolution of spatio-temporal phenomena over time. This paper discusses a different approach where morphing techniques are used to represent the evolution of spatio-temporal data in databases. First, the methods proposed in the spatio-temporal databases literature are presented and their main limitations are discussed with the help of illustrative examples. Then, the paper discusses the use of morphing techniques to handle spatio-temporal data, and the requirements and the challenges that must be investigated to allow the use of these techniques in databases. Finally, a set of examples is presented to compare the approaches investigated in this work. The need for benchmarking methodologies for spatio-temporal databases is also highlighted.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An evaluation of smoothing and remeshing techniques to represent the evolution of real-world phenomena",
        "doc_scopus_id": "85057150958",
        "doc_doi": "10.1007/978-3-030-03801-4_6",
        "doc_eid": "2-s2.0-85057150958",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Deformable moving objects",
            "Delaunay",
            "Edge flips",
            "Mesh quality",
            "Morphing techniques",
            "Real-world",
            "Remeshing method",
            "Remeshing technique"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2018.In this paper we investigate the use of morphing techniques to represent the continuous evolution of deformable moving objects, representing the evolution of real-world phenomena. Our goal is to devise processes capable of generating an approximation of the actual evolution of these objects with a known error. We study the use of different smoothing and remeshing methods and analyze various statistics to establish mesh quality metrics with respect to the quality of the approximation (interpolation). The results of the tests and the statistics that were collected suggest that the quality of the correspondence between the observations has a major influence on the quality and validity of the interpolation, and it is not trivial to compare the quality of the interpolation with respect to the actual evolution of the phenomenon being represented. The Angle-Improving Delaunay Edge-Flips method, overall, obtained the best results, but the Remeshing method seems to be more robust to abrupt changes in the geometry.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A framework for the management of deformable moving objects",
        "doc_scopus_id": "85044826926",
        "doc_doi": "10.1007/978-3-319-78208-9_17",
        "doc_eid": "2-s2.0-85044826926",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Earth-Surface Processes",
                "area_abbreviation": "EART",
                "area_code": "1904"
            },
            {
                "area_name": "Computers in Earth Sciences",
                "area_abbreviation": "EART",
                "area_code": "1903"
            }
        ],
        "doc_keywords": [
            "Compatible triangulations",
            "Deformable moving objects",
            "Interpolation method",
            "Morphing",
            "Moving objects",
            "Spatio-temporal data",
            "Spatio-temporal database",
            "Triangulated polygons"
        ],
        "doc_abstract": "© Springer International Publishing AG, part of Springer Nature 2018.There is an emergence of a growing number of applications and services based on spatiotemporal data in the most diverse areas of knowledge and human activity. The representation of the continuous evolution of moving regions, i.e., entities (or objects) whose position, shape and extent change continuously over time, is particularly challenging and the methods proposed in the literature to obtain such representation still present some issues. In this paper we present a framework for moving objects, in particular, moving regions, that uses the concept of mesh, i.e., a triangulated polygon, compatible triangulation and rigid interpolation methods to represent the continuous evolution of moving regions over time. We also present a spatiotemporal database extension for PostgreSQL that uses this framework and that allows to store moving objects data in a PostgreSQL database and to analyze and manipulate them using SQL. This extension can be smoothly integrated with PostGIS. Experiments show that our framework works with real data and provides a base for further work and investigation in this area.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representation of continuously changing data over time and space: Modeling the shape of spatiotemporal phenomena",
        "doc_scopus_id": "85016818033",
        "doc_doi": "10.1109/eScience.2016.7870891",
        "doc_eid": "2-s2.0-85016818033",
        "doc_date": "2017-03-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Environmental Science (miscellaneous)",
                "area_abbreviation": "ENVI",
                "area_code": "2301"
            },
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Agricultural and Biological Sciences (miscellaneous)",
                "area_abbreviation": "AGRI",
                "area_code": "1101"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Compatible triangulations",
            "Environmental science",
            "Morphing techniques",
            "phenomena",
            "Planar shape morphing",
            "Spatio-temporal data",
            "Spatio-temporal database",
            "Synthetic and real data"
        ],
        "doc_abstract": "© 2016 IEEE.There are numerous technologies and tools to acquire data related to the evolution of spatial phenomena over time. These data are typically organized as sequences of 2D geometric shapes obtained from observations taken at different times. The transformation of such sequences of 2D geometric shapes into spatiotemporal data representations, which can be easily processed and interpreted, has the potential to enable novel applications in fields as diverse as environmental sciences, climate sciences, biology or medicine. This paper focuses on the representation of moving 2D geometric shapes acquired at discrete times using continuous models of time and space. Using morphing techniques based on compatible triangulations, issues regarding the representation of spatiotemporal data in databases, as well as the influence of different design strategies on the fidelity of the approximations with respect to the modelled phenomena, are investigated. An experimental study using synthetic and real data was performed. The findings show that the use of triangulation based interpolation is a promising approach, because it allows creating continuous spatiotemporal representations that are more realistic than those obtained using the solutions proposed in previous work. Open issues regarding the representation of spatiotemporal data in information systems are also highlighted.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Morphing techniques for creating and representing spatiotemporal data in GIS",
        "doc_scopus_id": "84911865064",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911865064",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Environmental Engineering",
                "area_abbreviation": "ENVI",
                "area_code": "2305"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Database research",
            "Morphing",
            "Morphing techniques",
            "Remote sensing technology",
            "Satellite images",
            "Spatio-temporal data",
            "Spatio-temporal database",
            "Spatiotemporal data model"
        ],
        "doc_abstract": "With the development of remote sensing technologies allowing capturing and transmitting geo-referenced data repeatedly along time, there are many applications demanding for efficient tools to deal with spatiotemporal data. However database research on moving objects with extent has mainly focused on spatiotemporal data models and query languages leaving several issues to be solved regarding, for example, the acquisition of spatiotemporal data. This work deals with the application of 2D polygonal morphing techniques to create spatiotemporal data representations of moving objects that may change location, size or shape continuously over time. The aim is to transform a sequence of observations representing a moving object at different time instants into a continuous movement representation suitable to be loaded into spatiotemporal databases. This work also investigates several strategies to minimize users' intervention during the processing of a sequence of observations and presents an evaluation of the reliability of movement representations using real data. The movement of icebergs in the Antarctic seas is used as case study and the data sources are sequences of satellite images capturing the position and shape of the icebergs at different dates.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representation and management of spatiotemporal data in object-relational databases",
        "doc_scopus_id": "84863562974",
        "doc_doi": "10.1145/2245276.2245280",
        "doc_eid": "2-s2.0-84863562974",
        "doc_date": "2012-07-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Data representations",
            "Internal structure",
            "Moving objects",
            "Object-Relational Database",
            "Object-relational DBMS",
            "Re-utilization",
            "Spatial data",
            "Spatial functions",
            "Spatio-temporal data",
            "Storage requirements"
        ],
        "doc_abstract": "This paper deals with the design and implementation of a data model and operations for dealing with continuously changing spatial data in object-relational DBMS. The data model relies on abstract data types but we introduce modifications to the internal structure of the spatiotemporal data representations proposed in the literature, to reduce storage requirements and to enable the reutilization of data during the execution of the queries. We show how to implement spatiotemporal operations relying on the spatial functions released by the underlying DBMS and how to use the alternative data representations to reduce the volume of temporary data created in the evaluation of spatiotemporal operations. We also discuss on the advantages and disadvantages of the proposed solutions. © 2012 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Projects of fisioterapia and terapia ocupacional: A classification approach using text mining in R",
        "doc_scopus_id": "84923933085",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84923933085",
        "doc_date": "2009-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Classification approach",
            "Conceptual structures",
            "Fisioterapia",
            "Hierarchical information",
            "K-nearest neighbors",
            "Media communications",
            "Radioterapia",
            "Text mining"
        ],
        "doc_abstract": "Text mining applications is an area of growing interest in the context of academic research or in business intelligence. There is an enormous amount of textual data available which can be easily accessed via the Internet or downloaded from Databases. Several examples are scientific articles, academic reports, abstracts, books, letters, online forums, mailing lists, blogs and other media communication. There are several different methodologies and approaches in text mining. Assumptions which are based completely on the words in the text, based on the words but which allow some hierarchical information and complex approaches which assume the text has a conceptual structure. This paper presents the basics steps of text mining and a comparative study of two algorithms for reports' classification: K-Nearest Neighbor and Support Vector Machine (SVM). The case study was to identify final degree projects' reports of Fisioterapia and Terapia Ocupacional in the context of the Escola Superior de Tecnologia da Saúde do Porto and enabled to conclude that SVM is able to achieve better results for this classification problem.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A model of approximations for representing time-varying multidimensional data",
        "doc_scopus_id": "50249142498",
        "doc_doi": "10.1109/ICDEW.2008.4498302",
        "doc_eid": "2-s2.0-50249142498",
        "doc_date": "2008-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Abstract data type",
            "Access methods",
            "Data engineering",
            "Decision supports",
            "Hierarchical data structures",
            "International conferences",
            "Large data sets",
            "Multi-dimensional data",
            "New technologies",
            "Spatio-temporal databases",
            "Time-varying",
            "Time-varying data",
            "Widespread use"
        ],
        "doc_abstract": "The widespread use of new technologies for data acquisition and communications is disclosing large amounts of time-varying data that organizations wish to use for monitoring or decision support purposes. The efficient management of such large data sets depends largely on the use of appropriate data structures and access methods. This issue is an important topic of research in the spatiotemporal databases community. This paper presents a novel approach for the representation of large series of time-varying multidimensional data. It is based on a model of approximations that allows creating a hierarchical data structure by using different degrees of precision for each level. The hierarchical data structure allows representing an instance of a single series of discretely or continuously changing data as an abstract data type. The paper also shows how to use this approach to represent the movement of an object within a spatiotemporal database system. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of probabilistic queries in moving objects databases",
        "doc_scopus_id": "33750925699",
        "doc_doi": "10.1145/1140104.1140109",
        "doc_eid": "2-s2.0-33750925699",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Moving objects",
            "Spatiotemporal behavior",
            "Spatiotemporal databases",
            "Spatiotemporal uncertainty"
        ],
        "doc_abstract": "The representation of moving objects in spatial database systems has become an important research topic in recent years. As it is not realistic to track and store the location of objects at every time instant, one of the issues in this domain has to do with handling uncertainty in the location of moving objects. In this paper, we propose three statistical methods for computing probabilistic estimates about the location of a moving object at a certain time and show how to use them for evaluating probabilistic range queries. The focus is on applications dealing with the spatiotemporal behavior of non-network constrained moving objects, for monitoring or data-mining purposes, for instance. Copyright 2006 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Oporto: A realistic scenario generator for moving objects",
        "doc_scopus_id": "0035268347",
        "doc_doi": "10.1023/A:1011412005623",
        "doc_eid": "2-s2.0-0035268347",
        "doc_date": "2001-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The spatio-temporal database research community has just started to investigate benchmarking issues. On one hand we would rather have a benchmark that is representative of real world applications, in order to verify the expressiveness of proposed models. On the other hand, we would like a benchmark that offers a sizeable workload of data and query sets, which could obviously stress the strengths and weaknesses of a broad range of data access methods. This paper offers a framework for a spatio-temporal data sets generator, a first step towards a full benchmark for the large real world application field of \"smoothly\" moving objects with few or no restrictions in motion. The driving application is the modeling of fishing ships where the ships go in the direction of the most attractive shoals of fish while trying to avoid storm areas. Shoals are themselves attracted by plankton areas. Ships are moving points: Plankton or storm areas are regions with fixed center but moving shape; and shoals are moving regions. The specification is written in such a way that the users can easily adjust generation model parameters.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Query operations for moving objects database systems",
        "doc_scopus_id": "0034448244",
        "doc_doi": "10.1145/355274.355290",
        "doc_eid": "2-s2.0-0034448244",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Movement operations",
            "Moving objects",
            "Spatio-temporal databases",
            "Spatio-temporal uncertainty"
        ],
        "doc_abstract": "Geographical Information Systems were originally intended to deal with snapshots representing a single state of some reality but there are more and more applications requiring the representation and querying of time-varying information. This work addresses the representation of moving objects on GIS. The continuous nature of movement raises problems for representation in information systems due to the limited capacity of storage systems and the inherently discrete nature of measurement instruments. The stored information has therefore to be partial and does not allow an exact inference of the real-world object's behavior. To cope with this, query operations must take uncertainty into consideration in their semantics in order to give accurate answers to the users. The paper proposes a set of operations to be included in a GIS or a spatial database to make it able to answer queries on the spatio-temporal behavior of moving objects. The operations have been selected according to the requirements of real applications and their semantics with respect to uncertainty is specified. A collection of examples from a case study is included to illustrate the expressiveness of the proposed operations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representation and manipulation of moving points: An extended data model for location estimation",
        "doc_scopus_id": "0033427333",
        "doc_doi": "10.1559/152304099782330725",
        "doc_eid": "2-s2.0-0033427333",
        "doc_date": "1999-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In this paper we introduce a classification for spatio-temporal systems based on the properties of the represented objects. Building on this classification, we assert that complex objects can be derived from simpler ones, following an evolutionary approach which starts with the study of simple objects and ends by enriching them with new features. This paper focuses on the definition of a data model for the representation of moving points. The model is based on the decomposition of the trajectory of moving points into sections. The movement within each section of a trajectory is described by a variability function. Because for most systems is not possible to store the exact knowledge about the movement of an object, the answers to queries may be imprecise. We propose two additional approaches to deal with impression-the superset and the subset semantics-based on a maximum value for the variability function, and a smooth technique to integrate them in the model. Finally, we analyze certain functional aspects of the implementation of the data model in a Relational Database Management System (RDBMS) and outline directions for future research.",
        "available": false,
        "clean_text": ""
    }
]