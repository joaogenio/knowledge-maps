[
    {
        "doc_title": "Segmentation and Manipulation of Cork Strips in Bulk",
        "doc_scopus_id": "85107151302",
        "doc_doi": "10.1109/ICARSC52212.2021.9429769",
        "doc_eid": "2-s2.0-85107151302",
        "doc_date": "2021-04-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Computer vision techniques",
            "Conveyor belts",
            "Cork stoppers",
            "Industry sectors",
            "Motion planners",
            "Natural cork",
            "Punching machine",
            "Rgb-d cameras"
        ],
        "doc_abstract": "© 2021 IEEE.The production of cork stoppers is the largest application of natural cork, which is an ever-growing industry sector. Many attempts have been made to increase the automation of this process, such as the use of automated cork punching machines, but not all steps of this process are fully efficient such as the manipulation of cork strips prior to perforation, which is still a hand labor. This paper presents a system based on an RGBD camera and a 6 DoF robotic arm that manipulates cork strips which are disposed in bulk, either in a container or in a conveyor belt. It uses computer vision techniques to segment a single cork strip from the bunch and motion planners to control the robotic arm in order to grab the selected cork strip. On the experiments made, the system was able to correctly grab a cork strip with 92% success rate and with a frequency of 6 strips per minute.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Neural Network Classifier and Robotic Manipulation for an Autonomous Industrial Cork Feeder",
        "doc_scopus_id": "85115444254",
        "doc_doi": "10.1007/978-3-030-86230-5_34",
        "doc_eid": "2-s2.0-85115444254",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Convolutional neural network",
            "Cork",
            "Deep learning",
            "High quality",
            "Image processing technique",
            "Network robotics",
            "Neural networks classifiers",
            "Robotic manipulation",
            "Specific orientation",
            "Universal robot"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.This paper presents a solution for an autonomous cork puncher feeder with a robotic arm using image processing techniques and a convolutional neural network. Due to the need for cork strips to be inserted into the puncher with a specific orientation, to produce high quality cork stoppers, the identification of the orientation of each cork strip on the conveyor belt is a necessity. In response to this problem a convolutional neural network is used to analyse images processed with subtracted background, to create a robust solution for cork strips classification. In the tests carried out, a classification accuracy of 100% was obtained in a test data set with 12 different cork strips.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TIMAIRIS: Autonomous Blank Feeding for Packaging Machines",
        "doc_scopus_id": "85087543982",
        "doc_doi": "10.1007/978-3-030-34507-5_7",
        "doc_eid": "2-s2.0-85087543982",
        "doc_date": "2020-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Alternative solutions",
            "Computer vision system",
            "Current packaging",
            "Different shapes",
            "Industrial environments",
            "Mobile manipulator",
            "Modes of operation",
            "Multi-Modal Interactions"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Current packaging machine vendors do not provide any automated mechanism for blank feeding and the state of the art is to have a human operator dedicated to feed the blank piles to the packaging machine. This is a tedious, repetitive and tiring task. This also results in problems with unintentional errors, such as using the wrong pile of blanks. An alternative solution is the use of a fixed robotic arm surrounded by a protective cage. However, this solution is restricted to a single packaging machine, a unique type of blank shapes and does not cooperate with humans. TIMAIRIS is a joint effort between IMA S.p.A., Italy, (IMA) and the Universidade de Aveiro, Portugal, (UAVR), promoted by the European Robotics Challenges (EuRoC) project. Together, we propose a system based on a mobile manipulator for flexible, autonomous and collaborative blank feeding of packaging machines on industrial shop floor. The system provides a software architecture that allows a mobile robot to take high level decisions on how the task should be executed, which can depend on variables such as the number of packaging machines to feed and the rate of blank consumption at each one. Through a computer vision system, blanks of different shapes and sizes are correctly identified for adequate manipulation. The manipulation of the piles of blanks is performed using a single arm using compliant modes of operation to increase manipulation safety and robustness. Additionally, it has a safe navigation system that allows the robot to be integrated in an industrial environment where humans are present. Finally, it provides an enhanced multimodal interaction between human and robot that can be adapted to the environment and operator characteristics to make communication intuitive, redundant and safe.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs",
        "doc_scopus_id": "85070213536",
        "doc_doi": "10.1016/j.robot.2019.06.006",
        "doc_eid": "2-s2.0-85070213536",
        "doc_date": "2019-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "European robotics challenges (EuRoC)",
            "Manufacturing industries",
            "Mobile manipulation",
            "Realistic environments",
            "Robotics technology",
            "Scientific competition",
            "Skill-based"
        ],
        "doc_abstract": "© 2019As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2019-08-02 2019-08-02 2019-08-08 2019-08-08 2019-09-13T06:57:28 S0921-8890(17)30794-7 S0921889017307947 10.1016/j.robot.2019.06.006 S300 S300.1 FULL-TEXT 2020-01-13T11:13:04.440713Z 0 0 20191001 20191031 2019 2019-08-02T15:14:58.064687Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 120 120 C Volume 120 9 103227 103227 103227 201910 October 2019 2019-10-01 2019-10-31 2019 article fla © 2019 Published by Elsevier B.V. SKILLBASEDANYTIMEAGENTARCHITECTUREFOREUROPEANROBOTICSCHALLENGESINREALISTICENVIRONMENTSEUROCCHALLENGE2STAGEIIREALISTICLABS LIM G 1 Introduction 1.1 Motivation 1.2 Related work 2 European robotics challenges (EuRoC) 2.1 Challenge 2: shop floor logistics and manipulation 2.1.1 Benchmarking 2.1.2 Showcase: autonomous packaging 2.2 EuRoC platform 2.3 TIMAIRIS software architecture 3 Agent architecture 3.1 Skill-based anytime agent architecture 3.2 Resource management scheme 4 Solving benchmarking tasks 4.1 Task 1: production logistics 4.1.1 SLC detection 4.1.2 Manipulation and planning strategy 4.2 Task 2: product assembly 4.2.1 Fixture detection 4.2.2 Bolt, nut and washer detection 4.3 Manipulation and planning strategy 5 Solving showcase tasks 5.1 Manipulation of stacked non rigid objects 5.2 Manipulator for packaging 5.3 Showcase perception 5.4 Motion planing 5.5 Task planning 5.6 Safe human–robot collaboration 6 Challenge evaluation 6.1 Benchmark evaluation 6.2 Showcase evaluation 7 Conclusion Acknowledgments References PRATT 2013 10 12 G KITANO 1997 340 347 H PROCEEDINGSFIRSTINTERNATIONALCONFERENCEAUTONOMOUSAGENTS ROBOCUPROBOTWORLDCUPINITIATIVE SICILIANO 2014 1 7 B ISRROBOTIK201441STINTERNATIONALSYMPOSIUMROBOTICSPROCEEDINGS EUROCTHECHALLENGEINITIATIVEFOREUROPEANROBOTICS BISCHOFF 2010 15 16 R CULLY 2015 503 A HILDEBRANDT 2016 21 27 A AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2016INTERNATIONALCONFERENCE AFLEXIBLEROBOTICFRAMEWORKFORAUTONOMOUSMANUFACTURINGPROCESSESREPORTEUROPEANROBOTICSCHALLENGESTAGE1 ZADEH 2017 81 103 S BAKLOUTI 2017 9 14 E NILSSON 1999 205 226 K CHATZILYGEROUDIS 2018 236 250 K INSAURRALDE 2015 87 104 C DEAN 1988 49 54 T AAAIVOL88 ANALYSISTIMEDEPENDENTPLANNING GREFENSTETTE 1992 189 195 J MACHINELEARNINGPROCEEDINGS1992 APPROACHANYTIMELEARNING PEDROSA 2015 457 468 E PROGRESSINARTIFICIALINTELLIGENCE17THPORTUGUESECONFERENCEARTIFICIALINTELLIGENCEEPIA2015 ASKILLBASEDARCHITECTUREFORPICKPLACEMANIPULATIONTASKS AMARAL 2017 198 203 F AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2017IEEEINTERNATIONALCONFERENCE SKILLBASEDANYTIMEAGENTARCHITECTUREFORLOGISTICSMANIPULATIONTASKSEUROCCHALLENGE2STAGEIIREALISTICLABSBENCHMARKING LIM 2017 159 164 G AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2017IEEEINTERNATIONALCONFERENCE RICHROBUSTHUMANROBOTINTERACTIONGESTURERECOGNITIONFORASSEMBLYTASKS LIM 2017 15 27 G IBERIANROBOTICSCONFERENCE HUMANROBOTCOLLABORATIONSAFETYMANAGEMENTFORLOGISTICSMANIPULATIONTASKS MOKHTARI 2016 993 1005 V INTELLIGENTAUTONOMOUSSYSTEMS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES LIM 2019 G BALOGH 2005 17 R THRUN 2006 661 692 S BUEHLER 2009 M DARPAURBANCHALLENGEAUTONOMOUSVEHICLESINCITYTRAFFICVOL56 LIM 2017 336 341 G 2017IEEEINTERNATIONALCONFERENCEMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI NEURALREGULARIZATIONJOINTLYINVOLVINGNEURONSCONNECTIONSFORROBUSTIMAGECLASSIFICATION RUSSELL 2010 S ARTIFICIALINTELLIGENCEAMODERNAPPROACH LIM 2018 231 236 G 2018IEEEINTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC MOBILEMANIPULATIONFORAUTONOMOUSPACKAGINGINREALISTICENVIRONMENTSEUROCCHALLENGE2STAGEIISHOWCASE LIM 2019 1 11 G LIM 2013 387 395 G INTELLIGENTAUTONOMOUSSYSTEMSVOL12 ONTOLOGYREPRESENTATIONINSTANTIATIONFORSEMANTICMAPBUILDINGBYAMOBILEROBOT LIM 2011 492 509 G RUSU 2008 927 941 R RUSU 2009 R SEMANTIC3DOBJECTMAPSFOREVERYDAYMANIPULATIONINHUMANLIVINGENVIRONMENTS OLIVEIRA 2016 614 626 M PEDROSA 2016 35 40 E 2016INTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC ASCANMATCHINGAPPROACHSLAMADYNAMICLIKELIHOODFIELD COHENOR 1995 453 461 D RUSU 2011 1 4 R ROBOTICSAUTOMATIONICRA2011IEEEINTERNATIONALCONFERENCE 3DPOINTCLOUDLIBRARYPCL LOWE 1999 1150 1157 D COMPUTERVISION1999PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCEVOL2 OBJECTRECOGNITIONLOCALSCALEINVARIANTFEATURES TUDICO 2017 498 509 A PORTUGUESECONFERENCEARTIFICIALINTELLIGENCE IMPROVINGBENCHMARKINGMOTIONPLANNINGFORAMOBILEMANIPULATOROPERATINGINUNSTRUCTUREDENVIRONMENTS LIMX2019X103227 LIMX2019X103227XG 2021-08-08T00:00:00.000Z 2021-08-08T00:00:00.000Z © 2019 Published by Elsevier B.V. 2019-08-10T21:40:24.934Z S0921889017307947 National Funds China National Funds for Distinguished Young Scientists FCT - Foundation for Science and Technology UID/CEC/00127/2013 This work was supported by the EuRoC Project under Grant no. 608849 and by National Funds through the FCT - Foundation for Science and Technology , in the context of the project UID/CEC/00127/2013 . Dr. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D. degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Marie Curie individual fellow in the School of Computer Science at University of Manchester, UK. His research interests lie in the area of artificial intelligence and machine learning for autonomous robots, including perception, semantics, cognition and spatiotemporal representations on neuromorphic architectures. Eurico Pedrosa is a Post-Doc Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his Informatics Engineering degree from University of Aveiro in 2010 and a Computer Science Ph.D. degree from Aveiro University in 2018. His research interest are focused on intelligent robotics, robotic navigation including localization and mapping (SLAM), space representation using volumetric grids and most recently the application of radar sensors in indoor robotics. Filipe Amaral is a Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his MSc degree in Computer and Telematics Engineering from University of Aveiro in 2014. His current research interests are in the area of autonomous mobile robotics. Prof. Dr. Artur Pereira was born in Vila Nova de Famalicão, Portugal, in April 1960. He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 2003. He is currently an Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Instituto de Engenharia Electrónica e Informática de Aveiro. The main focus of his research is robotics at the architectural and software levels, with emphasis on simulation, navigation, localization, mapping, and machine learning. Nuno Lau is Assistant Professor at Aveiro University, Portugal and Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), where he leads the Intelligent Robotics and Systems group (IRIS). He got is Electrical Engineering Degree from Oporto University in 1993, a DEA degree in Biomedical Engineering from Claude Bernard University, France, in 1994 and the Ph.D. from Aveiro University in 2003. His research interests are focused on Intelligent Robotics, Artificial Intelligence, Multi-Agent Systems and Simulation. Nuno Lau participated in more than 15 international and national research projects, having the tasks of general or local coordinator in about half of them. Nuno Lau won more than 50 scientific awards in robotic competitions, conferences (best papers) and education. He has lectured courses at Phd and MSc levels on Intelligent Robotics, Distributed Artificial Intelligence, Computer Architecture, Programming, etc. Nuno Lau is the author of more than 150 publications in international conferences and journals. He was President of the Portuguese Robotics Society from 2015 to 2017, and is currently the Vice-President of this Society. Prof. Dr. José Luís Azevedo is currently Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Institute of Electronics and Informatics Engineering of Aveiro (IEETA). He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1998. His current research interests are in the area of cooperative autonomous mobile robotics. Prof. Dr. Bernardo Cunha was born in 1959 in Porto, Portugal. He earned his doctoral degree in electrical engineering at the University of Aveiro, Portugal, in 1999. He is a full time teacher at Universidade de Aveiro in the computer architecture area and an investigator at the Instituto de Engenharia Electrónica e Informática de Aveiro. Current research interests are centered in the area of cooperative autonomous mobile robotics. Simone Badini is a Mechanical Designer in the Research and Development department of IMA Spa since 2013. IMA Spa is a world leader company in the design and manufacture of automatic machines for the processing and packaging of pharmaceuticals, cosmetics, food, tea and coffee and tobacco. He got is M.Sc. degree in Mechanical Engineering from University of Bologna, Italy in 2012. He is currently project manager for the integration of cobot and autonomous mobile robot in the production lines for the IMA group. item S0921-8890(17)30794-7 S0921889017307947 10.1016/j.robot.2019.06.006 271599 2020-01-13T11:13:04.440713Z 2019-10-01 2019-10-31 true 3280747 MAIN 14 53587 849 656 IMAGE-WEB-PDF 1 gr11 88037 82 219 gr6 76125 134 219 gr10 99763 151 219 gr1 84425 55 219 gr12 78236 62 219 pic3 95379 163 140 gr13 84239 60 219 gr17 92453 164 193 gr2 91612 129 219 gr19 18466 78 219 fx1002 5080 40 219 gr9 80285 162 219 pic5 90620 164 140 pic8 92385 164 140 gr4 86357 129 219 gr7 83216 163 155 gr8 78279 144 219 pic4 91410 163 140 gr5 84112 164 138 gr3 101514 163 219 gr18 77492 164 204 pic1 90433 163 140 gr16 82949 47 219 fx1001 5682 68 219 gr14 84362 115 219 gr20 34694 98 219 pic2 92082 164 140 pic6 88104 163 140 gr21 22647 162 219 gr15 93327 108 219 pic7 96532 163 140 gr11 115147 142 378 gr6 102249 210 342 gr10 127663 261 378 gr1 119793 128 506 gr12 100014 106 376 pic3 101319 132 113 gr13 105403 103 376 gr17 149053 321 378 gr2 129409 223 378 gr19 42698 110 309 fx1002 27829 150 816 gr9 109290 277 375 pic5 98434 132 113 pic8 101766 132 113 gr4 113348 190 323 gr7 120919 318 302 gr8 117887 327 496 pic4 97712 132 113 gr5 117350 225 189 gr3 125523 242 325 gr18 101664 243 302 pic1 99455 132 113 gr16 120354 112 525 fx1001 36645 252 816 gr14 114768 194 370 gr20 52488 146 325 pic2 99552 131 112 pic6 98208 132 113 gr21 59417 279 378 gr15 119748 187 378 pic7 107374 132 113 gr11 364806 629 1674 gr6 175979 929 1514 gr10 495261 1154 1674 gr1 351578 568 2243 gr12 185445 470 1668 pic3 169551 583 500 gr13 214287 455 1666 gr17 622945 1422 1674 gr2 342433 988 1675 gr19 90504 487 1369 fx1002 103050 398 2169 gr9 245828 1230 1663 pic5 158735 584 500 pic8 168594 584 500 gr4 258891 842 1433 gr7 296978 1410 1339 gr8 281811 1449 2197 pic4 140145 583 500 gr5 232097 999 840 gr3 443350 1074 1440 gr18 160189 1077 1340 pic1 155377 583 500 gr16 409379 497 2326 fx1001 144071 670 2169 gr14 245424 859 1639 gr20 178264 647 1440 pic2 165958 583 499 pic6 138608 583 500 gr21 195782 1237 1675 gr15 306022 828 1676 pic7 174577 583 500 si27 26913 si33 1898 si34 1618 si31 7355 si3 2005 si14 1133 si6 2114 si38 3009 si21 1832 si37 4191 si18 3593 si36 1780 si16 8122 si2 1418 si8 1661 si1 1607 si9 4591 si26 2321 si19 1613 si32 6363 am 19107071 ROBOT 3227 103227 S0921-8890(17)30794-7 10.1016/j.robot.2019.06.006 Fig. 1 Human operators in a packaging industry. Fig. 2 KUKA KMR mobile manipulator. Fig. 3 Initial setup of the table for task 2. Fig. 4 Showcase environment in the gazebo simulator. Fig. 5 EuRoC software framework for C2 [23]. Fig. 6 Use case diagram of skill-based anytime agent architecture (SAAA). Fig. 7 A higher level overview of SAAA. Fig. 8 Sequence diagram of SAAA. Fig. 9 A skill-based architecture for safe human–robot collaboration. Fig. 10 Example of detection of two SLCs on a shelf. The top image is what is perceived by our detection system (plus depth). The bottom image contains a superimposed 3D model of the SLC for each detection, including the center of the SLC (red dot) and a possible pick point (green dot) . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Example of nut and bolt detection. The top image contains the detection of a nut and the bottom image contains the detection of a bolt. The same detection algorithm is used for nuts and bolts. Fig. 12 Washer detection in the fixture. The top image is what is acquired by the camera. The bottom image is the result of the segmentation by intensity. Fig. 13 Snapshots of bent blank piles. Fig. 14 Gripper and blank magazine in simulator. Fig. 15 Grpeer and blank magazine in the realistic environment. Fig. 16 Two pallets and blank piles and their detection results. The red lines in center and right images indicate the pose of pallets, while the green circles indicate the positions of blank piles. Fig. 17 A sequential result of the pose estimation of a blank pile. Fig. 18 Drawing pin filter to detect outlines. Fig. 19 A task plan for 9 blank piles on two pallets. Fig. 20 Human tracking and HRI during the EuRoC evaluation. Fig. 21 Interaction tree for the showcase task. Table 1 Benckmark task 1. Team Metric 1 Metric 2 Bonus Time AutoMAP 5 5 6.22 4:56 MTC-LU-UoB-Airbus 5 4 0 18:13 RSAII 5 5 3.32 9:14 NimbRo Logistics 5 5 3.45 8:54 TIMAIRIS 5 5 10 3:04 Table 2 Benckmark task 2. Team Metric 1 Metric 2 Bonus Time AutoMAP 5 5 4.16 15.01 MTC-LU-UoB-Airbus 0 1 0 8:54 RSAII 5 5 3.78 16:31 NimbRo Logistics 4 4 0 41:27 TIMAIRIS 5 5 10 6:15 Table 3 Quantifiable evaluation. Objectives Metrics Targets Events Percent (%) O1 O1M1 6+5+3 6+5+3 100 O1 O1M2 6 6 100 O1 O1M3 5 5 100 O1 O1M4 2 2 100 O2 O2M1 6 6 100 O2 O2M2 6 6 100 O2 O2M3 6 6 100 O2 O2M4 2 2 100 O3 O3M1 3 3 100 O3 O3M2 12 12 100 O4 O4M1 4 4 100 O4 O4M2 2 2 100 Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs Gi Hyun Lim a 1 Eurico Pedrosa a 1 Filipe Amaral a Artur Pereira a ⁎ Nuno Lau a ⁎ José Luís Azevedo a Bernardo Cunha a Simone Badini b a IEETA, Universidade de Aveiro, Aveiro, Portugal IEETA, Universidade de Aveiro Aveiro Portugal IEETA,Universidade de Aveiro, Aveiro, Portugal b IMA, Via Emilia 428-442, 40064 Ozzano dell’Emilia, Italy IMA Via Emilia 428-442 Ozzano dell’Emilia 40064 Italy IMA,Via Emilia 428-442, 40064 Ozzano dellEmilia, Italy ⁎ Corresponding authors. 1 These authors contributed equally to this work. As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging. Keywords Skill-based Anytime agent architecture Mobile manipulation Autonomous packaging European robotics challenges (EuRoC) 1 Introduction 1.1 Motivation Several robotics challenges and competitions have been launched for the exchange of research results and for comparative evaluations including manipulation, such as the DARPA Robotics Challenge [1], the RoboCup leagues [2] and the European Robotics challenges (EuRoC) project [3], since neither resources such as robots’ platforms, source codes and datasets are available to the public nor are simulators sufficiently mature to present real environments. To obtain a good result in a robotics challenge or competition, challengers need to develop a robotic system that completes given challenge tasks within a limited time. EuRoC is a research project where a robotic competition is conducted with the aim to develop and present solutions to the European manufacturing industry [3]. The EuRoC consortium has launched three industry-relevant challenges: C1 - Reconfigurable Interactive Manufacturing Cell, C2 - Shop Floor Logistics and Manipulation, and C3 - Plant Servicing and Inspection. In particular, the Challenge 2 (C2) addresses the SRA2009 [4] scenarios: Logistics and Robotic Co-Workers. Mobile manipulators are provided by the EuRoC host as a suggested solution to utilize as logistic carriers and dexterous manipulators. Each challenge team should consist of both a research group and an industry partner to show the use case in a realistic environment. Our work is being developed within the Challenge 2 context. The Challenge 2 host especially set the working time regulations to share the resources among all challenger teams and to make sure that they have the same amount of effective lab time in the realistic environment where the challengers access robotics platforms and benchmark infrastructures. As the use case scenario of TIMAIRIS, which is a collaborative challenge team between the Intelligent Robotics and Intelligent Systems (IRIS) research group from the University of Aveiro (UAVR) in Portugal and IMA S.p.A industry from Italy, an autonomous blank feeding task is investigated because it is not easily solved by a robot. Proof of such assumption is that the industrial state-of-the-art solution for this problem is to use human operators to perform it, as shown in Fig. 1. This is a tedious, repetitive and tiring task and human operators may occasionally refrain from collecting blank piles from more distance pallets. This paper presents a skill-based anytime agent architecture (SAAA) and the result of TIMAIRIS team in the second stage of C2. TIMAIRIS is one of 6 challenger teams advancing to the final stage out of 103 teams in the simulation contest, The second stage consists of three phases to be performed in a real environment: Benchmarking, Freestyle and Showcase. In the realistic labs of the EuRoC challenge tasks, the main constraint is time, not only the running time (online efficiency) in an evaluation matrix but also development time (offline efficiency) to complete the tasks in a limited time. Each challenger team is allowed to take a limited fixed time to access an experimental environment in which EuRoC hosts offer support and the mandatory robot platform is available. Especially, for the three phases of the stage II, each team has evenly-distributed eleven weeks (54 working days) to exclusively access the environment. Challenger teams are required to find efficient and robust skills and complete evaluations in the given time. SAAA provides opportunities to increase efficiency not only in autonomous execution for shop floor logistics and manipulation but also in its use of development during the challenge competition. 1.2 Related work To meet the requirements and constraints on a robotic challenge, challenger teams need an adaptive, flexible, robust and efficient robotics architecture [5,6]. Several reactive architectures are proposed to find a solution that makes a robot complete tasks on-time. By managing mission time, Zadeh et al. [7] developed an autonomous reactive architecture of unmanned vehicles in realistic ocean environment. Synchronization between high level mission and low level path planning is configured to control mission time and to guarantee termination of the mission with the best sequence of tasks fitted to available time. Baklouti et al. [8] proposed a reactive control architecture for wheelchair robot navigation without human intervention and prior knowledge of the world. Nilsson and Johansson [9] introduced a layered Open Robot Control (ORC) architecture to meet industrial demands such as computing efficiency and simple factory-floor operation with integration of online and offline robot programming. Offline programming typically done by robot programmers requires abstract modeling, whereas online programming typically done by production engineers or robot operators utilizes physical robots in real environments. Recently several trial and error methods are proposed to adapt for use by damaged robots [5,10]. Those focus on the online time challenge by reducing the number of trials to recover from damage to complete tasks. Cully et al. [5] introduced an intelligent trial and error algorithm which enables a robot to discover a compensatory behavior from damage without requiring pre-programmed contingency plans. The algorithm conducts experiments based on high-performing policies for the intact robot work on the damaged robot. While traditional reinforcement learning (RC) methods for robots need to reset their learning environments and robots to an initial state, Chatzilygeroudis et al. [10] proposed a reset-free trial-and-error learning for robot damage recovery. Insaurralde and Petillot [11] proposed a capability-oriented robot architecture that enables multiple unmanned vehicles to collaborate to autonomously carry out underwater intervention missions in a fault-tolerant manner. On the other hand, anytime approaches use iterative improvement techniques in problem solving, planning, scheduling [12] and learning [13]. Those address the online time challenge by returning results at any time. To adapt to realistic industrial environments, this paper proposes a skill-based anytime agent architecture (SAAA) by integrating previous work [14–17]. The architecture consists of a solver, modular skills and task representations. The separation of task s that organize orders of objects to assemble into graphs from an agent processing algorithm [18,19] allows a robot to start at any task state. It is not necessary to reset the robot and its environment to run from an initial state. During development, a robotics challenger team needs to repeat a subtask or to execute a skill from a specific state to refine the subtask or the skill until the time of evaluation. On the other hand a human co-worker will continue or restart a task after a task failure or a task completion at the runtime. To the best of our knowledge, robotic system architectures and anytime algorithms consider only online efficiency while the proposed architecture also takes offline efficiency into account. 2 European robotics challenges (EuRoC) Many robotic challenges have been so far launched to find competitive solutions for their applications [20]. The DARPA Grand Challenge [21] and its successive events, DARPA Urban Challenge [22] and DARPA Robotics Challenge [1], have drawn worldwide attention as robotic competitions for autonomous ground vehicles on an off-road course, for autonomous operation in a urban environment and for autonomous emergency maintenance robots, respectively. The main objective of the DARPA challenges is bridging the gap between fundamental discoveries of academia and military use. Since 1997, RoboCup [2] has been held annually to foster AI and intelligent robotics research. Initially the target of competition was a world cup with real robots, nowadays there are also other leagues such as RoboCup Industrial, RoboCup Rescue and RoboCup@Home. Especially, the RoboCup Industrial league defines two tasks: logistics and manipulation; but robots in this league have size constraints. To match the Strategic Research Agenda (SRA) for robotics [4], the EuRoC project was launched in 2014. It aims at exploiting synergies among all the actors of robotics and manufacturing to accelerate the transference of state-of-the-art technologies from academia to industry. After qualification, the members of each team are required to organize from both communities. The main motivation of the European Robotics Challenges (EuRoC) is to make use of robotics products and services by strengthening collaboration between the industrial and the research community [3]. The EuRoC is a research project based on robotics competitions that drives innovation in robotics and manufacturing through a number of application experiments. The Challenge Advisory Board of EuRoC is in charge of the evaluation of the competition project empowered on robotic platforms and benchmark infrastructures to get rid of contestants’ burden on platform-related low-level problems and maintenance. As a realization in this context, the EuRoC initiative has launched and run three challenges in parallel with different motivations and objectives. The Shop Floor Logistics and Manipulation or Challenge 2 (C2) covers application scenarios about robotic co-workers and logistics robots in industrial, professional service and domestic service sectors, which is matched to the Strategic Research Agenda (SRA) for robotics in Europe [4]. In the co-worker scenario, a robot becomes an assistant or collaborator that works literally hand-in-hand with its human counterparts in unstructured environments. Here, the robot needs to communicate with a human to take an order, to ask confirmation, and to reply to a question. Each EuRoC challenge is structured in three successive stages, over the period of 4 years: Stage I QUALIFYING: Simulation Contest, Stage II REALISTIC LABS: Benchmarking, Freestyle and Showcase and Stage III FIELD TESTS: Pilot Experiments. 2.1 Challenge 2: shop floor logistics and manipulation The Shop Floor Logistics and Manipulation challenge or Challenge 2 of EuRoC project consists of a assembly of tasks designed to be solved by a mobile robotic platform operating in industrial environment. This challenge is divided in three main stages where three different scenarios were designed and considered. Stages I and II were already completed and the Stage III will be done at the end of the EuRoC project. Briefly, the first stage was based on a stationary platform and the achieved solutions were integrated in a simulation environment. The Stage II consists in a scenario where a real Light-Weight-Robot (LBR iiwa) equipped with a two-finger jaw-gripper mounted on a mobile base (KUKA omniRob) was controlled through an intranet-based framework (similar to the one that was used in Stage I), as shown in Fig. 2. The final stage, Stage III, will be an application of all the achieved solutions in a real scenario of industry with real work pieces provided by an end-user, showing the real capabilities of this innovative solution and the contribution to the manufacturing dynamics improvement. 2.1.1 Benchmarking For the Stage II in the Challenge 2, the EuRoC host has provided a benchmark environment at DLR in a realistic factory set-up with elements from real end users. It consists of two tasks in simplified manufacturing environment: logistics and assembly. The first task of the Benchmarking phase addresses production logistics. In this task, the mobile manipulator has to bring 5 Small Load Carriers (SLCs) which are distributed in the room to a fixed target area on a table. One SLC is placed on the goal table out of the target area, two are placed on another table and two are located in a shelf. The room has enough space for the robot to move between the tables and the shelf. The start position is in the middle of the room. The setup of the room will be static over the development and evaluation. The SLCs are filled with a various number of nuts and washers from the Benchmarking task 2. For each correctly delivered SLC (its footprint has to be inside the target area) one point is awarded (max 5 points). For each SLC, if all parts in it remain within it during the transport to the goal area, another point is awarded (max 5 points). If the task is completed successfully extra points could be awarded for the execution time. The amount of points is inversely related to the best teams time (max 10 points). The second task of Benchmarking is about mobile manipulation for basic pre-assembly of products. In this task, the robot has to assemble five bolts. The layout of the room and the start position of the robot are the same as in the previous benchmark task. The assembly table is identified as “Pickup table”. On that table, there are 5 nuts, 5 bolts and 5 washers within a fixed area. Next to the parts are two fixtures attached to the table. One of the fixtures holds the washers and the other has a gap with the bolt’s head shape to allow the nut to be screwed. The parts are arranged to minimize occlusions (see Fig. 3). The bolts are upright, the nuts are flat and the washers are placed in the fixture to give a good orientation for grasping. The translation between the different parts may vary. The nuts and bolts are touching a virtual line parallel to the fixtures 10 cm and 20 cm behind, respectively. For each nut correctly screwed, one point is awarded (max 5 points). Applying a washer onto the bolt before screwing a nut gives another point (max 5 points). As in the previous task, if the task is completed successfully extra points are awarded for the execution time. Again, the amount of points is inversely related to the best teams time (max 10 points). 2.1.2 Showcase: autonomous packaging As the third and final challenge of the Stage II, the showcase runs in an simplified environment that, although not located in an industrial plant, includes the most important elements of the real environment where the task will be executed as a final product, as shown in Fig. 4. It uses real pallets with real piles of blanks and a prototype of the blank feeding mechanism of a packaging machine that includes all relevant features for the task to be performed. In this task the following issues are going to be covered: the platform will be able to recognize empty pallets, plan and replan its actions; an initial version of the multimodal interface will be used, integrating the possibility of gesture commands in the interaction with the platform (gestures will be used mostly for safe navigation); the robot will be able to navigate in an environment that includes a few humans and interact with them; the manipulation of the selected type of blanks will be demonstrated. To solve the blank feeding task using a mobile robot, several technical issues are identified. • Manipulation of stacked objects: The blank pile is not a unique rigid solid object. Not only it can bend under its own weight, but blanks can also easily break free from the pile due to the fact that a considerable amount of low friction blanks are stacked on top of each other. • Single arm manipulation: The manipulation of a blank pile is performed using a single arm with a gripper. In order not to miss any blanks out of a stack on a pallet, picking up the blank pile requires several manipulation steps and a specialized gripper. • Smooth placing of the blank pile: Blank piles have to be placed in the feeding mechanism avoiding hard collisions with the blanks that are already there and with the four blank guiding rods that prevent blanks from slipping out of the inclined magazine surface. • Shared workspace with humans: The robot has to be able to navigate in an environment that is shared with humans. This introduces safety concerns but, at the same time, it also rises the opportunity to take advantage of the human–robot proximity to explore human–robot interactions (HRI) to control the robot [16]. • Robust detection of blanks: Blanks are provided in untied piles, piled up on pallets close to each other, which can cause erroneous detection. • Handling global localization errors: The feeding of the blank magazine is an example of a manipulation action that requires a high level of precision for a proper feeding while preventing collisions between the blank magazine and the manipulator. 2.2 EuRoC platform As an objective of the EuRoC, robotics platforms and benchmark infrastructures have been developed to make challengers focus on the challenging research without efforts on platform-related low-level problems and maintenance. Especially for C2, two mobile manipulators are suggested to address logistics and robotic co-workers scenarios, as shown in Fig. 2. EuRoC hosts and robot manufacturers also provide programming and simulation frameworks, and open interfaces on lowest levels. Available EuRoC SW tools 2 2 are also provided for use in the EuRoC. Fig. 5 shows the EuRoC software framework for C2. Challengers are required to develop high-level software components with the similar functionalities but different scenarios to test and validate in meaningful contexts typically for shop floor logistics and manipulation in C2. 2.3 TIMAIRIS software architecture TIMAIRIS software is composed of three main components: a skill-based agent architecture (SAAA) [15], a human tracker and safety manager subsystem [17] and a realistic simulator [14]. TIMAIRIS software uses a skill-based anytime agent architecture [15] that has been evolving from the one used for Stage I simulation tasks [14]. The separation of task-dependent representations and a generic agent processing algorithm allow the robot to start at any task state. Human collaborators or robot operators need to repeat a task after recovering from failure or to test the skill from a given starting point. Having the same architecture performing successfully for such a different set of tasks demonstrates that this team solution is remarkably flexible and well adapted for EuRoC C2 platform and its capabilities. Ensuring safety, industrial robots need to share an environment with humans and to work hand in hand. To realize safe human–robot collaboration, a human tracker and safety manager subsystem has been integrated into the SAAA [17]. The human tracker keeps on tracking humans in a workspace. The safety manager infers whether it is in a safe state or not based on system states and human tracking information. Then, human–robot interaction module takes an order from the operator to resume or stop the paused task using gestures [16,24]. A realistic simulator has been developed for the Challenge 2 (see Fig. 4), using, as starting point, the Stage II simulator provided by the EuRoC host. The simulator allows the execution of the complete Challenge 2 tasks and was an essential tool in the development of TIMAIRIS’ Challenge 2 software. 3 Agent architecture 3.1 Skill-based anytime agent architecture To address the requirements and achieve the objectives of C2, a skill-based anytime agent architecture (SAAA) has been developed to solve the logistics and manipulation tasks [15]. This paper extends the previous architecture in the way that a plan manages more than one object and a robot explores workspace that cannot be covered by its cameras without moving the platform. In particular, the new architecture has been used to solve the Production Logistics and Product Assembly tasks previously described. These tasks have four objectives: perception, manipulation, planning and human robot interaction. Fig. 6 shows the use case diagram of SAAA at development time and/or runtime for human–robot collaboration. A robot and two types of humans are involved in the use case: a developer and a co-worker in a scenario for autonomous packaging. A robot developer sets a robot state to refine a skill which is executed at the state. It is possible that the developer repeat to execute the skill to improve performance and reduce runtime without reseting the robot and its environment. For example, a placing skill will be executed after picking and transport skills in pick-and-place tasks. To refine the placing skill, it is not efficient to run whole pick-and-place task by reseting to an initial state. A human co-worker may provide a command to start and stop a task or to change a sequence of objects to be delivered. A mobile manipulator (see Fig. 2) sequentially executes skills to complete the Production Logistics and Product Assembly task. In a higher level view of the skill-based framework, as shown in Fig. 7, the functional components are represented by boxes. A Perception module collects sensory data and processes them to extract information used by high-level modules. A Skill is the capacity of doing a particular task, such as, picking and placing an object or moving the end-effector of the manipulator to a desired pose. Perception modules and Skills collect and transmit sensory-motor data via Sensor interface and Effector interface, respectively. The Action Planner provides a plan for the target object. A plan contains a sequence of skills and their corresponding objects. The Solver is responsible for taking decisions on how to solve the current task based on the current sensory data and available Skills. To solve these tasks, an agent is required to perceive the environment through sensors and act upon that environment using actuators [25]. A skill-based agent architecture is implemented to develop a generic solution capable of handling shop floor logistics and assembly tasks by analyzing the properties of the environment. Henceforth, the proposed method has been utilized for several packing scenarios by increasing the realization during the EuRoC challenges [16,17,24,26,27]. Fig. 8 shows the sequence diagram of SAAA for autonomous packaging. To complete a task, a robot developer can set the state of a robot to start a new task from the initial state or to repeat the previous skill to continue the previously uncompleted task. When the robot start a task, the Solver [15] reads the Order graph [14] of the task and also tries to get a command from Interaction which is a module that tracks humans [17] and gets a command from a co-worker via gesture recognition [16]. The Solver requests a plan from Planner with the states of the robot and its environment, and executes a sequence of skills by following the plan. It continues until completing the task or terminated by the robot developer. when the robot stops, the developer repeats to set the state and to start the robot. The proposed algorithm is summarized in 1. In each task, the set O of objects to be manipulated, including their properties and place zones, is known in advance. The algorithm starts by building an Order Graph which represents the order of objects. The order is restricted by a direct acyclic graph (DAG), which represents a dependency graph between objects in terms of order of manipulation [14]. Leafs represent objects that need to be handled first. The dummy object λ is added to represent the graph root. To ensure that all object are eventually detected [28], a set of search poses S is estimated by the procedure buildSearchSpace with the insurances that all combined poses cover the whole working space. The set S is encoded as a circular list so that the search for poses never ends. Additionally, to improve the search [29], the vision system on the pan tilt unit is used to detect objects in the environment and the obtained poses are put in the head of S in the order defined by G . It may not detect any object, but, if it does, the system can gain in execution times due to good initial search poses. The algorithm then executes nested loops that, making the robot move around the search poses, finishing when only the root node ( λ ) remains in the Order Graph. Each search pose is explored to see if a leaf object is there. A plan is a tuple [19] consisting of a sequence of skills and their corresponding objects that allows to properly pick objects, move the end-effector of the manipulator and place the objects in the target position [30]. Those skills depend on a priori calculation of the pick and place pose. The specific plan depends on the task being solved. The Action Planner can estimate the state of task based on the input object. The plan could be related with several objects. For instance, in the assembly process several parts are added in sequence until the final assembly is produced. If the execution of the skill succeeds, the leaf corresponding to the processed object is removed from the graph. Because Order Graph and plan are separated from the agent processing algorithm, only three procedures are task dependent, buildOrderGraph, buildSearchSpace and makePlan. This formulation has two significant advantages. First, the agent can start execution at any task state, even if the agent meets a failure while execution, as it can continue the task after recovering from the failure. Second, to solve a task the developer only has to focus on the creation of a plan supported by a set of available skills. 3.2 Resource management scheme Fig. 9 shows an extension of the skill-based architecture for safe human–robot collaboration. The Human tracker keeps on tracking humans in the workspace by using laser scanners. The Human–Robot Interaction (HRI) module communicates with a human by recognizing gestures [24] and by providing information via multi-modal interfaces [16]. The Safety manager infers whether it is in a safe state or not based on system states and human tracking information. When the Safety manager decides to pause an executing task, it requests Solver to gaze at the nearest human operator, and to trigger HRI to interact with him. Then, HRI takes an order from the operator to resume or stop the paused task using gestures. To continuously monitor humans, the Human tracker and Safety manager need to be continuously active, while all other modules including skills and perceptions modules just run on request. That should cause conflicts over resources. For example, when the robot wants to recognize the pose of a blank magazine to feed a blank pile, a perception module tries to rotate the pan–tilt camera system to the blank magazine on the table. If, at the moment, a human operator approaches, the Safety manager also tries to rotate the same camera system to gaze the operator. Based on the skill-based agent architecture [14,15], a resource management scheme is added to the system architecture, as shown in Algorithm 2. When a robot starts, each module which needs to run continuously is launched and becomes a daemon. The Solver builds a resource map which lists all necessary resources for each daemon module. At every spin, which means a wake-up for all subscriptions, services, timers and so on in ROS (Robot Operating System), 3 3 the daemon checks the availability of its resources. If available, it runs normal procedures and release all resources at the end of the procedures. 4 Solving benchmarking tasks The objective of our team is to solve the tasks proposed for the EuRoC Benchmarking phase with high precision, accuracy and robustness, as fast as possible. Time is a key factor in solving the tasks at hand: not only it is an evaluation metric, but, for industrial applications, it a matter of productivity by reducing the time for development. In this section, we present our approach to solve two tasks of the Benchmarking phase -Production Logistics and Product Assembly- using the presented architecture. 4.1 Task 1: production logistics The goal of this task is to find all SLCs present in the room such as tables, shelves and workbenchs and place them in the designated target area in a workbench. Their approximate locations are known, on top of two tables and in a shelf, but their exact positions on those locations are unknown. To solve this task we start by designating several search poses that guarantees that eventually all SLCs are detected with the stereo camera mounted in the pan and tilt unit. Once an SLC is in sight, its pose should be detected, in order to pick it up. Then the SLC is placed somewhere else, in an intermediate or target area. This last step has several variations that affect the overall execution time because of the locations of SLCs. 4.1.1 SLC detection The detection of SLCs has two stages: first, color segmentation is used to define candidate regions in the image, that may contain at least an SLC; second, a 3D point cloud for each region is generated and matched against 3D template of an SLC to find its position and orientation, i.e. its pose. Color segmentation, in the HSV color space, is initially used to detect an SLC due to the SLC’s color homogeneity and good contrast with the environment. The output of the color segmentation is then used to generates blobs that represent SLC candidates. The resulting blobs must have a minimum size to be valid. The overlapping of SLCs in the image may generate a single blob for multiple SLCs, but that is not a concern as the disambiguation is deferred to the next stage. Once color segmentation is completed, the resulting blobs are used as masks to generate a point cloud for each candidate. While 3D point cloud generation and matching processes are heavy, the color segmentation is not. Thus, the SLC detection is time-optimized by reducing many candidates from the color segmentation. Let P be a point cloud and { p i } the set of points of P . The calculation of SLC’s pose relies on the processing of the point cloud P . To reduce the computational complexity of processing a point cloud with a high number of points, P is decimated by applying a voxelization filter. Then, to remove possible outliers in the point cloud a statistical outlier removal [31] is used. To address the possibility of SLCs overlap, the point cloud is divided into clusters using a euclidean cluster extractor [32], the cluster with the highest number of points being assigned to P and the remaining points being discarded – thanks to our agent architecture, discarded SLCs will be detected in the next cycle. The position of the SLC is approximately calculated from the centroid c of P , given by c = 1 n ∑ i = 1 n p i , while its orientation is initially provided by the Principal Component Analysis (PCA) of the projection of P in the X O Y plane [33]. However, this approach is not enough, as the view of the SLC may provide a partial point cloud that skews the centroid from the real center, and the orientation of the PCA has an ambiguity of π radians. The final pose of the SLC is calculated by matching P against a 3D template of the SLC that has its centroid in the origin and its bearing defined by the X axis. Before matching the point cloud, P is transformed to its origin, i.e. the inverse pose of P is applied to itself, then, the transformation that results from the matching between the template and P is the correction of the initial pose calculation. Note that because the orientation given by PCA is ambiguous, we do the matching with the initial orientation and with another rotated by π radians. To correct the pose, we use the matching transformation that provided the best matching. The algorithm used for matching is the adaptation of the scan matching algorithm proposed by Pedrosa et al. [34] to three dimensions. An example of SLC detection is shown in Fig. 10. 4.1.2 Manipulation and planning strategy To solve this task three manipulation skills were used. The pick_object and place_object are based on the skills trained in a simulation environment [14] with some minors adaptations. The pick_object_shelf skill is also derived from the pick_object skill but taking in account the space between the shelves of the shelf. So in this skill instead of approaching the object using a vertical movement it is approached at an angle of 45 degrees. This allows the arm to reach the objects on the shelf without hitting the upper level of the self. Our initial approach was to pick an SLC, hold it and deliver it to the goal area. With this approach the robot has to move across the room 4 times, 2 for the SLCs on the pickup table and 2 for the SLCs in the shelf. It takes around 15 min to complete the job, being most of this time spent on navigation between the goal table and the pickup table and shelf. In order to optimize this process a second approach was developed. Because a considerable amount of time was being spent on navigation, we manage to transport the SLCs on top of the robot platform. This way, navigation was reduced to one trip to each side: first, the two SLCs in the table are picked up and placed in the top of the robot; then, the robot moves to the shelf, pick up another SLC and also put it in its top surface; finally, the fourth SLC is picked up and held in the gripper, while the robot navigates to the target table. On the target table, the four SLCs are put in the target area, after which the fifth SLC, the one in the target table, is detected, picked and placed. With this improvement the time dropped to around 10 min. After this pick and place strategy was implemented, some improvements were accomplished, by parallelizing some steps. For example, after picking an SLC, while placing it on the top of the robot, movement to the next observation and picking position can be performed. Task time was now reduced to around 8 min. A significant amount of time was still being spent in picking the SLCs from the top of the robot and placing them on the goal area (3 SLCs from the top of the robot plus the fourth that was transported on the gripper). So our final solution was to rearrange the SLCs on top of the robot so that the manipulator can pick two at the same time. This way one of the picking from the robot’s body was eliminated. Also, by placing the fourth SLC on top of the robot aligned with the third and by picking both at once, the number of places in the target area was reduced to 3 – 2 places holding two SLCs and one holding only one SLC. The 4th SLC is placed on top of the robot while it is navigating to the target table. Also, during navigation the arm could pick the 3rd and 4th SLCs so that when the robot reaches the target table the SLCs are already grasped. These improvements dropped the time to 5 min. Furthermore, speed increasing was previously prepared through parameters in a configuration file. During the evaluation, 6 out of 7 attempts were successful and with decreasing times, which lead to the final task time of 3 min and 4 s just by increasing the speed of the movements. 4 4 4.2 Task 2: product assembly The goal of this task is to pre-assemble a set of bolts, nuts and washers using a single manipulator. Since the single arm manipulation is not able to assemble two parts, two fixtures are provided to help in the assembly: one contains the washers in an approximate upright position to facilitate picking; the other has a well with the shape of the bolt head shallow enough to hold the bolt in place when a torque is applied, i.e. it secures the bolt when screwing the nut. The specifications of the task includes the approximate locations of the assembly pieces and fixtures, but not their exact positions. To solve the task we start by defining an observation point for the camera that is mounted in the arm, that provides a complete view of all necessary elements for the task and allows the manipulation of all parts without changing the position of the platform. This is important as, if the platform does not move, the relative positions of all objects are maintained with high precision after the first detection (see Fig. 3). The robot has to detect the bolt, pick it and place it in the bolt fixture, then it has to detect the washer, pick it and place it in the bolt, and finally it has to detect a nut, pick it and screw it in the bolt. Positions are calculated using the pinhole model instead of the point cloud provided by the stereo rig. this strategy can be pursued because the dimensions of all elements in the task and a precise distance from the camera to the working table, that is inferred from the pose of the arm are known in advance. This is done to speed up detection, because the generation of depth information is computationally expensive. 4.2.1 Fixture detection The detection of the fixtures, although executed only once, is an important step. The robot does not start from the working area but has to navigate there. Thus, location errors are inevitable. The assembly area is well defined, therefore, the fixtures are used as reference points to the rest of the elements. The detection of the fixtures is done using HSV color segmentation. We start by detecting both fixtures from the observation point. Once the resulting blobs are obtained, for each blob the rotated rectangle that best encloses it is calculated. The detection from the observation point is not very precise, therefore using the information from the rotated rectangle, the camera is approximated to each fixture and the detection is repeated individually. The information about the fixture that contains the washers is used in the detection of the washers, as their relative position to the fixture is known and so the search space in the image can be reduced. The center of the rotated rectangle that derives from the bolt’s fixture coincides with screwing place. 4.2.2 Bolt, nut and washer detection The detection algorithm for the bolts and nuts is the same. We explore their shape similarity when viewed from the top, i.e. when the object is located at the image center. The detection of a bolt/nut starts by performing color segmentation, then blobs are created from the resulting segmentation. Only the blob closer to the image center and in the vicinity of the bolts virtual line is considered, the rest being discarded. We perform a shape analysis to find the vertices of its convex shape and to find afterwards two consecutive edges that resemble the bolt/nut hexagonal top in length and angle (Fig. 11). Those edges are then used to calculate the center of the bolt/nut, and its orientation. The orientation is required for the nut so that it can be picked up by its edges. The described approach assumes that the object to be detected is as much as possible in the center of the image. This is achieved by generating a hint list for bolts and nuts that gives a rough approximation of their positions. The list is populated by running the color segmentation once for the bolts and nuts from the observation position. The resulting blobs are then used as hints. When it is time to detect a bolt/nut the hint is used to approximate the camera to the object in focus. The detection of the washers uses a different strategy. Instead of color segmentation, we do an intensity filtering that keeps the pixels with higher intensity (Fig. 12). Then, from the resulting segmentation we extract several blobs validated by their size. The centers of these blobs are then used to calculate the positions of the washers. 4.3 Manipulation and planning strategy To place the bolt in the screwing fixture, the place_bolt skill takes advantage of the compliant movement of the arm. To insert the bolt head into the fixture, after aligning the bolt with the center of the fixture, the bolt is pushed against the fixture while rotating it. As soon as a drop in the bolt’s height is detected, the action finishes, as it means the bolt’s head entered the fixture. For the place_nut skill the strategy is similar. While using compliant movement of the arm, the robot starts rotating the nut while at the same time pushes it against the bolt tip. Based on how much the height of the nut dropped during the first rotation, the number of rotations is adjusted in order to be fully screwed on the bolt. In the pick_washer skill, since they are in an upright position slightly tilted, the gripper approaches the washers just like in a normal pick but with the fingers adjusted for the washer diameter. Then the gripper moves down slowly in order to push the adjacent washer and create space for the picking. Because they are tilted the robot pushes them in the opposite direction of the tilt in order to became upright and aligned with the fingers of the gripper. To place the washers on the bolt, the place_washer skill aligns the inner bottom edge of the washer with the top of the bolt, just touching it on the side. Then the washer is rotated by 60 degrees while pushing it against the bolt. This tilts the bolt a little bit making it push the washer to the correct position when it is released. The planning strategy for this task is straightforward. Not considering the perception parts, it corresponds to the following sequence of actions: first, a bolt is picked up and placed in the fixture; then, a washer is picked up and placed in the bolt; next, a nut is picked up and screwed in the bolt; finally, the assembly is picked up and placed in a target region. The same procedure applies to the other bolts, washers and nuts. The speed of the movements and the movements connecting the different skills were parameterized and optimized during evaluation, starting with safer speeds and proceeding to faster ones after the task had been completed with success. In 7 attempts to solve the task, only one of them failed to complete all the objectives. With this strategy the best achieved task time was 6 min and 15 s. 5 5 5 Solving showcase tasks 5.1 Manipulation of stacked non rigid objects Solving the showcase tasks introduced a number of technological issues. The blank pile is not a unique rigid solid object, as shown in Fig. 13. It is a stack of cardboards, that can bend under its own weight and can also be disrupted, since some blanks can break free from the pile, due to low friction between them. To prevent the breakdown of a pile, any manipulation procedure has to be aware of this fact. Blanks are provided in untied piles, piled up on pallets close to each other, which can cause erroneous detections. Blank piles have to be placed in a feeding mechanism avoiding hard collisions with blanks that can already be there and with the four blank guiding rods that prevent blanks from slipping out of the inclined magazine surface. This feeding task requires a high level of precision for a proper feeding, while preventing collisions between the blank magazine and the manipulator. The manipulation has to be performed using a single arm. Since the gripper provided by the host is not appropriate, a solution has to be revised, taking into account the aforementioned issues. Aside from the gripper design, a proper sequence of actions has to be planned to accomplish the showcase tasks. 5.2 Manipulator for packaging Regarding the gripper design, the TIMAIRIS team decided to used the gripper provided with the robotic arm and adapt it to the required manipulations. Two fingers were designed, able to grasp a pile of blanks with a maximum height of 80 mm. The closing force of 80N (or at least 50N) should be enough at least for manipulating the smaller piles. In order to grasp a pile, it should be partially dragged out of the table, as shown in Fig. 13. To do so, two thin shafts were attached to the fingers, as shown in Fig. 14(a) and Fig. 15(a). These thin shafts can slide along their own axes thanks to the presence of a spring, ensuring the existence of contact between the shafts and the interlayer and thus preventing the loss of any blank during the dragging of the pile. Warehouse or blank magazine is composed of a plate inclined by 55 ∘ and four aluminum legs that sustain the plate. The plate is ad hoc built for the industrial partner’s blanks or cardboards. On the plate, four thin rods are mounted that guide the blanks once the robot delivers the whole stack, as shown in Fig. 14(b) and Fig. 15(b). Moreover, a proximity sensor is added and connected to a led yellow light. The proximity sensor checks if the number of cardboards is below a given threshold, in which case the light is turned on, as shown in Fig. 15(b). 5.3 Showcase perception To accomplish the showcase tasks a number of perception activities must be implemented. The piles of blanks are put over two pallets, thus these pallets must be detected and located relative to the robot. The poses of the piles themselves must be precisely estimated, so the picking up could be well performed. Finally, the blank magazine must be detected and located in order to perform the feeding procedure. The detection of the pallets is done by processing the stereo images captured using the pan–tilt camera. It unfolds into three steps: first, voxel grid filtering [35] based on the height of the pallets is used to remove irrelevant points from the point cloud [36]; second, the filtered points are projected on a horizontal plane and clustering is applied to define candidate regions; finally, these regions are matched against a 3D template to find the poses of the pallets. The same approach is applied for a first rough estimation of the poses of the piles in the pallets, by using the average height of the piles and an appropriate template. Fig. 16 illustrates the detection of both pallets and piles. However, to reliably pick a blank pile up from a pallet, a robust pose estimation of piles is necessary. On the one hand, in real industry scenarios, the piles on a pallet are tightly-aligned and put close to each other. On the other hand, the patterns printed on the blanks, even for the same blank shape, are variable and can change with end users’ demands. Therefore, detection and pose estimation of blanks cannot rely on conventional approaches such as color segmentation or local feature matching such as SIFT [37]. The approximate piles’ poses estimated so far can be used to position the TCP camera, in order to obtain more reliable ones. The camera is positioned over a pile and an image is captured, as shown in Fig. 17(a). Then, a Canny edge detector is applied to that image, as shown in Fig. 17(b). The detected edges come from the blanks’ outline but also from the printed patterns on the top blank. A drawing pin filter which classifies each point with various number of neighbors is applied to detect the outline of a blank pile [26]. The kernel is defined as K D P = [ x i ] = p + h n , if x i is in the center h n , otherwise p > h × | [ x ] | , n = ∑ x i , where p and h are the values of pin and head, respectively. The p is larger than the sum of heads and n is the normalization factor of the kernel. In this work, a drawing pin kernel of size 5 × 5 with p = 50 and h = 1 is used, as shown in Fig. 18. The basic assumption is that the edges of outlines are straight lines and their points have few neighbors except those in the line, while the points in printed patterns have many neighbors. In the filtering algorithm, points with more than the kernel size are discarded, as shown in Fig. 17(c). Finally, to find the precise pose of the pile, it is matched against a template, starting from the roughly estimated pose in a spiral direction. Fig. 17(d) shows the result of the match. 5.4 Motion planing After grasping the blank pile, the arm is put in a transport pose. This pose is defined considering two requirements. In one side, the arm should be completely inside the robot footprint. This way, navigation to the blank magazine table will be much safer. In the other side, the arm pose should minimize the necessary motions to place the pile in the magazine. All of these manipulations have to be done while maintaining the blank pile in a pose that prevents it from breaking apart. In this challenge, motions have been generated in the simulator in advance and then the results are applied to the real robot. In order to achieve the best results specific filters had to be used to enhance and complete the trajectories that were derived from existing motion planners. Several different motion planners have also been tested and benchmarked for tasks of different complexities [38]. 5.5 Task planning The manipulation of the blank pile, aside from the gripper, also raises planning issues. The first relates to the order by which blank piles must be manipulated. Due to their dispositions in the pallet, certain blank piles are blocked by others and cannot be dragged before these are removed. Therefore, the order by which blank piles are manipulated has to be planned. The plan considers the blank piles available in all the pallets and gives preference to the pallet with the lowest number of blank piles, since an empty pallet can be replaced with a fully packed one. Fig. 19 shows a task plan with the sequence and dragging directions to pick up all blank piles on two pallets. The dragging itself is also a challenge. The gripper and the arm have to maintain pressure against the pallet while dragging the blank pile. Too much pressure and the pallet could move, not enough pressure and the pile can be disrupted. Therefore, the robotic arm has to be used in compliant mode and the pressure should be properly calculated. 5.6 Safe human–robot collaboration For small batch production, changing or adding new features, such as continuously changing printed patterns of blanks, can be a burden on both human operators and autonomous robots. Since industrial environments are noisy, where machines produce a continuous whirring sound, verbal communication is difficult for humans and impractical for robots. Thus, gestures have been considered as a practical alternative way of communication. Until now, gesture recognition systems in HRI have focused on small number of implicit interactions such as pointing and handing over. With respect to pointing, the target object must be in the field of view of both human and robot. Fig. 20 shows an example of human tracking and human–robot interaction during the EuRoC evaluation. When the robot detects an operator entering a region for safety handling, it stops the executing task and points the pan–tilt to the operator. To ask to fetch objects in a cluttered and unstructured environment, HRI systems need to have a rich vocabulary to designate an object [16]. Fig. 21 shows an interaction tree for the showcase task. That represents commands by sequentially encoding 8 gestures, which consist of 6 numbers 0 to 5 and thumb up/down. Gesture recognition module recognizes a one-hand gesture provided by the operator. Then HRI module tries to build a command with a sequence of gestures. Following the command, the robot continues or stops the paused task. 6 Challenge evaluation 6.1 Benchmark evaluation During the benchmark phase of the Stages II, all the Challenge 2 participants have been evaluated by developing same two tasks under the same conditions: the robot platform, EuRoC software architecture and working time. As the benchmark task 1, the robot has to pick up and place 5 SLCs into the target area to show the logistics capabilities with different payload from different location to a fixed place. The task has two metrics: a number of delivered SLCs regardless of whether parts are lost (metric 1) and a number of delivered SLCs without losing parts (metric 2). Table 1 shows the results, which are their best results out of three trials. The bonus is calculated as inverse proportion to the time difference from the best on that between the best and worst. As the benchmark task 2, the robot is required to screw nuts on 5 bolts placed in a fixed area on a table using a fixture. The task also has two metrics: a number of washers successfully put onto bolts (metric 1) and a number of bolts with a successfully screwed on nut (metric 2) Table 2 shows the results of the benchmark task 2. The bonus is calculated as same as that of the task 1 among the records of teams, who finish the assembly task. TIMAIRIS has completed the both tasks without failures in the shortest times. 6.2 Showcase evaluation TIMARIS’ showcase addresses all of these issues (and others) in the form of three challenges and two extra demos, with quantifiable metrics. The objectives and metrics are organized so that the highlights described above can be evaluated in a comprehensive and robust way. The first objective (O1) is focused on perception and includes 4 metrics: detecting the pose of blank piles, pallets and blank magazine (O1M1); recognizing the need for blank feeding (O1M2); identifying the number of piles in each pallet (O1M3); identifying the presence of humans in the vicinity of the robot (O1M4). The second objective is focused on manipulation and navigation and includes 4 metrics: correctly picking a blank pile (O2M1); transporting the piles to the blank magazine (O2M2); placing the blank piles in the blank magazine (O2M3); stopping manipulation and navigation if a human is close to the robot (O2M4). The third objective evaluates the planning capabilities and includes 2 metrics: providing a plan to pick the blank piles (O3M1); adapting navigation paths (O3M2). The last objective evaluates human robot interaction and includes 2 metrics: recognizing gesture commands provided by the human (O4M1); tracking a human for interaction (O4M2). The achieved metrics are presented in Table 3. TIMAIRIS completed the showcase with every objective and metric being accomplished as can be seen from Table 3 where targets is the number of trials in predefined task scenarios and events is the number of successes during the showcase evaluation. The metrics of O1M1 are composed of three metrics for detecting the pose of blank piles, pallets and blank magazine. In what regards Perception, Manipulation/Navigation and Planning, i.e. Objectives 1, 2 and 3 previously specified, all metrics have been achieved during the execution of the first part of the showcase evaluation that consisted of three challenges. Tracking humans with the pan–tilt camera, Metric 2 of Objective 4, was also achieved during this first part of the showcase evaluation. The gesture recognition was demonstrated during the first part of the showcase evaluation, where gesture commands have been used to start challenge execution. Safety has been demonstrated in manipulation and navigation several times. During the showcase, the gesture recognition, which includes automatic correction methods, has been presented independently and the results showed that it is very robust and adequate for this kind of interaction (see the linked video 6 6 ). The results obtained in the showcase phase provide an excellent base for the development of the pilot experiment. The pilot experiment environment is a real industrial setting but, as already referred, all the developments of showcase are directly applicable in this final environment. Some new features will have to be addressed that also depend on the speedup of the final prototype, such as considering several packaging machines and different types/shapes of blanks, enhanced safety and more challenging navigation issues. Still, the showcase results are a complete and very solid basis for the work that needs to be done in the pilot experiments. 7 Conclusion To enable the paradigm shift in the packaging industry, many developed technologies must be considered as the integration cannot be performed and only the complete set allows the execution of the task. The developed system has gradually evolved from the qualification stage in a limited time. During the Stage 2 of the EuRoC project, the proposed architecture has demonstrated the feasibility of practical use of for Shop Floor Logistics and Manipulation. It is believed that SAAA is efficient not only in autonomous execution for autonomous packaging tasks but also in its use of development during the competitive challenges and that the simple architecture and distributed skills make the system efficient. The results of benchmarking tasks in which TIMAIRIS took first place among 5 Stage 2 challenger teams was possible that our team just focused on improving individual skills to complete all tasks and to reduce running time. One of important development during the showcase is showing that autonomous blank feeding is possible in a realistic industrial environment. That is a pending task of the industry partner IMA S.p.A which is a world leading company in the design and manufacture of automatic machines for the processing and packaging. As the result of the Stage II evaluations, TIMAIRIS has been selected to advance to the final stage of the EuRoC project. Some new features will have to be addressed that also depend on the speedup of the final prototype to complete the pilot experiments. Still, the results of the Stage II are a complete and very solid basis for the work that needs to be done in the final stage of the EuRoC project. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was supported by the EuRoC Project under Grant no. 608849 and by National Funds through the FCT - Foundation for Science and Technology , in the context of the project UID/CEC/00127/2013. References [1] Pratt G. Manzo J. The DARPA robotics challenge [competitions] IEEE Robot. Autom. Mag. 20 2 2013 10 12 G. Pratt, J. Manzo, The DARPA robotics challenge [competitions], IEEE Robotics & Automation Magazine 20 (2) (2013) 10–12 [2] Kitano H. Asada M. Kuniyoshi Y. Noda I. Osawa E. Robocup: the robot world cup initiative Proceedings of the First International Conference on Autonomous Agents 1997 ACM 340 347 H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, E. Osawa, Robocup: The robot world cup initiative, in: Proceedings of the first international conference on Autonomous agents, ACM, 1997, pp. 340–347 [3] Siciliano B. Caccavale F. Zwicker E. Achtelik M. Mansard N. Borst C. Achtelik M. Jepsen N.O. Awad R. Bischoff R. EuRoC-The challenge initiative for european robotics ISR/Robotik 2014; 41st International Symposium on Robotics; Proceedings of 2014 VDE 1 7 B. Siciliano, F. Caccavale, E. Zwicker, M. Achtelik, N. Mansard, C. Borst, M. Achtelik, N. O. Jepsen, R. Awad, R. Bischoff, EuRoC-the challenge initiative for european robotics, in: ISR/Robotik 2014 41st International Symposium on Robotics; Proceedings of, VDE, 2014, pp. 1–7 [4] Bischoff R. Guhl T. The strategic research agenda for robotics in europe [industrial activities] IEEE Robot. Autom. Mag. 1 17 2010 15 16 R. Bischoff, T. Guhl, The strategic research agenda for robotics in europe [industrial activities], IEEE Robotics & Automation Magazine 1 (17) (2010) 15–16 [5] Cully A. Clune J. Tarapore D. Mouret J.-B. Robots that can adapt like animals Nature 521 7553 2015 503 A. Cully, J. Clune, D. Tarapore, J.-B. Mouret, Robots that can adapt like animals, Nature 521 (7553) (2015) 503 [6] Hildebrandt A.-C. Schuetz C. Wahrmann D. Wittmann R. Rixen D. A flexible robotic framework for autonomous manufacturing processes: report from the european robotics challenge stage 1 Autonomous Robot Systems and Competitions (ICARSC), 2016 International Conference on 2016 IEEE 21 27 A.-C. Hildebrandt, C. Schuetz, D. Wahrmann, R. Wittmann, D. Rixen, A flexible robotic framework for autonomous manufacturing processes: Report from the European Robotics Challenge Stage 1, in: Autonomous Robot Systems and Competitions (ICARSC), 2016 International Conference on, IEEE, 2016, pp. 21–27 [7] Zadeh S.M. Powers D.M. Sammut K. An autonomous reactive architecture for efficient AUV mission time management in realistic dynamic ocean environment Robot. Auton. Syst. 87 2017 81 103 S. M. Zadeh, D. M. Powers, K. Sammut, An autonomous reactive architecture for efficient auv mission time management in realistic dynamic ocean environment, Robotics and Autonomous Systems 87 (2017) 81–103 [8] Baklouti E. Amor N.B. Jallouli M. Reactive control architecture for mobile robot autonomous navigation Robot. Auton. Syst. 89 2017 9 14 E. Baklouti, N. B. Amor, M. Jallouli, Reactive control architecture for mobile robot autonomous navigation, Robotics and Autonomous Systems 89 (2017) 9–14 [9] Nilsson K. Johansson R. Integrated architecture for industrial robot programming and control Robot. Auton. Syst. 29 4 1999 205 226 K. Nilsson, R. Johansson, Integrated architecture for industrial robot programming and control, Robotics and Autonomous Systems 29 (4) (1999) 205–226 [10] Chatzilygeroudis K. Vassiliades V. Mouret J.-B. Reset-free trial-and-error learning for robot damage recovery Robot. Auton. Syst. 100 2018 236 250 K. Chatzilygeroudis, V. Vassiliades, J.-B. Mouret, Reset-free trial-and-error learning for robot damage recovery, Robotics and Autonomous Systems 100 (2018) 236–250 [11] Insaurralde C.C. Petillot Y.R. Capability-oriented robot architecture for maritime autonomy Robot. Auton. Syst. 67 2015 87 104 C. C. Insaurralde, Y. R. Petillot, Capability-oriented robot architecture for maritime autonomy, Robotics and Autonomous Systems 67 (2015) 87–104 [12] Dean T.L. Boddy M.S. An analysis of time-dependent planning. AAAI, vol. 88 1988 49 54 T. L. Dean, M. S. Boddy, An analysis of time-dependent planning., in: AAAI, Vol. 88, 1988, pp. 49–54 [13] Grefenstette J.J. Ramsey C.L. An approach to anytime learning Machine Learning Proceedings 1992 1992 Elsevier 189 195 J. J. Grefenstette, C. L. Ramsey, An approach to anytime learning, in: Machine Learning Proceedings 1992, Elsevier, 1992, pp. 189–195 [14] Pedrosa E. Lau N. Pereira A. Cunha B. A skill-based architecture for pick and place manipulation tasks Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015 2015 Springer 457 468 E. Pedrosa, N. Lau, A. Pereira, B. Cunha, A skill-based architecture for pick and place manipulation tasks, in: Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015, Springer, 2015, pp. 457–468 [15] Amaral F. Pedrosa E. Lim G.H. Shafii N. Pereira A. Azevedo J.L. Cunha B. Reis L.P. Badini S. Lau N. Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC challenge 2, stage II-realistic labs: benchmarking Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on 2017 IEEE 198 203 F. Amaral, E. Pedrosa, G. H. Lim, N. Shafii, A. Pereira, J. L. Azevedo, B. Cunha, L. P. Reis, S. Badini, N. Lau, Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC Challenge 2, Stage II-Realistic Labs: Benchmarking, in: Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on, IEEE, 2017, pp. 198–203 [16] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Dias P. Azevedo J.L. Cunha B. Reis L.P. Rich and robust human-robot interaction on gesture recognition for assembly tasks Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on 2017 IEEE 159 164 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, P. Dias, J. L. Azevedo, B. Cunha, L. P. Reis, Rich and robust human-robot interaction on gesture recognition for assembly tasks, in: Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on, IEEE, 2017, pp. 159–164 [17] Lim G.H. Pedrosa E. Amaral F. Dias R. Pereira A. Lau N. Azevedo J.L. Cunha B. Reis L.P. Human-robot collaboration and safety management for logistics and manipulation tasks Iberian Robotics Conference 2017 Springer 15 27 G. H. Lim, E. Pedrosa, F. Amaral, R. Dias, A. Pereira, N. Lau, J. L. Azevedo, B. Cunha, L. P. Reis, Human-robot collaboration and safety management for logistics and manipulation tasks, in: Iberian Robotics conference, Springer, 2017, pp. 15–27 [18] Mokhtari V. Lim G.H. Lopes L.S. Pinho A.J. Gathering and conceptualizing plan-based robot activity experiences Intelligent Autonomous Systems 13 2016 Springer 993 1005 V. Mokhtari, G. H. Lim, L. S. Lopes, A. J. Pinho, Gathering and conceptualizing plan-based robot activity experiences, in: Intelligent Autonomous Systems 13, Springer, 2016, pp. 993–1005 [19] Lim G.H. Shared representations of actions for alternative suggestion with incomplete information Robot. Auton. Syst. 2019 G. H. Lim, Shared representations of actions for alternative suggestion with incomplete information, Robotics and Autonomous Systems. [20] Balogh R. I am a robot–competitor: a survey of robotic competitions Int. J. Adv. Robot. Syst. 2 2005 17 R. Balogh, I am a robot–competitor: A survey of robotic competitions, International Journal of Advanced Robotic Systems 2 (2005) 17 [21] Thrun S. Montemerlo M. Dahlkamp H. Stavens D. Aron A. Diebel J. Fong P. Gale J. Halpenny M. Hoffmann G. Stanley: the robot that won the DARPA grand challenge J. Field Robot. 23 9 2006 661 692 S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann, et al., Stanley: The robot that won the darpa grand challenge, Journal of field Robotics 23 (9) (2006) 661–692 [22] Buehler M. Iagnemma K. Singh S. The DARPA Urban Challenge: Autonomous Vehicles in City Traffic, vol. 56 2009 Springer M. Buehler, K. Iagnemma, S. Singh, The DARPA urban challenge: autonomous vehicles in city traffic, Vol. 56, springer, 2009 [23] A. Dömel, S. Kriegel, M. Brucker, M. Suppa, Autonomous pick and place operations in industrial production, in: Ubiquitous Robots and Ambient Intelligence (URAI), 2015 12th International Conference on, 2015, pp. 356–356, [24] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Azevedo J.L. Cunha B. Neural regularization jointly involving neurons and connections for robust image classification 2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 IEEE 336 341 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, J. L. Azevedo, B. Cunha, Neural regularization jointly involving neurons and connections for robust image classification, in: 2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), IEEE, 2017, pp. 336–341 [25] Russell S. Norvig P. Artificial Intelligence: A Modern Approach third ed. 2010 Prentice Hall S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, 3rd Edition, Prentice Hall, 2010 [26] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Azevedo J.L. Cunha B. Badini S. Mobile manipulation for autonomous packaging in realistic environments: euroc challenge 2, stage ii, showcase 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC) 2018 IEEE 231 236 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, J. L. Azevedo, B. Cunha, S. Badini, Mobile manipulation for autonomous packaging in realistic environments: Euroc challenge 2, stage ii, showcase, in: 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), IEEE, 2018, pp. 231–236 [27] Lim G.H. Lau N. Pedrosa E. Amaral F. Pereira A. Luís Azevedo J. Cunha B. Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges Adv. Robot. 2019 1 11 G. H. Lim, N. Lau, E. Pedrosa, F. Amaral, A. Pereira, J. Luís Azevedo, B. Cunha, Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges, Advanced Robotics (2019) 1–11 [28] Lim G.H. Yi C. Suh I.H. Ko D.W. Hong S.W. Ontology representation and instantiation for semantic map building by a mobile robot Intelligent Autonomous Systems, vol. 12 2013 Springer 387 395 G. H. Lim, C. Yi, I. H. Suh, D. W. Ko, S. W. Hong, Ontology representation and instantiation for semantic map building by a mobile robot, in: Intelligent Autonomous Systems 12, Springer, 2013, pp. 387–395 [29] G.H. Lim, M. Oliveira, S.H. Kasaei, L.S. Lopes, Hierarchical nearest neighbor graphs for building perceptual hierarchies, in: 22nd International Conference on Neural Information Processing, ICONIP2015, 2015. [30] Lim G.H. Suh I.H. Suh H. Ontology-based unified robot knowledge for service robots in indoor environments IEEE Trans. Syst., Man, Cybern. A 41 3 2011 492 509 10.1109/TSMCA.2010.2076404 G. H. Lim, I. H. Suh, H. Suh, Ontology-based unified robot knowledge for service robots in indoor environments, Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on 41 (3) (2011) 492 –509 doi:101109/TSMCA.20102076404 [31] Rusu R. Marton Z. Blodow N. Dolha M. Towards 3D point cloud based object maps for household environments Robot. Auton. Syst. 56 11 2008 927 941 R. B. Rusu, Z. C. Marton, N. Blodow, M. Dolha, Towards 3D point cloud based object maps for household environments, Robotics and Autonomous Systems 56 (11) (2008) 927–941 [32] Rusu R.B. Semantic 3D object maps for everyday manipulation in human living environments (Ph.D. thesis) 2009 Computer Science department, Technische Universitaet Muenchen, Germany R. B. Rusu, Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments, Ph.D. thesis, Computer Science department, Technische Universitaet Muenchen, Germany (October 2009) [33] Oliveira M. Lopes L.S. Lim G.H. Kasaei S.H. Tomé A.M. Chauhan A. 3D object perception and perceptual learning in the RACE project Robot. Auton. Syst. 75 2016 614 626 M. Oliveira, L. S. Lopes, G. H. Lim, S. H. Kasaei, A. M. Tomé, A. Chauhan, 3d object perception and perceptual learning in the race project, Robotics and Autonomous Systems 75 (2016) 614–626 [34] Pedrosa E. Pereira A. Lau N. A scan matching approach to slam with a dynamic likelihood field 2016 International Conference on Autonomous Robot Systems and Competitions, ICARSC 2016 IEEE Portugal, Bragança 35 40 10.1109/ICARSC.2016.23 E. Pedrosa, A. Pereira, N. Lau, A Scan Matching Approach to SLAM with a Dynamic Likelihood Field, in: 2016 International Conference on Autonomous Robot Systems and Competitions (ICARSC), IEEE, Portugal, Bragança, 2016, pp. 35–40 doi:101109/ICARSC.201623 [35] Cohen-Or D. Kaufman A. Fundamentals of surface voxelization Graph. Models Image Process. 57 6 1995 453 461 D. Cohen-Or, A. Kaufman, Fundamentals of surface voxelization, Graphical models and image processing 57 (6) (1995) 453–461 [36] Rusu R.B. Cousins S. 3D is here: point cloud library (PCL) Robotics and Automation (ICRA), 2011 IEEE International Conference on 2011 IEEE 1 4 R. B. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: Robotics and Automation (ICRA), 2011 IEEE International Conference on, IEEE, 2011, pp. 1–4 [37] Lowe D.G. Object recognition from local scale-invariant features Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, vol. 2 1999 Ieee 1150 1157 D. G. Lowe, Object recognition from local scale-invariant features, in: Computer vision, 1999 The proceedings of the seventh IEEE international conference on, Vol. 2, Ieee, 1999, pp. 1150–1157 [38] Tudico A. Lau N. Pedrosa E. Amaral F. Mazzotti C. Carricato M. Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments Portuguese Conference on Artificial Intelligence 2017 Springer 498 509 A. Tudico, N. Lau, E. Pedrosa, F. Amaral, C. Mazzotti, M. Carricato, Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments, in: Portuguese Conference on Artificial Intelligence, Springer, 2017, pp. 498–509 Dr. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D. degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Marie Curie individual fellow in the School of Computer Science at University of Manchester, UK. His research interests lie in the area of artificial intelligence and machine learning for autonomous robots, including perception, semantics, cognition and spatiotemporal representations on neuromorphic architectures. Eurico Pedrosa is a Post-Doc Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his Informatics Engineering degree from University of Aveiro in 2010 and a Computer Science Ph.D. degree from Aveiro University in 2018. His research interest are focused on intelligent robotics, robotic navigation including localization and mapping (SLAM), space representation using volumetric grids and most recently the application of radar sensors in indoor robotics. Filipe Amaral is a Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his MSc degree in Computer and Telematics Engineering from University of Aveiro in 2014. His current research interests are in the area of autonomous mobile robotics. Prof. Dr. Artur Pereira was born in Vila Nova de Famalicão, Portugal, in April 1960. He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 2003. He is currently an Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Instituto de Engenharia Electrónica e Informática de Aveiro. The main focus of his research is robotics at the architectural and software levels, with emphasis on simulation, navigation, localization, mapping, and machine learning. Nuno Lau is Assistant Professor at Aveiro University, Portugal and Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), where he leads the Intelligent Robotics and Systems group (IRIS). He got is Electrical Engineering Degree from Oporto University in 1993, a DEA degree in Biomedical Engineering from Claude Bernard University, France, in 1994 and the Ph.D. from Aveiro University in 2003. His research interests are focused on Intelligent Robotics, Artificial Intelligence, Multi-Agent Systems and Simulation. Nuno Lau participated in more than 15 international and national research projects, having the tasks of general or local coordinator in about half of them. Nuno Lau won more than 50 scientific awards in robotic competitions, conferences (best papers) and education. He has lectured courses at Phd and MSc levels on Intelligent Robotics, Distributed Artificial Intelligence, Computer Architecture, Programming, etc. Nuno Lau is the author of more than 150 publications in international conferences and journals. He was President of the Portuguese Robotics Society from 2015 to 2017, and is currently the Vice-President of this Society. Prof. Dr. José Luís Azevedo is currently Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Institute of Electronics and Informatics Engineering of Aveiro (IEETA). He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1998. His current research interests are in the area of cooperative autonomous mobile robotics. Prof. Dr. Bernardo Cunha was born in 1959 in Porto, Portugal. He earned his doctoral degree in electrical engineering at the University of Aveiro, Portugal, in 1999. He is a full time teacher at Universidade de Aveiro in the computer architecture area and an investigator at the Instituto de Engenharia Electrónica e Informática de Aveiro. Current research interests are centered in the area of cooperative autonomous mobile robotics. Simone Badini is a Mechanical Designer in the Research and Development department of IMA Spa since 2013. IMA Spa is a world leader company in the design and manufacture of automatic machines for the processing and packaging of pharmaceuticals, cosmetics, food, tea and coffee and tobacco. He got is M.Sc. degree in Mechanical Engineering from University of Bologna, Italy in 2012. He is currently project manager for the integration of cobot and autonomous mobile robot in the production lines for the IMA group. "
    },
    {
        "doc_title": "Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges",
        "doc_scopus_id": "85066896804",
        "doc_doi": "10.1080/01691864.2019.1617780",
        "doc_eid": "2-s2.0-85066896804",
        "doc_date": "2019-07-03",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Industrial environments",
            "Industrial robotics",
            "Mobile manipulation",
            "Mobile manipulator",
            "Object manipulation",
            "Perception systems",
            "Pose estimation",
            "stacked objects"
        ],
        "doc_abstract": "© 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.Object manipulation tasks such as picking up, carrying and placing should be executed based on the information of objects which are provided by the perception system. A precise and efficient pose estimation system has been developed to address the requirements and to achieve the objectives for autonomous packaging, specifically picking up of stacked non-rigid objects. For fine pose estimation, a drawing pin shaped kernel and pinhole filtering methods are used on the roughly estimated pose of objects. The system has been applied in a realistic industrial environment as a challenging scenario for the Challenge 2–Shop Floor Logistics and Manipulation on a mobile manipulator in the context of the European Robotics Challenges (EuRoC) project.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Introduction to the Special Issue “Robotica 2016”",
        "doc_scopus_id": "85053448060",
        "doc_doi": "10.1007/s10846-018-0934-1",
        "doc_eid": "2-s2.0-85053448060",
        "doc_date": "2019-03-15",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-Robot Fast-Paced Coordination with Leader Election",
        "doc_scopus_id": "85070697946",
        "doc_doi": "10.1007/978-3-030-27544-0_2",
        "doc_eid": "2-s2.0-85070697946",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Global objective",
            "Leader election",
            "Multi-agent coordinations",
            "Multi-robot systems",
            "Real time constraints",
            "Soccer-playing robots",
            "Stochastic environment",
            "Task assignment"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.Coordination in Multi-Robot Systems is an active research line in Artificial Intelligence applied to Robotics. Through coordination, a team of robots can efficiently achieve their pre-defined global objective. From a wide range of multi-agent coordination sub-topics, one of the current open issues is task assignment and role selection in fast-paced environments. In homogeneous teams, where robots have the ability to dynamically change roles, working in highly dynamic and stochastic environments, it is important that any solution is able to perform and achieve results while complying with realtime constraints. In this paper, we balance the advantages and disadvantages of completely decentralised solutions and centralised ones, and then present our solution for leader election among a team, which is based on the Raft algorithm and tackles two of its limitations. The proposed solution was implemented in a real team of soccer-playing robots and the experimental results are thoroughly presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mobile manipulation for autonomous packaging in realistic environments: EuRoC challenge 2, stage II, showcase",
        "doc_scopus_id": "85048892736",
        "doc_doi": "10.1109/ICARSC.2018.8374188",
        "doc_eid": "2-s2.0-85048892736",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.European Robotics Challenges (EuRoC) project has been launched to find competitive solutions by exploiting synergies across research institutes to industrial end-users. This paper reports the research conducted by the TIMAIRIS team to fulfill EuRoC C2 Stage II tasks. TIMAIRIS is one of the 6 EuRoC finalists (Stage III) from an initial group of 102 teams. The packaging industry is very interested in recent advances in robotics but is still quite conservative in the way it uses automation, since shapes and printed patterns of blanks vary a lot to comply with end users' demands. The use of programmable logic controllers (PLCs) is widely common but the use of more sophisticated decision mechanisms is not so common. A shop floor logistics and manipulation system has been developed and demonstrated in a realistic environment for autonomous packaging.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human-Robot Collaboration and Safety Management for Logistics and Manipulation Tasks",
        "doc_scopus_id": "85042220407",
        "doc_doi": "10.1007/978-3-319-70836-2_2",
        "doc_eid": "2-s2.0-85042220407",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Human-robot collaboration",
            "Industrial context",
            "Manipulation task",
            "Mobile manipulator",
            "Reasoning methods",
            "Region-based filtering",
            "Safety concerns",
            "Safety management"
        ],
        "doc_abstract": "© Springer International Publishing AG 2018.To realize human-robot collaboration in manufacturing, industrial robots need to share an environment with humans and to work hand in hand. This introduces safety concerns but also provides the opportunity to take advantage of human-robot interactions to control the robot. The main objective of this work is to provide HRI without compromising safety issues in a realistic industrial context. In the paper, a region-based filtering and reasoning method for safety has been developed and integrated into a human-robot collaboration system. The proposed method has been successfully demonstrated keeping safety during the showcase evaluation of the European robotics challenges with a real mobile manipulator.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Neural regularization jointly involving neurons and connections for robust image classification",
        "doc_scopus_id": "85042368378",
        "doc_doi": "10.1109/MFI.2017.8170451",
        "doc_eid": "2-s2.0-85042368378",
        "doc_date": "2017-12-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Classification performance",
            "Classification results",
            "Experimental analysis",
            "Fully connected neural network",
            "Fully-connected layers",
            "Prediction performance",
            "Regularization methods",
            "Regularization technique"
        ],
        "doc_abstract": "© 2017 IEEE.This paper presents an integrated neural regularization method in fully-connected neural networks that jointly combines the cutting edge of regularization techniques; Dropout [1] and DropConnect [2]. With a small number of data set, trained feed-forward networks tend to show poor prediction performance on test data which has never been introduced while training. In order to reduce the overfitting, regularization methods commonly use only a sparse subset of their inputs. While a fully-connected layer with Dropout takes account of a randomly selected subset of hidden neurons with some probability, a layer with DropConnect only keeps a randomly selected subset of connections between neurons. It has been reported that their performances are dependent on domains. Image classification results show that the integrated method provides more degrees of freedom to achieve robust image recognition in the test phase. The experimental analyses on CIFAR-10 and one-hand gesture dataset show that the method provides the opportunity to improve classification performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time multi-object tracking on highly dynamic environments",
        "doc_scopus_id": "85026875635",
        "doc_doi": "10.1109/ICARSC.2017.7964072",
        "doc_eid": "2-s2.0-85026875635",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Dynamic environments",
            "Multi-object tracking",
            "Multiple moving objects",
            "Real time constraints",
            "Robotics research",
            "Soccer-playing robots",
            "Tracking objects",
            "Trade off"
        ],
        "doc_abstract": "© 2017 IEEE.An accurate representation of the environment is important to plan tasks and efficiently control an autonomous robot. Multi-object tracking has long been an important task in robotics research whenever an autonomous robot needs to plan its tasks on an environment with multiple moving objects. Specially when the application is defined by real-time constraints in a fast-paced environment, a trade-off between accuracy and performance is automatically imposed. This paper presents a solution for real-time multi-object tracking on stochastic and highly dynamic environments. Although not limited to this specific application domain, the implementation and evaluation of this solution was performed on a team of autonomous soccer-playing robots. The proposed solution is detailed from the detection and feature extraction phase to the general problem of efficiently tracking objects in the environment. Results on the real testbed are also presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC Challenge 2, Stage II - Realistic Labs: Benchmarking",
        "doc_scopus_id": "85026869156",
        "doc_doi": "10.1109/ICARSC.2017.7964075",
        "doc_eid": "2-s2.0-85026869156",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "Continuous development",
            "Effective solution",
            "Manipulation task",
            "Planning strategies",
            "Production quality",
            "Robotic technologies",
            "Scientific competition"
        ],
        "doc_abstract": "© 2017 IEEE.Nowadays, the increase of robotic technology application to industry scenarios is notorious. Proposals for new effective solutions are in continuous development once industry needs a constantly improvement in time as well as in production quality and efficiency. The EuRoC research project proposes a scientific competition in which research and industry manufacturers joint teams are encouraged to develop and test solutions that can solve several issues as well as be useful in manufacturing improvement. This paper presents the TIMAIRIS architecture and approach used in the Challenge 2 - Stage II - Benchmarking phase, namely regarding the perception, manipulation and planning strategy that was applied to achieve the tasks objectives. The used approach proved to be quite robust and efficient, which allowed us to rank first in the Benchmarking phase.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rich and robust human-robot interaction on gesture recognition for assembly tasks",
        "doc_scopus_id": "85026864168",
        "doc_doi": "10.1109/ICARSC.2017.7964069",
        "doc_eid": "2-s2.0-85026864168",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Human robot Interaction (HRI)",
            "Manufacturing enterprise",
            "Mobile manipulator",
            "Multiple feature fusion",
            "Puzzle games",
            "Robotics technology",
            "Small and medium sized enterprise"
        ],
        "doc_abstract": "© 2017 IEEE.The adoption of robotics technology has the potential to advance quality, efficiency and safety for manufacturing enterprises, in particular small and medium-sized enterprises. This paper presents a human-robot interaction (HRI) system that enables a robot to receive commands, provide information to a human teammate and ask them a favor. In order to build a robust HRI system based on gesture recognition, three key issues are addressed: richness, multiple feature fusion and failure verification. The developed system has been tested and validated in a realistic lab with a real mobile manipulator and a human teammate to solve a puzzle game.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Motivating first year students for an engineering degree",
        "doc_scopus_id": "85010380078",
        "doc_doi": "10.1109/CISPEE.2016.7777745",
        "doc_eid": "2-s2.0-85010380078",
        "doc_date": "2016-12-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Chemical Engineering (miscellaneous)",
                "area_abbreviation": "CENG",
                "area_code": "1501"
            },
            {
                "area_name": "Engineering (miscellaneous)",
                "area_abbreviation": "ENGI",
                "area_code": "2201"
            }
        ],
        "doc_keywords": [
            "Electronic technologies",
            "Engineering degrees",
            "First year students",
            "Portugal",
            "Study plans",
            "Telecommunications engineerings",
            "User perspectives"
        ],
        "doc_abstract": "© 2016 IEEE.When introducing the Bologna model, the University of Aveiro, in Portugal, added a new course to the Electronics and Telecommunications Engineering study plan, aimed at motivating students, both for the degree and for the profession. The course aims at providing new students with an introduction to Electronics and Telecommunications Engineering and to the main activities involved in the profession. For most of the students, the course is the first contact with electronic technology and equipment in a non-user perspective. In the academic year 2011-12, this course suffered a major reorganization. This paper presents the model, underlying assumptions and options followed when developing the new structure for the course. The main challenge was to achieve these goals, using a 'hands-on' approach in a course unit with more than 100 students, making students create useful and working circuits (it is hard to motivate young students based on abstractions), and having a meaningful approach to electronics, avoiding cookbook recipes and effectively creating knowledge for the students. The modifications seem to have been well received by the students, as expressed by the course success rate and the students' responses to the University quality assurance enquiries.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image scanning techniques for speeded-up color object detection",
        "doc_scopus_id": "84959285043",
        "doc_doi": "10.1201/b19241-52",
        "doc_eid": "2-s2.0-84959285043",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Color object detection",
            "Color segmentation",
            "Computational costs",
            "Computer vision algorithms",
            "Different resolutions",
            "Low resolution images",
            "Object detection algorithms",
            "Real time capability"
        ],
        "doc_abstract": "© 2016 Taylor & Francis Group, London.Nowadays there are many computer vision algorithms dedicated to solve the problem of object detection, from many different perspectives. Many of these algorithms take a considerable processing time even for low resolution images. Research areas that are more concerned about real-time capabilities of a system, such as the case of robotics for example, are focusing on adapting the existing algorithms or developing new ones that cope with time constraints. Color object detection algorithms are often used in time-constrained application since the color of an object is an important clue for its detection. In addition, color segmentation algorithms have a smaller computational cost than generic object detection algorithms. Most of the color object detection algorithms include a step of image scanning in search of pixels of the color of interest. In this paper we present a study on the influence of an image scanning pattern while searching for pixels of a certain color versus the image resolution in the results of three color object detection algorithms. We present results comparing different scanning approaches versus different resolution images for color object detection algorithms based on blob formation, contours detection and region growing clustering. Our experiments show that using search lines for color segmentation, followed by blob formation provides faster and more accurate results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the kicking accuracy in a soccer robot",
        "doc_scopus_id": "84955480270",
        "doc_doi": "10.1145/2695664.2695862",
        "doc_eid": "2-s2.0-84955480270",
        "doc_date": "2015-04-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Accuracy",
            "Friction parameters",
            "Heuristic approach",
            "Kicking device",
            "Middle-Size League",
            "Physical characteristics",
            "RoboCu",
            "Target direction"
        ],
        "doc_abstract": "Copyright 2015 ACM.Most of the soccer robots in the Middle Size League of Robo-Cup use electromagnetic kicking devices that allow to kick the ball with adjustable strength. In order to be efficient to score, the kicking strength should be calculated according to several parameters, namely the physical characteristics of the kicking device, the distance to the target, the velocity of the robot and the floor friction parameters. Moreover, the kicking decision should be taken at a moment in which the actual movement of the robot would result in the ball being released to the target direction, even if it is not physically aligned with it. This paper proposes an algorithm to improve the kicking accuracy, taking into account the described parameters, in a heuristic approach. The experimental results presented in this paper show the effectiveness of the proposed solution to improve the efficiency of the kicking device.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "RoboCup MSL - History, accomplishments, current status and challenges ahead",
        "doc_scopus_id": "84958546317",
        "doc_doi": "10.1007/978-3-319-18615-3_51",
        "doc_eid": "2-s2.0-84958546317",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The RoboCup Middle-Size League (MSL) is one of the founding leagues of the annual RoboCup competition. Ever since its birth it has been a league where development of hard- and software happens simultaneously in a real-world decentralized multi-robot soccer setting. Over the years the MSL achieved scientific results in robust design of mechatronic systems, sensor-fusion, tracking, world modelling and distributed multi-agent coordination. Because of recent rule changes which actively stimulate passing, matches in RoboCup MSL have become increasingly appealing to a general audience. Approximately five thousand spectators were present during last years final match. In this paper we present our plan to build on this momentum to further boost scientific progress and to attract new teams to the league. We also give a historical overview and discuss the current state of the MSL competition in terms of strengths, weaknesses, opportunities and threats.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UAVision: A modular time-constrained vision library for soccer robots",
        "doc_scopus_id": "84958540813",
        "doc_doi": "10.1007/978-3-319-18615-3_40",
        "doc_eid": "2-s2.0-84958540813",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The game of soccer is one of the main focuses of the RoboCup competitions, being a fun and entertaining research environment for the development of autonomous multi-agent cooperative systems. For an autonomous robot to be able to play soccer, first it has to perceive the surrounding world and extract only the relevant information in the game context. Therefore, the vision system of a robotic soccer player is probably the most important sensorial element, on which the acting of the robot is fully based. In this paper we present a new modular time-constrained vision library, named UAVision, that allows the use of video sensors up to a frame rate of 50 fps in full resolution and provides accurate results in terms of detection of the objects of interest for a robot playing soccer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A skill-based architecture for pick and place manipulation tasks",
        "doc_scopus_id": "84945908825",
        "doc_doi": "10.1007/978-3-319-23485-4_45",
        "doc_eid": "2-s2.0-84945908825",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Manipulation task",
            "Pick and place",
            "Product customization",
            "Specific tasks",
            "Unstructured environments"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Robots can play a significant role in product customization but they should leave a repetitive, low intelligence paradigm and be able to operate in unstructured environments and take decisions during the execution of the task. The EuRoC research project addresses this issue by posing as a competition to motivate researchers to present their solution to the problem. The first stage is a simulation competition where Pick & Place type of tasks are the goal and planning, perception and manipulation are the problems. This paper presents a skill-based architecture that enables a simulated moving manipulator to solve these tasks. The heuristics that were used to solve specific tasks are also presented. Using computer vision methods and the definition of a set of manipulation skills, an intelligent agent is able to solve them autonomously. The work developed in this project was used in the simulation competition of EuRoC project by team IRIS and enabled them to reach the 5th rank.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ECG monitoring via capacitive body coupled communications",
        "doc_scopus_id": "84906898111",
        "doc_doi": "10.1109/MeMeA.2014.6860047",
        "doc_eid": "2-s2.0-84906898111",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Capacitive couplings",
            "Ecg-electrocardiogram",
            "Near fields",
            "Range",
            "Trx-transceiver"
        ],
        "doc_abstract": "Personal Area Networks (PANs) used around the Human Body, often called Body Area Networks (BANs), are being developed in order to provide reliable communication services between personal devices, gadgets and sensors. Capacitive Body Coupled Communications (BCC) are promising to overcome existing problems of available commercial solutions (like Zigbee and Bluetooth), as they intrinsically have low power consumption and produce almost no interference. As BCC use near field Capacitive Coupling, the fields used to transfer energy are confined only to a short range around the source and the human body, and decrease at a much higher rate with distance than radiation fields. The short abrupt range characteristic can be advantageous for keeping the network safe and without causing interference in nearby PANs, but puts a big challenge in the receiver design, namely the requirements of gain, high input impedance, low noise and very low input capacitance. This paper presents an ECG acquisition system connected by BCC to a mobile device. This demonstration proves that BCC has 'enough performance' and is a promising solution for health-care monitoring. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UAVision: A modular time-constrained vision library for color-coded object detection",
        "doc_scopus_id": "84905852257",
        "doc_doi": "10.1007/978-3-319-09994-1_35",
        "doc_eid": "2-s2.0-84905852257",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Artificial vision system",
            "Computer vision library",
            "Human vision systems",
            "Object Detection",
            "Robotic soccer game",
            "Traffic sign detection",
            "Video sensors",
            "Vision library"
        ],
        "doc_abstract": "The ultimate goal of Computer Vision has been, for more than half of century, to create an artificial vision system that could imitate the human vision. The artificial vision system should have all the capabilities of the human vision system but must not carry the same flaws. Robotics and Automation are just two examples of research areas that use artificial vision systems as the main sensorial element. In these areas, the use of color-coded objects is very common since it relieves the burden of information processing while being an unobtrusive restraint of the environment. We present a novel computer vision library called UAVision that provides support for different video sensors technologies and all the necessary software for implementing an artificial vision system for the detection of color-coded objects. The experimental results that we present, both for the scenario of robotic soccer games and for traffic sign detection, show that our library can work at more than 50fps with images of 1 megapixel. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Self-calibration of colormetric parameters in vision systems for autonomous soccer robots",
        "doc_scopus_id": "84905509813",
        "doc_doi": "10.1007/978-3-662-44468-9_17",
        "doc_eid": "2-s2.0-84905509813",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Extract informations",
            "Humanoid soccer robots",
            "Initial configuration",
            "Intensity histograms",
            "Lighting intensity",
            "Robotic applications",
            "Statistical information",
            "Surrounding environment"
        ],
        "doc_abstract": "Vision is an extremely important sense for both humans and robots, providing detailed information about the environment. In the past few years, the use of digital cameras in robotic applications has been significantly increasing. The use of digital cameras as the main sensor allows the robot to capture the relevant information of the surrounding environment and take decisions. A robust vision system should be able to reliably detect objects and present an accurate representation of the world to higher-level processes, not only under ideal light conditions, but also under changing lighting intensity and color balance. To extract information from the acquired image, shapes or colors, the configuration of the colormetric camera parameters, such as exposure, gain, brightness or white-balance, among others, is very important. In this paper, we propose an algorithm for the self-calibration of the most important parameters of digital cameras for robotic applications. The algorithms extract some statistical information from the acquired images, namely the intensity histogram, saturation histogram and information from a black and a white area of the image, to then estimate the colormetric parameters of the camera. We present experimental results with two robotic platforms, a wheeled robot and a humanoid soccer robot, in challenging environments: soccer fields, both indoor and outdoor, that show the effectiveness of our algorithms. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera and the type and amount of light of the environment, both indoor and outdoor. © 2014 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "FIREMAN: Firefighter team breathing management system using android",
        "doc_scopus_id": "84885202051",
        "doc_doi": "10.1145/2493988.2494339",
        "doc_eid": "2-s2.0-84885202051",
        "doc_date": "2013-10-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Accurate estimation",
            "Breathing apparatus",
            "Cyber physical systems (CPSs)",
            "Firefighter",
            "Mobile/android",
            "Smart-phone applications",
            "System implementation",
            "Ventilation patterns"
        ],
        "doc_abstract": "In this paper we propose FIREMAN, a low cost system for online monitoring of firefighters ventilation patterns when using Self-Contained Breathing Apparatus (SCBA), based on a specific hardware device attached to SCBA and a Smartphone application. The system implementation allows the detection of relevant ventilation patterns while providing feasible and accurate estimation of SCBA air consumption.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling inductive coupling for wireless power transfer to integrated circuits",
        "doc_scopus_id": "84881498473",
        "doc_doi": "10.1109/WPT.2013.6556917",
        "doc_eid": "2-s2.0-84881498473",
        "doc_date": "2013-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Fuel Technology",
                "area_abbreviation": "ENER",
                "area_code": "2103"
            }
        ],
        "doc_keywords": [
            "Electromagnetic simulation",
            "Inductive couplings",
            "Integrated circuits (ICs)",
            "Low-power consumption",
            "Near fields",
            "Range",
            "Wireless communications",
            "Wireless power transfer"
        ],
        "doc_abstract": "With the trend of portable electronics to go battery-less, Inductive Coupling Wireless Power Transfer (ICW Power Transfer) is becoming commonplace. Inductive Coupling has shown to be a good technique for proximity and wireless power transfer in general, because it permits the easy use of impedance matching and resonance circuits. An important potential application is powering Integrated Circuits (ICs) from a PCB (Printed Circuit Board) without using conductive pins. While using ferromagnetic core materials improves coupling, conductive non-ferromagnetic ones like most IC's substrates, decrease coupling and ICW Power Transfer performance. In this paper we developed a simple theoretical model for ICW power transfer and compared it with 3D Electromagnetic simulations of an inductive link system to ICs. The results show that enough power can be supplied to very low power consumption ICs. As this technique can be also used to perform wireless communications, it opens the possibility to design ICs without pins at all. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of color spaces for user-supervised color classification in robotic vision",
        "doc_scopus_id": "85072926641",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85072926641",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Autonomous Mobile Robot",
            "Color classification",
            "Color image segmentation",
            "Graphical applications",
            "Image processing applications",
            "Robotic soccer players",
            "Robotic vision",
            "Supervised algorithm"
        ],
        "doc_abstract": "© 2013 CSREA Press. All rights reserved.Autonomous robots are becoming an integrated part of our daily life. The use of a robot for substituting man power in different activities that might be too dangerous, repetitive or time consuming, has become a common procedure nowadays. Robotic soccer is a research branch that focuses on developing autonomous mobile robots, using the game of soccer as a testing platform. The soccer environment in RoboCup competitions is still more controlled than the one of the regular soccer games among human teams. For the vision system of the robotic soccer players, the colors of the objects of interest are still important clues for their detection. Thus, most of the research teams attending the robotic soccer competitions, use manual or user-supervised color classification procedures before each game, in order to guarantee an accurate object detection during the game. This paper intends to be an evaluation of the most common color spaces used in image processing applications that are based on color segmentation. The paper presents a graphical application used for testing both the performance of human users in the classification of different colors, under different color spaces, as well as the performance of user supervised algorithms for color classification. The results acquired prove that the color spaces which separate the luminance information from the chromatic one, mainly the YUV color space, provide a more accurate outcome.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Self-calibration of colormetric parameters in vision systems for autonomous mobile robots",
        "doc_scopus_id": "85072918488",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85072918488",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Autonomous driving agents",
            "Autonomous Mobile Robot",
            "Camera calibration",
            "Colormetric",
            "Initial configuration",
            "Robotic vision",
            "Statistical information",
            "Surrounding environment"
        ],
        "doc_abstract": "© 2013 CSREA Press. All rights reserved.Vision is an extremely important sense for both humans and robots, providing detailed information about the environment. In the past few years, the use of digital cameras in robotic applications has been increasing significantly. The use of digital cameras as the main sensor allows the robot to take the relevant information from the surrounding environment and then take decisions. A robust vision system should be able to detect objects reliably and present an accurate representation of the world to higher-level processes, not only under ideal light conditions, but also under changing light intensity and color balance. In this paper, we propose an algorithm for the self-calibration of the most important parameters of digital cameras for robotic applications. The algorithm extracts statistical information from the acquired images, namely the intensity histogram, saturation histogram and information from a black and a white area of the image, to then estimate the colormetric parameters of the camera. We present experimental results obtained with several autonomous robotic platforms: two wheeled platforms, with different architectures of the vision system, a humanoid robots and an autonomous driving agent. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera and the type and amount of light of the environment, both indoor and outdoor.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From an autonomous soccer robot to a robotic platform for elderly care",
        "doc_scopus_id": "84861984252",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861984252",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Developed countries",
            "Elderly care",
            "Hardware and software",
            "Home care",
            "Independent living",
            "Innovative solutions",
            "Number of peoples",
            "Nursing homes",
            "Robotic platforms",
            "Robotic soccer",
            "Soccer robot"
        ],
        "doc_abstract": "Current societies in developed countries face a serious problem of aged population. The growing number of people with reduced health and capabilities, allied with the fact that elders are reluctant to leave their own homes to move to nursing homes, requires innovative solutions since continuous home care can be very expensive and dedicated 24/7 care can only be accomplished by more than one care-giver. This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A modular real-time vision module for humanoid robots",
        "doc_scopus_id": "84857012751",
        "doc_doi": "10.1117/12.911206",
        "doc_eid": "2-s2.0-84857012751",
        "doc_date": "2012-02-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Acquisition modules",
            "Camera calibration",
            "Camera parameter",
            "Client-server models",
            "Color calibration",
            "Color classification",
            "Color-coded environments",
            "Computational capability",
            "Field lines",
            "Hardware architecture",
            "Humanoid robot",
            "Its efficiencies",
            "Modular designs",
            "Modular vision",
            "Object Detection",
            "Processing capability",
            "Processing Time",
            "Real time",
            "Real time vision",
            "RoboCup",
            "Robotic platforms",
            "Robotic soccer",
            "Robotic vision",
            "Self calibration",
            "Self-calibration algorithms",
            "Soccer games",
            "Soccer-playing robots",
            "Vision systems",
            "Worst case scenario"
        ],
        "doc_abstract": "Robotic vision is nowadays one of the most challenging branches of robotics. In the case of a humanoid robot, a robust vision system has to provide an accurate representation of the surrounding world and to cope with all the constraints imposed by the hardware architecture and the locomotion of the robot. Usually humanoid robots have low computational capabilities that limit the complexity of the developed algorithms. Moreover, their vision system should perform in real time, therefore a compromise between complexity and processing times has to be found. This paper presents a reliable implementation of a modular vision system for a humanoid robot to be used in color-coded environments. From image acquisition, to camera calibration and object detection, the system that we propose integrates all the functionalities needed for a humanoid robot to accurately perform given tasks in color-coded environments. The main contributions of this paper are the implementation details that allow the use of the vision system in real-time, even with low processing capabilities, the innovative self-calibration algorithm for the most important parameters of the camera and its modularity that allows its use with different robotic platforms. Experimental results have been obtained with a NAO robot produced by Aldebaran, which is currently the robotic platform used in the RoboCup Standard Platform League, as well as with a humanoid build using the Bioloid Expert Kit from Robotis. As practical examples, our vision system can be efficiently used in real time for the detection of the objects of interest for a soccer playing robot (ball, field lines and goals) as well as for navigating through a maze with the help of color-coded clues. In the worst case scenario, all the objects of interest in a soccer game, using a NAO robot, with a single core 500Mhz processor, are detected in less than 30ms. Our vision system also includes an algorithm for self-calibration of the camera parameters as well as two support applications that can run on an external computer for color calibration and debugging purposes. These applications are built based on a typical client-server model, in which the main vision pipe runs as a server, allowing clients to connect and distantly monitor its performance, without interfering with its efficiency. The experimental results that we acquire prove the efficiency of our approach both in terms of accuracy and processing time. Despite having been developed for the NAO robot, the modular design of the proposed vision system allows it to be easily integrated into other humanoid robots with a minimum number of changes, mostly in the acquisition module. © 2012 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling capacitive coupling systems for Body Coupled Communications",
        "doc_scopus_id": "84908583774",
        "doc_doi": "10.4108/icst.bodynets.2012.250036",
        "doc_eid": "2-s2.0-84908583774",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Body area networks",
            "Body Coupled Communications",
            "Capacitive couplings",
            "Electromagnetic simulation",
            "Low Power",
            "Near field communications",
            "Near fields",
            "Personal wireless devices"
        ],
        "doc_abstract": "Copyright © 2012 ICST.With the exponential grow of personal wireless devices, issues like power consumption, communication security and interference immunity are gaining more and more importance. Body Area Networks (BAN), due to their reduced range, are by nature relatively low power, but as an answer to achieve even a more confined range, Near Field Communications (NFC) and Body Coupled Communications (BCC) have been proposed. While the first is used to link two devices that are physically very close, the BCC concept broadens the communication range to the region around the human body. One of the possible technologies supporting BCC is based upon relatively low frequency capacitive coupling between emitter and receiver. Understanding the connection between the physical layout of an electromagnetic system and its electrical model is important to understand its performance and critical aspects. However, although much information exists about antennas and their radiated energy, very few work has been done on how energy is transferred between the electrodes used in a capacitive communication system. In this paper we propose a simple model for estimating the gain of a capacitive coupling system. This model has been validated by 3D electromagnetic simulations and double checked with practical experiments performed in a controlled environment (faraday cage with only the minimum indispensable instrumentation equipment). The results are similar within an order of magnitude which, for the intended use of the model, proves to be accurate enough.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A mobile robotic platform for elderly care",
        "doc_scopus_id": "79960496492",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960496492",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Elderly care",
            "Hardware and software",
            "Independent living",
            "Mobile robotic",
            "Next generation network",
            "Robotic platforms",
            "Robotic soccer"
        ],
        "doc_abstract": "This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An efficient omnidirectional vision system for soccer robots: From calibration to object detection",
        "doc_scopus_id": "79952628276",
        "doc_doi": "10.1016/j.mechatronics.2010.05.006",
        "doc_eid": "2-s2.0-79952628276",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Multi-robot systems",
            "Omnidirectional vision system",
            "Robotic soccer team",
            "Robotic vision",
            "Self localization",
            "Shape based",
            "Technical challenges",
            "Vision systems"
        ],
        "doc_abstract": "Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-06-19 2010-06-19 2011-03-08T22:21:19 S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 S300 S300.1 FULL-TEXT 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-06-19T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 6 399 410 399 410 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright © 2010 Elsevier Ltd. All rights reserved. EFFICIENTOMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTSCALIBRATIONOBJECTDETECTION NEVES A 1 Introduction 2 Architecture of the vision system 3 Calibration of the vision system 3.1 Self-calibration of the digital camera parameters 3.1.1 Proposed algorithm 3.1.2 Experimental results 3.2 Distance map calibration 4 Color-based object detection 4.1 Color extraction 4.2 Object detection 4.3 Experimental results 5 Arbitrary ball detection 5.1 Related work 5.2 Proposed approach 5.3 Experimental results 6 Conclusions Acknowledgment References LIMA 2001 87 102 P ASTROM 1995 K PIDCONTROLLERSTHEORYDESIGNTUNING BAKER 1999 175 196 S BRESENHAM 1965 25 30 J TREPTOW 2004 41 48 A GRIMSON 1990 1255 1274 W NEVESX2011X399 NEVESX2011X399X410 NEVESX2011X399XA NEVESX2011X399X410XA item S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 true 1828663 MAIN 12 84396 849 656 IMAGE-WEB-PDF 1 si4 1540 53 289 si3 1436 46 293 si2 1250 46 238 si1 1092 46 192 gr10 45233 182 377 gr10 14653 106 219 gr11 18028 138 376 gr11 2966 81 219 gr12 43502 281 757 gr12 6024 81 219 gr13 31126 208 377 gr13 11667 121 219 gr14 7740 60 489 gr14 1994 27 219 gr15 19330 223 373 gr15 3452 131 219 gr16 43365 197 765 gr16 5658 56 219 gr17 11813 182 264 gr17 4751 151 219 gr18 9049 172 358 gr18 2922 105 219 gr19 8089 119 217 gr19 4702 121 219 gr2 27937 199 271 gr2 19723 161 219 gr20 23395 140 484 gr20 5238 63 219 gr21 24408 220 361 gr21 4625 133 219 gr3 57308 455 703 gr3 8598 142 219 gr4 25516 139 381 gr4 10182 80 219 gr5 22239 199 575 gr5 3068 76 219 gr6 26663 137 374 gr6 10562 80 219 gr7 61745 405 533 gr7 15882 164 215 gr8 53286 374 533 gr8 11776 154 219 gr9 33410 179 378 gr9 13228 104 219 gr1 39196 225 378 gr1 14502 130 219 MECH 1160 S0957-4158(10)00086-3 10.1016/j.mechatronics.2010.05.006 Elsevier Ltd Fig. 1 The CAMBADA team playing at RoboCup 2009, Graz, Austria. Fig. 2 On the left, a detailed view of the CAMBADA vision system. On the right, one of the robots. Fig. 3 Some experiments using the automated calibration procedure. At the top, results obtained starting with all the parameters of the camera set to zero. At the bottom, results obtained with all the parameters set to the maximum value. On the left, the initial image acquired. In the middle, the image obtained after applying the automated calibration procedure. On the right, the graphics showing the evolution of the parameters along the time. Fig. 4 On the left, an example of an image acquired with the camera parameters in auto-mode. On the right, an image acquired after applying the automated calibration algorithm. Fig. 5 The histogram of the intensities of the two images presented in Fig. 4. In (a) it is shown the histogram of the image obtained with the camera parameters set to the maximum value. (b) Shows the histogram of the image obtained after applying the automated calibration procedure. Fig. 6 On the left, an image acquired outdoors using the camera in auto-mode. As it is possible to observe, the colors are washed out. This happens because the camera’s auto-exposure algorithm tries to compensate the black region around the mirror. On the right, the same image with the camera calibrated using our algorithm. As can be seen, the colors and the contours of the objects are much more defined. Fig. 7 A screenshot of the tool developed to calibrate some important parameters of the vision system, namely the inverse distance map, the mirror and robot center and the regions of the image to be processed. Fig. 8 A screenshot of the interface to calibrate some important parameters need to obtain the inverse distance map (these parameters are described in [17]). Fig. 9 Acquired image after reverse-mapping into the distance map. On the left, the map was obtained with all misalignment parameters set to zero. On the right, after automatic correction. Fig. 10 A 0.5m grid, superimposed on the original image. On the left, with all correction parameters set to zero. On the right, the same grid after geometrical parameter extraction. Fig. 11 On the left, the position of the radial search lines used in the omnidirectional vision system, after detecting the center of the robot in the image using the tool described in this section. On the right, an example of a robot mask used to select the pixels to be processed, obtained with the same tool. White points represent the area that will be processed. Fig. 12 The software architecture of the omnidirectional vision system. Fig. 13 A screenshot of the application used to calibrate the color ranges for each color class using the HSV color space. Fig. 14 An example of a transition. “G” means green pixel, “W” means white pixel and “X” means pixel with a color different from green or white, for example resulting due to some noise or a not perfect color calibration. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 15 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 16 On the left, an example of an original image acquired by the omnidirectional vision system. In the center, the corresponding image of labels. On the right, the color blobs detected in the images. Marks over the ball point to the mass center. The several marks near the white lines (magenta) are the position of the white lines. The cyan marks are the position of the obstacles. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 17 Experimental results obtained by the omnidirectional system using the color ball detection. In this experiment, the ball was positioned in the center of the field, position (0,0). The robot performed a predefined trajectory while the position of the ball and the robot was recorded. Both axes in the graphics are in meters. Fig. 18 The circular Hough transform. a and b represent the parameter space that in this application are the radius of the ball and the distance to robot, respectively. Fig. 19 Example of a circle detection through the use of the circular Hough transform. Fig. 20 Example of a captured image using the proposed approach. The cross over the ball points out the detected position. In (b) the image (a), with the Canny edge detector applied. In (c), the image (b) after applying the circular Hough transform. Fig. 21 Experimental results obtained by the omnidirectional system using the morphological ball detection. In this experience, the ball was positioned in the penalty mark of the field. The robot performed a predefined trajectory while the position of the ball was recorded. Both axes in the graphics are in meters. Table 1 Statistical measures obtained for the images presented in Figs. 3 and 4. The initial values refer to the images obtained with the camera before applying the proposed automated calibration procedure. The final values refer to the images acquired with the cameras configured with the proposed algorithm. Experiment – ACM μ E MSV Parameters set to zero Initial 111.00 16.00 0.00 1.00 Final 39.18 101.95 6.88 2.56 Parameters set to maximum Initial 92.29 219.03 2.35 4.74 Final 42.19 98.59 6.85 2.47 Camera in auto-mode Initial 68.22 173.73 6.87 3.88 Final 40.00 101.14 6.85 2.54 An efficient omnidirectional vision system for soccer robots: From calibration to object detection António J.R. Neves ⁎ Armando J. Pinho Daniel A. Martins Bernardo Cunha ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. Keywords Robotic vision Omnidirectional vision systems Color-based object detection Shape-based object detection Vision system calibration 1 Introduction The Middle Size League (MSL) of RoboCup is a forum where several research areas have been challenged for proposing solutions to well-defined practical problems. The robotic vision is one of those areas and, for most of the MSL teams, it has become the only way of sensing the surrounding world. From the point of view of a robot, the playing field during a game provides a fast-changing scenery, where the teammates, the opponents and the ball move quickly and often in an unpredictable way. The robots have to capture these scenes through their cameras and have to discover where the objects of interest are located. There is no time for running complex algorithms. Everything has to be computed and decided in a small fraction of a second, for allowing real-time operation; otherwise, it becomes useless. Real-time is not the only challenge that needs to be addressed. Year after year, the initially well controlled and robot friendly environment where the competition takes place has become increasingly more hostile. Conditions that previously have been taken for granted, such as controlled lighting or easy to recognize color coded objects, have been relaxed or even completely suppressed. Therefore, the vision system of the robots needs to be prepared for adapting to strong lighting changes during a game, as well as, for example, for ball-type changes across games. In this paper, we provide a comprehensive description of the vision system of the MSL CAMBADA team (Fig. 1 ). Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture (CAMBADA) is the RoboCup MSL soccer team of the Institute of Electronics and Telematics Engineering of Aveiro (IEETA) research institute, University of Aveiro, Portugal. The team, which started officially in October 2003, won the 2008 MSL RoboCup World Championship and ranked 3rd in the 2009 edition. We start by presenting and explaining the hardware architecture of the vision system used by the robots of the CAMBADA team, which relies on an omnidirectional vision system (Section 2). Then, we proceed with the description of the approach that we have adopted regarding the calibration of a number of crucial parameters and in the construction of auxiliary data structures (Section 3). Concerning the calibration of the intrinsic parameters of the digital camera, we propose an automated calibration algorithm that is used to configure the most important features of the camera, namely, the saturation, exposure, white-balance, gain and brightness. The proposed algorithm uses the histogram of intensities of the acquired images and a black and a white area, known in advance, to estimate the referred parameters. We also describe a general solution to calculate the robot centered distances map, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. The soccer robots need to locate several objects of interest, such as the ball, the opponent robots and the teammates. Moreover, they also need to collect information for self-localization, namely, the position of the field white lines. For these tasks, we have developed fast and efficient algorithms that rely on color information. The color extraction algorithms are based on lookup tables and use a radial model for color object detection. Due to the severe restrictions imposed by the real-time constraint, some of the image processing tasks are implemented using a multi-threading approach and use special data structures to reduce the processing time. Section 4 provides a detailed description of these algorithms. As previously mentioned, the color codes assigned to the objects of interest tend to disappear as the competition evolves. For example, the usual orange ball used in the MSL will soon be replaced by an arbitrary FIFA ball, increasing the difficulty in locating one of the most important objects in the game. Anticipating this scenario, we developed a fast method for detecting soccer balls independently of their colors. In Section 5, we describe a solution based on the morphological analysis of the image. The algorithm relies on edge detection and on the circular Hough transform, attaining a processing time almost constant and complying with the real-time constraint. Its appropriateness has been clearly demonstrated by the results obtained in the mandatory technical challenge of the RoboCup MSL: 2nd place in 2008 and 1st place in 2009. 2 Architecture of the vision system The CAMBADA robots [1] use a catadioptric vision system, often named omnidirectional vision system, based on a digital video camera pointing at a hyperbolic mirror, as presented in Fig. 2 . We are using a digital camera Point Grey Flea 2, 1 Last accessed: 18/02/2010. 1 FL2-08S2C with a 1/3” CCD Sony ICX204 that can deliver images up to 1024×768 pixels in several image formats, namely RGB, YUV 4:1:1, YUV 4:2:2 or YUV 4:4:4. The hyperbolic mirror was developed by IAIS Fraunhofer Gesellschaft 2 Last accessed: 18/02/2010. 2 (FhG-AiS). Although the mirror was designed for the vision system of the FhG Volksbot 3 Last accessed: 18/02/2010. 3 we are achieving also an excellent result with it in our vision system. The use of omnidirectional vision systems have captured much interest in the last years, because it allows a robot to attain a 360° field of view around its central vertical rotation axis, without having to move itself or its camera. In fact, it has been a common solution for the main sensorial element in a significant number of autonomous mobile robot applications, as is the case of the MSL, where most of the teams have adopted this approach [2–9]. A catadioptric vision system ensures an integrated perception of all major target objects in the surrounding area of the robot, allowing a higher degree of maneuverability. However, this also implies higher degradation in the resolution with growing distances away from the robot, when compared to non-isotropic setups. 3 Calibration of the vision system An important task in the MSL is the calibration of the vision system. This includes the calibration of intrinsic parameters of the digital camera, the computation of the inverse distance map, the detection of the mirror and robot center and the definition of the regions of the image that have to be processed. Calibration has to be performed when environmental conditions change, such as playing in a different soccer field or when the lighting conditions vary over time. Therefore, there are adjustments that have to be made almost continuously, for example if the playing field is unevenly illuminated, or less frequently, when the playing field changes. Moreover, a number of adjustments have also to be performed when some of the vision hardware of the robot is replaced, such as the camera or the mirror. All these calibrations and adjustments should be robust, i.e., they should be as much as possible insensitive to small environmental variations, they should be fast to perform and they should be simple to execute, so that no special calibration expert is required to operate them. 3.1 Self-calibration of the digital camera parameters In a near future, it is expected that the MSL robots will have to play under natural lighting conditions and in outdoor fields. This introduces new challenges. In outdoor fields, the illumination may change slowly during the day, due to the movement of the sun, but also may change quickly in short periods of time due to a partial and temporally varying covering of the sun by clouds. In this case, the robots have to adjust, in real-time, both the color segmentation values as well as some of the camera parameters, in order to adapt to new lighting conditions [10]. The common approach regarding the calibration of the robot cameras in the MSL has been based on manual adjustments, that are performed prior to the games, or through some automatic process that runs offline using a pre-acquired video sequence. However, most (or even all) of the parameters remain fixed during the game. We propose an algorithm that does not require human interaction to configure the most important parameters of the camera, namely the exposure, the white-balance, the gain and the brightness. Moreover, this algorithm runs continuously, even during the game, allowing coping with environmental changes that often occur when playing. We use the histogram of intensities of the acquired images and a black and a white area, which location is known in advance, to estimate the referred parameters of the camera. Note that this approach differs from the well known problem of photometric camera calibration (a survey can be found in [11]), since we are not interested in obtaining the camera response values, but only to configure its parameters according to some measures obtained from the acquired images. The self-calibration process for a single robot requires a few seconds, including the time necessary to start the application. This is significantly faster than the usual manual calibration by an expert user, for which several minutes are needed. 3.1.1 Proposed algorithm The proposed calibration algorithm processes the image acquired by the camera and analyzes a white area in the image (a white area in a fixed place on the robot body, near the camera in the center of the image), in order to calibrate the white-balance. A black area (we use a part of the image that represents the robot itself, actually a rectangle in the upper left side of the image) is used to calibrate the brightness of the image. Finally, the histogram of the image intensities is used to calibrate the exposure and gain. The histogram of the intensities of an image is a representation of the number of times that each intensity value appears in the image. For an image represented using 8 bits per pixel, the possible values are between 0 and 255. Image histograms can indicate some aspects of the lighting conditions, particularly the exposure of the image and whether if it is underexposed or overexposed. The assumptions used by the proposed algorithm are the following: (i) The white area should appear white in the acquired image. In the YUV color space, this means that the average value of U and V should be close to 127, that is to say, the chrominance components of the white section should be as close to zero as possible. If the white-balance is not correctly configured, these values are different from 127 and the image does not have the correct colors. The white-balance parameter is composed by two values, WB_BLUE and WB_RED, directly related to the values of U and V, respectively. (ii) The black area should be black. In the RGB color space, this means that the average values of R, G and B should be close to zero. If the brightness parameter is too high, it is observed that the black region becomes blue, resulting in a degradation of the image. (iii) The histogram of intensities should be centered around 127 and should span all intensity values. Dividing the histogram into regions, the left regions represent dark colors, while the right regions represent light colors. An underexposed image will be leaning to the left, while an overexposed image will be leaning to the right in the histogram (for an example, see Fig. 5a). The values of the gain and exposure parameters are adjusted according to the characteristic of the histogram. Statistical measures can be extracted from the images to quantify the image quality [12,13]. A number of typical measures used in the literature can be computed from the image gray level histogram, namely, the mean (1) μ = ∑ i = 0 N - 1 iP i , μ ∈ [ 0 , 255 ] , the entropy (2) E = - ∑ i = 0 N - 1 P i log ( P i ) , E ∈ [ 0 , 8 ] , the absolute central moment (3) ACM = ∑ i = 0 N - 1 | i - μ | P i , ACM ∈ [ 0 - 127 ] and the mean sample value (4) MSV = ∑ j = 0 4 ( j + 1 ) x j ∑ j = 0 4 x j , MSV ∈ [ 0 - 5 ] , where N is the number of possible gray values in the histogram (typically, 256), P i is the relative frequency of each gray value and x j is the sum of the gray values in region j of the histogram (in the proposed approach we divided the histogram into five regions). When the histogram values of an image are uniformly distributed in the possible values, then μ ≈127, E ≈8, ACM ≈60 and MSV ≈2.5. In the experimental results we use these measures to analyze the performance of the proposed calibration algorithm. Moreover, we use the information of MSV to calibrate the exposure and the gain of the camera. The algorithm is depicted next. do do acquire image calculate the histogram of intensities calculate the MSV value if MSV<2.0 OR MSV>3.0 apply the PI controller to adjust exposure else apply the PI controller to adjust gain set the camera with new exposure and gain values while exposure or gain parameters change do acquire image calculate average U and V values of the white area apply the PI controller to adjust WB_BLUE apply the PI controller to adjust WB_RED set the camera with new white-balance parameters while white-balance parameters change do acquire image calculate average R, G and B values of the black area apply the PI controller to adjust brightness set the camera with new brightness value while brightness parameter change while any parameter changed The calibration algorithm configures one parameter at a time, proceeding to the next one when the current one has converged. For each of these parameters, a PI controller was implemented. PI controllers are used instead of proportional controllers as they result in better control, having no stationary error. The coefficients of the controller were obtained experimentally: first, the proportional gain was increased until the camera parameter started to oscillate. Then, it was reduced to about 70% of that value and the integral gain was increased until an acceptable time to reach the desired reference was obtained [14]. The algorithm stops when all the parameters have converged. More details regarding this algorithm can be found in [15]. 3.1.2 Experimental results To measure the performance of this calibration algorithm, tests have been conducted using the camera with different initial configurations. In Fig. 3 , results are presented both when the algorithm starts with the parameters of the camera set to zero, as well as when set to the maximum value. As can be seen, the configuration obtained after running the proposed algorithm is approximately the same, independently of the initial configuration of the camera. Moreover, the algorithm is fast to converge (it takes between 60 and 70 frames). In Fig. 4 , it is presented an image acquired with the camera in auto-mode. As can be seen, the image obtained using the camera with the parameters in auto-mode is overexposed and the white balance is not configured correctly. This is due to the fact that the camera analyzes the entire image and, as can be observed in Fig. 3, there are large black regions corresponding to the robot itself. Our approach uses a mask to select the region of interest, in order to calibrate the camera using exclusively the valid pixels. Moreover, and due to the changes in the environment when the robot is moving, leaving the camera in auto-mode leads to undesirable changes in the parameters of the camera, causing color classification problems. Table 1 presents the values of the statistical measures described in (1)–(4), regarding the experimental results presented in Fig. 3. These results confirm that the camera is correctly configured after applying the automated calibration procedure, since the results obtained are close to the optimal. Moreover, the algorithm converges always to the same set of parameters, independently of the initial configuration. According to the experimental results presented in Table 1, we conclude that the MSV measure is the best one for classifying the quality of an image. This is due to the fact that it is closer to the optimal values when the camera is correctly calibrated. Moreover, this measure can distinguish between two images that have close characteristics, as is the case when the camera is used in auto-mode. The good results of the automated calibration procedure can also be confirmed in the histograms presented in Fig. 5 . The histogram of the image obtained after applying the proposed automated calibration procedure (Fig. 5b) is centered near the intensity 127, which is a desirable property, as shown in Fig. 3 in the middle images. The histogram of the image acquired using the camera with all the parameters set to the maximum value (Fig. 5a) shows that the image is overexposed, leading that the majority of the pixels have bright colors. This algorithm has also been tested outdoors, under natural light. Fig. 6 shows that it works well even when the robot is under very different lighting conditions, showing its robustness. 3.2 Distance map calibration For most practical applications, the setup of the vision system requires the translation of the planar field of view at the camera sensor plane, into real world coordinates at the ground plane, using the robot as the center of this system. In order to simplify this non-linear transformation, most practical solutions adopted in real robots choose to create a mechanical geometric setup that ensures a symmetrical solution for the problem by means of a single viewpoint (SVP) approach. This, on the other hand, calls for a precise alignment of the four major points comprising the vision setup: the mirror focus, the mirror apex, the lens focus and the center of the image sensor. Furthermore, it also demands the sensor plane to be both parallel to the ground field and normal to the mirror axis of revolution, and the mirror foci to be coincident with the effective viewpoint and the camera pinhole respectively [16]. Although tempting, this approach requires a precision mechanical setup. In this section, we briefly present a general solution to calculate the robot centered distances map on non-SVP catadioptric setups, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. A detailed description of the algorithms can be found in [17] and a screenshot of the application is presented in Figs. 7 and 8 . This solution effectively compensates for the misalignment that may result either from a simple mechanical setup or from the use of low cost video cameras. The method can also extract most of the required parameters from the acquired image itself, allowing it to be used for self-calibration purposes. In order to allow further trimming of these parameters, two simple image feedback tools have been developed. The first one creates a reverse mapping of the acquired image into the real world distance map. A fill-in algorithm is used to integrate image data in areas outside pixel mapping on the ground plane. This produces a plane vision from above, allowing visual check of line parallelism and circular asymmetries (Fig. 9 ). The second generates a visual grid with 0.5m distances between both lines and columns, which is superimposed on the original image. This provides an immediate visual clue for the need of possible further distance correction (Fig. 10 ). With this tool, it is also possible to determine some other important parameters, namely the mirror center and the area of the image that will be processed by the object detection algorithms (Fig. 11 ). 4 Color-based object detection The algorithms that we propose for object detection can be split into three main modules, namely the Utility Sub-System, the Color Processing Sub-System and the Morphological Processing Sub-System, as shown in Fig. 12 . In the Color Processing Sub-System, proper color classification and extraction processes were developed, along with an object detection process to extract information from the acquired image, through color analysis. The Morphological Processing Sub-System presented in Section 5, is used to detect arbitrary FIFA balls independently of their colors. In order to satisfy the real-time constrains in the proposed image processing system, we implemented efficient data structures to process the image data [18,19]. Moreover, we use a two-thread approach to perform the most time consuming operations in parallel, namely the color classification and the color extraction, taking advantage of the dual core processor used by the laptop computers of our robots. 4.1 Color extraction Image analysis in the MSL is simplified, since objects are color coded. Black robots play with an orange ball on a green field that has white lines. Thus, the color of a pixel is a strong hint for object segmentation. We exploit this fact by defining color classes, using a look-up table (LUT) for fast color classification. The table consists of 16,777,216 entries (224, 8 bits for red, 8 bits for green and 8 bits for blue), each 8 bits wide, occupying a total of 16 MByte. Note that for other color spaces the table size would be the same, changing only the meaning of each component. Each bit expresses whether the color is within the corresponding class or not. This means that a certain color can be assigned to several classes at the same time. To classify a pixel, we first read the pixel’s color and then use the color as an index into the table. The 8-bit value read from the table is called the “color mask” of that pixel. The color calibration is performed in the HSV (Hue, Saturation and Value) color space, since it provides a single, independent, color spectrum variable. In the current setup, the image is acquired in RGB or YUV format and then is converted to an image of labels using the appropriate LUT. Fig. 13 presents a screenshot of the application used to calibrate the color ranges for each color class, using the HSV color space and a histogram based analysis. Certain regions of the image are excluded from analysis. One of them is the part in the image that reflects the robot itself. Other regions are the sticks that hold the mirror and the areas outside the mirror. These regions are found using the algorithm described in Section 3.2. An example is presented on the right of Fig. 11, where the white pixels indicate the area that will be processed. With this approach, we can reduce the time spent in the conversion and searching phases and we also eliminate the problem of finding erroneous objects in those areas. To extract color information from the image we use radial search lines, instead of processing the whole image. A radial search line is a line that starts at the center of the robot, with some angle, and ends at the limits of the image. In an omnidirectional system, the center of the robot is approximately the center of the image (see left of Fig. 11). The search lines are constructed based on the Bresenham line algorithm [20]. They are constructed once, when the application starts, and saved in a structure in order to improve the access to these pixels in the color extraction module. For each search line, we iterate through its pixels to search for transitions between two colors and areas with specific colors. The use of radial search lines accelerates the process of object detection, due to the fact that we only process part of the valid pixels. This approach has a processing time almost constant, independently of the information that is captured by the camera. Moreover, the polar coordinates, inherent to the radial search lines, facilitate the definition of the bounding boxes of the objects in omnidirectional vision systems. We developed an algorithm for detecting areas of a specific color which eliminates the possible noise that could appear in the image. For each radial scanline, it is performed a median filtering operation. Each time a pixel is found with a color of interest, the algorithm analyzes the pixels that follow (a predefined number). If it does not find more pixels of that color, it discards the pixel found and continues. When a predefined number of pixels with that color is found, it considers that the search line has that color. Regarding the ball detection, we created an algorithm to recover lost orange pixels due to the ball shadow cast over itself. As soon as we find a valid orange pixel in the radial sensor, the shadow recovery algorithm tries to search for darker orange pixels previously discarded in the color segmentation analysis. The search is conducted in each radial sensor, starting at the first orange pixel found when searching towards the center of the robot, limited to a maximum number of pixels. For each pixel analyzed, a comparison is performed using a wider region of the color space, in order to being able to accept darker orange pixels. Once a different color is found or the maximum number of pixels is reached, the search along the current sensor is completed and the next sensor is processed. In Fig. 16, we can see the pixels recovered by this algorithm (the orange blobs contain pixels that were not originally classified as orange). To accelerate the process of calculating the position of the objects, we put the color information that was found in each of the search lines into a list of colors. We are interested in the first pixel (in the corresponding search line) where the color was found and with the number of pixels with that color that have been found in the search line. Then, using the previous information, we separate the information of each color into blobs (Fig. 16 shows an example). After this, it is calculated the blob descriptor that will be used for the object detection module, which contains the following information: – Distance to the robot. – Closest pixel to the robot. – Position of the mass center. – Angular width. – Number of pixels. – Number of green and white pixels in the neighborhood of the blob. 4.2 Object detection The objects of interest that are present in a MSL game are: a ball, obstacles and the green field with white lines. Currently, our system detects efficiently all these objects with a set of simple algorithms that, using the color information collected by the radial search lines, calculate the object position and/or its limits in a polar representation (distance and angle). The algorithm that searches for the transitions between green pixels and white pixels is described next. If a non-green pixel is found in a radial scanline, we search for the next green pixel, counting the number of non-green pixels and the number of white pixels that meanwhile appeared. If these values are greater than a predefined threshold, the center of this region is considered a transition point corresponding to a position of a soccer field line. The algorithm is illustrated in Fig. 14 with an example. A similar approach has been described in [21]. The ball is detected using the following algorithm: (i) Separate the orange information into blobs. (ii) For each blob, calculate the information described previously. (iii) Perform a first validation of the orange blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only balls inside the field are detected. (iv) Validate the remaining orange blobs according to the number of pixels. As illustrated in Fig. 15 , it is known the relation between the pixel size at the ground plane and the distance to the center of the robot. Using this knowledge, we estimate the number of pixels that a ball should have according to the distance. (v) Following the same approach, the angular width is also used to validate the blobs. (vi) The ball candidate is the valid blob closest to the robot. The position of the ball is the mass center of the blob. To calculate the position of the obstacles around the robot, we use the following algorithm: (i) Separate the black information into blobs. (ii) Calculate the information for each blob. (iii) Perform a simple validation of the black blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only obstacles inside the field are detected. (iv) The position of the obstacle is given by the distance of the blob relatively to the robot. The limits of the obstacle are obtained using the angular width of the blob. More details regarding the detection and identification of obstacles can be found in [22]. Fig. 16 presents an example of an acquired image, the corresponding segmented image and the detected color blobs. As can be seen, the objects are correctly detected. The position of the white lines, the position of the ball and the information about the obstacles are then sent to the Real-time Database [1,23] and used, afterward, by the high level process responsible for the behaviors of the robots [24,25,22,26]. 4.3 Experimental results To experimentally measure the efficiency of the proposed algorithms, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. Note that the results in this test may be affected by errors in the localization algorithm and by some bumps while the robot is moving. The separate study of these sources of error has being left outside this experimental evaluation. However, they should be performed, for better understanding the several factors that influence the correct localization of the ball. The robot path across the field may be seen in Fig. 17 , along with the measured ball position. According to that data, it is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the effectiveness of the proposed algorithms. Our measures show a very high detection ratio (near 95%), and a good accuracy, with the average measures very close to the real ball position. In our experiments, we verified that the robots are able to detect the ball up to 6m with regular light conditions and a good color calibration, easy to obtain after applying the proposed automated calibration algorithm described in Section 3. The proposed algorithm has an almost constant processing time, independently of the environment around the robot, typically around 6ms. It needs approximately 35MBytes of memory. The experimental results were obtained using a camera resolution of 640×480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz and 1GB of memory. 5 Arbitrary ball detection The color codes tend to disappear as the competition evolves, increasing the difficulty posed to the vision algorithms. The color of the ball, currently orange, is the next color scheduled to become arbitrary. In this section, we propose a solution for overcoming this new challenge, i.e., a method for detecting balls independently of their colors. This solution is based on a morphological analysis of the image, being strictly directed to detect round objects in the field with specific characteristics, in this case the ball. Morphological object recognition through image analysis has became more robust and accurate in the past years, whereas still very time consuming even to modern personal computers. Because RoboCup is a real-time environment, available processing time can become a serious constraint when analyzing large amounts of data or executing complex algorithms. This section presents an arbitrary FIFA ball recognition algorithm, based on the use of image segmentation and the circular Hough transform. The processing time is almost constant and allows real-time processing. As far as we know, this approach has never been proposed. The experimental results obtained, as well as the classifications obtained by the CAMBADA team, seem to be very promising. Regarding the vision system described in Fig. 12, it is possible to specify whether to use the Morphological sub-system to detect the ball or the current color-based approach. Currently, in the MSL, the shape-based detection is only necessary in the mandatory challenge of the competition, although it will be incorporated in the rules in the next years. 5.1 Related work Many of the algorithms proposed during previous research work showed their effectiveness but, unfortunately, their processing time is in some cases over one second per video frame [27]. In [28], the circular Hough transform was presented in the context of colored ball detection as a validation step. However, no details about the implementation and experimental results have been presented. Hanek et al. [29] proposed a Contracting Curve Density algorithm to recognize the ball without color labeling. This algorithm fits parametric curve models to the image data by using local criteria based on local image statistics to separate adjacent regions. This method can extract the contour of the ball even in cluttered environments under different illumination, but the vague position of the ball should be known in advance. The global detection cannot be realized by this method. Treptow et al. [30] proposed a method to detect and track a ball without color information in real-time, by integrating the Adaboost Feature Learning algorithm into a condensation tracking framework. Mitri et al. [31] presented a scheme for color invariant ball detection, in which the edged filtered images serve as the input of an Adaboost learning procedure that constructs a cascade of classification and regression trees. This method can detect different soccer balls in different environments, but the false positive rate is high when there are other round objects in the environment. Coath et al. [32] proposed an edge-based arc fitting algorithm to detect the ball for soccer robots. However, the algorithm is used in a perspective camera vision system in which the field of view is far smaller and the image is also far less complex than that of the omnidirectional vision system used by most of the robotic soccer teams. More recently, Lu et al. [33] considered that the ball on the field can be approximated by an ellipse. They scan the color variation to search for the possible major and minor axes of the ellipse, using radial and rotary scanning, respectively. A ball is considered if the middle points of a possible major axis and a possible minor axis are very close to each other in the image. However, this method has a processing time that can achieve 150ms if the tracking algorithm fails, which might cause problems in real-time applications. 5.2 Proposed approach The proposed approach is presented in the top layer of Fig. 12. The search for potential ball candidates is conducted taking advantage of morphological characteristics of the ball (round shape), using a feature extraction technique known as the Hough transform. This is a technique for identifying the locations and orientations of certain types of features in a digital image [34]. The Hough transform algorithm uses an accumulator and can be described as a transformation of a point in the x, y-plane to the parameter space. The parameter space is defined according to the shape of the object of interest, in this case, the ball presents a rounded shape. First used to identify lines in images, the Hough transform has been generalized through the years to identify positions of arbitrary shapes by a voting procedure [35–37]. Fig. 18 shows an example of a circular Hough transform, for a constant radius, from the x, y-space to the parameter space. In Fig. 19 , we show an example of circle detection through the circular Hough transform. We can see the original image of a dark circle (known radius r) on a bright background (see Fig. 19a). For each dark pixel, a potential circle-center locus is defined by a circle with radius r and center at that pixel (see Fig. 19b). The frequency with which image pixels occur in the circle-center loci is determined (see Fig. 19c). Finally, the highest-frequency pixel represents the center of the circle with radius r. To feed the Hough transform process, it is necessary a binary image with the edge information of the objects. This image, Edges Image, is obtained using an edge detector operator. In the following, we present an explanation of this process and its implementation. To be possible to use this image processing system in real-time, and increase time efficiency, a set of data structures to process the image data has been implemented [18,19]. The proposed algorithm is based on three main operations: (i) Edge detection: this is the first image processing step in the morphological detection. It must be as efficient and accurate as possible in order not to compromise the efficiency of the whole system. Besides being fast to calculate, the intended resulting image must be absent of noise as much as possible, with well defined contours, and be tolerant to the motion blur introduced by the movement of the ball and the robots. Some popular edge detectors were tested, namely Sobel [38,39], Laplace [40,41] and Canny [42]. The tests were conducted under two distinct situations: with the ball standing still and with the ball moving fast through the field. The test with the ball moving fast was performed in order to study the motion blur effect in the edge detectors, on high speed objects captured with a frame rate of 30 frames per second. For choosing the best edge detector for this purpose, the results from the tests were compared taking into account the image of edges and processing time needed by each edge detector. On one hand, the real-time capability must be assured. On the other hand, the algorithm must be able to detect the edges of the ball independently of its motion blur effect. According to our experiments, the Canny edge detector was the most demanding in terms of processing time. Even so, it was fast enough for real-time operation and, because it provided the most effective contours, it was chosen. The parameters of the edge detector were obtained experimentally. (ii) Circular Hough transform: this is the next step in the proposed approach to find points of interest containing eventual circular objects. After finding these points, a validation procedure is used for choosing points containing a ball, according to our characterization. The voting procedure of the Hough transform is carried out in a parameter space. Object candidates are obtained as local maxima of a denoted Intensity Image (Fig. 20 c), that is constructed by the Hough Transform block (Fig. 12). Due to the special features of the Hough circular transform, a circular object in the Edges Image would produce an intense peak in Intensity Image corresponding to the center of the object (as can be seen in Fig. 20c). On the contrary, a non-circular object would produce areas of low intensity in the Intensity Image. However, as the ball moves away, its edge circle size decreases. To solve this problem, information about the distance between the robot center and the ball is used to adjust the Hough transform. We use the inverse mapping of our vision system [17] to estimate the radius of the ball as a function of distance. (iii) Validation: in some situations, particularly when the ball is not present in the field, false positives might be produced. To solve this problem and improve the ball information reliability, we propose a validation algorithm that discards false positives based on information from the Intensity Image and the Acquired Image. This validation algorithm is based on two tests against which each ball candidate is put through. In the first test performed by the validation algorithm, the points with local maximum values in the Intensity Image are considered if they are above a distance-dependent threshold. This threshold depends on the distance of the ball candidate to the robot center, decreasing as this distance increases. This first test removes some false ball candidates, leaving a reduced group of points of interest. Then, a test is made in the Acquired Image over each point of interest selected by the previous test. This test is used to eliminate false balls that usually appear in the intersection of the lines of the field and other robots (regions with several contours). To remove these false balls, we analyze a square region of the image centered in the point of interest. We discard this point of interest if the sum of all green pixels is over a certain percentage of the square area. Note that the area of this square depends on the distance of the point of interest to the robot center, decreasing as this distance increases. Choosing a square where the ball fits tightly makes this test very effective, considering that the ball fills over 90% of the square. In both tests, we use threshold values that were obtained experimentally. Besides the color validation, it is also performed a validation of the morphology of the candidate, more precisely a circularity validation. Here, from the candidate point to the center of the ball, it is performed a search of pixels at a distance r from the center. For each edge found between the expected radius, the number of edges at that distance are determined. By the size of the square which covers the possible ball and the number of edge pixels, it is calculated the edges percentage. If the edges percentage is greater than 70, then the circularity of the candidate is verified. The position of the detect ball is then sent to the Real-time Database, together with the information of the white lines and the information about the obstacles to be used, afterward, by the high level process responsible for the behaviors of the robots. 5.3 Experimental results Fig. 20 presents an example of the Morphological Processing Sub-System. As can be observed, the balls in the Edges Image (Fig. 20b) have almost circular contours. Fig. 20c) shows the resulting image after applying the circular Hough transform. Notice that the center of the balls present a very high peak when compared to the rest of the image. The ball considered was the closest to the robot, due to the fact that it has the high peak in the image. To ensure good results in the RoboCup competition, the system was tested with the algorithms described above. For that purpose, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. The results in this test may be affected by the errors in the localization algorithm and by the robot bumps while moving. These external errors are out of the scope of this study. The robot path in the field may be seen in Fig. 21 , along with the measured ball position. It is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the accuracy of the proposed algorithms. We obtained a very high detection ratio (near 90%) and a false positive rate around 0%, which is a very significant result. With the proposed approach, the omnidirectional vision system can detect the ball within this precision until distances up to 4 meters. The average processing time of the proposed approach was approximately 16ms. It needs approximately 40MBytes of memory. The experimental results have been obtained using a camera resolution of 640×480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz. 6 Conclusions This paper presents the omnidirectional vision system developed for the CAMBADA MSL robotic soccer team, from the calibration to the object detection. We presented several algorithms for the calibration of the most important parameters of the vision system and we proposed efficient color-based algorithms for object detection. Moreover, we proposed a solution for the detection of arbitrary FIFA balls, one of the current challenges in the MSL. The CAMBADA team won the last three editions of the Portuguese Robotics Festival, ranked 5th in RoboCup 2007, won the RoboCup 2008 and ranked 3rd in RoboCup 2009, demonstrating the effectiveness of our vision algorithms in a competition environment. As far as we know, no previous work has been published describing all the steps of the design of an omnidirectional vision system. Moreover, some of the algorithms presented in this paper are state-of-the-art, as demonstrated by the first place obtained in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. We are currently working in the automatic calibration of the inverse distance mapping and in efficient algorithms for autonomous color calibration, based on region growing. Regarding the object detection algorithms, as we have reduced the processing time to a few milliseconds, we are working on the acquisition of higher resolution images, capturing only a region of interest. The idea of work with higher image resolutions is to improve the object detection at higher distances. Moreover, we continue the development of algorithms for shape-based object detection, also to incorporate as a validation of the color-based algorithms. Acknowledgment This work was supported in part by the FCT (Fundação para a Ciência e a Tecnologia). References [1] Neves A, Azevedo J, Cunha NLB, Silva J, Santos F, Corrente G, et al. CAMBADA soccer team: from robot architecture to multiagent coordination. In: Vladan Papic editor. Robot soccer. Vienna, Austria: I-Tech Education and Publishing; 2010 [chapter 2]. [2] Zivkovic Z, Booij O. How did we built our hyperbolic mirror omni-directional camera-practical issues and basic geometry. Tech. rep. Intelligent Systems Laboratory, University of Amsterdam; 2006. [3] Wolf J. Omnidirectional vision system for mobile robot localization in the robocup environment. Master’s thesis. Graz University of Technology; 2003. [4] Menegatti E, Nori F, Pagello E, Pellizzari C, Spagnoli D. Designing an omnidirectional vision system for a goalkeeper robot. In: Proc of RoboCup 2001. Lecture notes in computer science, vol. 2377. Springer; 2001. p. 78–87. [5] Menegatti E, Pretto A, Pagello E. Testing omnidirectional vision-based monte carlo localization under occlusion. In: Proc of the IEEE intelligent robots and systems, IROS 2004; 2004. p. 2487–93. [6] P. Lima A. Bonarini C. Machado F. Marchese C. Marques F. Ribeiro Omni-directional catadioptric vision for soccer robots Robot Auton Syst 36 2–3 2001 87 102 [7] Liu F, Lu H, Zheng Z. A robust approach of field features extraction for robot soccer. In: Proc of the 4th IEEE Latin America robotic symposium, Monterry, Mexico; 2007. [8] Lu H, Zheng Z, Liu F, Wang X. A robust object recognition method for soccer robots. In: Proc of the 7th world congress on intelligent control and automation, Chongqing, China; 2008. [9] Voigtlrande A, Lange S, Lauer M, Riedmiller M. Real-time 3D ball recognition using perspective and catadioptric cameras. In: Proc of the 3rd European conference on mobile robots, Freiburg, Germany; 2007. [10] Mayer G, Utz H, Kraetzschmar G. Playing robot soccer under natural light: a case study. In: Proc of the RoboCup 2003. Lecture notes on artificial intelligence, vol. 3020. Springer; 2003. [11] Krawczyk G, Goesele M, Seidel H. Photometric calibration of high dynamic range cameras. Research Report MPI-I-2005-4-005. Max-Planck-Institut für Informatik, Stuhlsatzenhausweg 85, 66123 Saarbrücken, Germany; April 2005. [12] Shirvaikar MV. An optimal measure for camera focus and exposure. In: Proc of the IEEE southeastern symposium on system theory, Atlanta (USA); 2004. [13] Nourani-Vatani N, Roberts J. Automatic camera exposure control. In: Proc of the 2007 Australasian conference on robotics and automation, Brisbane, Australia; 2007. [14] K. Åström T. Hågglund PID controllers: theory, design, and tuning 2nd ed. 1995 Instruments Society of America [15] Neves AJR, Cunha AJPB, Pinheiro I. Autonomous configuration of parameters in robotic digital cameras. In: Proc of the 4th Iberian conference on pattern recognition and image analysis, IbPRIA-2009. Lecture notes in computer science, vol. 5524. Póvoa do Varzim, Portugal: Springer; 2009. p. 80–7. [16] S. Baker S.K. Nayar A theory of single-viewpoint catadioptric image formation Int J Comput Vis 2 1999 175 196 [17] Cunha B, Azevedo JL, Lau N, Almeida L. Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system. In: Proc of the RoboCup 2007. Lecture notes in computer science, vol. 5001. Atlanta (USA): Springer; 2007. p. 417–24. [18] Neves AJR, Martins DA, Pinho AJ. A hybrid vision system for soccer robots using radial search lines. In: Proc of the 8th conference on autonomous robot systems and competitions. Portuguese robotics open – ROBOTICA’2008, Aveiro, Portugal; 2008. p. 51–5. [19] Neves AJR. Corrente G, Pinho AJ. An omnidirectional vision system for soccer robots. In: Proc of the 2nd international workshop on intelligent robotics, IROBOT 2007. Lecture notes in artificial intelligence, vol. 4874. Springer; 2007. p. 499–507. [20] J.E. Bresenham Algorithm for computer control of a digital plotter IBM Syst J 4 1 1965 25 30 [21] Merke A, Welker S, Riedmiller M. Line base robot localisation under natural light conditions. In: Proc of the ECAI workshop on agents in dynamic and real-time environments, Valencia, Spain; 2002. [22] Silva J, Lau N, Rodrigues J, Azevedo JL, Neves AJR. Sensor and information fusion applied to a robotic soccer team. In: RoboCup 2009: robot soccer world cup XIII. Lecture notes in artificial intelligence. Springer; 2009. [23] Almeida L, Santos F, Facchinetti T, Pedreira P, Silva V, Lopes LS. Coordinating distributed autonomous agents with a real-time database: the CAMBADA project. In: Proc of the 19th international symposium on computer and information sciences, ISCIS 2004. Lecture notes in computer science, vol. 3280. Springer; 2004. p. 878–86. [24] Lau N, Lopes LS, Corrente G, Filipe N. Roles, positionings and set plays to coordinate a msl robot team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT’09. Lecture notes in computer science, vol. 5816. Aveiro, Portugal: Springer; 2009. p. 323–37. [25] Lau N, Lopes LS, Corrente G, Filipe N. Multi-robot team coordination through roles, positioning and coordinated procedures. In: Proc of the IEEE/RSJ international conference on intelligent robots and systems, MO, USA: St. Louis; 2009. p. 5841–48. [26] Silva J, Lau N, Neves AJR, Rodrigues J, Azevedo JL. Obstacle detection, identification and sharing on a robotic soccer team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT’09. Lecture notes in computer science, LNAI 5816. Aveiro, Portugal: Springer; 2009. p. 350–60. [27] Mitri S, Frintrop S, Pervolz K, Surmann H, Nuchter A. Robust object detection at regions of interest with an application in ball recognition. In: Proc of the 2005 IEEE international conference on robotics and automation, ICRA 2005, Barcelona, Spain; 2005. p. 125–30. [28] Jonker P, Caarls J, Bokhove W. Fast and accurate robot vision for vision based motion. In: RoboCup 2000: robot soccer world cup IV. Lecture notes in computer science. Springer; 2000. p. 149–58. [29] Hanek R, Schmitt T, Buck S. Fast image-based object localization in natural scenes. In: Proc of the 2002 IEEE/RSJ international conference on intelligent robotics and systems, Lausanne, Switzerland; 2002. p. 116–22. [30] A. Treptow A. Zell Real-time object tracking for soccer-robots without color information Robot Auton Syst 48 1 2004 41 48 [31] Mitri S, Pervolz K, Surmann H, Nuchter A. Fast color independent ball detection for mobile robots. In: Proc of the 2004 IEEE international conference on mechatronics and robotics, Aechen, Germany; 2004. p. 900–5. [32] Coath G, Musumeci P. Adaptive arc fitting for ball detection in RoboCup. In: Proc of the APRS workshop on digital image computing, WDIC 2003, Brisbane, Australia; 2003. p. 63–8. [33] Lu H, Zhang H, Zheng Z. Arbitrary ball recognition based on omni-directional vision for soccer robots. In: Proc of RoboCup 2008; 2008. [34] Nixon M, Aguado A. Feature extraction and image processing. 1st ed. Linacre House, Jordan Hill, Oxford OX2 8DP 225 Wildwood Avenue, Woburn, MA 01801-2041: Reed Educational and Professional Publishing Ltd.; 2002. [35] Ser PK, Siu WC. Invariant hough transform with matching technique for the recognition of non-analytic objects. In: IEEE international conference on acoustics, speech, and signal processing, ICASSP 1993, vol. 5; 1993. p. 9–12. [36] Zhang YJ, Liu ZQ. Curve detection using a new clustering approach in the hough space. In: IEEE international conference on systems, man, and cybernetics, 2000, vol. 4; 2000. p. 2746–51. [37] W.E.L. Grimson D.P. Huttenlocher On the sensitivity of the hough transform for object recognition IEEE Trans Pattern Anal Mach Intell 12 1990 1255 1274 [38] Zou J, Li H, Liu B, Zhang R. Color edge detection based on morphology. In: First international conference on communications and electronics, ICCE 2006; 2006. p. 291–3. [39] Zin TT, Takahashi H, Hama H. Robust person detection using far infrared camera for image fusion. In: Second international conference on innovative computing, information and control, ICICIC 2007; 2007. p. 310. [40] Zou Y, Dunsmuir W. Edge detection using generalized root signals of 2-d median filtering. In: Proc of the international conference on image processing, 1997, vol. 1; 1997. p. 417–9. [41] Blaffert T, Dippel S, Stahl M, Wiemker R. The laplace integral for a watershed segmentation. In: Proc of the international conference on image processing, 2000, vol. 3; 2000. p. 444–7. [42] Canny JF. A computational approach to edge detection. IEEE Trans Pattern Anal Mach Intell 8 (6). "
    },
    {
        "doc_title": "Vital-Jacket®: A wearable wireless vital signs monitor for patients' mobility in cardiology and sports",
        "doc_scopus_id": "77954389671",
        "doc_doi": "10.4108/ICST.PERVASIVEHEALTH2010.8991",
        "doc_eid": "2-s2.0-77954389671",
        "doc_date": "2010-07-14",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "European markets",
            "ISO 9001",
            "Manufacturing process",
            "MDD 42/93/CE certified",
            "Medical Devices",
            "Monitoring system",
            "Start-up companies",
            "T-shirts",
            "Vital sign",
            "Wearable electronics"
        ],
        "doc_abstract": "The Vital Jacket® (VJ) is a wearable vital signs monitoring system that joins textiles with microelectronics. After several years of development within the university lab, it has been licensed to a start-up company. Its evolutions have focused on cardiology and sports and scaled down from a jacket to a single T-shirt. The VJ manufacturing process has recently been certified to comply with the standards ISO9001 and ISO13485 and the cardiology version was approved as a Medical Device for the European market compliant with the MDD directive 42/93/CE, holding the CE1011 mark. The authors intend to wear VJs during the days of the congress to demonstrate its usefulness in first hand and will exemplify the different scenarios of use of this innovative wearable intelligent garment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Communicating among robots in the RoboCup Middle-Size League",
        "doc_scopus_id": "77951004199",
        "doc_doi": "10.1007/978-3-642-11876-0_28",
        "doc_eid": "2-s2.0-77951004199",
        "doc_date": "2010-04-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Best practice",
            "Communication protocols",
            "Fundamental component",
            "Limited bandwidth",
            "Mobile autonomous robots",
            "Multirobots",
            "RoboCup",
            "Robotic soccer",
            "Wireless communication protocols",
            "Wireless communications",
            "Wireless medium"
        ],
        "doc_abstract": "The RoboCup Middle-Size League robotic soccer competitions pose a real cooperation problem for teams of mobile autonomous robots. In the current state-of-practice cooperation is essential to overcome the opponent team and thus a wireless communication protocol and associated middleware are now fundamental components in the multi-robots system architecture. Nevertheless, the wireless communication has relatively low reliability and limited bandwidth. Since it is shared by both teams, it is a fundamental resource that must be used parsimoniously. Curiously, to the best of our knowledge, no previous study on the effective use of the wireless medium in actual game situations was done. In this paper we show how current teams use the wireless medium and we propose a set of best practices towards a more efficient utilization. Then, we present a communication protocol and middleware that follow such best practices and have been successfully used by one particular MSL team in the past four years. © 2010 Springer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Predictive control for behavior generation of omni-directional robots",
        "doc_scopus_id": "71049152967",
        "doc_doi": "10.1007/978-3-642-04686-5_23",
        "doc_eid": "2-s2.0-71049152967",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Behavior generation",
            "Continuous control",
            "Current peak",
            "Cycle time",
            "Discrete-time control",
            "Maximum acceleration",
            "Maximum velocity",
            "Medium size leagues",
            "Motion stability",
            "Omni-directional motion",
            "Omnidirectional robots",
            "Physical constraints",
            "Predictive control",
            "RoboCup",
            "Robot controls",
            "Robotic soccer team",
            "System delay"
        ],
        "doc_abstract": "This paper describes the approach developed by the CAMBADA robotic soccer team to address physical constraints regarding omni-directional motion control, with special focus on system delay. CAMBADA robots carry inherent delays which associated with discrete time control results in non-instant, non-continuous control degrading the performance over time. Besides a natural maximum velocity, CAMBADA robots have also a maximum acceleration limit implemented at software level to provide motion stability as well as current peaks avoidance on DC motors. Considering the previous constraints, such as the cycle time and the overall sensor-action delay, compensations can be made to improve the robot control. Since CAMBADA robots are among the slowest robots in the RoboCup Medium Size League, such compensations can help to improve both several behaviours as well as a better field coverage formation. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Autonomous configuration of parameters in robotic digital cameras",
        "doc_scopus_id": "68749090043",
        "doc_doi": "10.1007/978-3-642-02172-5_12",
        "doc_eid": "2-s2.0-68749090043",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous configuration",
            "Camera parameter",
            "Initial configuration",
            "Making decision",
            "Robotic applications",
            "Surrounding environment"
        ],
        "doc_abstract": "In the past few years, the use of digital cameras in robotic applications has been increasing significantly. The main areas of application of these robots are the industry and military, where these cameras are used as sensors that allow the robot to take the relevant information of the surrounding environment and making decisions. To extract information from the acquired image, such as shapes or colors, the configuration of the camera parameters, such as exposure, gain, brightness or white-balance, is very important. In this paper, we propose an algorithm for the autonomous setup of the most important parameters of digital cameras for robotic applications. The proposed algorithm uses the intensity histogram of the images and a black and a white area, known in advance, to estimate the parameters of the camera. We present experimental results that show the effectiveness of our algorithms. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system",
        "doc_scopus_id": "50249091617",
        "doc_doi": "10.1007/978-3-540-68847-1_43",
        "doc_eid": "2-s2.0-50249091617",
        "doc_date": "2008-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous mobile robots",
            "Back-propagation",
            "Catadioptric vision",
            "Distance maps",
            "Distributed architectures",
            "General solutions",
            "International symposium",
            "Mathematical properties",
            "Mirror surfaces",
            "Mobile robotics",
            "Non-SVP catadioptric",
            "Omnidirectional vision",
            "RoboCup",
            "Robot vision",
            "Robot-soccer",
            "Robotic vision",
            "Visualization",
            "World Cup"
        ],
        "doc_abstract": "The use of single viewpoint catadioptric vision systems is a common approach in mobile robotics, despite the constraints imposed by those systems. A general solution to calculate the robot centered distances map on non-SVP catadioptric setups, exploring a back-propagation ray-tracing approach and the mathematical properties of the mirror surface is discussed in this paper. Results from this technique applied in the robots of the CAMBADA team (Cooperative Autonomous Mobile Robots with Advanced Distributed Architecture) are presented, showing the effectiveness of the solution. © 2008 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hierarchical distributed architectures for autonomous mobile robots: A case study",
        "doc_scopus_id": "47849085902",
        "doc_doi": "10.1109/EFTA.2007.4416889",
        "doc_eid": "2-s2.0-47849085902",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Actuation systems",
            "Autonomous mobile robots",
            "Case studies",
            "Complex control",
            "Controller-area network",
            "Distributed architectures",
            "Distributed hardware",
            "Distributed sensing",
            "Dynamic environments",
            "Emerging technologies",
            "Global coordination",
            "Hardware architectures",
            "International conferences",
            "Micro-controller",
            "Mutual interference",
            "Portugal",
            "Soccer robots",
            "Vision sensing"
        ],
        "doc_abstract": "Robots are becoming commonplace in unstructured and dynamic environments, ranging from homes to offices, public sites, catastrophe sites, military scenarios. Achieving adequate performance in such circumstances requires complex control architectures, mixing adequately deliberative and reactive capabilities. This mixing needs to be properly addressed from both the software and hardware architectures point of view and, particularly, the mapping of the former onto the latter, in order to reduce mutual interference between concurrent behaviors and support the desired coordination with adequate level of reactivity. This paper discusses the benefits of using hierarchical distributed hardware architectures and presents the case study of the CAMBADA soccer robots developed at the University of Aveiro, Portugal. These robots use a distributed hardware architecture with a central computer to carry out vision sensing, global coordination and deliberative functions and a low-level distributed sensing and actuation system based on a set of simple microcontroller nodes interconnected with a Controller Area Network (CAN). © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "VICAID: Development and evaluation of a palmtop-based job aid for workers with severe developmental disabilities",
        "doc_scopus_id": "0035365167",
        "doc_doi": "10.1111/1467-8535.00198",
        "doc_eid": "2-s2.0-0035365167",
        "doc_date": "2001-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The development of technologies to assist people with severe and profound developmental disabilities to engage in constructive activity without constant support from carers may assist such persons to participate fully in vocational and other occupational activities. We describe the development, evaluation and use of VICAID, a system based on a radically simplified palmtop computer. The VICAID system enables a person with a developmental disability to access pictorial instructions designed to help him or her in the accurate completion of tasks. It also provides reminders to access the instructions, and/or alerts a job coach or supervisor should the worker's interaction with the system suggest that they are having difficulty with a task. Evaluative studies show that (1) the system is more effective than alternative supports (pictorial instructions presented in booklets) in maintaining accurate task performance, (2) that the system is preferred to such booklets by most users with severe disabilities, and (3) that it can be used in real work settings. Future development of the system will require attention to be paid to issues of training job-coaches and support workers in its setting-up and maintenance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Promoting independent task performance by persons with severe developmental disabilities through a new computer-aided system",
        "doc_scopus_id": "0033771782",
        "doc_doi": "10.1177/0145445500245005",
        "doc_eid": "2-s2.0-0033771782",
        "doc_date": "2000-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            },
            {
                "area_name": "Clinical Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3203"
            },
            {
                "area_name": "Arts and Humanities (miscellaneous)",
                "area_abbreviation": "ARTS",
                "area_code": "1201"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This study involved two experiments. In Experiment I, a computer-aided system for promoting task performance by 6 persons with severe developmental disabilities was compared with a card system. The computer-aided system was portable and presented pictorial task instructions (one instruction per step) and prompts. In Experiment 2, the same system was used, but the number of instruction occasions was reduced. In one condition, the system presented all the instructions used in Experiment 1 but mostly in clusters rather than individually. In another, the system presented part of the Experiment 1 instructions. Three Experiment 1 participants also served in Experiment 2. Experiment 1 results indicated all 6 participants had higher percentages of correct steps with the computer system and preferred it to the card system. Experiment 2 results indicated that the condition in which the instructions were clustered was more effective for maintaining correct task performance. Implications of the findings were discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of a computer-aided system providing pictorial task instructions and prompts to people with severe intellectual disability",
        "doc_scopus_id": "0033007839",
        "doc_doi": "10.1046/j.1365-2788.1999.43120165.x",
        "doc_eid": "2-s2.0-0033007839",
        "doc_date": "1999-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Rehabilitation",
                "area_abbreviation": "MEDI",
                "area_code": "2742"
            },
            {
                "area_name": "Arts and Humanities (miscellaneous)",
                "area_abbreviation": "ARTS",
                "area_code": "1201"
            },
            {
                "area_name": "Neurology",
                "area_abbreviation": "NEUR",
                "area_code": "2808"
            },
            {
                "area_name": "Neurology (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2728"
            },
            {
                "area_name": "Psychiatry and Mental Health",
                "area_abbreviation": "MEDI",
                "area_code": "2738"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The present study extended the evaluation of a computer-aided system providing pictorial instructions and prompts to promote task performance in people with severe intellectual disability. Four people were presented with two sets of tasks. The participants used the computer-aided system for one set and a card (control) system for the other. The results indicate that the computer-aided system was more effective than the card system with all participants. Three of the participants preferred the computer-aided system, while one favoured the card system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A palmtop-based job aid for workers with severe intellectual disabilities",
        "doc_scopus_id": "0033048649",
        "doc_doi": "10.3233/tad-1999-10106",
        "doc_eid": "2-s2.0-0033048649",
        "doc_date": "1999-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Rehabilitation",
                "area_abbreviation": "MEDI",
                "area_code": "2742"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Supported employment services have made open employment a meaningful option for persons with intellectual disabilities. To date, however, few people with more severe intellectual disabilities have benefited from these opportunities. Many such persons can, with systematic instruction, rapidly learn complex work tasks, but need long-term support to maintain an acceptable level of performance over time. We describe a new system, based on a 'palmtop' computer, to assist people with severe intellectual disabilities to perform complex work tasks. Using the system, a worker with severe intellectual disabilities uses a radically simplified palmtop computer to access a series of pictorial instructions guiding him/her through the steps in a task. For users who may become distracted from job tasks, devices similar to a commercial radio-pager remind the user to request the next instruction after a pre-set time has expired since the previous instruction. Should the user not respond to the prompt, the job coach or supervisor receives a similar alert. We present data from a series of six single case experiments in which the system was evaluated in use by workers with severe intellectual disabilities in real work settings. The results show that the system supports higher levels of work accuracy and pace than simpler support systems such as booklets of picture instructions, and that the prompting capacity of the system is useful to workers who may become distracted from job tasks. In addition, the capacity of the system to deliver instructions in clusters tailored to the needs of the individual user increases the utility of the system for persons with varying support needs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SIGIF: A digital signal interchange format with application in neurophysiology",
        "doc_scopus_id": "0030896155",
        "doc_doi": "10.1109/10.568917",
        "doc_eid": "2-s2.0-0030896155",
        "doc_date": "1997-05-12",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In this paper, the authors describe a biomedical digital-signal interchange format. The format supports both raw and processed data, multiple segments, several signal structures and representations, and an open architecture. Its versatility and adaptability allows the software to take advantage of any particular features of the acquisition hardware. The format has been used and improved in routine work during a five-year period involving the cooperation between two hospitals and one engineering research center. In order to support the format, an object oriented C language library has been developed and is also shortly described.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Instrument for anti-aliased display of biological signals",
        "doc_scopus_id": "0029428446",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0029428446",
        "doc_date": "1995-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "An instrument graphical reconstruction of biological signal is presented. A high quality anti-aliasing techniques is used, together with features for optimum reproduction of the traditional EEG Paper Tracing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SIGIF: a digital signal interchange format for biological signals",
        "doc_scopus_id": "0027876085",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027876085",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Biological signals",
            "Digital signal interchange format",
            "Multiple epochs"
        ],
        "doc_abstract": "A biomedical digital signal format is presented. The format supports both raw and processed data, multiple epochs, several signal structures and representations and an open architecture. In order to support the format, an object oriented C language library is also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integration of multimedia information in a clinical neurophysiology department",
        "doc_scopus_id": "0027875248",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027875248",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Biological signal integration",
            "Multimedia information"
        ],
        "doc_abstract": "The integration in a computerized system of multimedia information used in a neurophysiology department is presented. Special attention is given to biological signal integration in the system due to its difficulty and absence of standard interface with acquisition equipment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inexpensive instrument for real time synchronization of image and digital signal data",
        "doc_scopus_id": "0027843276",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027843276",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Digital signal data",
            "Real time synchronization"
        ],
        "doc_abstract": "An inexpensive instrument for time synchronization of digitally recorded signal and analogue full motion video is presented. The instrument communicates with a remote Host computer through a serial line using a simple protocol.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An instrument for time synchronisation of data stored by unrelated equipment, in the clinical neurophysiology environment",
        "doc_scopus_id": "85067032653",
        "doc_doi": "10.1109/IEMBS.1992.5761388",
        "doc_eid": "2-s2.0-85067032653",
        "doc_date": "1992-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cognitive tests",
            "Eeg machines",
            "Electro-encephalogram (EEG)",
            "Multi channel",
            "Multimedia information"
        ],
        "doc_abstract": "© 1992 IEEE.Today neurophysiology laboratories take advantage from different, general purpose equipment, to store multimedia information pertinent to the ongoing exams [1]. Along with patient's Electroencephalogram (EEG) written down on paper by means of multi-channel polygraphs generically known as EEG machines, it is also common to record the image of the patient throughout the exam, using commercial cameras and video tape recorders (VTR's). Computerised equipment, namely personal computers, are also getting more important every day within these environments; time or frequency domain analysis of the EEG, or support for cognitive tests, are currently performed using these machines.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of a statistical lossless compression algorithm for the Electroencephalogram",
        "doc_scopus_id": "0345451301",
        "doc_doi": "10.1109/IEMBS.1992.5761654",
        "doc_eid": "2-s2.0-0345451301",
        "doc_date": "1992-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    }
]