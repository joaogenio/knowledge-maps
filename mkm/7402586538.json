[
    {
        "doc_title": "Microbiome: A forgotten target of environmental micro(nano)plastics?",
        "doc_scopus_id": "85124529562",
        "doc_doi": "10.1016/j.scitotenv.2022.153628",
        "doc_eid": "2-s2.0-85124529562",
        "doc_date": "2022-05-20",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Environmental Engineering",
                "area_abbreviation": "ENVI",
                "area_code": "2305"
            },
            {
                "area_name": "Environmental Chemistry",
                "area_abbreviation": "ENVI",
                "area_code": "2304"
            },
            {
                "area_name": "Waste Management and Disposal",
                "area_abbreviation": "ENVI",
                "area_code": "2311"
            },
            {
                "area_name": "Pollution",
                "area_abbreviation": "ENVI",
                "area_code": "2310"
            }
        ],
        "doc_keywords": [
            "Dysbiosis",
            "Ecosystem",
            "Human health",
            "Immune response",
            "In-vitro",
            "Microbiome",
            "Microbiotas",
            "Microplastics",
            "Nanoplastics",
            "Pollution, human health",
            "Animals",
            "Mammals",
            "Microbiota",
            "Microplastics",
            "Plastics",
            "Soil",
            "Water Pollutants, Chemical"
        ],
        "doc_abstract": "© 2022 Elsevier B.V.Microplastics (MPs) and nanoplastics (NPs) are emerging pollutants in different environmental compartments (air, soil and water) and that may induce several ecotoxicological effects on organisms and their microbiota. A considerable number of studies has been addressing and highlighting the effects of MPs/NPs on biochemical, molecular and behavior effects of aquatic organisms. However, less attention has been focused on microbiota. Here, a critical overview of published studies focusing on microorganisms affected by MPs and NPs after in vitro or in vivo exposure is provided. Available studies regarding the properties of MPs/NPs, microbial phyla, experimental conditions, techniques employed, and effects are summarized. The link between microbiota disruption and other effects on other hosts (e.g., crustaceans, fish, and mammals) as also analyzed. Overall, the literature review shows that most studies with microorganisms were performed in vitro (MPs: 44.11%; NPs: 23.52%) in comparison with in vivo tests (MPs: 32.35%; NPs: 11.76%). The most studied MP/NPs were polystyrene particles, generally spheres, with sizes <50 μm and concentrations ranged between 100 and 1000 mg L−1. The most studied main phyla were Proteobacteria, Bacteroidetes, Firmicutes, and Actinobacteria. MPs/NPs induced microbiome composition disruption, immune response (i.e., immune modulator release, immune cells activation and inflammatory response), enzyme activity changes (i.e., catalase, urease, dehydrogenase, alkaline phosphatase, and fluorescein diacetate hydrolase) and gene expression changes. The immune responses changes were related to microbiome disruption. Research gaps are highlighted and recommendations for future research indicated that microbiome is sensitive to MP/NPs and microbiome disruption can be a valuable tool to assess the risk of plastic particles to human and environmental health.",
        "available": true,
        "clean_text": "serial JL 271800 291210 291781 291792 31 Science of The Total Environment SCIENCETOTALENVIRONMENT 2022-02-03 2022-02-03 2022-02-10 2022-02-10 2022-06-03T00:52:05 S0048-9697(22)00720-3 S0048969722007203 10.1016/j.scitotenv.2022.153628 S300 S300.3 FULL-TEXT 2022-06-10T23:38:43.187341Z 0 0 20220520 2022 2022-02-03T16:27:23.805046Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure e-component body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst misctext primabst pubtype ref specialabst 0048-9697 00489697 true 822 822 C Volume 822 143 153628 153628 153628 20220520 20 May 2022 2022-05-20 2022 article rev © 2022 Elsevier B.V. All rights reserved. MICROBIOMEAFORGOTTENTARGETENVIRONMENTALMICRONANOPLASTICS SANTOS A 1 Introduction 2 Methodological approach 3 Results and discussion 3.1 Historical analysis 3.2 Types of MPs and NPs 3.3 Techniques approach 3.4 Size of the MP/NPs 3.5 Shape of the MP/NPs 3.6 MP/NPs concentration and exposure length 3.7 Effects on microorganisms in in vitro tests 3.8 Effects on microorganisms in in vivo tests 3.9 Systemic effects after microbiota exposure to MP/NPs 3.10 Microbiota as an emerging biomarker for assessing the effects of MP/NPs 4 Conclusion and perspectives CRediT authorship contribution statement Acknowledgments References ANWAR 2020 1 20 H ARRIETA 2015 307 M AUGUSTE 2020 1 8 M AUTA 2018 15 21 H AVIO 2015 18 26 C BARON 2018 6506 6511 Y BERCIK 2011 599 U701 P BHATTACHARYA 2010 16556 16561 P BLASING 2018 422 435 M BOTTERELL 2019 98 110 Z BOUWMEESTER 2015 8932 8947 H BROWNE 2007 559 566 M BROWNE 2008 5026 5031 M BROWNE 2011 9175 9179 M CAPORASO 2010 1 7 J CARPENTER 1972 1240 1241 E CHAE 2017 624 632 Y CHAE 2019 2 8 Y CHEN 2004 10044 10048 H CHEN 2019 856 864 X COLE 2013 6646 6655 M CORMIER 2019 1 14 B COSTA 2019 173 184 P COZAR 2014 10239 10244 A ECKERT 2018 495 502 E EERKESMEDRANO 2015 63 82 D ENYOH 2020 1454 1461 C FENG 2018 949 961 X FENG 2019 3072 3079 L FRANK 2007 13780 13785 D FRANZELLITTI 2019 37 51 S FRINGER 2020 1 11 V GAMBARDELLA 2018 313 321 C GILBERT 2016 94 103 J GOODRICH 2014 250 262 J GRANBY 2018 430 443 K GRAY 2017 3074 3080 A GRENHAM 2011 S GRIGORAKIS 2018 10796 10802 S GU 2020 1 8 H HEINLAAN 2020 M HENAOMEJIA 2012 179 U67 J HIRT 2020 1 23 N HONDA 2016 75 84 K HUANG 2005 3157 3162 M HUANG 2019 Y HUERTALWANGA 2016 2685 2691 E IMRAN 2019 846 857 M JANDHYALA 2015 8836 8847 S JEONG 2016 8849 8857 C JIANG 2018 16 23 J JIN 2018 322 329 Y JIN 2019 308 317 Y JOHANSEN 2019 392 400 M JOVANOVIC 2017 510 515 B KLEINTEICH 2018 J KNIGHT 2018 R KURCHABA 2020 N LAMBERT 2016 265 268 S LEAR 2021 1 19 G LEI 2018 1 8 L LEVY 2017 219 232 M LEY 2006 1022 1023 R LI 2015 190 195 J LI 2019 37 46 J LIANG 2019 1 10 Y LIN 2020 D LIU 2019 836 846 Z LU 2018 449 458 L LU 2019 94 100 L LUSHER 2013 94 99 A MACHADO 2020 1 14 M MAK 2019 1 10 C MIAO 2019 8 10 L MIAO 2019 1 8 L NG 2018 1377 1388 E OBERBECKMANN 2020 209 232 S OBERBECKMANN 2017 S FATEIMPACTMICROPLASTICSINMARINEECOSYSTEMS HITCHHIKINGMICROORGANISMSMICROPLASTICSINBALTICSEA PACO 2017 10 15 A PEDA 2016 251 256 C PROKIC 2019 37 46 M QIAO 2019 R QIAO 2019 246 253 R QIN 2020 R RAM 2020 1 12 B RAMADAN 2021 M RILLIG 2012 6453 6454 M RILLIG 2018 1128 1131 M RINNIELLA 2019 1 23 E ROCHMAN 2014 656 661 C RODRIGUEZSEIJO 2017 495 503 A ROUND 2009 313 323 J RUMMEL 2017 258 267 C DESA 2018 1029 1039 L SAYES 2007 163 180 C SCHNABL 2014 1513 1524 B SCHWABE 2013 800 812 R SEELEY 2020 1 10 M SEKIROV 2010 859 904 I SUN 2018 131 139 M SUN 2018 1378 1385 X SUSSARELLU 2016 2430 2435 R TAMBOLI 2004 1 4 C URSELL 2014 1470 1476 L VANCAUWENBERGHE 2013 495 499 L VIRSEK 2017 301 309 M WAN 2019 646 658 Z WANG 2020 1 8 J WATTS 2014 8823 8830 A WEISS 2017 2959 2977 G WEXLER 2017 A WIEDNER 2020 315 324 K WRIGHT 2017 6634 6647 S WRIGHT 2013 R1031 R1033 S WRIGHT 2013 483 492 S XIAO 2020 163 173 T YAN 2020 122795 W YAN 2020 Y YANG 2020 L YI 2020 0 3 M ZETTLER 2013 7137 7146 E ZHANG 2020 J ZHANG 2020 109852 Y ZHU 2018 302 310 D ZHU 2018 408 415 B SANTOSX2022X153628 SANTOSX2022X153628XA 2024-02-10T00:00:00.000Z 2024-02-10T00:00:00.000Z © 2022 Elsevier B.V. All rights reserved. 0 2022-06-07T05:09:53.777Z CAPES 306329/2020-4 88882.386366/2019-01 CAPES Coordenação de Aperfeiçoamento de Pessoal de Nível Superior National Council for Scientific and Technological Development 433553/2018-9 CNPq Conselho Nacional de Desenvolvimento Científico e Tecnológico MCTIC 28/2018 MCTIC Ministério da Ciência, Tecnologia, Inovações e Comunicações This work was supported by National Council for Scientific and Technological Development - CNPq (MCTIC/CNPq N° 28/2018 ; n° 433553/2018-9 ) and Coordination of Improvement of Higher-Level Personnel - CAPES (n° 88882.386366/2019-01 ). Rocha T.L. is granted with productivity scholarship from CNPq (proc. n. 306329/2020-4). item S0048-9697(22)00720-3 S0048969722007203 10.1016/j.scitotenv.2022.153628 271800 2022-06-03T00:34:46.061533Z 2022-05-20 true 4005801 MAIN 14 52290 849 656 IMAGE-WEB-PDF 1 ga1 true 32749 155 500 gr1 42962 270 669 gr2 49844 387 536 gr3 78192 264 802 gr4 75887 560 758 gr5 46277 358 536 gr6 339589 762 758 gr7 59646 339 625 ga1 true 6384 68 219 gr1 5028 88 219 gr2 7194 158 219 gr3 8331 72 219 gr4 7898 162 219 gr5 6735 146 219 gr6 15709 164 163 gr7 7108 119 219 ga1 true 278747 687 2213 gr1 328348 1195 2961 gr2 329863 1713 2372 gr3 780402 1171 3553 gr4 308128 1489 2014 gr5 405421 1585 2372 gr6 3047662 3374 3356 gr7 451095 1502 2766 mmc1 mmc1.docx docx 55177 APPLICATION STOTEN 153628 153628 S0048-9697(22)00720-3 10.1016/j.scitotenv.2022.153628 Elsevier B.V. Fig. 1 Methodological screening of articles concerning the effects of microplastics (MPs) and nanoplastics (NPs) on the microbiota. Fig. 1 Fig. 2 Progression of microplastics (MPs) and nanoplastics (NPs) studies over the years. Dotted line denotes the date of the first studies addressing the on the effects of MP/NPs on microorganisms. Fig. 2 Fig. 3 Timeline of publications with microplastics (MPs) and nanoplastics (NPs) studies affecting microorganisms. Fig. 3 Fig. 4 The frequency (%) of studies with microplastics (MPs) and nanoplastics (NPs) affecting microorganisms according to plastic composition (A), experimental condition (in vivo and in vitro tests) (B), techniques (C), and organisms (D). Legend: ribosomal intergenic spacer analysis (ARISA), average well color development (AWCD), water-stable aggregates (WSA), fluorescein diacetate (FDA), sherlock microbial identification system (SMIS), phospholipid fatty acids (PFLAs), growth-based viability (GBV), microbial growth inhibition assay (MGIA), microbial viability assay (MVA), kinetic acute bioluminescence inhibition assay (KABIA) and protozoan viability assay (PVA). Fig. 4 Fig. 5 The number of studies (%) with microplastics (MPs) and nanoplastics (NPs) and micro-organisms by Phylum level. Fig. 5 Fig. 6 The number of studies (%) with microplastics (MPs) and nanoplastics (NPs) and microorganisms related to the effects caused by different MP/NPs types at the phylum level. Fig. 6 Fig. 7 Insertion of the microbiota at the levels of biological organization. Fig. 7 Review Microbiome: A forgotten target of environmental micro(nano)plastics? Andressa Liberal Santos Methodology Data curation Formal analysis Writing – review & editing a Cândido Carvalho Rodrigues Data curation Writing – review & editing a Miguel Oliveira Data curation Writing – review & editing b Thiago Lopes Rocha Conceptualization Supervision Data curation Writing – review & editing a ⁎ a Laboratory of Environmental Biotechnology and Ecotoxicology, Institute of Tropical Pathology and Public Health, Federal University of Goiás, Goiânia, Goiás, Brazil Laboratory of Environmental Biotechnology and Ecotoxicology Institute of Tropical Pathology and Public Health Federal University of Goiás Goiânia Goiás Brazil Laboratory of Environmental Biotechnology and Ecotoxicology, Institute of Tropical Pathology and Public Health, Federal University of Goiás, Goiânia, Goiás, Brazil b Department of Biology & CESAM, University of Aveiro, 3810-193 Aveiro, Portugal Department of Biology & CESAM University of Aveiro Aveiro 3810-193 Portugal Department of Biology & CESAM, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author at: Universidade Federal de Goiás, Instituto de Patologia Tropical e Saúde Pública, Rua 235, Setor Universitário, Goiânia, Goiás CEP: 74605050, Brazil. Universidade Federal de Goiás Instituto de Patologia Tropical e Saúde Pública Rua 235, Setor Universitário Goiânia Goiás CEP: 74605050 Brazil Editor: Henner Hollert Microplastics (MPs) and nanoplastics (NPs) are emerging pollutants in different environmental compartments (air, soil and water) and that may induce several ecotoxicological effects on organisms and their microbiota. A considerable number of studies has been addressing and highlighting the effects of MPs/NPs on biochemical, molecular and behavior effects of aquatic organisms. However, less attention has been focused on microbiota. Here, a critical overview of published studies focusing on microorganisms affected by MPs and NPs after in vitro or in vivo exposure is provided. Available studies regarding the properties of MPs/NPs, microbial phyla, experimental conditions, techniques employed, and effects are summarized. The link between microbiota disruption and other effects on other hosts (e.g., crustaceans, fish, and mammals) as also analyzed. Overall, the literature review shows that most studies with microorganisms were performed in vitro (MPs: 44.11%; NPs: 23.52%) in comparison with in vivo tests (MPs: 32.35%; NPs: 11.76%). The most studied MP/NPs were polystyrene particles, generally spheres, with sizes <50 μm and concentrations ranged between 100 and 1000 mg L−1. The most studied main phyla were Proteobacteria, Bacteroidetes, Firmicutes, and Actinobacteria. MPs/NPs induced microbiome composition disruption, immune response (i.e., immune modulator release, immune cells activation and inflammatory response), enzyme activity changes (i.e., catalase, urease, dehydrogenase, alkaline phosphatase, and fluorescein diacetate hydrolase) and gene expression changes. The immune responses changes were related to microbiome disruption. Research gaps are highlighted and recommendations for future research indicated that microbiome is sensitive to MP/NPs and microbiome disruption can be a valuable tool to assess the risk of plastic particles to human and environmental health. Graphical abstract Unlabelled Image Keywords Dysbiosis Ecosystem Microbiota Microorganisms Pollution, human health 1 Introduction Plastics are widely used for their attractive properties such as durability, low cost, low thermal conductivity and malleability. Over the past 60 years, there has been a rapid increase in plastic production (Bouwmeester et al., 2015; Lei et al., 2018). The production rate has grown exponentially in recent decades leading to accumulation and dispersion across the planet (Lu et al., 2019). In the environment, physical, chemical and biological agents can lead to the fragmentation of plastics, originating microplastics (MP) (Cózar et al., 2014; Wright and Kelly, 2017) and nanoplastics (NP) (Lambert and Wagner, 2016; Wright and Kelly, 2017). Ng et al. (2018) defined MPs as plastic particles with a diameter between 100 nm and 5 mm, while NPs are considered those with sizes between 1 and 100 nm. In addition, MP/NPs can originate from manufactured products such as cosmetics, synthetic clothing, car tires (Browne et al., 2011; Paço et al., 2017; Lei et al., 2018), industrial production processes and personal care products (Jin et al., 2018). MPs enter aquatic environments mainly through domestic sewage discharge (Jiang, 2018) and can be found in different regions such as water column and sediments (Van Cauwenberghe et al., 2013; Eerkes-Medrano et al., 2015; Li et al., 2015; Paço et al., 2017; Wright and Kelly, 2017; Lei et al., 2018). A literature review by Chae and An (2017) revealed the presence of plastic particles on the surface of oceans, beach sands, deep waters and lakes in North America, Europe, Asia, Latin America, Africa and Oceania. However, the presence of MP/NPs is not limited to the aquatic environment and can also be found in aerial and terrestrial ecosystems (Rillig, 2012; Huerta Lwanga et al., 2016; Bläsing and Amelung, 2018). MP/NPs can be ingested by various aquatic organisms such as zooplankton, mussels, crabs, marine worms and fish (Browne et al., 2008; Cole et al., 2013; Lusher et al., 2013; Watts et al., 2014; Wright et al., 2013a; Qiao et al., 2019a). Due to their small size, they can accumulate in the digestive systems of these organisms and reach the circulatory system (Avio et al., 2015; Grigorakis and Drouillard, 2018; Jin et al., 2019). Studies have reported that MP/NP ingestion by fish can not only cause physical harm (Jovanović et al., 2017; Pedà et al., 2016), but also block the digestive tract, reduce growth rates, alter the enzyme antioxidant activities, induce oxidative stress, affect reproduction, immune function and lipid metabolism (Wright et al., 2013b; Jeong et al., 2016; Sussarellu et al., 2016; Rodriguez-Seijo et al., 2017; Jin et al., 2018). In addition, the ability of MP/NP to induce microbiota dysbiosis in the gut in organisms such as zebrafish and mice has also been reported (Jin et al., 2018; Lu et al., 2018; Qiao et al., 2019a). The interaction between MP/NPs and microorganisms has drawn the attention of researchers (Oberbeckmann et al., 2017; Rillig and Bonkowski, 2018; Lu et al., 2019; Lear et al., 2021). In the field of environmental biotechnology, the ability of bacteria and fungi to degrade plastic particles has been studied (Paço et al., 2017; Auta et al., 2018; Zhang et al., 2020a). The composition of microbial communities on the surface of MP/NPs particles was also addressed (Oberbeckmann and Labrenz, 2020), in what was called “plastisphere” (Zettler et al., 2013). The variation of bacterial communities in MP/NPs can also affect the buoyancy of plastics, influencing the dispersion of MP/NPs in the aquatic environment (Chen et al., 2019). The potential capacity of MP/NPs to transport antimicrobials to microbial ecosystems, generating changes in community composition or leading to the acquisition of antimicrobial resistance has also been the subject of research (Sun et al., 2018a; Lu et al., 2019; Imran et al., 2019; Ram and Kumar, 2020; Zhang et al., 2020b). All these studies demonstrate that there are several and important connections between these small particles and the microorganisms that should be better explored. Despite the available studies on the environmental distribution and ecotoxicological effects of MP/NPs (Chae and An, 2017; de Sá et al., 2018; Lu et al., 2019), knowledge about their effects on the microbiome remains limited. Thus, the present study aimed to: (i) critically review the information available in the literature in terms of microbiome disruption, in animals and in the environment, after exposure to MP/NPs; (ii) summarize and discuss current trends in laboratory research in relation to types of polymers MP/NPs and their characteristics, exposure conditions, microorganisms and techniques used for identification, analysis, as well as effects (on microorganisms or habitat or host); (iii) indicate research gaps and future perspectives. 2 Methodological approach A survey of the available published peer-reviewed literature was conducted on December 31st of 2020, using the Thompson Reuters database ISI Web of Science and ScienceDirect. A combination of keywords was used as criteria (“microplastic”, “nanoplastic”, “microorganisms”, “microbiota”, “microbiome”, “bacteria”, “archaea”, “fungi” and “protozoa” in any topic, title, or text words). The search yielded a total of 2540 publications. The abstracts of those candidate articles were read to screen for eligibility, leading to the selection of 108 articles, which were screened for eligibility based on being studies associated with microorganisms. A careful analysis of the full text of the articles revealed that 71 articles were eligible and 37 were excluded resulting in 34 studies that make up this review. Then, the following inclusion criteria were considered: experimental articles; laboratory studies; articles in which there was exposure with MP/NPs to analyze the effect of MP/NPs alone or MP/NPs in the presence of other contaminants. The exclusion criteria were: articles written in a language other than English; articles whose analysis was exclusively on the chemical composition of the MP/NPs; articles whose theme was biodegradation of MP/NPs by microorganisms; studies concerning the effects of the microbiome on environmental behavior and fate of MP/NPs; articles that did not specify the items covered in this review. Thus, a total of 34 studies selected for this critical review as shown in Fig. 1 . Available studies were described according to the following criteria: (i) host; (ii) microorganism; (iii) type of MP/NPs; (vi) shape and color; (v) size range; (vi) fluorescence; (vii) concentration; (viii) exposure period; (ix) source; (x) analytical techniques used; (xi) reported effects. The studies tested different polymers that were classified into polyethylene (PE) both high and low density, polystyrene (PS), polypropylene (PP), polyvinyl chloride (PVC) (Lei et al., 2018), polyamide (PA) (Wiedner and Polifka, 2020), polyacrylonitrile (PAN) (Liang et al., 2019), polyethersulfone (PES) (Qin et al., 2020), polyurethane (PU), Polylactic acid (PLA) (Seeley et al., 2020) and polybutylene terephthalate-hexane acid (PBAT) (Li et al., 2019). The shapes reported were classified into microspheres, nanospheres, particles, beads, fragments, and fibers (de Sá et al., 2018). The use of fluorescent MP/NP was defined as “yes” or “no”. The MP sizes were classified as <50 μm; 50–100 μm; 100–200 μm; 200–400 μm and > 1000 μm; or not related (NR). 3 Results and discussion 3.1 Historical analysis The approach of the toxicity of plastic particles (0.25–0.5 cm) was first discussed in 1972, reporting the potential impact of the presence of these plastic particles on Sea of Sargasso, bringing concerns about their increase on aquatic environments (Carpenter and Smith, 1972). NPs started being studied in 2005, observing the biodegradation, crystallization behavior, thermal stability and water adsorption by NPs (Huang et al., 2005). The discussion of the MP toxicity surfaced in 2007 (Fig. 2 ) with a learned discourse about their being emerging contaminants of potential concern, due to the poorly understood effects of these particles on the environment (Browne et al., 2007). The following year, the same group of researchers exposed the marine mussel Mytilus edulis to PE microspheres (2.0, 3.0 or 9.6 μm; 0 0.51 g L−1; 12 h). These tested MPs were able to accumulate in the intestine and be translocated to the circulatory system of this marine bivalve (Browne et al., 2008), confirming the potential risk of these plastic particles for aquatic fauna. On the other hand, the first study with a biological approach involving NPs was done in 2010 (Fig. 2) and investigated the physical adsorption of charged polystyrene NPs (20 nm; 0.08–0.8 mg mL−1; 2 h). These NPs inhibited the photosynthetic activity of the Chlorella and Scenedesmus algae, and induced the reactive oxygen species (ROS) production (Bhattacharya et al., 2010). Concerns about the harmful effects of MPs and NPs over microorganisms was carried out only in 2018 (Jin et al., 2018; Gambardella et al., 2018) (Fig. 3 ). These studies showed the induction of dysbiosis and intestinal inflammation in adult zebrafish (Danio rerio) after exposure to PS-MPs (0.5 and 50 μm) (Jin et al., 2018), and the sub-lethal effects, (i.e., growth inhibition) in the genus Vibrio (Phylum Proteobacteria) after in vitro exposure to 0.1 μm PS-NPs (Gambardella et al., 2018). Also, in vitro studies with bacterial communities (of unspecified phylum) exposed to 212–250 μm PE-MPs (Kleinteich et al., 2018) reported changes in their microbial communities. In the same year, studies were carried out to assess the effects of PS-MPs (0.5 and 50 μm) on intestinal microbiota of mammals (mice), reporting dysbiosis induction and hepatic lipid metabolism disorder (Lu et al., 2018). Afterwards, the growth inhibition, oxidative stress and membrane destruction were observed in Synechococcus elongatus after exposure to PS-NPs (50 nm) (Feng et al., 2019). 3.2 Types of MPs and NPs In this review, 61.76% of studies were done using only MPs, 26.47% using only NPs, and 11.76% using both MP/NPs. The most studied types of MPs and NPs to access their toxicity on microorganisms are PS (MPs: 38.23%; NPs: 29.41%) and PE (MPs: 26.47%; NPs: 5.88%) (Fig. 4A). Studies with MPs also analyzed the toxicity of PVC (11.76%), PP (5.88%), PES (2.94%), PAN (2.94%), PU (2.94%), PLA (2.94%), PA (2.94%), and PBAT (2.94%), in contrast with studies using NPs that only investigated three types of plastics: PS (29.41%), PE (5.88%), and PP (2.94%) (Fig. 4A). It becomes clear that the available information on the effects of MPs and NPs is mainly limited to a few polymers, which are the most used in human activities and consequently generate more environmental pollution. Thus, revised data showed that further ecotoxicological studies with different types of NPs are needed. 3.3 Techniques approach The toxic effects of MP/NPs were mainly analyzed using in vitro tests (MPs: 44.11%; NPs: 23.52%) (Fig. 4B). The in vitro toxicity of MP/NPs was mainly analyzed in microorganisms isolated from microbial cultures, while the in vivo studies analyzed microorganisms isolated in the intestine (MPs: 26.47%; NPs: 2.94%), throughout the organism (MPs: 5.88%; NPs: 2.94%), one article isolated the microorganisms from the feces of Litopenaeus vannamei (NPs: 2.94%) and one for microorganisms isolated from the hemolymph of Mytilus galloprovincialis (NPs: 2.94%). This demonstrates that studies with animal models are still scarce and deserve greater attention in future research. The different approaches found aimed to determine changes in the composition of the microbiota or in the function of specific microorganisms after exposures to MP/NPs. Furthermore, the tests used could be divided into culture-dependent and culture-independent analyses. Microbial culture-dependent techniques are approaches based on the growth and isolation of bacteria under controlled laboratory conditions. Such techniques have been a gold standard for identifying bacterial species for many years (Feng et al., 2018). Several methodologies were classified as culture-dependent were not limited to the microbial culture itself, but also included additional analyzes carried out later with the use of staining systems or detection equipment. Among the microbial culture-dependent analysis techniques explored in this review are the Microcounting (Gambardella et al., 2018), Flow cytometry (Chae et al., 2019), Pulse Amplitude Modulation (PAM) fluorometry (Miao et al., 2019b), Average Well Color Development (AWCD) (Miao et al., 2019a), Spectrophotometry (Feng et al., 2019), Fourier Transform Infrared spectroscopy (FTIR), Nutrient Auto Analyzer, Transmission Electron Microscopy (TEM) (Sun et al., 2018b), Electroactivity assay (Li et al., 2019), Water Stable Aggregates (WSA), Fluorescein Diacetate Hydrolysis (FDA) (Liang et al., 2019), Sherlock Microbial Identification System® (Lin et al., 2020), Phospholipid Fatty Acid Analysis (PLFA) (Wiedner and Polifka, 2020), Growth-based Viability assay (GBV) (Fringer et al., 2020), Microbial Growth Inhibition assay (MGIA), Microbial Viability assay (MVA), Kinetic Acute Bioluminescence Inhibition assay (KABIA) and Protozoan Viability assay (PVA) (Heinlaan et al., 2020) (Fig. 4C). Despite the fact that several microorganisms were not cultivable for a long time, culture methods in the field of phylogeny made a great advance, which today allows us to cultivate previously uncultivated organisms (Feng et al., 2018). In addition, several analyzes found were not limited to the phenotypic approach, allowing to investigate for example the possible interference of MP/NPs in functional and metabolic analyzes of microorganisms such as the production of chlorophyll-a in photosynthetic bacteria (Miao et al., 2019b) or observe MP/NPs-induced toxicity markers, such as altered cell size, granularity, and bacterial viability (Chae et al., 2019). The methodologies found were adequate for the proposed researches with MP/NPs; however, based on the types of approaches carried out, it is noted that the culture-dependent methods are usually indicated for analyzes whose target of the toxicity of the MP/NPs are specific microorganisms as in the studies carried out with Halomonas alkaliphila bacteria exposed to 50 and 55 nm PS-NPs (20, 40, 80, 160 and 320 mg L −1) and 1 μm PS-MPs (20, 40, 80, 160 and 320 mg L−1) for 2 h (Feng et al., 2019). The analysis of this study allowed the characterization of the main functional groups (proteins, polysaccharides and nucleic acids) and extracellular polymeric substances after exposure. Another example was the study whose target was the S. elongatus bacterium that was exposed to 50 nm PS-NH2-NPs (2 9 μg mL−1) for 48 h, in which the main effect found was the inhibition of bacterial growth. Studies in which the target microorganism in the samples was unknown demonstrated that they required molecular investigations, with the exception of the study performed with the Sherlock™ Microbial Identification System (SMIS). This technique was used to analyze changes in the microbial community and the effects of bioaccumulation through food webs in soil exposed for 287 days to 37.13 μm PE-MPs (5 15 g m−2) (Lin et al., 2020). Most culture-independent techniques for microbiota analysis are based on 16S rRNA gene analysis (Feng et al., 2018). Studies of the 16S rRNA gene provide insight into which microbial taxa are present in a given sample because it is an excellent bacterial phylogenetic marker (Caporaso et al., 2010). Method microbiome analysis data and standards are advancing rapidly (Knight et al., 2018), such as first, second and third generation genomic sequencing techniques (Xiao and Zhou, 2020). These approaches are important, as most microorganisms are not cultivable (Feng et al., 2018). In the toxicity studies of MP/NPs on microbiomes, the Next Generation Sequencing (NGS) stands out, which belongs to the second generation, with about 55% among the independent studies of microbial culture (Lu et al., 2018, Zhu et al., 2018a, Zhu et al., 2018b, Jin et al., 2018, Liu et al., 2019, Qiao et al., 2019a, Qiao et al., 2019b, Wan et al., 2019, Huang et al., 2019, Jin et al., 2019, Yang et al., 2020, Auguste et al., 2020, Kurchaba et al., 2020, Yan et al., 2020a, Yan et al., 2020b, Gu et al., 2020, Yi et al., 2020, Seeley et al., 2020, Wang et al., 2020), with MiSeq (Illumina, USA) being the main platform used. Differently from the culture-dependent techniques, the genomic sequencing approach proved to be very promising and adequate in the evaluation of phylogenetic alteration in face of exposure to MP/NPs, as in all the studies that addressed it, it was possible to detect alterations in the composition of microbiomes. However, it should be noted that this technique generates a large amount of data, and environmental factors and complementary methods of analysis can affect the results (Feng et al., 2018). Therefore, it is important to combine bioinformatics analysis for data refinement and interpretation. Furthermore, other culture-independent techniques allowed complementary analyzes such as RT-PCR for gene expression (Machado et al., 2020). 3.4 Size of the MP/NPs Studies were conducted with different sized MPs (1–3140 μm) and NPs (26–9900 nm). The sizes found in greater quantities were < 50 μm (73.52%), while 8.82% used MPs with 50–100 μm, 14.7% with 200–400 μm, and 8.82% used MPs > 1000 μm. Most of in vivo studies performed analyzes with particles “< 50 μm”, and only one study realized analysis of MPs with 80 and 250 μm. The preference for analyzes with smaller particles can be explained by the fact that the smaller MPs (~ 3.0 mm) have a greater ability to translocate within the body of organisms, reaching other regions of the body. Browne et al. (2008) reported that smaller MPs (< 3.0 μm; 0.51 g L−1) translocated more easily than larger MPs (> 3.0 or 9.6 μm; 0.51 g L−1) in the marine mussel Mytilus edulis after 3 days of exposure. Furthermore, small plastic particles are easier to be absorbed by biological processes due to the large surface/volume ratio (Prokić et al., 2019). The MP sizes can be directly linked with their biological effects, such as disturbances in energy metabolism, oxidative balance, antioxidative capacity, and DNA damage (Prokić et al., 2019; Zhang et al., 2020b). Besides, Botterell et al. (2019) observed that the MP size is one of the factors related to their bioavailability to zooplanktons. The NPs are more likely to induce toxicological effects in cells than MPs since their smaller size allows them to permeate lipid membranes and even biological barriers (Costa et al., 2019). 3.5 Shape of the MP/NPs The literature review showed that the shape of MP/NPs as factors that can determine their toxicity. Most studies were carried out with microspheres or nanospheres (29.41%). The term “particles” without the morphology description was often used (26.47%), followed by beads (23.52%), fragments (8.82%), fibers (8.82%), granule (2.94%), membrane (2.94%), and pieces (2.94%). In 5.88% of the studies, more than one MP/NPs format was used, and in only two studies (5.88%), the format of the MP/NPs used was not specified. The MP uptake changes according to their morphology. For example, Gray and Weinstein (2017) assessed the uptake of acute exposure of Palaemonetes pugio to three different shapes of PE and PS-MPs (spheres, fragments and fibers), with sizes ranging from 30 to 165 μm, demonstrating that ingestion of fragments was greater than spheres and fibers, and that of spheres was greater than fiber. 3.6 MP/NPs concentration and exposure length The magnitude of the toxicity of pollutants, such as MPs and NPs, directly depends of the concentration and time of exposure (Chen et al., 2004; Sayes et al., 2007). The MP/NPs concentrations greatly varies in the revised studies, ranging from 100 μg L−1 to 1000 mg L−1 (Table S1). The exposure time in tests with microorganisms also varies in the revised studies (Table S1). The studies were mainly conducted between 4 and 12 h (11.76%), 24 and 48 h (11.76%), between 3 and 7 days (26.47%), 10 to 21 days (35.29%), 30 to 90 days (23.52%), and only one study with 287 days. The animal model Danio rerio (zebrafish) was studied with exposure time of 7 and 14 days, while mice were studied for 5 and 6 weeks. This difference may be related to the development time of each organism. It is thus clear that longer exposure periods should be tested as they represent a more environmentally relevant approach. Taking into account that, for other contaminants, the some of the effects are observed only after generational exposures, this should also be considered for plastics. The magnitude of the toxicity of pollutants, such as MPs and NPs, directly depends of the concentration and time of exposure (Chen et al., 2004; Sayes et al., 2007). The MP/NPs concentrations greatly varies in the revised studies, ranging from 100 μg L−1 to 1000 mg L−1 (Table S1). The exposure time in tests with microorganisms also varies in the revised studies (Table S1). The studies were mainly conducted between 4 and 12 h (11.76%), 24 and 48 h (11.76%), between 3 and 7 days (26.47%), 10 to 21 days (35.29%), 30 to 90 days (23.52%), and only one study with 287 days. The animal model Danio rerio (zebrafish) was studied with exposure time of 7 and 14 days, while mice were studied for 5 and 6 weeks. This difference may be related to the development time of each organism. It is thus clear that longer exposure periods should be tested as they represent a more environmentally relevant approach. Taking into account that, for other contaminants, the some of the effects are observed only after generational exposures, this should also be considered for plastics. 3.7 Effects on microorganisms in in vitro tests Part of the analyzed studies involving the toxicity of MP/NPs on microorganisms was carried out in vitro using pre-existing microbial inoculum or microorganisms isolated from the environment. In this analysis, the taxon of microorganisms found in the studies were bacteria (73.91%), fungi (13.04%), protozoa (8.69%) and unspecified (4.34%). No virus studies were found. These proportions are relatively close to the global distribution of biomass of taxons (Bar-On et al., 2018), which indicates that bacteria have been more studied as a target of MP/NPs toxicity as they are more commonly found. Studies involving viruses and MP/NPs were found only in approaches in which small plastic particles carried viruses (Enyoh et al., 2020). Nonetheless, among the total number of microorganisms studied, the phyla that appeared in the greatest number of studies are all bacterial: Proteobacteria (70.58%), Firmicutes (58.82%) and Bacteroidetes (58.82%) (Fig. 5 ). The number of studies using in vitro tests to analyze toxic effects caused by MP/NPs on microorganisms was greater than in vivo tests (67.64% versus 32.35%, respectively) (Fig. 4B). Studies that performed in vitro tests observed changes in the composition of the microbial community (Qin et al., 2020), alteration of the microbial metabolic functional diversity (Miao et al., 2019a), oxidative stress increase (Kurchaba et al., 2020), or changes in the state of aggregation (Liang et al., 2019). The increase in microbial diversity and wealth was demonstrated in aerobic granular sludge after 12 h exposure to 0.25 mm PES-MPs (0.1–0.5 g L−1) (Qin et al., 2020). Interestingly, as it addresses wastewater treatment systems by activated sludge, the greater the microbial diversity in the sludge, the better the treatment yield. In this study, the exposure to PES-MPs increased the Firmicutes abundance, having a microbial transformation effect on ammonia nitrogen, producing endospores to resist these extreme environmental threats. Similarly, the microbial community present in soils exposed to 2 mm, 3 mm and 800 nm PE and PP-MPs (2 and 20 mg g−1) for 24 h increased their diversity. The PP and PE-MPs altered the microbial community structure, leading to the enrichment of Acidobacteria and Bacteroidetes, and depletion of Deinococcus-Thermus and Chloroflexi (Yi et al., 2020). Analogously, the 15–90 days exposure of the soil microbiota to 0.01 mm PE-MPs (2000 fragments kg−1) increased Acidobacterias, Bacteroidetes, and Proteobacteria (Huang et al., 2019). A 14 days exposure to 212–250 μm exposure PE-MPs (2 and 20 mg g−1) has been reported to affect the bacterial community composition in unpolluted freshwater sediments. Despite these effects, disruption of microecology is not a desired effect. In addition, the results indicated that the MPs can serve as a vehicle for hydrophobic pollutants, but the bioavailability of these pollutants is reduced due to adsorption, suggesting that MPs can decrease the effects of polycyclic aromatic hydrocarbons (PHAs) on microorganisms (Kleinteich et al., 2018); however, the toxic potentials of MP/NPs for susceptible microorganisms are still present. The marine bacterium H. alkaliphila (Proteobacteria) exposed for 2 h to 50 and 55 nm PS-NP (20–320 mg L−1) and to 1 μm PS-MP (20–320 mg L−1) showed that both particles inhibited bacterial growth at high concentrations, but only NPs inhibited bacterial growth at a lower concentration (80 mg L−1). Therefore, results suggest greater toxicity of PS-NPs than PS-MPs, and it must be linked to the facility of NPs in transposing membranes (Prokić et al., 2019). The toxicity occurs through the interruption of ecological functions of the bacterium H. alkaliphila (Sun et al., 2018b). The toxicity studies of MP/NPs were not restricted to planktonic microorganisms, approaches involving biofilms have also been found. The microbial functional diversity of freshwater biofilms was reduced after 24 h exposure to 100 nm PS-NPs (1, 5, and 10 mg L−1), indicating an inhibitory effect of NPs on the metabolic diversity of the biofilm. The total metabolic functions of carbon remained constant with high concentrations of NPs, but for some specific carbon sources, such as esters, there was a change in utilization capacity revealing toxicity dependent on the type of carbon (Miao et al., 2019a). Metabolic profiles and signaling pathways for freshwater cyanobacteria S. elongatus were explored in response to short-term exposure (48 h) to 50 nm amino-modified PS-NPs (2–9 μg mL−1). These NPs induced toxic effects to S. elongatus, leading to growth inhibition, oxidative stress and membrane destruction (Feng et al., 2019); therefore the toxicity proved to be independent of the life form of the microorganisms (free living or in community). The impacts of 100 nm and 500 nm PS-NPs (5–100 mg L−1) and 1 μm and 9 μm PS-MPs (5–100 mg L−1) exposure on freshwater biofilms for 3 h indicated that the NPs at high concentration (100 mg L−1) decreased chlorophyll A and enzymatic activity (β-glucosidase and leucine aminopeptidase), suggesting negative effects on carbon and nitrogen cycling, while larger particle sizes exhibited negligible effects. Thus, it was possible to verify the toxicity of PS-NPs in primary production and nutrient cycling in aquatic ecosystems (Miao et al., 2019b), which can cause an imbalance in the microecology and even affect higher trophic levels. Toxicity analyzes by MP/NPs in specific genera, with specific functions, were also explored. The influence of 4 mm PBAT and PVC-MPs (5% w/w) on the activity of Geobacter metallireducens GS15 with ferrihydrite or ferric citrate as respective electron receptors were performed for 2 39 days. This proteobacteria is considered the main regulator of the biogeochemical iron cycle. The results suggested that PBAT and PVC-MPs can delay the electroactivity of G. metallireducens GS15 and that the coverage PVC-MPs by products containing iron was lower than that of PBAT-MPs. Therefore, the PVC-MPs show potential for inhibiting the process of iron cycling (Li et al., 2019). Another approach found was with taxon fungus. The effects on the fungal aggregation stage of the soil of phylum Ascomycota after exposure to 0.37 3.14 mm PAN-MPs (0.4% w/w) under different temperatures were investigated during 2 and 7 days (Liang et al., 2019). These fungi are considered essential to maintain the quality of the soil, as they guarantee the ideal state of soil aggregation. The percentage of stable aggregation in water (WSA) and hydrolysis of fluorescein diacetate (FDA) was evaluated as an indicator of fungal biomass. The addition of PAN MPs was the main factor that influenced the decrease in the WSA percentage and with the increase in temperature (25 to 28 °C), this percentage decreased even more. This study highlights the important role of abiotic factors like temperature on the potential effects of MP/NPs. The interactive effects of climate change and the presence of microfibers are important to assess the effects of global changes in aggregation and other soil processes (Liang et al., 2019). The effects on enzyme activities (catalase and urease) and bacterial community structure of soil amended with 0.01 mm PE MPs, and the colonization of MPs were evaluated after exposure for 15–90 days. The results showed that the amendment of soil with PE-MPs increased the activities of catalase and urease after 30 and 90 days, and altered the composition of the bacterial community of the soil. Data indicates that PE-MPs can induce toxic effects in the soils by altering its functional properties (Huang et al., 2019). Approaches involving the toxicity of MP/NPs for gene expression were also found. Synechococcus sp. PCC 7002 after 5- and 10-days exposure to 1–4 μm PE-MPs (0.00050 g mL−1) and 200–9900 nm PE-NPs (0.00050 g mL−1) showed alterations in their gene expression (esterase and hydrolase genes), increased their cell viability in exposure to PE-MPs and decreased their cell viability in exposure to PE-NPs. Also, the PE-MPs showed the appearance of biofilm morphology and extracellular polysaccharides (EPS) fibril formation in their surface (Machado et al., 2020). Changes in gene expression (nitrification and denitrification genes) were also found in microbial sediment communities after 7- and 16-days exposure to 53–300 μm PE, PVC, PU, and PLA-MPs (1.5 g microcosmo−1) (Seeley et al., 2020). A small decrease in Shewanella oneidensis viability was observed after 4-, 8-, and 12-h exposure to 160 nm PS-NPs (18.75–300 mg L−1) under aerobic conditions, while no significant effects were found under anaerobic conditions (Fringer et al., 2020). Despite a great diversity of effects generated by different types of MP/NPs in several different microorganisms, Heinlaan et al. (2020) performed an approach with organisms of different biological complexity in the same study: bacteria (Escherichia coli MG1655, Staphylococcus aureus 6538 and Vibrio fischeri NRRL B-11177), yeast (Sacchamoryces cerevisiae) and protozoan (Tetrahymena thermophila BIII) exposed to 26 nm PS-NPs (0.0001 100 mg L−1) and 102 nm PS-NPs (0.01–100 mg L−1) with time exposure of 30 min for V. fischeri and 24 min for the others organisms. Results showed toxic effects (growth inhibition) only to V. fischeri NRRL B-11177 after exposure to PS-NPs at 21.6 ± 6.1 mg L−1. Wiedner and Polifka (2020) also showed that 80 days exposure to 90–100 μm PP, PE, PS, and PA-MPs (16.6 mg ha−1) changed the microbial community and increased the variability of Phospholipid fatty acids (PLFAs) for bacteria and protozoa. 3.8 Effects on microorganisms in in vivo tests The number of published works on living organisms and their microbiota is summarized in Fig. 4D. The taxon of microorganisms found in the studies in vivo after exposure to MP/NPs were bacteria (93.33%) and unspecified (6.66%). No study reported fungi, protozoa and viruses in the microbiota studied after exposure to MP/NPs. This is probably due to the fact that bacteria typically dominate the cellular fractions of microbiota, which induces researchers to use predominantly the 16S rRNA gene in sequencing. For analysis of other taxa, other necessary ones require the use of the 18S rRNA gene for fungi or microarrays for viruses (Goodrich et al., 2014). The loss of diversity, configuring dysbiosis, has been reported on microorganisms after in vivo exposure to MP/NPs (Jin et al., 2018; Lu et al., 2018; Qiao et al., 2019a; Wan et al., 2019; Liu et al., 2019; Jin et al., 2019) (Fig. 6 ). In 66.66% of the studies carried out in vivo, this analysis occurred in the intestines, being, in these cases, called intestinal dysbiosis (Fig. 6). In 20% of the studies, the analysis of the alteration of the microbial community occurred using the entire body of the animal (pool) (Fig. 6). There is still analysis of the microbiome in 6.6% of the studies through hemolymph and in 6.6% using collected feces. Dysbiosis typically features one or more of the following non-mutually exclusive characteristics: the bloom of pathobionts, loss of commensals, and loss of diversity (Levy et al., 2017). The gut microbiota is subject to natural variations induced by the changing supply of nutrients, drugs, the immune system, and the intestinal mucosa. The action of stress factors such as oxidative stress, the induction of bacteriophages, and the secretion of bacteriocins amplify the changes in microbial composition leading to decreased diversity and outgrowth of specific bacterial taxa (Weiss and Hennet, 2017). The increase in oxidative stress generated by 21 days exposure to 5 μm PS microbeads (50 μg L−1 and 500 μg L−1) was also observed in zebrafish (D. rerio) (Qiao et al., 2019b). About 93.33% of the total in vivo studies reported changes in the microbial communities induced by MP/NP exposure (Lu et al., 2018; Wan et al., 2019; Kurchaba et al., 2020; Gu et al., 2020; Auguste et al., 2020). Liu et al. (2019) demonstrated that the crab (Eriocheir sinensis) exposed during 21 days to 5 μm PS-MPs (0.04–40 mg L−1) has a decrease in Firmicutes and Bacteroidetes in the microbiome, while the phylum Proteobacteria increased. Similar data have been reported in a study conducted in humans during research on inflammatory bowel diseases, indicating that abnormal microbiotas, characterized by depletion of commensal bacteria such as Firmicutes and Bacteroidetes, can represent a spectrum of Crohn's disease (Frank et al., 2007). Therefore, MP/NPs exposure may induce diseases associated with depletion of the intestinal microbiota. On the other hand, effects of 10 μm PS-MPs exposure (1000 items L−1) for 21 days on the microbial community of the digestive tract of mussels Mytilus coruscus were evaluated and results revealed that Bacteroidetes and Firmicutes had the greatest diversity of Operational Taxonomic Units (OTUs) and exposure showed non-significant effects of MP ingestion on the dominant microorganisms of the mussel gut (Yang et al., 2020). In male rat intestines, a decrease in the phylum Firmicutes was found after 5 weeks exposure to 0.5 and 50 μm PS-MPs (1000 μg L−1) (Lu et al., 2018). In humans, the ratio between Firmicutes and Bacteroidetes has implied a greater predisposition to the disease state, when obese people have a relative lower proportion of Bacteroidetes in their gut microbiome (Ley et al., 2006; Jandhyala et al., 2015). Therefore, the change between the proportions of these phyla induced by exposure to MP/NPs may be associated with a disease development in the male mice gut (Lu et al., 2018). Jin et al. (2019) also demonstrated that the male mice exposed for 6 weeks to 50 μm PS-MPs and 0.5 μm NPs (100 and 1000 μg L−1) showed intestinal barrier dysfunction and intestinal dysbiosis in which the content of Actinobacteria decreased significantly. The NP toxicity on the intestinal microbiome of the soil oligochaete Enchytraeus crypticus was analyzed after exposure for 7 days to 0.05 to 0.1 μm PS-NPs (0.025, 0.5 and 10%) (Zhu et al., 2018b). A significant change was observed in the microbiome of oligochaetes fed with 10% NPs, with significant decreases in the relative abundance of the Rhizobiaceae, Xanthobacteraceae and Isosphaeraceae families. These families contain key microbes that contribute to the nitrogen cycle and the decomposition of organic matter. Nonetheless, the effects of exposure to 80 and 250 μm PVC-MPs (1 g MP kg−1) for 56 days on the intestinal microbiota of Folsomia candida were analyzed. The intestinal microbiota of this Collembola consists mainly of the phyla Actinobacteria, Bacteroidetes, Proteobacteria and Firmicutes. In this study there was a significant increase in microbiota diversity in the intestine, while growth and reproduction were inhibited (Zhu et al., 2018a). These results indicate that PVC-MPs can be toxic and impact non-target species through changes in their microbiota. For aquatic animals, there were studies like L. vannamei that were exposed to 44 nm PS-NPs (50 μg mL−1; 21 days) through a simulated food chain and evaluated the effects on the physical, biochemical and nutritional characteristics of the shrimp over the course of exposure (Chae et al., 2019) and the study carried out in Eriocheir sinensis that evaluated the impact of exposure to 5 μm PS-MPs (0.04, 0.4, 4, and 40 mg L−1; 7, 14 and 21 days) (Liu et al., 2019). For L. vonnamei, the levels of some essential amino acids (asparagine, glutamine, proline, histidine, cysteine and arginine) and fatty acids decreased. The activities of glutathione-S-transferase (GST) and superoxide dismutase (SOD) changed revealing oxidative stress. Furthermore, microbial activity (analyzed by means of cell size, cell granularity, and cell viability) in the intestine significantly increased. Such analyzes show that NP pollution can interfere with changing nutrition in marine food resources, indirectly causing potential health implications for human consumers (Chae et al., 2019). For E. sinensis, effects such as decrease of content or activity of factors related to immunity, alteration of gene expression and alteration of the microbiome with a decrease in Firmicutes and Bacteroidetes and an increase in Fusobacteria and Proteobacteria were effects that characterized the toxicity of PS-MPs (Liu et al., 2019). The in vivo exposure of M. galloprovincialis to NH2-modified 50 nm PS-NPs (10 μg L−1; 96 h) induced changes in the microbiome and immune inhibition (decreased phagocytosis and lysozyme). These results indicated that these particles are toxic to mussels, and when negatively regulating the immune system, can favor potentially pathogenic bacteria such as those belonging to the genera Arcobacter-like, Psychrobium and Vibrio (Auguste et al., 2020). As for studies on fish, the exposure of D. rerio adults to beads, fragments, and fibers of 10, 15 and 30 μm PS-MPs (0.5 to 8.0 mg mg−1) for 24 h promoted intestinal dysbiosis with bacteria belonging to the phylum Proteobacteria significantly decreased and Fusobacteria significantly increased (Qiao et al., 2019a). Intestinal dysbiosis was also observed in D. rerio adults exposed to 0.5 and 50 μm PS-MP and PS-NPs (100 and 1000 mg L−1), with a reported decrease in Bacteroides and Proteobacteria and an increase in Firmicutes (Jin et al., 2018). Similarly, the medaka fish (O. melastigma) after exposure to 2.5 μm PS beads (100 μg L−1) for 30 days revealed intestinal dysbiosis. At the phylum level, there were changes in Proteobacteria abundance in both male and female fish after exposure to PS-MPs. However, at the genus level, the relative abundance of gut microbiota of male and female fish was significantly different, indicating that PS-MPs may also be related to changes in the sex-linked microbiome (Yan et al., 2020a). The zebrafish adult exposed for 21 days to 5 μm PS-MP exposure (50 and 500 μg L−1) showed several intestinal changes, such as villi and epithelial damage, intestinal inflammation, oxidative stress (increased catalase - CAT and SOD activities) and alterations in the intestinal microbiome and in the profiles metabolic amino acids (proline, leucine, lysine, threonine, alanine, phenylalanine, glutamine, tyrosine, and ornithine) and lipids (propylene glycol, linoleic acid, palmitic acid, carnitine, triglycerides, and Trimethylamine N-oxide - TMAO) (Qiao et al., 2019b), indicating the potential toxicity of PS-MPs to intestinal health. Furthermore, zebrafish larvae exposed for 10 days to 14–45 μm PE-MPs (120 mg L−1) also presented intestinal dysbiosis with an increased abundance of Bacteroidetes (Kurchaba et al., 2020). Furthermore, alteration of the intestinal microbiome in D. rerio larvae exposed to 5 and 50 μm PS-MP/NPs (100 and 1000 mg L−1) also was reported. At phylum level, it was observed that MPs of 5 μm increased Bacteroidetes, while MPs of 50 μm decreased Bacteroidetes, which indicates that the size of the MPs interferes with toxicity to zebrafish microbiota (Wan et al., 2019). These results confirm that the zebrafish is a suitable in vivo model system to assess the effects of MP/NPs on the microbiome. The effects of PS-NPs in the intestinal health and growth performance were analyzed also were investigated in the marine fish Larimichthys crocea (juveniles) (Gu et al., 2020). Fish exposed to 100 nm PS-NPs (10, 104, 106 items L−1) for 14 days showed a decrease in growth and enzymatic activity (lysozyme), as well as changes in the microbiome, favoring the growth of pathogenic bacteria (Parabacteroides and Alistipes) (Gu et al., 2020), confirming that PS-NPs are harmful to the fish health. Deregulation of intestinal mucosa homeostasis can lead to a multitude of diseases, and as the intestinal microbiota is a key to mucosal homeostasis, its imbalance is consequently related to the progression of disorders (Sekirov et al., 2010). Thus, the results indicated in this review showed that MP/NPs exposure can induce intestinal mucosa homeostasis changes in terrestrial and aquatic organisms, confirming their potential toxicological risk. 3.9 Systemic effects after microbiota exposure to MP/NPs Some of the effects generated by the MP/NPs found occurred directly on the microorganisms; however other effects were observed in the hosts. This raises questions about the influence of altered microbiota on animal/human health. The microbiota and the host form a ‘superorganism’ complex, in which symbiotic relationships can confer benefits to the host. However, defects in the host's regulatory circuits that control bacterial detection and homeostasis, or changes in the microbiome, through environmental changes, can disrupt this symbiotic relationship and promote disease (Schwabe and Jobin, 2013). Some previous studies have already reported that dysbiosis can, for example, affect the nervous system in rats (Bercik et al., 2011), as there is communication between the intestine and the brain (Grenham et al., 2011). Dysbiosis can also trigger immunological disorders (Honda and Littman, 2016), inflammatory bowel diseases (Tamboli et al., 2004), obesity (Henao-Mejia et al., 2012), asthma (Arrieta et al., 2015), liver diseases (Schnabl and Brenner, 2014), and cancer (Anwar et al., 2020). In addition to the direct effect of altering the microbiome, other indirect effects were also found inflammation induction (Jin et al., 2018), hepatic lipid disorders (Lu et al., 2018), increase of membranous permeability (Qiao et al., 2019a), neurotoxicity (Wan et al., 2019) and increase of oxidative stress mediator (L-FABP and SOD1) (Kurchaba et al., 2020). Most studies with MP/NPs are focused on the effects caused directly to the organisms (de Sá et al., 2018), but an important issue to be addressed refers to the effects triggered after the alteration of the microbiota of these organisms, since the alteration of the microbiota has already demonstrated involvement in a considerable number of complex diseases that reside in different (and distant) organs of the intestine (Sekirov et al., 2010). Jin et al. (2018) showed that 6 weeks exposure PS-MPs (100 and 1000 μg L−1) and 0.5 μm NPs (100 and 1000 μg L−1) induced inflammation by increasing IL-1a, IL-1b, and IFN in the intestine of D. rerio. Besides, it has been reported an increase in permeability and disruption of metabolism in D. rerio exposed for 24 h to beads, fragments, and fibers PS-MPs (10, 15 and 30 μm; 0.5 to 8.0 mg mg−1) (Qiao et al., 2019a). It should be noted that in the first study, dysbiosis was promoted, a fact that led to an increase in Stenotrophomonas, Ralstonia, Vogesella, and Plesiomonas, bacteria considered as opportunistic pathogens and which can lead to inflammatory conditions. The induction of inflammation is comprehensive since the microbiota interacts extensively with the immune system (Round and Mazmanian, 2009). In the second study, dysbiosis was also promoted, but in this case, there was an increase in the number of bacteria of the genus Gordonia (Phylum Actinobacteria), which have a strong ability to catabolize components directly related to plastics, and it is known that synthetic polymers are energy-rich and theoretically represent a good source of energy and carbon for microorganisms (Oberbeckmann and Labrenz, 2020). Zebrafish larvae exposed for 7 days to 5 and 50 μm PS-MPs (100 and 1000 mg L−1) showed changes in energy, glycolipid, and nucleic acid metabolism, as well as inflammatory response, neurotoxicity, and oxidative stress (Wan et al., 2019). Similarly, zebrafish larvae exposed for 4 or 10 days to 10–45 μm PE-MPs (20 mg L−1) showed an increase in L-FABP (Kurchaba et al., 2020), which is a marker used to indicate kidney disorders and has already been found in response to exposure to silica by ceramic workers (Ramadan et al., 2021). Liu et al. (2019) demonstrated a decrease in the activity/content of factors related to immunity (haemocyanin, alkaline phosphatase, phenoloxidase, lysozyme and acid phosphatase) and gene expression changes (Hc and LSZ genes) in the E. sinensis exposed to sterile 5 μm PS microspheres (0.04 to 40 mg L−1; 7, 14 and 21 days). Gene expression changes induced by MP/NPs exposure also were reported in O. melastigma (Yan et al., 2020a), Oryzias latipes (Rochman et al., 2014), and D. rerio (Mak et al., 2019; Cormier et al., 2019). Jin et al. (2019) reported dysbiosis, intestinal barrier dysfunction and metabolic disorders on pathways of functional genes for gut microbiota in male rats exposed to PS-MPs (5 μm; 100 and 1000 mg L−1) for 6 weeks. Besides, Lu et al. (2018) showed that PS-MPs (0.5 μm; 1000 mg L−1) exposure for 5 weeks decreased the levels of mucin secretion, hepatic triglyceride (TG) and total cholesterol (HCT), and induced liver disorders in male rats. Granby et al. (2018) also observed inhibition of detoxification in the liver of the fish D. labrax exposed to PE-MPs (125–250 μm; 2%) for 40 days. Analyzing at the phylum level, Proteobacteria, besides being the phylum with the highest percentage of studies, was the phylum related to the greatest variety of MP/NPs (PE, PS and PVC and PBAT, PE and PP and PES) (Fig. 6) and related effects such as dysbiosis/alteration of the microbiome, immune induction, metabolism disruption, neurotoxicity (host), oxidative stress, mucosal damage (host), alteration of gene expression and alteration of enzyme activity (Lu et al., 2018; Wan et al., 2019; Qiao et al., 2019a; Jin et al., 2019). Only PA and PAN-MP/NPs were not related to the phylum Proteobacteria. Among the phyla that appear among the highest percentages of studies found, Bacteroides, Chloroflexi, and Firmicutes, there were studies with PE, PS, PES and PE, and PP and the effect profile found between Bacteroides and Firmicutes was very similar (dysbiosis/alteration of the microbiome, immune induction, metabolism disruption, neurotoxicity (host), oxidative stress, mucosal damage (host), alteration of gene expression and enzyme activity changes). Both Bacteroides and Firmicutes are phyla related to the maintenance of intestinal health (Wexler and Goodman, 2017), as they carry out the fermentation of amino acids and modulate the immune system (Rinniella et al., 2019). Therefore, when they are reduced, they can alter intestinal and systemic homeostasis. This may explain the similarity in the effect profile caused by MP/NPs. On the other hand, that of the phylum Chloroflexi was not associated with effects of neurotoxicity (host) and oxidative stress. The effects caused by MPs on microorganisms can be caused directly by solid particles, for example, through mechanical stress during adsorption of the particle on the cell wall or indirectly through chemicals associated with plastics (Rummel et al., 2017). MP/NPs can also be vectors for antimicrobials allowing environmental bacteria to be eliminated or acquire antimicrobial resistance (Sun et al., 2018a; Imran et al., 2019; Lu et al., 2019; Zhang et al., 2020b). Several other interactions between MPs and some organic pollutants have already been reported (Viršek et al., 2017; Lei et al., 2018; Eckert et al., 2018; Johansen et al., 2019). 3.10 Microbiota as an emerging biomarker for assessing the effects of MP/NPs The MPs/NPs can affect gut microbiota because these particle plastics can enter and accumulate in the gastric intestinal tract (Jin et al., 2019). Based on this, from an environmental perspective, the gut microbiota disruption can be an important biomarker for toxicological assessment (Table S1). The MPs/NPs can affect gut microbiota because these particle plastics can enter and accumulate in the gastric intestinal tract (Jin et al., 2019). Based on this, from an environmental perspective, the gut microbiota disruption can be an important biomarker for toxicological assessment (Table S1). Due to the sensitivity of the intestinal microbiota, it has become a new toxicological target for some pollutants and may act as a potential means to indirectly affect host health and as a new type of environmental pollutant, the toxicity of MPs to organisms has been extensively studied (Lu et al., 2019). Due to the specific biochemical interactions of microorganisms with their hosts and their systemic integration into host biology, the intestinal microbiome has been considered as a new organic system (Anwar et al., 2020), being compared to an organ (Ursell et al., 2014). The taxonomic composition of the microbiota might be most important, and this could be influenced by the overall diversity of species or by the presence of particular taxa, either of which can distinguish healthy individuals from those with disease states (Gilbert et al., 2016). Franzellitti et al. (2019) resumed the effects at the molecular, cellular, and system levels as well as effects on apical endpoints observed in aquatic organisms interacting with MPs. As the microbiota can be considered an organ due to its great complexity, in this review we propose the inclusion of the microbiota in two levels of biological organization: organ and community (Fig. 7 ), which can be a biomarker to assess the effects of MP/NPs. 4 Conclusion and perspectives When exploring the toxicity of MP/NPs on microorganisms in vitro and in vivo assays and the microbiota as a potential biomarker, we realized that this is a theme that goes beyond the interaction between particles and microbes, revealing to generate influence at different levels of biological organization, being an emerging and highly relevant field. The growth in the number of publications in the last years is indicative of a promising future for this research field. Factors related to the toxicity of MP/NPs mentioned were: polymer type, polymer shape, polymer size, fluorescent color, concentration, exposure time, type of sample, analysis technique, effects generated, microorganisms involved and nature of the test (in vivo or in vitro). There was no sexual distinction between the publications with an in vivo approach included in this review. Studies analyzing the toxicity of MPs addressed 10 types of polymers, while the toxicity of NPs was analyzed for only 3 types of polymers. Despite this, studies have shown that the toxicity of NPs is greater than that of MPs due to their ability to cross biological membranes. It is important to emphasize that for MPs the percentages of in vitro assays were also higher than in vivo assays, which demonstrates that analyzes in different in vivo models need to be further explored. Several techniques for the analysis of microorganisms have been used, but methodologies for sequencing genetic material stand out. Among all the publications analyzed here, 38 phyla of microorganisms were found being studied for the toxicity of MP/NPs and the main effects caused by MP/NPs on microorganisms were dysbiosis, metabolism disruptions, oxidative stress, altered immune response, altered enzymatic activity, altered gene expression and altered aggregation stage. It should also be emphasized the need to diversify studies taking into account the role of organisms in the environment. This review brings a new approach that addresses the effects of MP/NPs on microorganisms after different tests. MPs have been studied relatively intensely in aquatic environments and are recognized as potentially harmful to biota. Most studies are focused on the effects of MPs/NPs directly on the organisms, but some studies are already investigating the effects generated by MP/NPs on the microbiota of these organisms (and the consequence of this interference) or cultures of microorganisms isolated from the environment (and the consequence for ecosystems). The microbiota of an organism is related to several factors such as genotype, physiology, immune system, pathobiology and the host's lifestyle, in addition to members of transient communities, thus correlations between the interaction of MP/NPs and such factors should be noted for possible relevance in toxicity studies. Analyzes for metabolite-specific toxicity and related pathogens need to be better studied, especially for NPs that have greater potential for toxicity as they can cross the mucus barrier, while MPs are not absorbed. Despite this, Hirt and Body-Malapel (2020) related the MPs remain attached to the intestinal mucus layer and come into direct contact with the apical part of intestinal epithelial cells. This can lead to inflammation gut and local effects on the immune system, therefore, despite being less toxic than NPs, the MPs also need attention as they present toxicity to the intestinal epithelium. Microbiomes are associated with several diseases and important health conditions such as gut microbes and gastrointestinal tract diseases (inflammatory bowel disease, gastric cancer, colorectal cancer), cardiovascular diseases, influence in integrity system (role of the gut microbiota in skin homeostasis, dyshomeostasis due to dysbiosis), Influence in pulmonary health (asthma and allergies, viral and bacterial respiratory infections), pregnancy and brain physiology (Anwar et al., 2020), thus revealing a great potential as a toxicity biomarker if these microorganisms are affected by pollutants such as MP/NPs. Although these health conditions are generally mainly studied in humans, no work included in this review addressed toxicity studies generated by MP/NPs in humans. Therefore, analyzes regarding the toxicity of MP/NPs for the human microbiota must be studied. MP/NPs can be ingested by organisms directly or indirectly through trophic transfer. Thus, the concomitant ingestion of MP/NPs in the trophic chains associated with the type of food increases the possibility of altering bacterial communities. We also highlight the importance of animal study models for the analysis as to the influence of changes in the microbiota in other health problems. Studies with environmental samples are also important, as microorganisms play a critical role in controlling the source-sink behavior and cycling of major and minor elements in the biosphere. Biofilms have the potential to impose strong control over the fate of particles such as MP/NPs. It was observed that analyzes of the interaction MP/NPs and biofilms still have a low incidence, being found in only 4% of the studies and related PS-NP caused a decrease in microbial metabolic functional diversity. Biofilms can be found in nature as well as in organisms. Thus, the toxicity of MP/NPs for biofilms also requires more attention in future studies. Despite this, other studies involving biofilms address the ability of biofilm to modify the surface of MP/NPs influencing the buoyancy and fate of these particles in water. Based on the results of this study, we recommend: a) Perform more studies of MP/NPs effects on the microbiota using organisms at different trophic levels and environmentally relevant concentrations; b) Assess the in vitro toxicity of MP/NPs with environmentally relevant microorganisms; c) Investigate the effects of MP/NPs in viruses, bacteria, cyanobacteria, fungi, and protozoa; d) Assess the trophic transfer of MP/NPs and its effect on microbiota; e) Perform studies regarding the influence on prey digestion capacity during feed contaminated with MP/NPs and effects on predator microbiota; f) Analyzes of the effects of bioaccumulation and toxicokinetics of MP/NPs in microbiota of freshwater, estuarine, and marine invertebrates and vertebrates; g) Constant updating of the concentrations and environmentally relevant conditions of MP/NPs in aquatic, terrestrial and air ecosystems; h) In-depth analysis of the impact on human health due to the consumption of contaminated food and its effects on the microbiota; i) Perform more studies on the toxicity of MP/NPs in biofilms. The following are the supplementary data related to this article. Table S1 Effects of microplastics (MPs) and nanoplastics (NPs) on the microbiota. Table S1 Supplementary data to this article can be found online at CRediT authorship contribution statement Andressa Liberal Santos: Methodology, Data curation, Formal analysis, Writing – review & editing. Cândido Carvalho Rodrigues: Data curation, Writing – review & editing. Miguel Oliveira: Data curation, Writing – review & editing. Thiago Lopes Rocha: Conceptualization, Supervision, Data curation, Writing – review & editing. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was supported by National Council for Scientific and Technological Development - CNPq (MCTIC/CNPq N° 28/2018; n° 433553/2018-9) and Coordination of Improvement of Higher-Level Personnel - CAPES (n° 88882.386366/2019-01). Rocha T.L. is granted with productivity scholarship from CNPq (proc. n. 306329/2020-4). References Anwar et al., 2020 H. Anwar S. Irfan G. Hussain M. Naeem Faisal H. Muzaffar I. Mustafa I. Mukhtar S. Malik M. Irfan Ullah Gut microbiome: a new organ system in body Parasitol. Microbiol. Res. 2020 1 20 10.5772/intechopen.89634 Anwar, H., Irfan, S., Hussain, G., Naeem Faisal, M., Muzaffar, H., Mustafa, I., Mukhtar, I., Malik, S., Irfan Ullah, M. Gut Microbiome: A New Organ System in Body. Parasitol. Microbiol. Res. 1–20, 2020. doi:10.5772/intechopen.89634 Arrieta et al., 2015 M.C. Arrieta L.T. Stiemsma P.A. Dimitriu L. Thorson S. Russell S. Yurist-Doutsch B. Kuzeljevic M.J. Gold H.M. Britton D.L. Lefebvre P. Subbarao P. Mandhane A. Becker K.M. McNagny M.R. Sears T. Kollmann CHILD Study Investigators W.W. Mohn S.E. Turvey B.B. Finlay Early infancy mirobial and metabolic alterations affect risk of childhood asthma Sci. Transl. Med. 7 2015 307 10.1126/scitranslmed.aab2271 Arrieta, M.C., Stiemsma, L.T., Dimitriu, P.A., Thorson, L., Russell, S., Yurist-Doutsch, S., Kuzeljevic, B., Gold, M. J., Britton, H. M., Lefebvre, D. L., Subbarao, P., Mandhane, P., Becker, A., McNagny, K. M., Sears, M. R., Kollmann, T.; CHILD Study Investigators, Mohn, W. W., Turvey, S. E., Finlay, B. B. Early infancy mirobial and metabolic alterations affect risck of childhood asthma. Science Translational Medicine, v. 7, n. 307, 2015. doi: 10.1126/scitranslmed.aab2271 Auguste et al., 2020 M. Auguste A. Lasa T. Balbi A. Pallavicini L. Vezzulli L. Canesi Impact of nanoplastics on hemolymph immune parameters and microbiota composition in Mytilus galloprovincialis Mar. Environ. Res. 159 2020 1 8 10.1016/j.marenvres.2020.105017 Auguste, M., Lasa, A., Balbi, T., Pallavicini, A., Vezzulli, L., Canesi, L. Impact of nanoplastics on hemolymph immune parameters and microbiota composition in Mytilus galloprovincialis. Marine Environmental Research, v. 159, p. 1-8, 2020. doi: doi:10.1016/j.marenvres.2020.105017 Auta et al., 2018 H.S. Auta C.U. Emenike B. Jayanthi S.H. Fauziah Growth kinetics and biodeterioration of polypropylene microplastics by Bacillus sp. and Rhodococcus sp. isolated from mangrove sediment Mar. Pollut. Bull. 127 2018 15 21 10.1016/j.marpolbul.2017.11.036 Auta, H. S., Emenike, C. U., Jayanthi, B. & Fauziah, S. H. Growth kinetics and biodeterioration of polypropylene microplastics by Bacillus sp. and Rhodococcus sp. isolated from mangrove sediment. Mar. Pollut. Bull. v. 127, p. 15–21, 2018. doi: 10.1016/j.marpolbul.2017.11.036 Avio et al., 2015 C.G. Avio S. Gorbi F. Regoli Experimental development of a new protocol for extraction and characterization of microplastics in fish tissues: first observations in commercial species from Adriatic Sea Mar. Environ. Res. 111 2015 18 26 10.1016/j.marenvres.2015.06.014 Avio, C. G., Gorbi, S. & Regoli, F. Experimental development of a new protocol for extraction and characterization of microplastics in fish tissues: First observations in commercial species from Adriatic Sea. Marine Environmental Research, v. 111, p. 18–26, 2015. doi: 10.1016/j.marenvres.2015.06.014 Bar-On et al., 2018 Y.M. Bar-On R. Phillips R. Milo The biomass distribution on Earth PNAS 115 2018 6506 6511 10.1073/pnas.1711842115 Bar-On, Y. M., Phillips, R., Milo, R. The biomass distribution on Earth, PNAS, v. 115, p. 6506-6511, 2018. doi: 10.1073/pnas.1711842115 Bercik et al., 2011 P. Bercik E. Denou J. Collins W. Jackson J. Lu J. Jury Y.K. Deng P. Blennerhassett J. Macri K.D. McCoy The intestinal microbiota affect central levels of brasin-derived neurotropic factor and behavior in mice Gastroenterology 141 2 2011 599 U701 10.1053/j.gastro.2011.04.052 Bercik, P., Denou, E., Collins, J., Jackson, W., Lu, J., Jury, J., Deng, Y.K., Blennerhassett, P., Macri, J., McCoy, K. D. The Intestinal Microbiota Affect Central Levels of Brasin-Derived Neurotropic Factor and Behavior in Mice. Gastroenterology, v. 141, n. 2, p. 599-U701, 2011. doi: 10.1053/j.gastro.2011.04.052 Bhattacharya et al., 2010 P. Bhattacharya S. Lin J.P. Turner P.C. Ke Physical adsorption of charged plastic nanoparticles affects algal photosynthesis J. Phys. Chem. C 114 2010 16556 16561 10.1021/jp1054759 Bhattacharya, P., Lin, S., Turner, J. P., Ke, P. C. Physical Adsorption of Charged Plastic Nanoparticles Affects Algal Photosynthesis. The Journal of Physical Chemistry C, v. 114, p. 16556–16561, 2010. doi:10.1021/jp1054759 Bläsing and Amelung, 2018 M. Bläsing W. Amelung Plastics in soil: analytical methods and possible sources Sci. Total Environ. 612 2018 422 435 10.1016/j.scitotenv.2017.08.086 Bläsing, M. & Amelung. W. Plastics in soil: Analytical methods and possible sources. Sci. Total Environ., v. 612, p. 422–435, 2018. doi: 10.1016/j.scitotenv.2017.08.086 Botterell et al., 2019 Z.L.R. Botterell N. Beaumont T. Dorrington M. Steinke R.C. Thompson P.K. Lindeque Bioavailability and effects of microplastics on marine zooplankton: a review Environ. Pollut. 245 x 2019 98 110 10.1016/j.envpol.2018.10.065 Botterell, Z. L. R., Beaumont, N., Dorrington, T., Steinke, M., Thompson, R. C., Lindeque, P. K. Bioavailability and effects of microplastics on marine zooplankton: A review. Environmental Pollution. v. 245, n. x, p. 98-110, 2019. doi: 10.1016/j.envpol.2018.10.065 Bouwmeester et al., 2015 H. Bouwmeester P.C. Hollman R.J. Peters Potential health impact of environmentally released micro- and nanoplastics in the human food production chain: experiences from nanotoxicology Environ. Sci. Technol. 49 15 2015 8932 8947 10.1021/acs.est.5b01090 Bouwmeester, H., Hollman, P.C., Peters, R. J. Potential health impact of environmentally released micro- and nanoplastics in the human food production chain: experiences from nanotoxicology. Environ. Sci. Technol., v. 49, n. 15, p. 8932–8947, 2015. doi: 10.1021/acs.est.5b01090 Browne et al., 2007 M.A. Browne T. Galloway R. Thompson Microplastic-an emerging contaminant of potential concern? Integr. Environ. Assess. Manag. 3 2007 559 566 10.1002/ieam.5630030412 Browne, M. A., Galloway, T., Thompson, R. Microplastic-an emerging contaminant of potential concern? Integr. Environ. Assess. Manag., v. 3, p. 559–566, 2007. doi: 10.1002/ieam.5630030412 Browne et al., 2008 M.A. Browne A. Dissanayake T.S. Galloway D.M. Lowe R.C. Thompson Ingested microscopic plastic translocates to the circulatory system of themussel, Mytilus edulis (L.) Environ. Sci. Technol. 42 2008 5026 5031 10.1021/es800249a Browne, M. A., Dissanayake, A., Galloway, T. S., Lowe, D. M., Thompson, R. C. Ingested microscopic plastic translocates to the circulatory system of themussel, Mytilus edulis (L.). Environ. Sci. Technol., v. 42, p. 5026–5031, 2008. doi: 10.1021/es800249a Browne et al., 2011 M.A. Browne P. Crump S.J. Niven E. Teuten A. Tonkin T. Galloway R. Thompson Accumulation of microplastic on shorelines worldwide: sources and sinks Environ. Sci. Technol. 45 21 2011 9175 9179 10.1021/es201811s Browne, M. A., Crump, P., Niven, S. J., Teuten, E., Tonkin, A., Galloway, T., Thompson, R. Accumulation of microplastic on shorelines worldwide: sources and sinks. Environ. Sci. Technol., v. 45, n. 21, p. 9175–9179, 2011. doi: 10.1021/es201811s Caporaso et al., 2010 J.G. Caporaso C.L. Lauber W.A. Walters D. Berg-Lyons C.A. Lozupone P.J. Turnbaughd N. Fierer R. Knight Global patterns of 16S rRNA diversity at a depth of millions of sequences per sample PNAS 2010 1 7 10.1073/pnas.1000080107 Caporaso, J. G., Lauber, C. L., Walters W. A., Berg-Lyons, D., Lozupone C. A., Turnbaughd, P. J., Fierer, N., Knight, R. Global patterns of 16S rRNA diversity at a depth of millions of sequences per sample. PNAS, p. 1-7, 2010. doi: 10.1073/pnas.1000080107 Carpenter and Smith, 1972 E.J. Carpenter K.L. Smith Plastics on the Sargasso sea surface Science 175 4027 1972 1240 1241 10.1126/science.175.4027.1240 Carpenter, E. J., & Smith, K. L. Plastics on the Sargasso Sea Surface. Science, v. 175, n. 4027, p. 1240–1241, 1972. doi: 10.1126/science.175.4027.1240 Chae and An, 2017 Y. Chae Y.J. An Effects of micro- and nanoplastics on aquatic ecosystems: current research trends and perspectives Mar. Pollut. Bull. 124 2017 624 632 10.1016/j.marpolbul.2017.01.070 Chae, Y. & An, Y. J. Effects of micro- and nanoplastics on aquatic ecosystems: Current research trends and perspectives. Mar. Pollut. Bull., v. 124, p. 624–632, 2017. doi: 10.1016/j.marpolbul.2017.01.070 Chae et al., 2019 Y. Chae D. Kim M. Choi Y. Cho Y. An Impact of nano-sized plastic on the nutritional value and gut microbiota of whiteleg shrimp Litopenaeus vannamei via dietary exposure Environ. Int. 130 2019 2 8 10.1016/j.envint.2019.05.042 Chae, Y., Kim, D., Choi M., Cho, Y., An, Y. Impact of nano-sized plastic on the nutritional value and gut microbiota of whiteleg shrimp Litopenaeus vannamei via dietary exposure. Environment International, v. 130, p. 2-8, 2019. doi: doi:10.1016/j.envint.2019.05.042 Chen et al., 2004 H.-T. Chen M.F. Neerman A.R. Parrish E.E. Simanek Cytotoxicity, hemolysis, and acute in vivo toxicity of dendrimers based on melamine, candidate vehicles for drug delivery J. Am. Chem. Soc. 126 2004 10044 10048 10.1021/ja048548j Chen, H.-T., Neerman, M. F., Parrish, A. R. & Simanek, E. E. Cytotoxicity, hemolysis, and acute in vivo toxicity of dendrimers based on melamine, candidate vehicles for drug delivery. J. Am. Chem. Soc, v. 126, p. 10044–10048, 2004. doi: 10.1021/ja048548j Chen et al., 2019 X. Chen X. Xiong X. Jiang H. Shi C. Wu Sinking of floating plastic debris caused by biofilm development in a freshwater lake Chemosphere 222 2019 856 864 10.1016/j.chemosphere.2019.02.015 Chen, X., Xiong, X., Jiang, X., Shi, H. & Wu, C. Sinking of floating plastic debris caused by biofilm development in a freshwater lake. Chemosphere, v. 222, p. 856–864, 2019. doi: 10.1016/j.chemosphere.2019.02.015 Cole et al., 2013 M. Cole P. Lindeque E. Fileman C. Halsband R. Goodhead J. Moger Microplastic ingestion by zooplankton Environ. Sci. Technol. 47 2013 6646 6655 10.1021/es400663f Cole, M., Lindeque, P., Fileman, E., Halsband, C., Goodhead, R., Moger, J. Microplastic ingestion by zooplankton. Environ. Sci. Technol., v. 47, p. 6646–6655, 2013. doi: 10.1021/es400663f Cormier et al., 2019 B. Cormier A. Batel J. Cachot M. Begout T. Braunbeck X. Cousin S.H. Keiter Multi-laboratory hazard assessment of contaminated microplastic particles by means of enhanced fish embryo test with the zebrafish (Danio rerio) Front. Environ. Sci. 7 2019 1 14 10.3389/fenvs.2019.00135 Cormier, B., Batel, A., Cachot, J., Begout, M., Braunbeck, T., Cousin, X., Keiter, S.H. Multi-Laboratory Hazard Assessment of Contaminated Microplastic Particles by Means of Enhanced Fish Embryo Test With the Zebrafish (Danio rerio). Front. Environ. Sci., v. 7, p. 1–14, 2019. doi:10.3389/fenvs.2019.00135 Costa et al., 2019 P.J. Costa V. Reis A. Paço M. Costa A.C. Duarte T. Rocha-Santos Micro(nano)plastics – analytical challenges towards risk evaluation Trends Anal. Chem. 111 2019 173 184 10.1016/j.trac.2018.12.013 Costa, P.J., Reis, V., Paço, A., Costa, M., Duarte, A. C., & Rocha-Santos, T. Micro(nano)plastics – analytical challenges towards risk evaluation. Trends in Analytical Chemistry. v. 111, p. 173-184, 2019. doi:10.1016/j.trac.2018.12.013 Cózar et al., 2014 A. Cózar F. Echevarría J.I. González-Gordillo X. Irigoien B. Úbeda S. Hernández-León A.T. Palma S. Navarro J. Garcia-de-Lomas A. Ruiz M.L. Fernandez-de-Pelles C.M. Duarte Plastic debris in the open ocean Proc. Natl. Acad. Sci. U. S. A. 111 28 2014 10239 10244 10.1073/pnas.1314705111 Cózar, A.; Echevarría, F.; González-Gordillo, J. I.; Irigoien, X.; Úbeda, B.; Hernández-León, S.; Palma, A. T.; Navarro, S.; Garcia-de-Lomas, J.; Ruiz, A.; Fernandez-de-Pelles, M. L.; Duarte, C. M. Plastic debris in the open ocean. Proc. Natl. Acad. Sci. U. S. A., v. 111, n. 28, p. 10239−10244, 2014. doi: 10.1073/pnas.1314705111 Eckert et al., 2018 E.M. Eckert A. Di Cesare M.T. Kettner M. Arias-Andres D. Fontaneto H. Grossart G. Corno Microplastics increase impact of treated wastewater on freshwater microbial community Environ. Pollut. 234 2018 495 502 10.1016/j.envpol.2017.11.070 Eckert, E. M., Di Cesare, A., Kettner, M. T., Arias-Andres, M., Fontaneto, D., Grossart, H., Corno, G. Microplastics increase impact of treated wastewater on freshwater microbial community. Environ. Pollut., v. 234, p. 495–502, 2018. doi: doi:10.1016/j.envpol.2017.11.070 Eerkes-Medrano et al., 2015 D. Eerkes-Medrano R.C. Thompson D.C. Aldridge Microplastics in freshwater systems: a review of the emerging threats, identification of knowledge gaps and prioritisation of research needs Water Res. 75 2015 63 82 10.1016/j.watres.2015.02.012 Eerkes-Medrano, D., Thompson, R. C., Aldridge, D. C. Microplastics in freshwater systems: a review of the emerging threats, identification of knowledge gaps and prioritisation of research needs. Water Res., v. 75, p. 63–82, 2015. doi: 10.1016/j.watres.2015.02.012 Enyoh et al., 2020 C.E. Enyoh A.W. Verla E.N. Verla Novel coronavirus (SARC-CoV-2) and airborne microplastic J. Mater. Environ. Sci. 11 2020 1454 1461 10.5281/zenodo.3738451 Enyoh, C., E., Verla, A., W., Verla, E. N. Novel Coronavirus (SARC-CoV-2) and Airborne Microplastic, J. Mater. Environ. Sci., v. 11, p. 1454-1461, 2020. doi: 10.5281/zenodo.3738451 Feng et al., 2018 X. Feng W. Ding L. Xiong L. Guo J. Sun P. Xiao Recent advancements in intestinal microbiota analyses: a review for non-microbiologists Curr.Med.Sci. 38 2018 949 961 10.1007/s11596-018-1969-z Feng, X., Ding, W., Xiong, L., Guo, L., Sun, J., Xiao, P. Recent Advancements in Intestinal Microbiota Analyses: A Review for Non-Microbiologists. Current Medical Science, v. 38, p. 949-961, 2018. doi: 10.1007/s11596-018-1969-z Feng et al., 2019 L. Feng J. Li E.G. Xu X. Sun F. Zhu Z. Ding H. Tian S. Dong P. Xia X. Yuan Short-term exposure to positively charged polystyrene nanoparticles causes oxidative stress and membrane destruction in cyanobacteria Environ. Sci. Nano 6 2019 3072 3079 10.1039/C9EN00807A Feng, L., Li, J., Xu, E.G., Sun, X., Zhu, F., Ding, Z., Tian, H., Dong, S., Xia, P., Yuan, X. Short-term exposure to positively charged polystyrene nanoparticles causes oxidative stress and membrane destruction in cyanobacteria. Environ. Sci. Nano, v. 6, p. 3072–3079, 2019. doi: 10.1039/C9EN00807A Frank et al., 2007 D.N. Frank A.L.S. Amand R.A. Feldman E.C. Boedeker N. Harpaz N.R. Pace Molecular-phylogenetic characterization of microbial community imbalances in human inflammatory bowel diseases Proc. Natl. Acad. Sci. U. S. A. 104 2007 13780 13785 10.1073/pnas.0706625104 Frank, D. N., Amand, A. L. S., Feldman, R. A., Boedeker, E. C., Harpaz, N., Pace, N. R. Molecular-phylogenetic characterization of microbial community imbalances in human inflammatory bowel diseases. Proc. Natl. Acad. Sci. U. S. A. v. 104, p. 13780–13785, 2007. doi: 10.1073/pnas.0706625104 Franzellitti et al., 2019 S. Franzellitti L. Canesi M. Auguste R.H.G.R. Wathsala E. Fabbri Microplastic exposure and effects in aquatic organisms: a physiological perspective Environ. Toxicol. Pharmacol. 68 2019 37 51 10.1016/j.etap.2019.03.009 Franzellitti, S., Canesi, L., Auguste, M., Wathsala, R. H. G. R., Fabbri, E. Microplastic exposure and effects in aquatic organisms: A physiological perspective. Environmental Toxicology and Pharmacology, v. 68, p. 37–51, 2019. doi: 10.1016/j.etap.2019.03.009 Fringer et al., 2020 V.S. Fringer L.P. Fawcett D.M. Mitrano M.A. Maurer-Jones Impacts of nanoplastics on the viability and riboflavin secretion in the model bacteria Shewanella oneidensis Front. Environ. Sci. 8 2020 1 11 10.3389/fenvs.2020.00097 Fringer, V.S., Fawcett, L.P., Mitrano, D.M., Maurer-Jones, M.A. Impacts of Nanoplastics on the Viability and Riboflavin Secretion in the Model Bacteria Shewanella oneidensis. Front. Environ. Sci. 8, 1–11, 2020. doi:10.3389/fenvs.2020.00097 Gambardella et al., 2018 C. Gambardella S. Morgana M. Bramini A. Rotini L. Manfra L. Migliore V. Piazza F. Garaventa M. Faimali Ecotoxicological effects of polystyrene microbeads in a battery of marine organisms belonging to different trophic levels Mar. Environ. Res. 141 2018 313 321 10.1016/j.marenvres.2018.09.023 Gambardella, C., Morgana, S., Bramini, M., Rotini, A., Manfra, L., Migliore, L., Piazza, V., Garaventa, F., Faimali, M. Ecotoxicological effects of polystyrene microbeads in a battery of marine organisms belonging to different trophic levels. Marine Environmental Research. v. 141, p. 313–321, 2018. doi: 10.1016/j.marenvres.2018.09.023 Gilbert et al., 2016 J.A. Gilbert R.A. Quinn J. Debelius Z.Z. Xu J. Morton N. Garg J.K. Jansson P.C. Dorrestein R. Knight Microbiome-wide association studies link dynamic microbial consortia to disease Nature 535 7610 2016 94 103 10.1038/nature18850 Gilbert, J. A., Quinn, R. A., Debelius, J., Xu, Z. Z., Morton, J., Garg, N., Jansson, J. K., Dorrestein, P. C., Knight, R. Microbiome-wide association studies link dynamic microbial consortia to disease. Nature, v. 535, n. 7610, p. 94–103, 2016. doi:10.1038/nature18850. Goodrich et al., 2014 J.K. Goodrich S.C. Di Rienzi A.C. Poole O. Koren W.A. Walters J.G. Caporaso R. Knight R.E. Ley Conducting a microbiome study Cell 158 2014 250 262 10.1016/j.cell.2014.06.037 Goodrich, J. K., Di Rienzi, S. C., Poole, A. C., Koren, O., Walters, W. A., Caporaso, J. G., Knight, R., Ley, R. E. Conducting a Microbiome Study, Cell, v. 158, p. 250-262, 2014. doi: 10.1016/j.cell.2014.06.037 Granby et al., 2018 K. Granby S. Rainieri R.R. Rasmussen M.J.J. Kotterman J.J. Sloth T.L. Cederberg A. Barranco A. Marques B.K. Larsen The influence of microplastics and halogenated contaminants in feed on toxicokinetics and gene expression in European seabass (Dicentrarchus labrax) Environ. Res. 164 2018 430 443 10.1016/j.envres.2018.02.035 Granby, K., Rainieri, S., Rasmussen, R. R., Kotterman, M. J. J., Sloth, J. J., Cederberg, T. L., Barranco, A., Marques, A., Larsen, B. K. The influence of microplastics and halogenated contaminants in feed on toxicokinetics and gene expression in European seabass (Dicentrarchus labrax). Environ. Res., v. 164, p. 430–443, 2018. doi: 10.1016/j.envres.2018.02.035 Gray and Weinstein, 2017 A.D. Gray J.E. Weinstein Size- and shape-dependent effects of microplastic particles on adult daggerblade grass shrimp (Palaemonetes pugio) Environ. Toxicol. Chem. 36 2017 3074 3080 10.1002/etc.3881 Gray, A.D., Weinstein, J.E. Size- and shape-dependent effects of microplastic particles on adult daggerblade grass shrimp (Palaemonetes pugio). Environ. Toxicol. Chem. 36, 3074–3080, 2017. doi:10.1002/etc.3881 Grenham et al., 2011 S. Grenham G. Clarke J.F. Cryan T.G. Dinan Brain-gut-microbe communication in health and disease Front. Physiol. 2 94 2011 10.3389/fphys.2011.00094 Grenham, S., Clarke, G., Cryan, J. F., Dinan, T. G. Brain-gut-microbe communication in health and disease. Frontiers in Physiology, v. 2, n. 94, 2011. doi: 10.3389/fphys.2011.00094 Grigorakis and Drouillard, 2018 S. Grigorakis G. Drouillard Effect of microplastic amendment to food on diet assimilation efficiencies of PCBs by fish Environ.Sci.Technol. 52 2018 10796 10802 10.1021/acs.est.8b02497 Grigorakis, S. & Drouillard, G. Effect of microplastic amendment to food on diet assimilation efficiencies of PCBs by fish. Environmental Science & Technology, v. 52, p. 10796-10802, 2018. doi: 10.1021/acs.est.8b02497 Gu et al., 2020 H. Gu S. Wang X. Wang X. Yu M. Hu W. Huang Y. Wang Nanoplastics impair the intestinal health of the juvenile large yellow croaker Larimichthys crocea J. Hazard. Mater. 397 2020 1 8 10.1016/j.jhazmat.2020.122773 Gu, H., Wang, S., Wang, X., Yu, X., Hu, M., Huang, W., Wang, Y. Nanoplastics impair the intestinal health of the juvenile large yellow croaker Larimichthys crocea. Journal of Hazardous Materials, v. 397, p. 1-8, 2020. doi: 10.1016/j.jhazmat.2020.122773 Heinlaan et al., 2020 M. Heinlaan K. Kasemets V. Aruoja I. Blinova O. Bondarenko A. Lukjanova A. Khosrovyan I. Kurvet M. Pullerits M. Sihtmäe G. Vasiliev H. Vija A. Kahru Hazard evaluation of polystyrene nanoplastic with nine bioassays did not show particle-specific acute toxicity Sci. Total Environ. 707 2020 10.1016/j.scitotenv.2019.136073 Heinlaan, M., Kasemets, K., Aruoja, V., Blinova, I., Bondarenko, O., Lukjanova, A., Khosrovyan, A., Kurvet, I., Pullerits, M., Sihtmäe, M., Vasiliev, G., Vija, H., Kahru, A. Hazard evaluation of polystyrene nanoplastic with nine bioassays did not show particle-specific acute toxicity. Sci. Total Environ, v. 707, 2020. doi:10.1016/j.scitotenv.2019.136073 Henao-Mejia et al., 2012 J. Henao-Mejia E. Elinav C.C. Jin L.M. Hao W.Z. Mehal T. Strowig C.A. Thaiss A.L. Kau S.C. Eisenbarth M.J. Jurczak J.P. Camporez G.I. Shulman H.M. Hoffman R.A. Flavell Inflammasome-mediated dysbiosis regulates progression of NAFLD and obesity Nature 482 2012 179 U67 10.1038/nature10809 Henao-Mejia, J., Elinav, E., Jin, C.C., Hao, L.M., Mehal, W.Z., Strowig, t., Thaiss, C. A., Kau, A. L., Eisenbarth, S. C., Jurczak, M. J., Camporez, J. P., Shulman, G. I., Hoffman, H. M., Flavell, R. A. Inflammasome-mediated dysbiosis regulates progression of NAFLD and obesity. Nature, v. 482, p. 179-U67, 2012. doi:10.1038/nature10809 Hirt and Body-Malapel, 2020 N. Hirt M. Body-Malapel Immunotoxicity and intestinal effects of nano- and microplastics: a review of the literature Part. Fibre Toxicol. 17 2020 1 23 10.1186/s12989-020-00387-7 Hirt, N., Body-Malapel, M. Immunotoxicity and intestinal effects of nano- and microplastics: a review of the literature. Part. Fibre Toxicol. v. 17, p. 1–23, 2020. doi:10.1186/s12989-020-00387-7 Honda and Littman, 2016 K. Honda D.R. Littman The microbiota in adaptive immune homeostasis and disease Nature 535 7610 2016 75 84 10.1038/nature18848 Honda, K. & Littman, D.R. The microbiota in adaptive immune homeostasis and disease. Nature, v. 535, n. 7610, p. 75-84, 2016. doi: 10.1038/nature18848 Huang et al., 2005 M. Huang J. Yu X. Ma P. Jin High performance biodegradable thermoplastic starch—EMMT nanoplastics Polymer 46 2005 3157 3162 10.1016/j.polymer.2005.01.090 Huang M., Yu J., Ma X., Jin P. High performance biodegradable thermoplastic starch—EMMT nanoplastics. Polymer, v. 46, p. 3157-3162, 2005. doi:10.1016/j.polymer.2005.01.090 Huang et al., 2019 Y. Huang Y. Zhao J. Wang M. Zhang W. Jia X. Qin LDPE microplastic films alter microbial community composition and enzymatic activities in soil Environ. Pollut. 254 2019 10.1016/j.envpol.2019.112983 Huang, Y., Zhao, Y., Wang, J., Zhang, M., Jia, W., Qin, X. LDPE microplastic films alter microbial community composition and enzymatic activities in soil. Environ. Pollut., v. 254, 2019. doi: 10.1016/j.envpol.2019.112983 Huerta Lwanga et al., 2016 E. Huerta Lwanga H. Gertsen H. Gooren P. Peters T. Salanki M. van der Ploeg E. Besseling A.A. Koelmans V. Geissen Microplastics in the terrestrial ecosystem: implications for Lumbricus terrestris (Oligochaeta, Lumbricidae) Environ. Sci. Technol. 5 2016 2685 2691 10.1021/acs.est.5b05478 Huerta Lwanga, E., Gertsen, H., Gooren, H., Peters, P., Salanki, T., van der Ploeg, M., Besseling, E., Koelmans, A. A., Geissen, V. Microplastics in the terrestrial ecosystem: implications for Lumbricus terrestris (Oligochaeta, Lumbricidae). Environ. Sci. Technol., v. 5, p. 2685–2691, 2016. doi: 10.1021/acs.est.5b05478. Imran et al., 2019 M. Imran K.R. Das M.M. Naik Co-selection of multi-antibiotic resistance in bacterial pathogens in metal and microplastic contaminated environments: an emerging health threat Chemosphere 215 2019 846 857 10.1016/j.chemosphere.2018.10.114 Imran, M., Das, K. R., Naik, M. M. Co-selection of multi-antibiotic resistance in bacterial pathogens in metal and microplastic contaminated environments: An emerging health threat. Chemosphere, v. 215, p. 846–857, 2019. doi 10.1016/j.chemosphere.2018.10.114 Jandhyala et al., 2015 S.M. Jandhyala R. Talukdar C. Subramanyam H. Vuyyuru M. Sasikala D.N. Reddy Role of the normal gut microbiota World J. Gastroenterol. 21 2015 8836 8847 10.3748/wjg.v21.i29.8787 Jandhyala, S. M., Talukdar, R., Subramanyam, C., Vuyyuru, H., Sasikala, M., Reddy D.N. Role of the normal gut microbiota. World J. Gastroenterol., v. 21, p. 8836–8847, 2015. doi: 10.3748/wjg.v21.i29.8787 Jeong et al., 2016 C.B. Jeong E.J. Won H.M. Kang M.C. Lee D.S. Hwang U.K. Hwang B. Zhou S. Souissi S.J. Lee J.S. Lee Microplastic size-dependent toxicity, oxidative stress induction, and p-JNK and p-p38 activation in the monogonont rotifer (Brachionus koreanus) Environ. Sci. Technol. 50 2016 8849 8857 10.1021/acs.est.6b01441 Jeong, C. B., Won, E. J., Kang, H. M., Lee, M. C., Hwang, D. S., Hwang, U. K., Zhou, B., Souissi, S., Lee, S. J., Lee, J. S. Microplastic size-dependent toxicity, oxidative stress induction, and p-JNK and p-p38 activation in the monogonont rotifer (Brachionus koreanus). Environ. Sci. Technol., v. 50, p. 8849-8857, 2016. doi: 10.1021/acs.est.6b01441 Jiang, 2018 J.Q. Jiang Occurrence of microplastics and its pollution in the environment: a review Sustain. Prod. Consum. 13 2018 16 23 10.1016/j.spc.2017.11.003 Jiang, J. Q. Occurrence of microplastics and its pollution in the environment: A review. Sustain. Prod. Consum., v. 13, p. 16–23, 2018. doi:10.1016/j.spc.2017.11.003 Jin et al., 2018 Y. Jin J. Xia Z. Pan J. Yang W. Wang Z. Fu Polystyrene microplastics induce microbiota dysbiosis and inflammation in the gut of adult zebrafish Environ. Pollut. 235 2018 322 329 10.1016/j.envpol.2017.12.088 Jin, Y., Xia, J., Pan, Z., Yang, J., Wang, W., Fu, Z. Polystyrene microplastics induce microbiota dysbiosis and inflammation in the gut of adult zebrafish. Environ. Pollut., v. 235, p. 322–329, 2018. doi:10.1016/j.envpol.2017.12.088 Jin et al., 2019 Y. Jin L. Lu W. Tu T. Luo Z. Fu Impacts of polystyrene microplastic on the gut barrier, microbiota and metabolism of mice Sci. Total Environ. 649 2019 308 317 10.1016/j.scitotenv.2018.08.353 Jin, Y., Lu, L., Tu, W., Luo, T. & Fu, Z. Impacts of polystyrene microplastic on the gut barrier, microbiota and metabolism of mice. Sci. Total Environ., v. 649, p. 308–317, 2019. doi: 10.1016/j.scitotenv.2018.08.353 Johansen et al., 2019 M.P. Johansen E. Prentice T. Cresswell N. Howell Biofilm-enhanced adsorption of strong and weak cations onto different microplastic sample types: use of spectroscopy, microscopy and radiotracer methods Water Res. 158 2019 392 400 10.1016/j.watres.2019.04.029 Johansen, M. P., Prentice, E., Cresswell, T., Howell, N. Biofilm-enhanced adsorption of strong and weak cations onto different microplastic sample types: Use of spectroscopy, microscopy and radiotracer methods. Water Res., v. 158, p. 392–400, 2019. doi:10.1016/j.watres.2019.04.029 Jovanović et al., 2017 B. Jovanović K. Gökdağ O. Güven Y. Emre E.M. Whitley A.E. Kideys Ingestion of microplastics by fish and its potential consequences from a physical perspective Mar. Pollut. Bull. 3 2017 510 515 10.1002/ieam.1913 Jovanović, B., Gökdağ, K., Güven, O., Emre, Y. Whitley, E. M. Kideys, A. E. Ingestion of Microplastics by Fish and Its Potential Consequences from a Physical Perspective. Marine Pollution Bulletin, v. 3, p. 510-515, 2017. doi: 10.1002/ieam.1913 Kleinteich et al., 2018 J. Kleinteich S. Seidensticker N. Marggrander C. Zarf Microplastics reduce short-term effects of environmental contaminants. Part II: polyethylene particles decrease the effect of polycyclic aromatic hydrocarbons on microorganisms Int. J. Environ. Res. Public Health 15 2018 10.3390/ijerph15020287 Kleinteich, J., Seidensticker, S., Marggrander, N., Zarf, C. Microplastics reduce short-term effects of environmental contaminants. Part II: Polyethylene particles decrease the effect of polycyclic aromatic hydrocarbons on microorganisms. Int. J. Environ. Res. Public Health, v. 15, 2018. doi: 10.3390/ijerph15020287 Knight et al., 2018 R. Knight A. Vrbanac B.C. Taylor A. Aksenov C. Callewaert J. Debelius A. Gonzalez T. Kosciolek L. McCall D. McDonald A.V. Melnik J.T. Morton J. Navas R.A.Q. Quinn J.G. Sanders A.D. Swafford L.R. Thompson A. Tripathi Z.Z. Xu J.R. Zaneveld Q. Zhu J.G. Caporaso P.C. Dorrestein Best practices for analysing microbiomes Microbiome 2018 10.1038/s41579-018-0029-9 Knight, R., Vrbanac, A., Taylor, B. C., Aksenov, A., Callewaert, C., Debelius, J., Gonzalez, A., Kosciolek, T., McCall, L., McDonald, D., Melnik, A. V., Morton, J. T., Navas, J., Quinn, R. A. Q., Sanders, J. G., Swafford, A. D., Thompson, L. R., Tripathi, A., Xu, Z. Z., Zaneveld, J. R., Zhu, Q., Caporaso, J. G., Dorrestein, P. C. Best practices for analysing microbiomes, Microbiome, 2018. doi: 10.1038/s41579-018-0029-9 Kurchaba et al., 2020 N. Kurchaba B.J. Cassone C. Northam B.F. Ardelli C.M.R. LeMoine Effects of MP polyethylene microparticles on microbiome and inflammatory response of larval zebrafish Toxics 8 2020 10.3390/toxics8030055 Kurchaba N, Cassone BJ, Northam C, Ardelli BF, LeMoine CMR. Effects of MP Polyethylene Microparticles on Microbiome and Inflammatory Response of Larval Zebrafish. Toxics. v. 8, 2020. doi: 10.3390/toxics8030055. Lambert and Wagner, 2016 S. Lambert M. Wagner Characterisation of nanoplastics during the degradation of polystyrene Chemosphere 145 2016 265 268 10.1016/j.chemosphere.2015.11.078 Lambert, S. & Wagner, M. Characterisation of nanoplastics during the degradation of polystyrene. Chemosphere, v. 145, p. 265−268, 2016. doi: doi:10.1016/j.chemosphere.2015.11.078 Lear et al., 2021 G. Lear J.M. Kingsbury S. Franchini V. Gambarini S.D.M. Maday J.A. Wallbank L. Weaver O. Pantos Plastic and the microbiome: impacts and solutions Environ. Microb. 16 2021 1 19 10.1186/s40793-020-00371-w Lear, G., Kingsbury, J., M., Franchini, S., Gambarini, V., Maday, S., D., M., Wallbank, J., A., Weaver, L., Pantos, O., 2021. Plastic and the microbiome: impacts and solutions. Environmental microbiome, v. 16, p. 1-19. Lei et al., 2018 L. Lei S. Wu S. Lu M. Liu Y. Song Z. Fu H. Shi K.M. Raley-Susman D. He Microplastic particles cause intestinal damage and other adverse effects in zebrafish Danio rerio and nematode Caenorhabditis elegans Sci. Total Environ. 619–620 2018 1 8 10.1016/j.scitotenv.2017.11.103 Lei, L., Wu, S., Lu, S., Liu, M., Song, Y., Fu, Z., Shi, H., Raley-Susman, K. M., He, D. Microplastic particles cause intestinal damage and other adverse effects in zebrafish Danio rerio and nematode Caenorhabditis elegans. Science of the Total Environment, v. 619–620, p. 1–8, 2018. doi: 10.1016/j.scitotenv.2017.11.103 Levy et al., 2017 M. Levy A.A. Kolodziejczyk C.A. Thaiss E. Elinav Dysbiosis and the immune system Nat. Rev. Immunol. 17 4 2017 219 232 10.1038/nri.2017.7 Levy, M., Kolodziejczyk, A. A., Thaiss, C. A., & Elinav, E. Dysbiosis and the immune system. Nature Reviews Immunology, v., 17, n. 4, p. 219–232, 2017. doi:10.1038/nri.2017.7 Ley et al., 2006 R.E. Ley P.J. Turnbaugh S. Klein J.I. Gordon Human gut microbes associated with obesity Nature 444 7122 2006 1022 1023 10.1038/4441022a Ley, R. E., Turnbaugh, P. J., Klein, S., & Gordon, J. I. Human gut microbes associated with obesity. Nature, v. 444(7122), p. 1022–1023, 2006. doi:10.1038/4441022a Li et al., 2015 J. Li D. Yang L. Li K. Jabeen H. Shi Microplastics in commercial bivalves from China Environ. Pollut. 207 2015 190 195 10.1016/j.envpol.2015.09.018 Li, J., Yang, D., Li, L., Jabeen, K., Shi, H. Microplastics in commercial bivalves from China. Environ. Pollut., v. 207, p. 190–195, 2015. doi: 10.1016/j.envpol.2015.09.018 Li et al., 2019 J. Li F. Liu C. Yang S. Zheng L. Xiao J. Li C. Tu Y. Luo Inhibition effect of polyvinyl chloride on ferrihydrite reduction and electrochemical activities of Geobacter metallireducens J. Basic Microbiol. 60 2019 37 46 10.1002/jobm.201900415 Li, J., Liu, F., Yang, C., Zheng, S., Xiao, L., Li, J., Tu, C., Luo, Y. Inhibition effect of polyvinyl chloride on ferrihydrite reduction and electrochemical activities of Geobacter metallireducens. J. Basic Microbiol., v. 60, p. 37–46, 2019. doi: 10.1002/jobm.201900415 Liang et al., 2019 Y. Liang A. Lehmann M.B. Ballhausen L. Muller M.C. Rillig Increasing temperature and microplastic fibers jointly influence soil aggregation by Saprobic fungi Front. Microbiol. 10 2019 1 10 10.3389/fmicb.2019.02018 Liang, Y., Lehmann, A., Ballhausen, M. B., Muller, L., Rillig, M. C. Increasing Temperature and Microplastic Fibers Jointly Influence Soil Aggregation by Saprobic Fungi. Front. Microbiol., v. 10, p. 1–10, 2019. doi: 10.3389/fmicb.2019.02018 Lin et al., 2020 D. Lin G. Yang P. Dou S. Qian L. Zhao Y. Yang N. Fanin Microplastics negatively affect soil fauna but stimulate microbial activity: insights from a field-based microplastic addition experiment Proc. R. Soc. B Biol. Sci. 287 2020 10.1098/rspb.2020.1268 Lin, D., Yang, G., Dou, P., Qian, S., Zhao, L., Yang, Y., Fanin, N. Microplastics negatively affect soil fauna but stimulate microbial activity: insights from a field-based microplastic addition experiment. Proc. R. Soc. B Biol. Sci., v. 287, 2020. doi:10.1098/rspb.2020.1268 Liu et al., 2019 Z. Liu P. Yu M. Cai D. Wu M. Zhang M. Chen Y. Zhao Effects of microplastics on the innate immunity and intestinal microflora of juvenile Eriocheir sinensis Sci. Total Environ. 685 2019 836 846 10.1016/j.scitotenv.2019.06.265 Liu, Z., Yu, P., Cai, M., Wu, D., Zhang, M., Chen, M., Zhao, Y. Effects of microplastics on the innate immunity and intestinal microflora of juvenile Eriocheir sinensis. Sci. Total Environ., v. 685, p. 836–846, 2019. doi: 10.1016/j.scitotenv.2019.06.265 Lu et al., 2018 L. Lu Z. Wan T. Luo Z. Fu Y. Jin Polystyrene microplastics induce gut microbiota dysbiosis and hepatic lipid metabolism disorder in mice Sci. Total Environ. 631–632 2018 449 458 10.1016/j.scitotenv.2018.03.051 Lu, L., Wan, Z., Luo, T., Fu, Z., Jin, Y. Polystyrene microplastics induce gut microbiota dysbiosis and hepatic lipid metabolism disorder in mice. Sci. Total Environ., 631–632, p. 449–458, 2018. doi: 10.1016/j.scitotenv.2018.03.051 Lu et al., 2019 L. Lu T. Luo Y. Zhao C. Cai Z. Fu Y. Jin Interaction between microplastics and microorganism as well as gut microbiota: a consideration on environmental animal and human health Sci. Total Environ. 667 2019 94 100 10.1016/j.scitotenv.2019.02.380 Lu, L., Luo, T., Zhao, Y., Cai, C., Fu, Z., Jin, Y. Interaction between microplastics and microorganism as well as gut microbiota: A consideration on environmental animal and human health. Sci. Total Environ., v. 667, p. 94–100, 2019. doi: 10.1016/j.scitotenv.2019.02.380 Lusher et al., 2013 A.L. Lusher M. McHugh R.C. Thompson Occurrence of microplastics in the gastrointestinal tract of pelagic and demersal fish from the English Channel Mar. Pollut. Bull. 67 2013 94 99 10.1016/j.marpolbul.2012.11.028 Lusher, A. L., McHugh, M., Thompson, R. C. Occurrence of microplastics in the gastrointestinal tract of pelagic and demersal fish from the English Channel. Mar. Pollut. Bull., v. 67, p. 94–99, 2013. doi:10.1016/j.marpolbul.2012.11.028 Machado et al., 2020 M.C. Machado G.V. Vimbela T.T. Silva-Oliveira A. Bose A. Tripathi The response of Synechococcus sp. PCC 7002 to micro-/nano polyethylene particles - investigation of a key anthropogenic stressor PLoS One 15 2020 1 14 10.1371/journal.pone.0232745 Machado, M.C., Vimbela, G. V., Silva-Oliveira, T.T., Bose, A., Tripathi, A. The response of Synechococcus sp. PCC 7002 to micro-/nano polyethylene particles - Investigation of a key anthropogenic stressor. PLoS One, v. 15, p. 1–14, 2020. doi:10.1371/journal.pone.0232745 Mak et al., 2019 C.W. Mak K. Ching-Fong Yeung K.M. Chan Acute toxic effects of polyethylene microplastic on adult zebrafish Ecotoxicol. Environ. Saf. 182 2019 1 10 10.1016/j.ecoenv.2019.109442 Mak, C. W., Ching-Fong Yeung, K., Chan, K. M. Acute toxic effects of polyethylene microplastic on adult zebrafish. Ecotoxicol. Environ. Saf., v. 182, p. 1–10, 2019. doi: 10.1016/j.ecoenv.2019.109442 Miao et al., 2019a L. Miao S. Guo Z. Liu S. Liu G. You H. Qu J. Hou Effects of nanoplastics on freshwater biofilm microbial metabolic functions as determined by BIOLOG ECO microplates Int. J. Environ. Res. Public Health 16 2019 8 10 10.3390/ijerph16234639 Miao, L., Guo, S., Liu, Z., Liu, S., You, G., Qu, H., Hou, J. Effects of nanoplastics on freshwater biofilm microbial metabolic functions as determined by BIOLOG ECO microplates. Int. J. Environ. Res. Public Health, v. 16, p. 8–10, 2019a. doi: 10.3390/ijerph16234639 Miao et al., 2019b L. Miao J. Hou G. You Z. Liu S. Liu T. Li Y. Mo S. Guo H. Qu Acute effects of nanoplastics and microplastics on periphytic biofilms depending on particle size, concentration and surface modification Environ. Pollut. 255 2019 1 8 10.1016/j.envpol.2019.113300 Miao, L., Hou, J., You, G., Liu, Z., Liu, S., Li, T., Mo, Y., Guo, S., Qu, H. Acute effects of nanoplastics and microplastics on periphytic biofilms depending on particle size, concentration and surface modification. Environmental Pollution, v. 255, p. 1-8, 2019b. doi:10.1016/j.envpol.2019.113300 Ng et al., 2018 E.L. Ng E. Huerta Lwanga S.M. Eldridge P. Johnston H.W. Hu V. Geissen D. Chen An overview of microplastic and nanoplastic pollution in agroecosystems Sci. Total Environ. 627 2018 1377 1388 10.1016/j.scitotenv.2018.01.341 Ng, E.L., Huerta Lwanga, E., Eldridge, S.M., Johnston, P., Hu, H.W., Geissen, V., Chen, D. An overview of microplastic and nanoplastic pollution in agroecosystems. Sci. Total Environ., v. 627, p. 1377–1388, 2018. doi:10.1016/j.scitotenv.2018.01.341 Oberbeckmann and Labrenz, 2020 S. Oberbeckmann M. Labrenz Marine microbial assemblages on microplastics: diversity, adaptation, and role in degradation Annu. Rev. Mar. Sci. 12 2020 209 232 10.1146/annurev-marine-010419-010633 Oberbeckmann, S. & Labrenz, M. Marine Microbial Assemblages on Microplastics: Diversity, Adaptation, and Role in Degradation. Ann. Rev. Mar. Sci., v. 12, p. 209–232, 2020. doi:10.1146/annurev-marine-010419-010633 Oberbeckmann et al., 2017 S. Oberbeckmann K. Kesy B. Kreikemeyer M. Labrenz Hitchhiking microorganisms on microplastics in the Baltic Sea Fate And Impact of Microplastics in Marine Ecosystems 72 2017 10.1016/b978-0-12-812271-6.00070-3 Oberbeckmann, S., Kesy, K., Kreikemeyer, B., & Labrenz, M. Hitchhiking Microorganisms on Microplastics in the Baltic Sea. Fate and Impact of Microplastics in Marine Ecosystems, v. 72, 2017. doi: 10.1016/b978-0-12-812271-6.00070-3 Paço et al., 2017 A. Paço K. Duarte J.P. da Costa P.S. Santos R. Pereira M.E. Pereira A.C. Freitas A.C. Duarte T.A. Rocha-Santos Biodegradation of polyethylene microplastics by the marine fungus Zalerion maritimum Sci. Total Environ. 586 2017 10 15 10.1016/j.scitotenv.2017.02.017 Paço, A., Duarte, K., da Costa, J. P., Santos, P. S., Pereira, R., Pereira, M. E., Freitas, A. C., Duarte, A. C., Rocha-Santos, T. A. Biodegradation of polyethylene microplastics by the marine fungus Zalerion maritimum. Sci. Total Environ., v. 586, p. 10–15, 2017. doi: 10.1016/j.scitotenv.2017.02.017 Pedà et al., 2016 C. Pedà L. Caccamo M.C. Fossi F. Gai F. Andaloro L. Genovese A. Perdichizzi T. Romeo G. Maricchiolo Intestinal alterations in European sea bass Dicentrarchus labrax (Linnaeus, 1758) exposed to microplastics: preliminary results Environ. Pollut. 212 2016 251 256 10.1016/j.envpol.2016.01.083 Pedà, C., Caccamo, L., Fossi, M. C., Gai, F., Andaloro, F., Genovese, L., Perdichizzi, A., Romeo, T., Maricchiolo, G. Intestinal alterations in European sea bass Dicentrarchus labrax (Linnaeus, 1758) exposed to microplastics: Preliminary results. Environmental Pollution, v. 212, p. 251–256, 2016. doi: 10.1016/j.envpol.2016.01.083 Prokić et al., 2019 M.D. Prokić T.B. Radovanović J.P. Gavrić C. Faggio Ecotoxicological effects of microplastics: examination of biomarkers, current state and future perspectives TrACTrends Anal. Chem. 111 2019 37 46 10.1016/j.trac.2018.12.001 Prokić, M. D., Radovanović, T. B., Gavrić, J. P., Faggio, C. Ecotoxicological effects of microplastics: Examination of biomarkers, current state and future perspectives. TrAC - Trends Anal. Chem., v. 111, p. 37–46, 2019. doi:10.1016/j.trac.2018.12.001 Qiao et al., 2019a R. Qiao Y. Deng S. Zhang M.B. Wolosker Q. Zhu H. Ren Y. Zhang Accumulation of different shapes of microplastics initiates intestinal injury and gut microbiota dysbiosis in the gut of zebrafish Chemosphere 236 2019 10.1016/j.chemosphere.2019.07.065 Qiao, R., Deng, Y., Zhang, S., Wolosker, M.B., Zhu, Q., Ren, H., Zhang, Y. Accumulation of different shapes of microplastics initiates intestinal injury and gut microbiota dysbiosis in the gut of zebrafish. Chemosphere, v. 236, 2019a. doi: 10.1016/j.chemosphere.2019.07.065 Qiao et al., 2019b R. Qiao C. Sheng Y. Lu Y. Zhang H. Ren B. Lemos Microplastics induce intestinal inflammation, oxidative stress, and disorders of metabolome and microbiome in zebrafish Sci. Total Environ. 662 2019 246 253 10.1016/j.scitotenv.2019.01.245 Qiao, R., Sheng, C., Lu, Y., Zhang, Y., Ren, H., Lemos, B. Microplastics induce intestinal inflammation, oxidative stress, and disorders of metabolome and microbiome in zebrafish. Science of the Total Environment, v. 662, p. 246-253, 2019b. doi:10.1016/j.scitotenv.2019.01.245 Qin et al., 2020 R. Qin C. Su W. Liu L. Tang X. Li X. Deng A. Wang Z. Chen Effects of exposure to polyether sulfone microplastic on the nitrifying process and microbial community structure in aerobic granular sludge Bioresour. Technol. 302 2020 10.1016/j.biortech.2020.122827 Qin, R., Su, C., Liu, W., Tang, L., Li, X., Deng, X., Wang, A., Chen, Z. Effects of exposure to polyether sulfone microplastic on the nitrifying process and microbial community structure in aerobic granular sludge. Bioresour. Technol., v. 302, 2020. doi: 10.1016/j.biortech.2020.122827 Ram and Kumar, 2020 B. Ram M. Kumar Correlation appraisal of antibiotic resistance with fecal, metal and microplastic contamination in a tropical Indian river, lakes and sewage Clean Water 3 2020 1 12 10.1038/s41545-020-0050-1 Ram, B. & Kumar, M. Correlation appraisal of antibiotic resistance with fecal, metal and microplastic contamination in a tropical Indian river, lakes and sewage. Clean Water, v. 3, p. 1–12, 2020. doi: 10.1038/s41545-020-0050-1 Ramadan et al., 2021 M.A. Ramadan M. Abdelgwad M.M. Fouad Predictive value of novel biomarkers for chronic kidney disease among workers occupationally exposed to silica Toxicol. Ind. Health x x 2021 10.1177/0748233721990304 Ramadan, M. A., Abdelgwad, M., Fouad, M. M. Predictive value of novel biomarkers for chronic kidney disease among workers occupationally exposed to silica. Toxicology and Industrial Health, v. x, n. x, 2021. doi: 10.1177/0748233721990304. Rillig, 2012 M.C. Rillig Microplastic in terrestrial ecosystems and the soil? Environ. Sci. Technol. 12 2012 6453 6454 10.1021/es302011r Rillig, M.C. Microplastic in terrestrial ecosystems and the soil? Environ. Sci. Technol., v. 12, p. 6453–6454, 2012. doi:10.1021/es302011r. Rillig and Bonkowski, 2018 M.C. Rillig M. Bonkowski Microplastic and soil protists: a call for research Environ. Pollut. p. 241 2018 1128 1131 10.1016/j.envpol.2018.04.147 Rillig, M. C. & Bonkowski, M. Microplastic and soil protists: A call for research. Environ. Pollut., p. 241, p. 1128–1131, 2018. doi: 10.1016/j.envpol.2018.04.147 Rinniella et al., 2019 E. Rinniella M. Cintoni P. Raoul L.R. Lopetuso F. Scaldaferri G. Pulcini G.A.D. Miggiano A. Gasbarrini M.C. Mele Food components and dietary habits: keys for a healthy gut microbiota composition Nutrients 11 2393 2019 1 23 10.3390/nu11102393 Rinniella, E., Cintoni, M., Raoul, P., Lopetuso, L.R., Scaldaferri, F., Pulcini, G., Miggiano, G.A.D., Gasbarrini, A., Mele, M.C. Food Components and Dietary Habits: Keys for a Healthy Gut Microbiota Composition Nutrients, v. 11, n. 2393, p. 1-23, 2019. doi:10.3390/nu11102393 Rochman et al., 2014 C.M. Rochman T. Kurobe I. Flores S.J. Teh Early warning signs of endocrine disruption in adult fish from the ingestion of polyethylene with and without sorbed chemical pollutants from the marine environment Sci. Total Environ. 493 2014 656 661 10.1016/j.scitotenv.2014.06.051 Rochman, C. M., Kurobe, T., Flores, I., Teh, S. J. Early warning signs of endocrine disruption in adult fish from the ingestion of polyethylene with and without sorbed chemical pollutants from the marine environment. Sci. Total Environ., v. 493, p. 656–661, 2014. doi: 10.1016/j.scitotenv.2014.06.051 Rodriguez-Seijo et al., 2017 A. Rodriguez-Seijo J. Lourenço T.A.P. Rocha-Santos J.D. Costa A.C. Duarte H. Vala R. Pereira Histopathological and molecular effects of microplastics in Eisenia andrei Bouché Environ. Pollut. 220 2017 495 503 10.1016/j.envpol.2016.09.092 Rodriguez-Seijo, A., Lourenço, J., Rocha-Santos, T. A. P., Costa, J. D., Duarte, A. C., Vala, H., Pereira, R. Histopathological and molecular effects of microplastics in Eisenia andrei Bouché. Environ. Pollut, v. 220, p. 495–503, 2017. doi: 10.1016/j.envpol.2016.09.092 Round and Mazmanian, 2009 J.L. Round S.K. Mazmanian The gut microbiota shapes intestinal immune responses during health and disease Nat. Rev. Immunol. 9 2009 313 323 10.1038/nri2515 Round, J. L. & Mazmanian, S. K. The gut microbiota shapes intestinal immune responses during health and disease. Nat. Rev. Immunol., v. 9, p. 313–323, 2009. doi: 10.1038/nri2515. Rummel et al., 2017 C.D. Rummel A. Jahnke E. Gorokhova D. Kühnel M. Schmitt-Jansen Impacts of biofilm formation on the fate and potential effects of microplastic in the aquatic environment Environ. Sci. Technol. Lett. 4 2017 258 267 10.1021/acs.estlett.7b00164 Rummel, C. D., Jahnke, A., Gorokhova, E., Kühnel, D., Schmitt-Jansen, M. Impacts of biofilm formation on the fate and potential effects of microplastic in the aquatic environment. Environ. Sci. Technol. Lett., v. 4, p. 258–267, 2017. doi: 10.1021/acs.estlett.7b00164 de Sá et al., 2018 L.C. de Sá M. Oliveira F. Ribeiro T.L. Rocha M.N. Futter Studies of the effects of microplastics on aquatic organisms: What do we know and where should we focus our efforts in the future? Sci. Total Environ. 645 2018 1029 1039 10.1016/j.scitotenv.2018.07.207 de Sá, L. C., Oliveira, M., Ribeiro, F., Rocha, T. L., Futter, M. N. Studies of the effects of microplastics on aquatic organisms: What do we know and where should we focus our efforts in the future? Sci. Total Environ., v. 645, p. 1029–1039, 2018. doi: 10.1016/j.scitotenv.2018.07.207 Sayes et al., 2007 C.M. Sayes K.L. Reed D.B. Warheit Assessing toxicity of fine and nanoparticles: comparing in vitro measurements to in vivo pulmonary toxicity profles Toxicol. Sci. 97 2007 163 180 10.1093/toxsci/kfm018 Sayes, C. M., Reed, K. L., Warheit, D. B. Assessing toxicity of fine and nanoparticles: comparing in vitro measurements to in vivo pulmonary toxicity profles. Toxicol. Sci., v. 97, p. 163–180, 2007. doi: 10.1093/toxsci/kfm018 Schnabl and Brenner, 2014 B. Schnabl D.A. Brenner Interactions between the intestinal microbiome and liver diseases Gastroenterology 146 6 2014 1513 1524 10.1053/j.gastro.2014.01.020 Schnabl, B., Brenner, D.A. Interactions Between the Intestinal Microbiome and Liver Diseases. Gastroenterology, v. 146, n. 6, p. 1513-1524, 2014. doi: 10.1053/j.gastro.2014.01.020 Schwabe and Jobin, 2013 R.F. Schwabe C. Jobin The microbiome and cancer Nat. Rev. Cancer 13 2013 800 812 10.1038/nrc3610 Schwabe, R. F., & Jobin, C. The microbiome and cancer. Nature Reviews Cancer, v. 13, p. 800–812, 2013. doi:10.1038/nrc3610 Seeley et al., 2020 M.E. Seeley B. Song R. Passie R.C. Hale Microplastics affect sedimentary microbial communities and nitrogen cycling Nat. Commun. 11 2020 1 10 10.1038/s41467-020-16235-3 Seeley, M.E., Song, B., Passie, R., Hale, R.C. Microplastics affect sedimentary microbial communities and nitrogen cycling. Nat. Commun. v. 11, p. 1–10, 2020. doi:10.1038/s41467-020-16235-3 Sekirov et al., 2010 I. Sekirov S.L. Russell M. Caetano L. Antunes B.B. Finlay Gut microbiota in health and disease Physiol. Rev. 90 2010 859 904 10.1152/physrev.00045.2009 Sekirov, I., Russell, S. L., Caetano M Antunes, L., Finlay, B. B. Gut microbiota in health and disease. Physiol. Rev., v. 90, p. 859–904, 2010. doi: 10.1152/physrev.00045.2009 Sun et al., 2018a M. Sun M. Ye W. Jiao Y. Feng P. Yu M. Liu J. Jiao X. He K. Liu Y. Zhao J. Wu X. Jiang F. Hu Changes in tetracycline partitioning and bacteria/phage-comediated ARGs in microplastic-contaminated greenhouse soil facilitated by sophorolipid J. Hazard. Mater. 345 2018 131 139 10.1016/j.jhazmat.2017.11.036 Sun, M., Ye, M., Jiao, W., Feng, Y., Yu, P., Liu, M., Jiao, J., He, X., Liu, K., Zhao, Y., Wu, J., Jiang, X., Hu, F. Changes in tetracycline partitioning and bacteria/phage-comediated ARGs in microplastic-contaminated greenhouse soil facilitated by sophorolipid. J. Hazard. Mater., v. 345, p. 131–139, 2018a. doi: 10.1016/j.jhazmat.2017.11.036 Sun et al., 2018b X. Sun B. Chen Q. Li N. Liu B. Xia L. Zhu K. Qu Toxicities of polystyrene nano- and microplastics toward marine bacterium Halomonas alkaliphila Sci. Total Environ. 642 2018 1378 1385 10.1016/j.scitotenv.2018.06.141 Sun, X., Chen, B., Li, Q., Liu, N., Xia, B., Zhu, L., Qu, K. Toxicities of polystyrene nano- and microplastics toward marine bacterium Halomonas alkaliphila. Science of the Total Environment, v. 642, p. 1978-1385, 2018b. doi: doi:10.1016/j.scitotenv.2018.06.141 Sussarellu et al., 2016 R. Sussarellu M. Suquet Y. Thomas C. Lambert C. Fabioux M.E.J. Pernet N.L. Goïc V. Quillien C. Mingant Y. Epelboin C. Corporeau J. Guyomarch J. Robbens I. Paul-Pont P. Soudant A. Huvet Oyster reproduction is affected by exposure to polystyrene microplastics Proc. Natl. Acad. Sci. U. S. A. 113 2016 2430 2435 10.1073/pnas.1519019113 Sussarellu, R., Suquet, M., Thomas, Y., Lambert, C., Fabioux, C., Pernet, M. E. J., Goïc, N. L., Quillien, V., Mingant, C., Epelboin, Y., Corporeau, C., Guyomarch, J., Robbens, J., Paul-Pont, I., Soudant, P., Huvet, A. Oyster reproduction is affected by exposure to polystyrene microplastics. Proc. Natl. Acad. Sci. USA, v. 113, p. 2430-2435, 2016. doi: 10.1073/pnas.1519019113 Tamboli et al., 2004 C.P. Tamboli C. Neut P. Desreumaux J.F. Colombel Dysbiosis in inflammatory bowel disease Gut 53 1 2004 1 4 10.1136/gut.53.1.1 Tamboli, C. P., Neut, C., Desreumaux, P., Colombel, J. F. Dysbiosis in inflammatory bowel disease. Gut, v. 53, n. 1, p. 1-4, 2004. doi: 10.1136/gut.53.1.1 Ursell et al., 2014 L.K. Ursell H.J. Haiser W. Van Treuren N. Garg L. Reddivari J. Vanamala P.C. Dorrestein P.J. Tumbaugh R. Knight The intestinal metabolome: an intersection between microbiota and host Gastroenterology 146 6 2014 1470 1476 10.1053/j.gastro.2014.03.001 Ursell, L. K., Haiser, H. J., Van Treuren, W., Garg, N., Reddivari, L., Vanamala, J., Dorrestein, P. C., Tumbaugh, P. J., Knight, R. The Intestinal Metabolome: An Intersection Between Microbiota and Host. Gastroenterology, v. 146, n.6, p. 1470–1476, 2014. doi: 10.1053/j.gastro.2014.03.001. Van Cauwenberghe et al., 2013 L. Van Cauwenberghe A. Vanreusel J. Mees C.R. Janssen Microplastic pollution in deep-sea sediments Environ. Pollut. 182 2013 495 499 10.1016/j.envpol.2013.08.013 Van Cauwenberghe, L., Vanreusel, A., Mees, J., Janssen, C. R. Microplastic pollution in deep-sea sediments. Environ. Pollut., v. 182, p. 495–499, 2013. doi: doi:10.1016/j.envpol.2013.08.013 Viršek et al., 2017 M.K. Viršek M.N. Lovšin Š. Koren A. Kržan M. Peterlin Microplastics as a vector for the transport of the bacterial fish pathogen species Aeromonas salmonicida Mar. Pollut. Bull. 125 2017 301 309 10.1016/j.marpolbul.2017.08.024 Viršek, M. K., Lovšin, M. N., Koren, Š., Kržan, A., Peterlin, M. Microplastics as a vector for the transport of the bacterial fish pathogen species Aeromonas salmonicida. Mar. Pollut. Bull., v. 125, p. 301–309, 2017. doi: 10.1016/j.marpolbul.2017.08.024 Wan et al., 2019 Z. Wan C. Wang J. Zhou M. Shen X. Wang Z. Fu Y. Jin Effects of polystyrene microplastics on the composition of the microbiome and metabolism in larval zebrafish Chemosphere 217 2019 646 658 10.1016/j.chemosphere.2018.11.070 Wan, Z., Wang, C., Zhou, J., Shen, M., Wang, X., Fu, Z., Jin, Y. Effects of polystyrene microplastics on the composition of the microbiome and metabolism in larval zebrafish. Chemosphere, v. 217, p. 646–658, 2019. doi: 10.1016/j.chemosphere.2018.11.070 Wang et al., 2014 J. Wang M. Huang Q. Wang Y. Sun Zhao Z. Yanran Y. Huang LDPE microplastics significantly alter the temporal turnover of soil microbial communities Sci. Total Environ. 726 2020 1 8 10.1016/j.scitotenv.2020.138682 Wang, J., Huang, M., Wang, Q., Sun, Y., Zhao, Yanran, Z., Huang, Y, 2020. LDPE microplastics significantly alter the temporal turnover of soil microbial communities. Science of the Total Environment, v. 726, p. 1-8. Watts et al., 2014 A.J.R. Watts C. Lewis R.M. Goodhead S.J. Beckett J. Moger C.R. Tyler Uptake and retention of microplastics by the shore crab Carcinus maenas Environ. Sci. Technol. 48 2014 8823 8830 10.1021/es501090e Watts, A. J. R., Lewis, C., Goodhead, R. M., Beckett, S. J., Moger, J., Tyler, C. R. Uptake and retention of microplastics by the shore crab Carcinus maenas. Environ. Sci. Technol., v. 48, p. 8823–8830, 2014. doi:10.1021/es501090e Weiss and Hennet, 2017 G.A. Weiss T. Hennet Mechanisms and consequences of intestinal dysbiosis Cell. Mol. Life Sci. 74 16 2017 2959 2977 10.1007/s00018-017-2509-x Weiss, G. A., Hennet, T. Mechanisms and consequences of intestinal dysbiosis. Cellular and Molecular Life Sciences, v. 74, n. 16, p. 2959–2977, 2017. doi:10.1007/s00018-017-2509-x. Wexler and Goodman, 2017 A.G. Wexler A.L. Goodman An insider's perspective: bacteroides as a window into the microbiome Nat. Microbiol. 2 5 2017 10.1038/nmicrobiol.2017.26 Wexler, A. G., & Goodman, A. L. An insider’s perspective: Bacteroides as a window into the microbiome. Nature Microbiology, v. 2, n. 5, 2017. doi:10.1038/nmicrobiol.2017.26. Wiedner and Polifka, 2020 K. Wiedner S. Polifka Effects of microplastic and microglass particles on soil microbial community structure in an arable soil (Chernozem) Soil 6 2020 315 324 10.5194/soil-6-315-2020 Wiedner, K., Polifka, S. Effects of microplastic and microglass particles on soil microbial community structure in an arable soil (Chernozem). Soil, v. 6, p. 315–324, 2020. doi:10.5194/soil-6-315-2020 Wright and Kelly, 2017 S.L. Wright F.J. Kelly Plastic and human health: a micro issue? Environ. Sci. Technol. 51 2017 6634 6647 10.1021/acs.est.7b00423 Wright, S. L. & Kelly, F. J. Plastic and Human Health: A Micro Issue? Environ. Sci. Technol., v. 51, p. 6634–6647, 2017. doi: 10.1021/acs.est.7b00423 Wright et al., 2013a S.L. Wright D. Rowe R.C. Thompson T.S. Galloway Microplastic ingestion decreases energy reserves in marine worms Curr. Biol. 23 2013 R1031 R1033 10.1016/j.cub.2013.10.068 Wright, S. L., Rowe, D., Thompson, R. C., Galloway, T. S. Microplastic ingestion decreases energy reserves in marine worms. Curr. Biol., v. 23, p. R1031–R1033, 2013a. doi:10.1016/j.cub.2013.10.068 Wright et al., 2013b S.L. Wright R.C. Thompson T.S. Galloway The physical impacts of microplastics on marine organisms: a review Environ. Pollut. 178 2013 483 492 10.1016/j.envpol.2013.02.031 Wright, S. L., Thompson, R. C., Galloway, T. S. The physical impacts of microplastics on marine organisms: a review. Environ. Pollut., v. 178, p. 483-492, 2013b. doi: 10.1016/j.envpol.2013.02.031 Xiao and Zhou, 2020 T. Xiao W. Zhou The third generation sequencing: the advanced approach to genetic diseases Transl. Pediatr. 9 2020 163 173 10.21037/tp.2020.03.06 Xiao, T. & Zhou, W. The third generation sequencing: the advanced approach to genetic diseases. Transl Pediatr, v. 9, p. 163-173, 2020. doi: 10.21037/tp.2020.03.06 Yan et al., 2020a W. Yan N. Hamid S. Deng P.P. Jia D.S. Pei Individual and combined toxicogenetic effects of microplastics and heavy metals (Cd, Pb, and Zn) perturb gut microbiota homeostasis and gonadal development in marine medaka (Oryzias melastigma) J. Hazard. Mater. 397 2020 122795 10.1016/j.jhazmat.2020.122795 Yan, W., Hamid, N., Deng, S., Jia, P.P., Pei, D.S. Individual and combined toxicogenetic effects of microplastics and heavy metals (Cd, Pb, and Zn) perturb gut microbiota homeostasis and gonadal development in marine medaka (Oryzias melastigma). J. Hazard. Mater. v. 397, 122795, 2020a. doi:10.1016/j.jhazmat.2020.122795 Yan et al., 2020b Y. Yan Z. Chen F. Zhu C. Zhu C. Wang C. Gu Effect of polyvinyl chloride microplastics on bacterial community and nutrient status in two agricultural soils Bull. Environ. Contam. Toxicol. 2020 10.1007/s00128-020-02900-2 Yan, Y., Chen, Z., Zhu, F., Zhu, C., Wang, C., Gu, C. Effect of Polyvinyl Chloride Microplastics on Bacterial Community and Nutrient Status in Two Agricultural Soils. Bull. Environ. Contam. Toxicol. 2020b. doi:10.1007/s00128-020-02900-2 Yang et al., 2020 L. Yang L. Lv H. Liu M. Wang Y. Sui Y. Wang Effects of ocean acidification and microplastics on microflora community composition in the digestive tract of the thick shell mussel Mytilus coruscus through 16S RNA gene sequencing Bull. Environ. Contam. Toxicol. 2020 10.1007/s00128-020-03022-5 Yang, L., Lv, L., Liu, H., Wang, M., Sui, Y., Wang, Y. Effects of Ocean Acidification and Microplastics on Microflora Community Composition in the Digestive Tract of the Thick Shell Mussel Mytilus coruscus Through 16S RNA Gene Sequencing. Bulletin of Environmental Contamination and Toxicology, 2020. doi:10.1007/s00128-020-03022-5 Yi et al., 2020 M. Yi S. Zhou L. Zhang S. Ding The effects of three different microplastics on enzyme activities and microbial communities in soil Water Environ. Res. 2020 0 3 10.1002/wer.1327 Yi, M., Zhou, S., Zhang, L. & Ding, S. The effects of three different microplastics on enzyme activities and microbial communities in soil. Water Environ. Res., p. 0–3, 2020. doi:10.1002/wer.1327. Zettler et al., 2013 E.R. Zettler T.J. Mincer L.A. Amaral-Zettler Life in the “plastisphere”: microbial communities on plastic marine debris Environ. Sci. Technol. 47 2013 7137 7146 10.1021/es401288x Zettler, E. R., Mincer, T. J., Amaral-Zettler, L. A. Life in the “plastisphere”: microbial communities on plastic marine debris. Environ. Sci. Technol., v. 47, p. 7137–7146, 2013. doi: 10.1021/es401288x. Zhang et al., 2020a J. Zhang D. Gao Q. Li Y. Zhao L. Li H. Lin Q. Bi Y. Zhao Biodegradation of polyethylene microplastic particles by the fungus Aspergillus flavus from the guts of wax moth Galleria mellonella Sci. Total Environ. 704 2020 10.1016/j.scitotenv.2019.135931 Zhang, J., Gao, D., Li, Q., Zhao, Y., Li, L., Lin, H., Bi, Q., Zhao, Y. Biodegradation of polyethylene microplastic particles by the fungus Aspergillus flavus from the guts of wax moth Galleria mellonella. Sci. Total Environ., v. 704, 2020a. doi: 10.1016/j.scitotenv.2019.135931 Zhang et al., 2020b Y. Zhang J. Lu J. Wu J. Wang Y. Luo Potential risks of microplastics combined with superbugs: enrichment of antibiotic resistant bacteria on the surface of microplastics in mariculture system Ecotoxicol. Environ. Saf. 187 2020 109852 10.1016/j.ecoenv.2019.109852 Zhang, Y., Lu, J., Wu, J., Wang, J. & Luo, Y. Potential risks of microplastics combined with superbugs: Enrichment of antibiotic resistant bacteria on the surface of microplastics in mariculture system. Ecotoxicol. Environ. Saf., v. 187, p. 109852, 2020b. doi: 10.1016/j.ecoenv.2019.109852 Zhu et al., 2018a D. Zhu Q. Chen X. An X. Yang P. Christie X. Ke L. Wu Y. Zhu Exposure of soil collembolans to microplastics perturbs their gut microbiota and alters their isotopic composition Soil Biol. Biochem. 116 2018 302 310 10.1016/j.soilbio.2017.10.027 Zhu, D., Chen, Q., An, X., Yang, X., Christie P., Ke, X., Wu, L., Zhu, Y. Exposure of soil collembolans to microplastics perturbs their gut microbiota and alters their isotopic composition. Soil Biology and Biochemistry, v. 116, p. 302-310, 2018a. doi: 10.1016/j.soilbio.2017.10.027 Zhu et al., 2018b B. Zhu Y. Fang D. Zhu P. Christie X. Ke Y. Zhu Exposure to nanoplastics disturbs the gut microbiome in the soil oligochaete Enchytraeus crypticus* Environ. Pollut. 239 2018 408 415 10.1016/j.envpol.2018.04.017 Zhu, B., Fang, Y., Zhu, D., Christie, P., Ke, X., Zhu, Y. Exposure to nanoplastics disturbs the gut microbiome in the soil oligochaete Enchytraeus crypticus*. Environmental Pollution, v. 239, p. 408-415, 2018b. doi: doi:10.1016/j.envpol.2018.04.017 "
    },
    {
        "doc_title": "A Robust 3D-Based Color Correction Approach for Texture Mapping Applications",
        "doc_scopus_id": "85125064447",
        "doc_doi": "10.3390/s22051730",
        "doc_eid": "2-s2.0-85125064447",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3d-modeling",
            "Color mapping function",
            "Colour corrections",
            "Correction approaches",
            "Image histograms",
            "Joint image histogram",
            "Mapping applications",
            "Multiple image",
            "Point-clouds",
            "Texture mapping"
        ],
        "doc_abstract": "© 2022 by the authorsLicensee MDPI, Basel, Switzerland.Texture mapping of 3D models using multiple images often results in textured meshes with unappealing visual artifacts known as texture seams. These artifacts can be more or less visible, depending on the color similarity between the used images. The main goal of this work is to produce textured meshes free of texture seams through a process of color correcting all images of the scene. To accomplish this goal, we propose two contributions to the state-of-the-art of color correction: a pairwise-based methodology, capable of color correcting multiple images from the same scene; the application of 3D information from the scene, namely meshes and point clouds, to build a filtering procedure, in order to produce a more reliable spatial registration between images, thereby increasing the robustness of the color correction procedure. We also present a texture mapping pipeline that receives uncorrected images, an untextured mesh, and point clouds as inputs, producing a final textured mesh and color corrected images as output. Results include a comparison with four other color correction approaches. These show that the proposed approach outperforms all others, both in qualitative and quantitative metrics. The proposed approach enhances the visual quality of textured meshes by eliminating most of the texture seams.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Editorial: Special issue on Autonomous Driving and Driver Assistance Systems — Some main trends",
        "doc_scopus_id": "85109560838",
        "doc_doi": "10.1016/j.robot.2021.103832",
        "doc_eid": "2-s2.0-85109560838",
        "doc_date": "2021-10-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2021-06-12 2021-06-12 2021-07-12 2021-07-12 2021-09-02T10:55:37 S0921-8890(21)00117-2 S0921889021001172 10.1016/j.robot.2021.103832 S300 S300.1 FULL-TEXT 2021-09-02T10:29:16.817797Z 0 0 20211001 20211031 2021 2021-06-12T04:38:12.168906Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav body acknowledge affil articletitle auth authfirstini authfull authlast grantnumber grantsponsor grantsponsorid pubtype ref 0921-8890 09218890 true 144 144 C Volume 144 7 103832 103832 103832 202110 October 2021 2021-10-01 2021-10-31 2021 simple-article edi © 2021 Published by Elsevier B.V. EDITORIALSPECIALISSUEAUTONOMOUSDRIVINGDRIVERASSISTANCESYSTEMSMAINTRENDS SANTOS V 1 Introduction 2 Trends in this special issue Perception Deep learning Planing, control and management 3 Conclusions Acknowledgments References RATO 2021 103714 D BERSANI 2021 103662 M ZHAO 2020 103597 X BARADARANKHALKHALI 2020 103596 M OLIVEIRA 2020 103558 M ABDENNOUR 2021 103707 N LOBIANCO 2020 103623 L ALMEIDA 2020 103605 T LIKMETA 2020 103568 A LUCET 2021 103706 E KANAPRAM 2020 103652 D FERNANDEZ 2020 103624 F QIN 2020 103606 Z SANTOSX2021X103832 SANTOSX2021X103832XV 2023-07-12T00:00:00.000Z 2023-07-12T00:00:00.000Z © 2021 Published by Elsevier B.V. 2021-07-14T22:52:35.066Z FCT UID/CEC/00127/2020 FCT Fundação para a Ciência e a Tecnologia Generalitat de Catalunya CIDIS-205-2020 FIEC-16-2018 Generalitat de Catalunya This work has been supported by: the Spanish Government under Project TIN2017-89723-P , the “ CERCA Programme/ Generalitat de Catalunya ”, the ESPOL projects CIDIS-205-2020 and FIEC-16-2018 , FCT — Foundation for Science and Technology , in the context of project UID/CEC/00127/2020 . The authors gratefully acknowledge the support of the CYTED Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” ( REF-518RT0559 ). 0 item S0921-8890(21)00117-2 S0921889021001172 10.1016/j.robot.2021.103832 271599 2021-09-02T10:29:16.817797Z 2021-10-01 2021-10-31 true 328924 MAIN 3 68383 849 656 IMAGE-WEB-PDF 1 ROBOT 103832 103832 S0921-8890(21)00117-2 10.1016/j.robot.2021.103832 Editorial Editorial: Special issue on Autonomous Driving and Driver Assistance Systems — Some main trends Vitor Santos IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Angel D. Sappa ⁎ 1Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador 1Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863 Guayaquil Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador 2Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Bellaterra, Barcelona, Spain 2Computer Vision Center, Universitat Autònoma de Barcelona Bellaterra Barcelona 08193 Spain Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain ⁎ Corresponding editor. Miguel Oliveira IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Arturo de la Escalera Universidad Carlos III de Madrid, Escuela Politécnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid), Spain Universidad Carlos III de Madrid, Escuela Politécnica Superior C/ Butarque, 15 - 28911 Leganes (Madrid) Spain Universidad Carlos III de Madrid, Escuela Politcnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid) Spain 1 Introduction This special issue covers several topics associated to the main point of Autonomous Driving and Driving Assistance Systems, and most papers span a wide range of concerns. Therefore, there is not a single set of categories where papers can be associated to. So, in order to simplify the overview, a simple clustering was done. Clearly, perception is the most common theme. A relevant focus is also the learning based techniques, especially the Deep Learning front, which serves mainly as a tool for several fields of the entire robotics research domain, sometimes to solve new problems and some other times to tackle classical issues of perception and navigation that still remain challenging. A third large scope of the papers cover topics like planning and control up to full systems management; that covers higher levels of software organization and the integration of complete or very large systems covering many fronts from perception, to data fusion and process and task management at the level of the Intelligent Transportation Systems (ITS) realm. So, the 13 papers of this special issue are divided primarily into these three groups: 1. Perception [1] [2] [3] [4] [5] 2. Deep Learning [6] [7] [8] [9] 3. Planing, control and management [10] [11] [12] [13] As mentioned, this categorization is not very strict since topics overlap and in some cases more than one of these topics is covered in the paper. The intended idea is mainly to mark some major trends observed in the Autonomous Driving community. 2 Trends in this special issue Next sections provide a further insight of each paper within its major category but where their remainder components are also addressed. Perception In this section, there is one paper that focuses on the classical problem of Simultaneous Localization and Mapping (SLAM) [3]. Other papers include the detection or tracking of vehicles, obstacles or the road [2,4] [1]. Perception is carried out using several different sensors, from RGB sensors [3,4] to LiDAR [1] and RaDAR [2]. The fusion of information from these sensors requires accurate estimations of the geometric transformations between them. This problem of the calibration of the sensors in intelligent vehicles is addressed in [5]. Rato and Santos propose a new methodology for the detection of road curbs based on the density of the accumulated point cloud generated by the motion of the vehicle equipped with a 4-Layer LiDAR. Results from the algorithm are compared with data annotated on top of satellite images, which show that the detection is accurate [1]. Khalkhali et al. [4] propose a method that combines Situation Assessment (SA) information extracted from surveillance cameras in a driving environment with Kalman Filter (KF) equations. The resulting modified model is able to track objects in different situation scenarios based on SA information and state estimation from KF. Results in video sequences from different datasets show an average 25 percent performance improvement in vehicle tracking [4]. Zhao et al. [3] address the problem of SLAM from a perspective of enhancing the robustness of the method to challenging situations such as large-baseline motion, textureless environments, and great illumination changes. Several improvements are proposed across the pipeline, including the usage of more robust visual features, online photometric calibration, and a multi-scale analysis. Results indicate that the system improves both the accuracy and robustness of localization. Bersani et al. [2] present an integrated algorithm for the estimation of ego-vehicle and obstacles’ positioning and motion along a given road, modeled in curvilinear coordinates. Data from RaDARs and a LiDAR is fused in order to identify and track obstacles. The algorithm is tested and validated in a prototype intelligent vehicle. Oliveira et al. [5] propose a novel framework for the calibration of multi-sensor and multi-modal intelligent vehicle platforms. The proposed methodology is general and can be used for any intelligent vehicle. Results include a case study where the calibration of the four sensors of and intelligent vehicle is carried out. Deep learning The usage of machine learning based solutions is growing in most of computer vision applications, in the current special issue four papers are deep learning-based approaches, which represent about the 30% of publications. Two of these papers are focused on road and objects detection through deep learning models while the remaining papers are focused on the driving topic as detailed next. Regarding the publications focused on road and objects detection both of them target the lane detection problem under different schemes. Almeida et al. [8] propose a new representation based on the usage of two simultaneous deep learning techniques for road and lane detection in the context of mixed scenarios (i.e., structured, unstructured, lane based, curb based limits, etc.). The proposed approach has been evaluated in five different road scenarios (e.g., roundabout entrance, highway scenario, road occlusion, etc.) achieving a better overall performance than each algorithm individually applied. The proposed approach allows to change the number of simultaneous algorithms used in the detection, each of the used algorithms can be updated or replaced by another with more confidence. In contrast to the approach presented above, Lo Bianco et al. [7] propose a single architecture for detecting both road lane and road participants (e.g., pedestrian, vehicles). The proposed multi-task approach takes advantage of detected features to reduce computational requirements, hence a real time performance is reached even in configurations with limited hardware. The proposed approach is validated through a newly generated public dataset that contains about 20K images of different real scenarios. Obtained results show both the validity of the proposed multi-task model for road applications as well as the good overall performance in a real-time computation, which make it suitable for on-board operations. On the other hand, the publications tackling the driving action propose deep learning based approaches for driver identification [6] and a decision-making strategy for autonomous driving [9]. In Abdennour et al. [6] the authors propose to identify driver by means of a lighweight deep learning model that is trained with just the CAN-Bus vehicle data. The proposed method requires less than two hours of training to achieve an accuracy higher than the 99%. The driving problem is also tackled in Likmeta et al. [9], but in this case for autonomous vehicles by proposing a high-level decision-making system. The authors propose a combination of traditional rule-based strategies together with a reinforcement learning (RL) model. The usage of handcrafted rule-based controllers allows to always determine why a given decision was made; on the other hand the RL architecture enable to deal with complex scenarios, which are usually difficult to interpret. The best features of each approach are combined by designing parametric rule-based controllers, where rules can be provided by domain experts and their parameters are learned via RL. The manuscript presents an extensive numerical simulation in both highways and urban scenarios showing the validity of proposed approach. Planing, control and management There are two papers concerning issues related to parking systems: [10] [13], one for abnormality detection based on Self-awareness [11], and the last one shows how to use a cognitive multi-layer map to develop collaborative human–machine systems [12]. Lucet et al. [10] present an autonomous bus navigation and parking system in a bus depot. Several peculiarities make this case harder than cars: the environment is a confined area, dimensions and weight higher and the need of a better accuracy. The authors use a predictive controller, based on its model linearized around the changing path curvature value, to perform accurate curved paths tracking. The controller has been implemented in an industrial vehicle and tested under realistic conditions. Quin et al. [13] present an Automated Valet Parking system. The methodology is based on the directional graph search and it is divided into three parts: global path planning, path coordination strategy and parking path planning. The global path planning uses a directional Hybrid A* algorithm. The path coordination strategy gives a transitional path to connect the end node of the global path to the parking planning start node. The parking path planning generates a parking path to guide the vehicle using a modified C-type vertical parking path planning algorithm. The system is validated using simulations on Matlab and PreScan. Kanaram et al. [11] use self-awareness for the detection of abnormalities instead of a manually programmed set of nested conditions checking for their presence. Multi-sensory time-series data are used to develop Dynamic Bayesian Network models used for state prediction and the detection of abnormalities. Real experiments of autonomous vehicles performing various tasks under real conditions are used to develop and test the algorithm. Fernandez et al. [12] introduce a cognitive layer called Associated Reality to enhance the information, knowledge and communication processes needed for Advanced Driver Assistance Systems (ADAS) and Automated and Autonomous Vehicles. The proposed architecture includes an augmented Local Dynamic Map and an augmented Graph Database. They show its use for vehicle localization and mapping in road tunnels. 3 Conclusions For some time now, it is clear that the Autonomous Driving challenges and the Driver Assistance Systems share similar tools to a common purpose, which is essentially to ensure the safety of drivers, passengers and all agents in the road. Perception continues to be the most common topic, which is clear from the proportion of papers in this special issue which are devoted to this topic. Multi-modality seems to be a trend too, and issues of calibration in multi-sensorial and multi-modality setups are also found as relevant concerns by authors. After perception, authors address the problem of modeling and controlling vehicles and related systems, both on board or on the road. The classical approaches, especially in planning and controlling are still exploited and improved by authors, but it is undeniable that learning techniques, mainly Deep Learning, seem to have established as critical tools, mainly in perception for classification, with very good results in semantic segmentation of images, which used to be a huge challenge some years ago. As a final corollary, and to bring the challenge to higher levels, we also find works, usually based on large projects, that combine many of the tools described an others to propose entire setups and demonstrators, ready to merge in full featured Intelligent Transportation Systems. We have not yet observed abundant full end-to-end systems, but there is a feeling that the path is being left open to that, perhaps to be covered in future versions of these special issues in ADDAS. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work has been supported by: the Spanish Government under Project TIN2017-89723-P, the “CERCA Programme/ Generalitat de Catalunya”, the ESPOL projects CIDIS-205-2020 and FIEC-16-2018, FCT — Foundation for Science and Technology, in the context of project UID/CEC/00127/2020. The authors gratefully acknowledge the support of the CYTED Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” (REF-518RT0559). References [1] Rato D. Santos V. Lidar based detection of road boundaries using the density of accumulated point clouds and their gradients Robot. Auton. Syst. 138 2021 103714 D. Rato, V. Santos, Lidar based detection of road boundaries using the density of accumulated point clouds and their gradients, Robotics and Autonomous Systems 138 (2021) 103714. [2] Bersani M. Mentasti S. Dahal P. Arrigoni S. Vignati M. Cheli F. Matteucci M. An integrated algorithm for ego-vehicle and obstacles state estimation for autonomous driving Robot. Auton. Syst. 139 2021 103662 M. Bersani, S. Mentasti, P. Dahal, S. Arrigoni, M. Vignati, F. Cheli, M. Matteucci, An integrated algorithm for ego-vehicle and obstacles state estimation for autonomous driving, Robotics and Autonomous Systems 139 (2021) 103662. [3] Zhao X. Liu L. Zheng R. Ye W. Liu Y. A robust stereo feature-aided semi-direct slam system Robot. Auton. Syst. 132 2020 103597 X. Zhao, L. Liu, R. Zheng, W. Ye, Y. Liu, A robust stereo feature-aided semi-direct slam system, Robotics and Autonomous Systems 132 (2020) 103597. [4] Baradaran Khalkhali M. Vahedian A. Sadoghi Yazdi H. Vehicle tracking with Kalman filter using online situation assessment Robot. Auton. Syst. 131 2020 103596 M. Baradaran Khalkhali, A. Vahedian, H. Sadoghi Yazdi, Vehicle tracking with kalman filter using online situation assessment, Robotics and Autonomous Systems 131 (2020) 103596. [5] Oliveira M. Castro A. Madeira T. Pedrosa E. Dias P. Santos V. A ros framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Robot. Auton. Syst. 131 2020 103558 M. Oliveira, A. Castro, T. Madeira, E. Pedrosa, P. Dias, V. Santos, A ros framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach, Robotics and Autonomous Systems 131 (2020) 103558. [6] Abdennour N. Ouni T. Amor N.B. Driver identification using only the can-bus vehicle data through an rcn deep learning approach Robot. Auton. Syst. 136 2021 103707 N. Abdennour, T. Ouni, N. B. Amor, Driver identification using only the can-bus vehicle data through an rcn deep learning approach, Robotics and Autonomous Systems 136 (2021) 103707. [7] Lo Bianco L.C. Beltrán J. López G.F. García F. Al-Kaff A. Joint semantic segmentation of road objects and lanes using convolutional neural networks Robot. Auton. Syst. 133 2020 103623 L. C. Lo Bianco, J. Beltrán, G. F. López, F. García, A. Al-Kaff, Joint semantic segmentation of road objects and lanes using convolutional neural networks, Robotics and Autonomous Systems 133 (2020) 103623. [8] Almeida T. Lourenco B. Santos V. Road detection based on simultaneous deep learning approaches Robot. Auton. Syst. 133 2020 103605 T. Almeida, B. Lourenço, V. Santos, Road detection based on simultaneous deep learning approaches, Robotics and Autonomous Systems 133 (2020) 103605. [9] Likmeta A. Metelli A.M. Tirinzoni A. Giol R. Restelli M. Romano D. Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving Robot. Auton. Syst. 131 2020 103568 A. Likmeta, A. M. Metelli, A. Tirinzoni, R. Giol, M. Restelli, D. Romano, Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving, Robotics and Autonomous Systems 131 (2020) 103568. [10] Lucet E. Micaelli A. Russotto F.-X. Accurate autonomous navigation strategy dedicated to the storage of buses in a bus center Robot. Auton. Syst. 136 2021 103706 E. Lucet, A. Micaelli, F.-X. Russotto, Accurate autonomous navigation strategy dedicated to the storage of buses in a bus center, Robotics and Autonomous Systems 136 (2021) 103706. [11] Kanapram D.T. Marin-Plaza P. Marcenaro L. Martin D. de la Escalera A. Regazzoni C. Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection Robot. Auton. Syst. 134 2020 103652 D. T. Kanapram, P. Marin-Plaza, L. Marcenaro, D. Martin, A. de la Escalera, C. Regazzoni, Self-awareness in intelligent vehicles: Feature based dynamic bayesian models for abnormality detection, Robotics and Autonomous Systems 134 (2020) 103652. [12] Fernandez F. Sanchez A. Velez J.F. Moreno B. Associated reality: A cognitive human–machine layer for autonomous driving Robot. Auton. Syst. 133 2020 103624 F. Fernandez, A. Sanchez, J. F. Velez, B. Moreno, Associated reality: A cognitive human–machine layer for autonomous driving, Robotics and Autonomous Systems 133 (2020) 103624. [13] Qin Z. Chen X. Hu M. Chen L. Fan J. A novel path planning methodology for automated valet parking based on directional graph search and geometry curve Robot. Auton. Syst. 132 2020 103606 Z. Qin, X. Chen, M. Hu, L. Chen, J. Fan, A novel path planning methodology for automated valet parking based on directional graph search and geometry curve, Robotics and Autonomous Systems 132 (2020) 103606. "
    },
    {
        "doc_title": "A general approach to hand-eye calibration through the optimization of atomic transformations",
        "doc_scopus_id": "85103787190",
        "doc_doi": "10.1109/TRO.2021.3062306",
        "doc_eid": "2-s2.0-85103787190",
        "doc_date": "2021-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Atomic transformation",
            "Calibration algorithm",
            "Calibration problems",
            "Calibration procedure",
            "Initial configuration",
            "Nonlinear least squares methods",
            "Optimization procedures",
            "Robot operating system"
        ],
        "doc_abstract": "© 2021 IEEE.This article proposes a general approach to solve the hand-eye calibration problem. The system is general since it is able to calibrate any number of cameras and, moreover, is able to simultaneously perform the calibration of several instances of the two common hand-eye calibration use cases: eye-on-hand and eye-to-base. The calibration is solved with a nonlinear least squares method, and the reprojection error is used as a metric to guide the optimization procedure. Our approach is seamlessly integrated with the robot operating system framework and allows for the interactive positioning of sensors and labeling of data, facilitating both the data acquisition and labeling and the calibration procedures. Results show that the proposed approach is able to handle any calibration use case with a minimal initial configuration. The approach is compared with several other state-of-the-art hand-eye calibration algorithms. Results show that the proposed approach produces very accurate calibrations when compared to the state of the art.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Camera to LiDAR calibration approach through the optimization of atomic transformations",
        "doc_scopus_id": "85103644407",
        "doc_doi": "10.1016/j.eswa.2021.114894",
        "doc_eid": "2-s2.0-85103644407",
        "doc_date": "2021-08-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Atomic transformation",
            "Detection sensors",
            "Geometric optimization",
            "Light detection and ranging",
            "Multiple cameras",
            "Multiple sensors",
            "Ranging sensors",
            "Sensor modality",
            "State of the art"
        ],
        "doc_abstract": "© 2021 Elsevier LtdThis paper proposes a camera-to-3D Light Detection And Ranging calibration framework through the optimization of atomic transformations. The system is able to simultaneously calibrate multiple cameras with Light Detection And Ranging sensors, solving the problem of Bundle. In comparison with the state-of-the-art, this work presents several novelties: the ability to simultaneously calibrate multiple cameras and LiDARs; the support for multiple sensor modalities; the calibration through the optimization of atomic transformations, without changing the topology of the input transformation tree; and the integration of the calibration framework within the Robot Operating System (ROS) framework. The software pipeline allows the user to interactively position the sensors for providing an initial estimate, to label and collect data, and visualize the calibration procedure. To test this framework, an agricultural robot with a stereo camera and a 3D Light Detection And Ranging sensor was used. Pairwise calibrations and a single calibration of the three sensors were tested and evaluated. Results show that the proposed approach produces accurate calibrations when compared to the state-of-the-art, and is robust to harsh conditions such as inaccurate initial guesses or small amount of data used in calibration. Experiments have shown that our optimization process can handle an angular error of approximately 20 degrees and a translation error of 0.5 meters, for each sensor. Moreover, the proposed approach is able to achieve state-of-the-art results even when calibrating the entire system simultaneously.",
        "available": true,
        "clean_text": "serial JL 271506 291210 291817 291820 291862 291866 291870 291883 31 Expert Systems with Applications EXPERTSYSTEMSAPPLICATIONS 2021-03-18 2021-03-18 2021-04-02 2021-04-02 2021-09-11T14:55:41 S0957-4174(21)00335-3 S0957417421003353 10.1016/j.eswa.2021.114894 S300 S300.1 FULL-TEXT 2021-09-11T14:12:26.883148Z 0 0 20210815 2021 2021-03-18T13:20:21.732409Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref 0957-4174 09574174 true 176 176 C Volume 176 47 114894 114894 114894 20210815 15 August 2021 2021-08-15 2021 article fla © 2021 Elsevier Ltd. All rights reserved. ACAMERALIDARCALIBRATIONAPPROACHTHROUGHOPTIMIZATIONATOMICTRANSFORMATIONS PINTODEAGUIAR A 1 Introduction 2 Proposed approach 2.1 Objective function 2.1.1 Camera modality sub-function 2.1.2 3D LiDAR sub-function 2.2 Normalization of multi-modal residuals 3 Calibration framework 3.1 Calibration configuration 3.2 Initial parameter estimation 3.3 Labeling data 3.4 Collecting data 3.5 Visualizing the optimization 4 Results 4.1 Methodology 4.2 Metrics 4.3 Evaluation 4.4 Impact of the number of collections used for training 4.5 Impact of the number of incomplete collections used for training 4.6 Impact of the accuracy of the initial estimate 5 Conclusions CRediT authorship contribution statement Acknowledgements References AGARWAL 2010 29 42 S COMPUTERVISIONECCV2010 BUNDLEADJUSTMENTINLARGE DEAGUIAR 2020 105535 A ALVAREZ 2014 1532 1542 S BADUE 2021 113816 C BESL 1992 239 256 P BRADSKI 2000 120 125 G DURRANTWHYTE 2006 99 110 H FABBRI 2020 1 R IEEETRANSACTIONSPATTERNANALYSISMACHINEINTELLIGENCE CAMERAPOSEESTIMATIONUSINGFIRSTORDERCURVEDIFFERENTIALGEOMETRY FISCHLER 1981 381 395 M FOOTE 2013 T 2013IEEECONFERENCETECHNOLOGIESFORPRACTICALROBOTAPPLICATIONSTEPRA TFTRANSFORMLIBRARY FREMONT 2008 V 2008IEEEINTERNATIONALCONFERENCEMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMS EXTRINSICCALIBRATIONBETWEENAMULTILAYERLIDARACAMERA FURGALE 2013 P 2013IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS UNIFIEDTEMPORALSPATIALCALIBRATIONFORMULTISENSORSYSTEMS GAO 2003 930 943 X GARRIDOJURADO 2016 481 491 S GIRSHICK 2006 57 68 M SELECTEDPAPERSFREDERICKMOSTELLER UNBIASEDESTIMATESFORCERTAINBINOMIALSAMPLINGPROBLEMSAPPLICATIONS GUINDEL 2017 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS GUINDEL 2017 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS HORNEGGER 1999 J PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISION REPRESENTATIONISSUESINMLESTIMATIONCAMERAMOTION HU 2019 D 2019IEEECVFCONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR DEEPCHARUCODARKCHARUCOMARKERPOSEESTIMATION HUANG 2020 134101 134110 J HUANG 2009 L 2009IEEEINTELLIGENTVEHICLESSYMPOSIUM ANOVELMULTIPLANARLIDARCOMPUTERVISIONCALIBRATIONPROCEDUREUSING2DPATTERNSFORAUTOMATEDNAVIGATION SUKIM 2019 52 E LIAO 2017 Y 20172NDINTERNATIONALCONFERENCEADVANCEDROBOTICSMECHATRONICSICARM JOINTKINECTMULTIPLEEXTERNALCAMERASSIMULTANEOUSCALIBRATION MAJUMDER 2018 165 172 S MELENDEZPASTOR 2017 28 38 C MIRZAEI 2012 452 467 F OLIVEIRA 2020 103558 M PANDEY 2010 336 341 G DEPAULA 2014 1997 2007 M PENATESANCHEZ 2013 2387 2400 A PRADEEP 2014 211 225 V EXPERIMENTALROBOTICS CALIBRATINGAMULTIARMMULTISENSORROBOTABUNDLEADJUSTMENTAPPROACH REHDER 2016 383 398 J ROMERORAMIREZ 2018 38 47 F DOSSANTOS 2016 429 444 F SANTOS 2019 684 698 L SARIFF 2006 N 20064THSTUDENTCONFERENCERESEARCHDEVELOPMENT OVERVIEWAUTONOMOUSMOBILEROBOTPATHPLANNINGALGORITHMS STURM 2014 610 613 P COMPUTERVISIONAREFERENCEGUIDE PINHOLECAMERAMODEL VERMA 2019 3906 3912 S 2019IEEEINTELLIGENTTRANSPORTATIONSYSTEMSCONFERENCEITSC AUTOMATICEXTRINSICCALIBRATIONBETWEENACAMERAA3DLIDARUSING3DPOINTPLANECORRESPONDENCES WANG 2017 851 W ZHOU 2018 L 2018IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS AUTOMATICEXTRINSICCALIBRATIONACAMERAA3DLIDARUSINGLINEPLANECORRESPONDENCES ZUNIGANOEL 2019 2862 2869 D PINTODEAGUIARX2021X114894 PINTODEAGUIARX2021X114894XA 2023-04-02T00:00:00.000Z 2023-04-02T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 2021-06-03T23:22:39.891Z FCT-Foundation for Science and Technology DFA/BD/5318/2020 FST Foundation for Science and Technology André Silva Pinto de Aguiar thanks the FCT-Foundation for Science and Technology, Portugal for the Ph.D. Grant DFA/BD/5318/2020. 0 item S0957-4174(21)00335-3 S0957417421003353 10.1016/j.eswa.2021.114894 271506 2021-09-11T14:12:26.883148Z 2021-08-15 true 9751065 MAIN 16 57422 849 656 IMAGE-WEB-PDF 1 gr13 45835 216 756 gr11 28097 274 677 gr12 59533 303 538 gr8 27086 384 378 gr9 42489 272 378 gr10 58390 207 713 gr1 29720 318 557 gr2 55394 479 355 gr3 38297 253 622 gr4 37647 227 623 gr5 26654 299 579 gr6 40044 273 623 gr7 40265 371 623 gr13 5245 63 219 gr11 5067 89 219 gr12 23664 123 219 gr8 5563 164 161 gr9 31085 157 219 gr10 12344 64 219 gr1 7208 125 219 gr2 18248 163 121 gr3 8216 89 219 gr4 8040 80 219 gr5 6728 113 219 gr6 11125 96 219 gr7 8820 131 219 gr13 351003 956 3346 gr11 183510 1214 2997 gr12 236019 859 1526 gr8 85285 1089 1071 gr9 138857 770 1071 gr10 390325 917 3155 gr1 178096 1411 2468 gr2 186632 1360 1008 gr3 285181 1120 2756 gr4 283146 1007 2760 gr5 154239 1324 2563 gr6 333409 1212 2761 gr7 305954 1645 2760 si1 1410 si10 2986 si100 5085 si101 3098 si102 5085 si103 11404 si104 26868 si105 2558 si106 1532 si107 1801 si108 4612 si109 4612 si11 2695 si110 8384 si111 9023 si112 20021 si113 10650 si114 3851 si115 9840 si116 10690 si117 1605 si118 10873 si119 14075 si12 5109 si120 10023 si121 22946 si122 25518 si123 2953 si124 12549 si125 7059 si126 9217 si127 20950 si128 4004 si129 10219 si13 2691 si130 17821 si131 5303 si132 5414 si133 7411 si134 6972 si135 12531 si136 7930 si137 7827 si138 16004 si139 14295 si14 2550 si140 23792 si141 16827 si142 18205 si143 15700 si144 9279 si145 11992 si15 2986 si16 2695 si17 5109 si18 2691 si19 2691 si2 1547 si20 2986 si21 2987 si22 5110 si23 3561 si24 1577 si25 1577 si26 1577 si27 1577 si28 7752 si29 3636 si3 1604 si30 5795 si31 31502 si32 13339 si33 4040 si34 3746 si35 4040 si36 3238 si37 13902 si38 6183 si39 1577 si4 14074 si40 52652 si41 1986 si42 8772 si43 7253 si44 1342 si45 1532 si46 8310 si47 1801 si48 8877 si49 3673 si5 2691 si50 4440 si51 3673 si52 3673 si53 23103 si54 4440 si55 4557 si56 5742 si57 18062 si58 1657 si59 3051 si6 2550 si60 2420 si61 2953 si62 3474 si63 12159 si64 5856 si65 3474 si66 8631 si67 14386 si68 1797 si69 1532 si7 2986 si70 20483 si71 35017 si72 14473 si73 3043 si74 1801 si75 4612 si76 6993 si77 1532 si78 1801 si79 5742 si8 2695 si80 7284 si81 2953 si82 18315 si83 19115 si84 1412 si85 1412 si86 1549 si87 7284 si88 15206 si89 4609 si9 5109 si90 8698 si91 37769 si92 11842 si93 18597 si94 9944 si95 3272 si96 4819 si97 13888 si98 5861 si99 27245 am false 7565333 ESWA 114894 114894 S0957-4174(21)00335-3 10.1016/j.eswa.2021.114894 Elsevier Ltd Fig. 1 (a) AgRob V16 model and the respective referential frames represented as red-green–blue axes. (b): Transformation tree for AgRob V16 robotic platform. The majority of the frames not to be calibrated were omitted for simplicity. Each sensor has an associated atomic transformation, denoted by the solid edges. Dashed edges denote transformations that are not optimized (they can be static or dynamic). Each sensor has a corresponding link to which the data it collects is attached, denoted in the figure by solid thin ellipses. Very few approaches in the literature are capable of calibrating such a system while preserving the initial structure of the transformation graph. Fig. 2 Difference between the initial position of the pattern corners, and the final position of these same projected points, after the optimization has been completed. Squares denote the position of the detected pattern corners; crosses denote the initial position of each projected corner; points denote the current position of the projected points. Fig. 3 Two examples of the pattern boundary points extraction from LiDAR data. The colored points represent the clustered LiDAR scans considering the spherical component θ , and the crosses represent the boundary points extracted using the maximum and minimum values of the spherical component ϕ , for each cluster. Fig. 4 (a): calibration pattern and respective ground truth boundary points represented as blue lines; (b): misaligned boundary points observed by the 3D LiDAR with the ground-truth points at the start of the calibration procedure; (c): optimization result - ground-truth points and projected boundary points aligned. It is noteworthy that the orthogonal distance aligned the z coordinate of the ground-truth pattern and 3D LiDAR points. Fig. 5 Example of the developed interactive tool operation for AgRob V16 system. Here, the reference frames of the cameras that compose the stereo camera system and the 3D LiDAR sensor were moved. When the user positions the sensors in the desired pose, the initial estimate for the pose of each sensor is saved to use in calibration. Fig. 6 (a): labeling image data using a charuco pattern; (b): labeling 3D LiDAR data on pattern (solid blue points) using the semi-automatic proposed approach. Note that, our approach works with partial detections, i.e., collections of data where the pattern is not fully detected. This figure presents an example of an partial detection for each type of sensor. For the camera, only a portion of the corners of the pattern were detected. For the 3D LiDAR, the pattern is not fully observable due to the low vertical resolution. Even though, our approach is able to calibrate the system with these detections. Fig. 7 (a): ROS calibration configuration - simultaneous visualization of all the collections of data and respective alignment between ground-truth points and labeled points; (b): graphics representing the objective function minimization - individual residuals value, and the total error vs iterations. Fig. 8 Chain of transformations representing the generic configuration to extract an atomic transformation from a sensor to sensor calibration. Preserving the chain of transformation of the anchored sensor, and taking into account the sensor to sensor calibration T ̂ , we recover the atomic transformation child 1 T parent 1 . Fig. 9 Camera reprojection error from one camera image into the other. Squares represent the expected corner pixel coordinates, and crosses the projected result. Fig. 10 3D LiDAR to camera reprojection error metric calculation. (a) represents the annotation procedure, where four classes are annotated, each one representing a single side of the pattern; (b) red curves represent the approximation of each one of the classes by a polynomial function, in order to account for the distortion in the image; (c) reprojection error calculation - blue dots represent the 3D LiDAR pattern boundary reprojected points and yellow lines the difference of each one of the points with the labelled ground truth. Fig. 11 Reprojection error dispersion for the camera to camera calibration using ATOM full configuration. Each color represents the error associated with one individual collection. (a) is the representation of this error before calibrating (using the initial guess), and (b) after the calibration. Fig. 12 Point cloud projection into the left camera image using the 3D LiDAR to camera calibration. The color represents the points depth. It is worth noting that, this procedure was done using the ATOM full calibration result, with a collection from the test dataset, just like in all the evaluation pipeline. If the system is accuratly calibrated, changes in point’s color, which denotes a variation of the measured range, should coincide with transitions between far and near objects in the image. Fig. 13 Impact of the initial angle (a) and distance (b) estimation error on the optimization error and execution time. The considered optimization error is the root means squared (RMS) error. Table 1 Description of the datasets used for train and evaluation. Two datasets were used for training (calibration) and one for testing (evaluation). The datasets contain incomplete collections, i.e., collections were the pattern is not detected by at least one sensor, and partial collections, i.e., collections were the pattern is partially detected by at least one sensor. Dataset Nr. Collections Observations Total Incomplete cPartial train-1 42 5 c38 Dataset contains incomplete collections. train-2 56 0 c24 The dataset does not contain the point cloud generated by the ZED camera. test-3 15 0 c8 Dataset with low number of collections, only used for test. Table 2 Summary of the methods used and evaluated in the experiments. Method Calibration Properties OpenCV (Bradski, 2000) pairwise reprojection error, intrinsics calibration Stereo camera factory calibration pairwise reprojection error, intrinsics calibrations ICP average (Besl & McKay, 1992) pairwise reprojection error, average of result of all collections ICP best (Besl & McKay, 1992) pairwise reprojection error, best result of all collections ATOM pairwise [this paper] pairwise reprojection error, angle-axis, intrinsics calibration ATOM full [this paper] full calibration reprojection error, angle-axis, intrinsics calibration Table 3 Performance comparison of methods for camera to camera calibration. Method Train dataset e R (rad) e t (m) e x (px) e y (px) e rms (px) OpenCV (Bradski, 2000) train-1 Not able to calibrate due to partial pattern detections. sATOM pairwise 0.009 0.003 0.551 ± 0.800 0.780 ± 1.090 1.049 ATOM full 0.008 0.003 0.547 ± 0.759 0.638 ± 1.034 0.974 OpenCV (Bradski, 2000) train-2 0.006 0.003 0.582 ± 0.648 0.622 ± 0.966 0.863 ATOM pairwise 0.010 0.006 0.655 ± 1.055 0.677 ± 0.982 1.020 ATOM full 0.008 0.005 0.594 ± 0.912 0.696 ± 1.033 0.974 ZED’s factory calibration - 0.007 0.006 2.220 ± 0.765 0.486 ± 0.823 1.757 Table 4 Performance comparison of methods for camera to 3D LiDAR calibration. Method Type Train dataset e x (px) e y (px) e rms (px) ICP average (Besl & McKay, 1992) left camera - 3D LiDAR train-1 47.210 ± 31.374 19.058 ± 28.233 44.307 ICP best (Besl & McKay, 1992) left camera - 3D LiDAR 9.111 ± 11.950 2.625 ± 7.967 10.492 ATOM pairwise right camera - 3D LiDAR 3.054 ± 4.727 1.031 ± 2.689 3.869 ATOM pairwise left camera - 3D LiDAR 3.648 ± 4.846 1.260 ± 2.869 4.101 ATOM full right camera - 3D LiDAR 3.351 ± 4.874 0.950 ± 2.279 3.811 ATOM full left camera - 3D LiDAR 3.398 ± 4.923 1.100 ± 2.602 3.942 ICP average (Besl & McKay, 1992) left camera - 3D LiDAR train-2 - - - ICP best (Besl & McKay, 1992) left camera - 3D LiDAR - - - ATOM pairwise right camera - 3D LiDAR 7.574 ± 6.393 1.776 ± 3.181 6.715 ATOM pairwise left camera - 3D LiDAR 7.560 ± 5.535 1.619 ± 2.795 6.432 ATOM full right camera - 3D LiDAR 7.702 ± 5.441 1.648 ± 2.781 6.537 ATOM full left camera - 3D LiDAR 8.117 ± 5.692 1.687 ± 2.838 6.765 Table 5 Impact of the number of collections in ATOM’s full calibration performance. The dashed entries correspond to results not generated since, for the camera-to-LiDAR case, the mean rotation and translation errors are not available. Type Nr. Collections e R (rad) e t (rad) e x (px) e y (px) e rms (px) camera - camera 1 0.051 0.053 8.205 ± 7.201 16.089 ± 3.346 13.534 5 0.017 0.016 3.540 ± 4.575 1.108 ± 1.449 4.233 10 0.009 0.004 1.377 ± 1.318 0.932 ± 0.991 1.645 20 0.008 0.003 0.515 ± 0.719 0.658 ± 1.056 0.976 30 0.008 0.003 0.547 ± 0.759 0.638 ± 1.034 0.974 right camera - 3D LiDAR 1 - - 4.348 ± 6.314 1.936 ± 4.431 5.487 5 - - 4.202 ± 5.179 1.144 ± 2.342 4.466 10 - - 3.305 ± 4.742 0.984 ± 2.387 3.820 20 - - 3.355 ± 4.744 1.005 ± 2.412 3.774 30 - - 3.352 ± 4.874 0.950 ± 2.279 3.811 left camera - 3D LiDAR 1 - - 5.126 ± 6.686 1.527 ± 3.435 5.566 5 - - 3.381 ± 4.447 1.058 ± 2.503 3.730 10 - - 2.937 ± 4.430 1.115 ± 2.791 3.712 20 - - 3.411 ± 4.887 1.258 ± 2.959 4.046 30 - - 3.398 ± 4.924 1.100 ± 2.602 3.942 Table 6 Impact of the number of incomplete collections in ATOM full calibration performance. The dashed entries correspond to results not generated since, for the camera-to-LiDAR case, the mean rotation and translation errors are not available. Type Nr. Collections e R (rad) e R (rad) e x (px) e x (px) e rms (px) Complete Incomplete camera - camera 10 0 0.011 0.011 1.159 ± 1.479 0.848 ± 1.182 1.457 10 2 0.006 0.003 0.821 ± 0.711 0.606 ± 0.900 1.031 10 4 0.009 0.006 1.680 ± 2.001 0.784 ± 0.945 2.017 10 5 0.010 0.005 0.778 ± 0.990 0.641 ± 0.908 1.076 right camera - 3D LiDAR 10 0 - - 5.104 ± 6.517 1.370 ± 3.085 5.496 10 2 - - 3.272 ± 4.800 1.068 ± 2.634 3.920 10 4 - - 3.414 ± 4.934 1.040 ± 2.515 3.964 10 5 - - 3.734 ± 4.956 1.111 ± 2.540 4.069 left camera - 3D LiDAR 10 0 - - 5.113 ± 6.479 1.535 ± 3.430 5.576 10 2 - - 3.112 ± 4.665 1.192 ± 2.970 3.930 10 4 - - 2.995 ± 4.541 1.137 ± 2.886 3.849 10 5 - - 3.720 ± 5.012 1.274 ± 2.907 4.169 A Camera to LiDAR calibration approach through the optimization of atomic transformations André Silva Pinto de Aguiar Conceptualization Methodology Software Writing - original draft a c ⁎ Miguel Armando Riem de Oliveira Conceptualization Methodology Software Writing - review & editing b Eurico Farinha Pedrosa Conceptualization Methodology Software Writing - review & editing b Filipe Baptista Neves dos Santos Writing - review & editing Supervision a a INESC TEC – INESC Technology and Science; 4200-465 Porto, Portugal INESC TEC – INESC Technology and Science; 4200-465 Porto Portugal INESC TEC - INESC Technology and Science; 4200-465, Porto, Portugal b Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal c School of Science and Technology, University of Trás-os-Montes e Alto Douro; 5000-801 Vila Real, Portugal School of Science and Technology University of Trás-os-Montes e Alto Douro; 5000-801 Vila Real Portugal School of Science and Technology, University of Trás-os-Montes e Alto Douro; 5000-801 Vila Real, Portugal ⁎ Corresponding author. This paper proposes a camera-to-3D Light Detection And Ranging calibration framework through the optimization of atomic transformations. The system is able to simultaneously calibrate multiple cameras with Light Detection And Ranging sensors, solving the problem of Bundle. In comparison with the state-of-the-art, this work presents several novelties: the ability to simultaneously calibrate multiple cameras and LiDARs; the support for multiple sensor modalities; the calibration through the optimization of atomic transformations, without changing the topology of the input transformation tree; and the integration of the calibration framework within the Robot Operating System (ROS) framework. The software pipeline allows the user to interactively position the sensors for providing an initial estimate, to label and collect data, and visualize the calibration procedure. To test this framework, an agricultural robot with a stereo camera and a 3D Light Detection And Ranging sensor was used. Pairwise calibrations and a single calibration of the three sensors were tested and evaluated. Results show that the proposed approach produces accurate calibrations when compared to the state-of-the-art, and is robust to harsh conditions such as inaccurate initial guesses or small amount of data used in calibration. Experiments have shown that our optimization process can handle an angular error of approximately 20 degrees and a translation error of 0.5 meters, for each sensor. Moreover, the proposed approach is able to achieve state-of-the-art results even when calibrating the entire system simultaneously. Keyword Computer vision Geometric optimization Atomic transformations 1 Introduction Nowadays, autonomous robotic systems are endowed with high-quality onboard sensors of different modalities, i.e. sensors that output different types of data, such as cameras and Light Detection And Ranging (LiDAR) sensors. To move autonomously while safely avoiding any kind of obstacle, these vehicles need to perform complex tasks such as Simultaneous Localization and Mapping (Durrant-Whyte & Bailey, 2006) and Path Planning (Sariff & Buniyamin, 2006) which require calibrated sensor data. The quality of the onboard sensors data is also crucial, since the robot should have a clear perception of the environment. For example, in agriculture, the automation of tasks such as crop monitoring or harvesting is a complex challenge that requires data of high quality sensors (Santos et al., 2016), such as, for example long-range 3D LiDARs. To perform data fusion, and take advantage of all the sensors present in the robotic system, it is essential to know the spatial relationship between all sensors (Melendez-Pastor, Ruiz-Gonzalez, & Gomez-Gil, 2017; Majumder & Pratihar, 2018). To do so, the most common approach is to perform extrinsic calibration, i.e., to compute the transformation between the sensors’ reference frames. The standard approach to perform extrinsic calibration is to find associations between data incoming from each sensor to be calibrated. Thus, a cost function that minimizes the error between associations is used. Most of the calibration procedures use a pattern that can be detected independently of the sensor modality, so that data correspondences can be found. Using these concepts, camera to 3D LiDAR extrinsic calibration have been approached in several works. A minority of works perform calibration without using a pattern. In those, the characteristics of the environment are used as features to compute intrinsic and extrinsic calibration. In autonomous driving, for example, lane detection and vanishing point tracking are common approaches (Badue et al., 2021; de Paula, Jung, & da Silveira, 2014; Álvarez, Llorca, & Sotelo, 2014). The great majority of the works found in the literature follow a pairwise calibration between a monocular camera and a 3D LiDAR, a concept introduced by Huang and Barth (2009). In this work, the extrinsic coefficients are computed solving a closed-form equation, and refined with a maximum likelihood estimation. Similarly, Verma, Berrio, Worrall, and Nebot (2019) use a standard chessboard to calibrate a perspective/fisheye camera and a 3D LiDAR using a Genetic Algorithm. Wang, Sakurada, and Kawaguchi (2017) propose a work where the corners of the pattern are automatically detected for both a panoramic camera and a 3D LiDAR so that the calibration can be performed. For the LiDAR case, authors propose a detection based on the intensity of reflectance of the beams. Fremont and Bonnifait (2008) and Guindel, Beltran, Martin, and Garcia (2017) use circle-based patterns to perform the extrinsic calibration. Mirzaei, Kottas, and Roumeliotis (2012) propose the estimation of a 3D LiDAR intrinsic parameters, as well as the extrinsic calibration with a monocular camera, through the minimization of a non-linear least squares cost function. The calibration is used to build photorealistic 3D reconstruction of indoor and outdoor scenes. Pandey, McBride, Savarese, and Eustice (2010) calibrate a 3D LiDAR with an omnidirectional camera also using a standard planar pattern. To calibrate the system, the sensors should observe the pattern from at least three different points of view. With this input, the extrinsic coefficients are calculated with a non-linear optimization technique. With the same purpose, Huang and Grizzle (2020) use a pattern of known dimension and geometry and estimates the pattern to LiDAR pose automatically using a fitting algorithm. Although all these works perform successful extrinsic calibrations between 3D range sensors and monocular cameras, pairwise calibration is a major shortcoming since most robotic systems present more than two sensors to be calibrated. In a system with more than two sensors, one would have to combine multiple pairwise calibrations. The problem is that the number of the combinations of pairwise calibrations required grow quickly. For example, Zhou, Li, and Kaess (2018) presente a system with a 3D LiDAR and a stereo camera system. However, to calibrate the three sensors (LiDAR and two cameras), two calibrations have to be performed: LiDAR to left camera, and LiDAR to the right camera. In the same way, with the purpose of fusing point clouds of multiple stereo cameras, Dhall, Chelani, Radhakrishnan, and Krishna (2017) use a 3D LiDAR to perform pairwise calibration with all the cameras in the system. Only after obtaining the transformation between the range sensor and each camera of the stereo system, the transformation between the stereo cameras can be found. Then the point clouds can be fused. Similarly, Kim and Park (2019) perform six pairwise calibrations between a 3D LiDAR and six monocular cameras mounted in an hexagonal plate that constitute an omnidirectional camera. To overcome this limitation, few works exist that consider multi-sensor, multi-modal calibration in a non-pairwise fashion. For example, Zuniga-Noel, Ruiz-Sarmiento, Gomez-Ojeda, and Gonzalez-Jimenez (2019) propose a method to estimate the extrinsic calibration between multiple sensors such as LiDARs, depth cameras and RGB cameras. The calibration procedure is separated in two parts: a motion-based approach that estimates 2D extrinsic parameters and a method that uses the observation of the ground plane to estimate the remaining ones. It is worth noting that this framework requires the robotic platform to be moving. Liao, Li, Ju, Liu, and Jiang (2017) propose a joint objective function to simultaneously calibrate three RGB cameras with respect to an RGB-D camera. Rehder, Siegwart, and Furgale (2016), propose an approach for joint estimation of both temporal offsets and spatial transformations between sensors. This approach is one of few that is not designed for a particular set of sensors, since its methodology does not rely on unique properties of specific sensors. It is able to calibrate systems containing both cameras and LiDARs. Pradeep, Konolige, and Berger (2014), present a joint calibration of the joint offsets and the sensors locations for a PR2 robot. This method takes sensor uncertainty into account and is modelled in a similar way to the bundle adjustment problem. The two major shortcomings of the state-of-the art in extrinsic calibration are: most of the methods perform pairwise calibration, which can be exhaustive for a robotic system with many sensors to be calibrated; and the majority of the works are focused on specific sensor modalities, rather than working in a more general way. To overcome these issues, this work proposes an extrinsic calibration framework with the following contributions: • Support for calibration of multiple sensors (i.e. N ⩾ 2 ); • The ability to handle multiple sensor modalities; • Calibration without changing the topology of the input transformation tree; • Integration of the system within the Robot Operating System (ROS) framework, with interactive tools to collect data, set the initial estimates, and visualize the calibration. Our previous works have focused on the calibration of intelligent vehicles. These platforms are often characterized by the large amount of sensors of different modalities mounted onboard. As such, these previous works presented a methodology based on atomic transformations for multi-sensor, multi-modal robotic systems (Guindel, Beltran, Martin, & Garcia, 2017; Oliveira, Castro, Madeira, Dias, & Santos, 2019; Oliveira et al., 2020). In this work, we extend our framework - Atomic Transformation Optimization Method (ATOM) 1 1 (Oliveira et al., 2019) - to also consider the calibration of 3D LiDARs along with the other supported modalities. Atomic transformations are geometric transformations that are not aggregated, i.e., they are indivisible. As such, this article presents the methodologies implemented to simultaneously calibrate a 3D LiDAR sensor with multiple cameras using a Bundle Adjustment optimization scheme (Agarwal, Snavely, Seitz, & Szeliski, 2010). As our approach is not focused on a single robotic platform, we present the calibration of a different robotic platform in comparison with our previous works - the agricultural robot AgRob V16 (de Aguiar, dos Santos, dos Santos, de Jesus Filipe, & de Sousa, 2020; Santos et al., 2019). The remainder of this paper is organized as follows: Section 2 details the optimization procedure and how it is cast as a Bundle Adjustment problem; Section 3 describes the ROS (Quigley et al., 2009) calibration setup, i.e., the steps from the robotic platform configuration until the data collection; Section 4 details the experimental results; and finally, Section 5 provides conclusions and future work. 2 Proposed approach ATOM is a calibration framework that simultaneously calibrates sensors of different modalities though the optimization of atomic transformations. This concept is supported by a well-defined optimization pipeline, that defines a set of optimization parameters and minimizes a cost function that supports different input modalities. This function f, which depends on the optimization parameters Φ , is known as the objective function. Our approach minimizes f to calibrate all the sensors of generic multi-modal robotic platforms simultaneously. In this process, the definition of a tree graph which contains topological information about the relationship between reference frames is required. In this tree, nodes are reference frames and edges correspond to the transformation between nodes. This data structure allows for the definition of unique paths between the graph nodes, i.e., enables an efficient retrieval of the transformations between any two frames in the tree. Fig. 1 represents the robotic platform to be calibrated (AgRob V16) with its respective referential frames, and the transformation tree considered for the calibration. The design of the transformation tree leads to the definition of the optimization parameters to calibrate the system. An extrinsic calibration can be viewed as a pose estimation problem, where the pose of each sensor is estimated. Thus, the set of parameters to optimize Φ , must together define the pose of each sensor. To perform such a calibration, we propose to maintain the initial structure of the transformation tree, calibrating only one atomic transformation per sensor. Since the system contains camera sensors, it is also possible to introduce the intrinsic parameters of each camera in the set Φ . In this way, the set of parameters will be composed of different modalities and so, there is the need to design an objective function that is able to characterize sensors of in a multi-modal fashion. As previously discussed, pairwise approaches for projecting the objective function result in complex graphs with many combinations of relationship definitions. For every existing pair of sensors, these relationships must be established according to the combined modality of the pair of sensors, which leads to a problem of scalability for which there is no solution in the literature. To solve this issue, we formulate our solution in Bundle Adjustment problem, in that the structure of the objective function is designed in a sensor to calibration pattern paradigm. Also, for every collection of data, the transformation that takes the corners of the calibration pattern to the world is optimized. In other words, the poses of the pattern are jointly estimated with the poses of the sensors. To perform this iterative procedure, the set of optimization parameters Φ must be initialized. The first guess for the pattern pose is obtained w.r.t. one of the cameras to be calibrated, resulting in a transformation cam i T p , where p denotes pattern and a T b represents the transformation from frame b to a. As will be detailed later on, our calibration framework allows the definition of an initial guess for the pose of each sensor and, consequently, for the transformations to be calibrated. With this definition, it is possible to compute the pose of any particular sensor j as an aggregate homogeneous transformation w A s j , obtained from the chain of transformations for that particular sensor present in the topological transformation graph: (1) w A s j = ∏ i ∈ R i T i + 1 = ∏ i ∈ K i T i + 1 · child T ^ parent · ∏ i ∈ L i T i + 1 , where child T parent represents the transformation to be calibrated, i T i + 1 for i ∈ K represent the prior links to the frame parent, i T i + 1 for i ∈ L the later to the frame child, and R is the set that contains all the frames present in the chain of transformation of sensor j. So, to obtain the homogeneous transformation from the pattern to the world, the following calculation is applied: (2) w T p = w A cam m · cam m T p , where p refers to the pattern, w states for world, and cam m for the mth camera sensor. So, the set of parameters Φ to be optimized is composed of the transformation represented in (2) along with the poses of each sensor to be calibrated, and, in the case of cameras, their intrinsics and distortion parameters: (3) Φ = x m = 1 , r m = 1 , i m = 1 , d m = 1 , … , x m = M , r m = M , i m = M , d m = M ︷ cameras , x n = 1 , r n = 1 , … , x n = N , r n = N ︷ LiDARs , . . . ︷ Other modalities , x k = 1 , r k = 1 , … , x k = K , r k = K ︷ Calibration pattern , where m refers to the mth camera to be calibrated, n states for the nth LiDAR to be calibrated, k refers to the pattern detection of the kth collection of data, x is a translation vector [ t x , t y , t z ] , r is a rotation represented thought the angle-axis parameterization [ r 1 , r 2 , r 3 ] (where the vector r is used to represent the axis and its norm represents the angle), i is a vector with each camera intrinsic parameters [ c x , c y , f x , f y ] , and d is a vector of each camera distortion coefficients [ d 0 , d 1 , d 2 , d 3 , d 4 ] . The intrinsic and distortion parameters of each camera can be initialized using any camera calibration toolbox, or in some cases, these parameters are also provided by the manufacture. The angle-axis parameterization was chosen because it has three components and three degrees of freedom, which means that it does not introduce more sensitivity than the one inherent to the problem itself (Hornegger & Tomasi, 1999), unlike the rotation matrix which has nine degrees of freedom for the also three components, or the euler angles that loose a degree of freedom when two axis are aligned. Using angle-axis representation, we have six optimization parameters per sensor that represent the pose of each one, i.e., the geometric transformation that will be calibrated. The optimization procedure, as will be explained, consists of the minimization of an objective function by the definition of residuals that are calculated as an error (in pixels for RGB cameras and in meters for 3D LiDARs) between the re-projected position of the calibration pattern, and the position of the pattern detected by each sensor. 2.1 Objective function To be able to consider multiple modalities in the same optimization process, we propose to structure the objective function F ( Φ ) as a composition of as many sub-functions f i ( . ) as desired modalities. The objective function F ( Φ ) from (4) is minimized using a non-linear least squares approach. 2 2 In this work we used the least-squares solver provided by Least-squares finds a local minimum of a scalar cost function, with bounds on variables, by having an m-dimensional real residual function on n real variables. As such, we choose this minimization approach as its is the best fit for our problem. So, for each new modality added to the calibration, a sub-function associated with it is designed and incorporated in F ( Φ ) , which allows for the minimization of the error associated with the pose of sensor of that specific modality. This is one of the reasons why the proposed approach is scalable. Thus, the optimization procedure can be defined as: (4) arg min Φ F ( Φ ) = arg min Φ 1 2 ∑ i ‖ f i ( { Φ i } ) ‖ 2 , where f i ( . ) is the objective sub-function for the i-th sensor considering the set of k optimization parameters { Φ i } . Thus, the final cost to be minimized is computed by the sum of the squared sub-function values for each set of optimization parameters. The value for all these sub-functions is a vector with the residuals associated to whit re-projection of the points of the calibrated pattern. For our use-case, the goal is to calibrate a stereo camera system (two cameras) and a 3D LiDAR sensor. So, the objective function is composed of the vector values of three sub-functions, two for the cameras and one for the 3D LiDAR. Each sub-function is detailed in the next sub-sections. 2.1.1 Camera modality sub-function When the sensors to be calibrated are cameras, their calibration is performed as a bundle adjustment (Agarwal et al., 2010), as described in our previous work, Oliveira et al. (2020). Thus, the created sub-function is based on the average geometric error corresponding to the image distance (in pixels) between a projected point and a detected one. So, the goal of the cost sub-function for camera sensors is to adjust the initial estimate for the intrinsic and distortion parameters, and position of the pattern corners, in order to minimize the re-projection error f cam , given by: (5) f cam = ‖ x c = 1 - x ̂ c = 1 ‖ . . . , ‖ x c = C - x ̂ c = C ‖ , where ‖ . ‖ represents the Euclidean distance between two vectors, c is the index of the pattern corners, x c denotes the ground-truth pixel coordinates of the measured points given by the pattern detection, and x ̂ are the projected points, given by the relationship between a 3D point in the world and its projection onto the image. To perform such calibration, 3D pattern points have to be found and re-projected onto the image plane. For each collection of data, the camera(s) to be calibrated capture the pattern. By knowing the real size of the pattern, and the size of each square that composes it, the 3D coordinates of the corners can be found in the local pattern reference frame. Then, each corner is located in the plane z = 0 , since the corners are in the XoY plane of the local pattern reference frame. Thus, each corner in the local pattern referential frame p p is transformed to the camera referential frame as follows: (6) p cam = cam T w · w T p · p p . Note that, p cam and p p are homogeneous vectors of the 3D corner coordinates in each reference frame, so that (6) is valid. Finally, to re-project each 3D corner from camera’s reference frame p c = i cam to the image plane, taking into account each camera intrinsic and distortion parameters, the pinhole camera model (Sturm, 2014) is used: (7) x ̂ c = i = K · p c = i cam ″ , where K is the matrix that contains the intrinsic parameters i and, (8) p c = i cam ′ = p c = i cam · 1 z cam = x cam z cam , y cam z cam , 1 T , (9) p c = i cam ″ = x cam ′ · ( 1 + d 0 l 2 + d 1 l 2 + d 4 l 6 ) + 2 d 2 · x cam ′ y cam ′ + d 3 · ( l 2 + 2 x cam ′ 2 ) y cam ′ · ( 1 + d 0 l 2 + d 1 l 2 + d 4 l 6 ) + d 2 · ( l 2 + 2 y cam ′ 2 ) + 2 d 3 · x cam ′ y cam ′ , where l = ( x cam z cam ) 2 + ( y cam z cam ) 2 , and d j is the jth component of the distortion vector d . From (6)–(9), it is possible to conclude that all the desired parameters to optimize are being considered: the pattern to world transformation w T p present in (6) that can be computed as the inverse of (2); the world to camera transformation cam T w also present in (6); and finally, the intrinsic i and distortion parameters d considered in (7)–(9). The use of these parameters to project the 3D corners in the image plane, together with the minimization of the geometric re-projection error, lead to the parameter configuration that optimize the sub-function f cam . Thus, it is expected that the re-projected points become closer to the ground-truth corners during the optimization. Fig. 2 shows the difference between the initial position of the pattern corners, and the final position of these same projected points, after the optimization has been completed. It is possible to observe that the pixels corresponding to the projection of the final position of the points (dots in Fig. 2) almost perfectly match the ground-truth point (squares in Fig. 2). 2.1.2 3D LiDAR sub-function For the case of 3D LiDARs, the sub-function f lidar considers two types of residuals: orthogonal distance and limit points distance. To compute both, this approach also uses the calibration pattern and, in specific, its boundary points. As will be detailed later on, our calibration framework has a semi-automatic labelling procedure that allows to save, for each collection of data, the LiDAR 3D points that are on the pattern. As in case of the camera sensor, this approach formulates the cost sub-function by minimizing the residuals w.r.t. some ground-truth. Here, the ground-truth 3D points are, once again, generated in the pattern reference frame by knowing the three dimensional structure of the pattern, such as its height and width. Thus, by knowing the size of the pattern, the size of each pattern square, and the pattern origin (bottom left corner), the coordinates of the boundary points defined in the local pattern’s reference frame are computed. It is important to note that, the size of the board between the pattern grid and the end of the physical pattern had to be measured so that this step could be implemented. Also, as explained before, all the calculated pattern limit points have coordinate z = 0 , since the pattern’s reference frame is in the XoY plane. After calculating the ground-truth boundary points of the pattern, two things are required: the pattern boundary points observed by the 3D LiDAR sensor and the homogeneous transformation that converts 3D points from the LiDAR referential frame to the local pattern reference frame. Given a set of labelled 3D LiDAR cartesian points on the pattern p lidar = x c = i , y c = i , z c = i , the boundary points are calculated using a spherical parameterization for each 3D point. After computing the spherical coordinates of each 3D LiDAR point on pattern p s , lidar = r c = i , θ c = i , ϕ c = i , two limit points are calculated considering the set of 3D LiDAR points on pattern belonging to a given horizontal scan of the original point cloud. As the labelled set of 3D LiDAR points on pattern is an unordered point cloud, the horizontal scans are computed by clustering the points considering their θ value. So, points with the same θ value belong to the same horizontal scan. Finally, to extract the two limit points per horizontal scan, the ϕ component maximum and minimum values of each set are computed, resulting in the two most distant points, corresponding to points in the pattern boundaries. The result of this procedure is represented in Fig. 3 . The final step before computing the residuals that constitute the cost sub-function f lidar is to convert the set of labelled 3D LiDAR points on pattern, as well as the computed boundary points, to the patterns’ reference frame. This is done using the homogeneous transformations computed in (1) and (2): (10) p p = p T w · w A lidar · p lidar , where p T w is the transformation matrix from the world to the pattern reference frame, and w A lidar the transformation matrix from the world to the LiDAR sensor reference frame. Similarly to the cameras’ case, (10) shows that the optimization parameters include the sensor pose and the pattern pose. After this, the two residual types can be computed. The first, orthogonal distance, is the absolute z value of the coordinates of the projected 3D LiDAR points. As they are on patterns’ referential frame, it is intended that their z coordinate is zero. Therefore, any value different from zero means that the optimization parameters (sensor pose and pattern pose) are not yet correct. The second residual type is the Euclidean distance of x and y components between the ground-truth pattern boundary points, and the LiDAR 3D points on pattern boundary calculated as described before. For each LiDAR point on the pattern boundary, the residual is computed as the distance between x and y coordinates, in the calibration pattern frame, of the respective LiDAR boundary point and the closest point that belongs to the limit of the physical board that is being detected. This being said, the 3D LiDAR cost sub-function is as follows: (11) f lidar = z l = 1 lidar , p , ‖ p q = 1 , xy boardlimit - p q = 1 , xy p ‖ , . . . , z l = L lidar , p , ‖ p q = Q , xy boardlimit - p q = Q , xy p ‖ , where z l = i lidar , p is the z coordinate of the ith 3D LiDAR point projected into the patterns’ referential frame, p q = j , xy boardlimit are the x and y coordinates of the jth 3D LiDAR boundary point on the same referential, and p q = j , xy p are the x and y coordinates of the corresponding ground-truth boundary point. Fig. 4 shows the ground-truth pattern boundary points representation (blue lines on the left of Fig. 4), the calculated boundary points at the start of the calibration procedure (blue circles on the middle of Fig. 4), and the result of the optimization procedure with the ground-truth points and the boundary points observed by the LiDAR aligned (right representation on Fig. 4). 2.2 Normalization of multi-modal residuals We propose a full calibration method where sensors of different modalities contribute to a global vector residual of residuals. While the camera sub-function provides a set of residuals that are expressed in pixels, the LiDAR sub-function provides a set of residual expressed in meters. This mismatch in units of measurements may display highly disparities in error magnitudes, which could result in unwanted behaviours in the optimization processes due to differences in scale. For example, a residual of 1 pixel has higher influence (or weight) in the optimization path than a residual of 0.5 meters. Yet, our knowledge about the system tells us that the opposite should be considered. As result, the parameters that influence the residuals with higher scale will dominate the optimization, while the other parameters are perceived to already be close to their optimal state. To handle the different scales in multi-model residuals in Eq. 4, we employ a normalization factor to the optimization. Let C = { c } be the set of existing residual classes (e.g C = { pixels , meters } ) and c ( i ) ∈ C is the residual class for the ith sensor, then the optimization is defined as (12) arg min Φ F ( Φ ) = arg min Φ 1 2 ∑ i f i ( { Φ i } ) η c ( i ) 2 , where η c ( i ) is the normalization factor for residuals created by the sensor sub-function f i . Note that the normalization factor η c ( i ) is defined per residual class and not per sensor. The normalization values per class are given by the arithmetic mean of the same class residuals before the optimization. For example, the normalization for the class pixels, with η c ( i ) = η pixels , is given by (13) η pixels = 1 n ∑ j ‖ f j ( { Φ j } ) ‖ 1 : ∀ f j ∈ { pixels } , where n is the total number of residuals that are part of the considered class and ‖ . ‖ 1 is the L1 norm. Note that the normalization factors are constant values during optimization, calculated once with the residuals that result from the initial guess. 3 Calibration framework The ROS (Quigley et al., 2009) has become the standard framework for the development of robotic solutions. As referenced before, the proposed calibration procedure requires the creation of a transformation tree, from which atomic transformations are optimized. For this purpose, ROS provides a tree graph referred to as tf tree (Foote, 2013). With this tool, it is possible to define a data structure as the one present in Fig. 1. Also, the Robot Operating System Visualization (RVIZ) tool supports additional functionalities, such as robot visualization, collision detection, etc. In fact, this visualization procedure is interactive, in that if any transformation between two links changes, the robotic platforms and sensors affected by these links change its pose accordingly. This interactive procedure is possible since the optimizations’ cost function always recomputes the aggregate transformations. Therefore, a change in one atomic transformation in the chain affects the global sensor pose, and consequently, the error to minimize. So, if atomic transformations change due to the calibration procedure, the tf tree will automatically adjust the robots and sensors poses accordingly. It should be emphasized that, due to all these functionalities, the calibration procedure should not change the structure of the tf tree. Our approach preserves the predefined structure of the tf tree, since, during optimization, only the values of some atomic transformations contained in the chain are estimated, securing the topology of the tree. To the best of our knowledge, our approach is one of few which maintains the structure of the transformation graph before and after optimization. Given all of the above, we state an extensive integration with ROS as a key component of the proposed approach. The ROS calibration framework is segmented in five main components: configuration, initial estimate, data labelling, data collection, optimization procedure. Each will be described in detail in the following sections. 3.1 Calibration configuration The configuration defines the parameters which will be used throughout the calibration procedure, from the definition of the sensors to be calibrated to a description of the calibration pattern. The proposed approach, detailed in Section 2, is based on the optimization of atomic transformations. These were combined through the use of the topological information contained in a tree. The transformation tree is generated from a ROS Unified Robot Description Format (URDF). Additional information must be given to define which, out of the set of atomic transformations, will be optimized during the calibration procedure. Also, a description of the calibration pattern must be provided. All this information is defined in a calibration configuration file. 3.2 Initial parameter estimation Optimization procedures suffer from the known problem of local minima. This problem tends to occur when the initial solution is far from the optimal parameter configuration, and may lead to failure in finding adequate parameter values. To avoid this, the setup of the a plausible initial guess for the entire parametric optimization system is essential. Our approach supports different modalities of parameters, as stated in (3). Thus, each modality requires a specific type of initialization. For cameras intrinsic i and distortion d parameters, the initialization is performed using any state-of-the-art camera calibration toolbox, or using the calibration provided by the manufacture. To initialize the transformations of sensors in general, we developed an interactive tool which parses the configuration URDF file and creates a 3D visualization tool for ROS interactive marker associated with each sensor. Fig. 5 shows an example of the developed tool. Here, we can see the user changing each sensor reference frame, dragging the respective interactive markers. With this tool the user can move and rotate the markers relative to each sensor. This provides a simple, interactive method to easily generate plausible first guesses for the poses of the sensors. Immediate visual feedback is provided to the user by the observation of the 3D models of the several components of the robot model and how they are put together, e.g. where each camera or LiDAR is positioned w.r.t. the vehicle. Also, for multi-sensor systems, it is possible to observe how well the data from a pair of sensors overlap. An example of this procedure can be watched at Concerning the atomic transformations associated with the calibration pattern w T p present in (2), these are initialized by defining a new branch in the transformation tree which connects the pattern to the frame to which it is fixed. For example, for AgRob V16 case, w T p is estimated through (2) where cam m T p is estimated solving the Perspectiva-n-Point (PnP) for the detected pattern corners (Gao, Hou, Tang, & Cheng, 2003; Fabbri, Giblin, & Kimia, 2020; Penate-Sanchez, Andrade-Cetto, & Moreno-Noguer, 2013), and w A cam m by deriving its topology from the tf tree and using the initial values for each atomic transformation in the chain. 3.3 Labeling data The labeling of data refers to the annotation of the portions of data which captures the calibration pattern. A labeling procedure is executed for the data of each sensor, and can be automatic, semi-automatic or even manual in some cases. The information that is stored depends on the modality of the sensor, but for cameras it is always the pixel coordinates of the corners observed in the pattern. The standard calibration pattern that is used for camera calibration is a chessboard pattern. The images are labelled using one of the many available image-based chessboard detectors (Czyzewski, 2017). Our system is also compatible with charuco boards (Garrido-Jurado, Muñoz-Salinas, Madrid-Cuevas, & Medina-Carnicer, 2016). These have the advantage of being able to detect the pattern even when it is partially occluded. Also in this case we make use of off the shelf detectors, (e.g. Romero-Ramirez, Muñoz-Salinas, & Medina-Carnicer, 2018; Hu, DeTone, & Malisiewicz, 2019). For the calibration of AgRob V16, a labeling algorithm for 3D LiDARs was developed. This method is semi-automatic and is initialized by a setup of a seed point. To label the pattern points viewed by the 3D LiDAR, the user drags an interactive marker to a point located in the pattern - the seed point. This constitutes the non-automatic stage of the procedure. After that, the algorithm clusters a set of points (that are intended to belong to the pattern) using an Euclidean distance threshold computed using the a priori known dimensions of the pattern. Despite being simple and fast, this approach reveals lack of precision, since it includes many outliers in the labeling procedure. The pattern used is rectangular. Thus, the Euclidean distance threshold has to be higher than the smaller side of the rectangle. This means that, if the pattern is close to another object, points from this object will be labeled as pattern points. To overcome this issue, a Random Sample Consensus (RANSAC) (Fischler & Bolles, 1981) algorithm is executed to fit the set of labeled points in a plane (the pattern plane), eliminating the outliers. RANSAC is an iterative algorithm, and it is performed a maximum number of times M. To find points that belong to the plane, the point to plane distance is computed in each iteration i for each point j as (14) D ij = | a i x j + b i y j + c i z j + d i | a i 2 + b i 2 + c i 2 where p j = [ x j , y j , z j ] T is the jth point on the cluster. With this, a point is considered as inlier if its distance to the plane D ij is smaller than a given threshold D threshold . The final set of inliers corresponds to the one found in the iteration i that gives the higher number of points belonging to the plane. Fig. 6 shows an example of the labeling procedure for cameras and 3D LiDARs proposed in ATOM. It is worth noting that, our approach works with partial detections, as represented in the figure. This interactive data labeling procedure is showcased in 3.4 Collecting data In most robotic systems, the data coming from the sensors is streamed at different frequencies. However, to compute the associations between the data of multiple sensors, temporal synchronization of the sensor data is required. Of course, this only becomes an issue when calibrating multi-sensor robotic systems. For now, the synchronization problem is solved trivially by collecting data (and the corresponding labels) at user defined moments in which the scene has remained static for a certain period of time. In static scenes, the problem of data de-synchronization is not observable, which warrants the assumption that for each captured collection the sensor data is ‘adequately’ synchronized. This can be done using, for example, a tripod to held the pattern before collecting each snapshot of data (Rehder et al., 2016; Furgale, Rehder, & Siegwart, 2013). In this work, the problem is approached in a simpler way, where the pattern is hold by the user, as shown in Fig. 2, remaining static by sufficient amount of time to ensure the synchronization between all the sensors. To save the scene data captured by all the sensors in the calibration system, the user can do it with just two mouse clicks on the interactive ROS-based tool (on RVIZ) developed. We refer to these recordings of data as data collections. Each one of them contains the values of all atomic transformations that exist in the system at a given timestamp, a copy of the robot configuration file, sensor data and labels, and high level information about each sensor, such as the topological transformation chain, extracted from the transformation tree. This information is stored in a dataset file that will be read by the optimization procedure afterwards. Also, a video showing the procedure for collecting data is provided for AgRob V16 calibration here It should be pointed out that, the set of collections should contain as many different poses as possible. As such, collections should preferably have different distances and orientations w.r.t. the calibration pattern, so that the calibration returns more accurate results. This concern is common to the majority of calibration procedures. 3.5 Visualizing the optimization The immediate visualization of the calibration is essential for several reasons: it provides the user the necessary data so that he can detect failures on the calibration, such as outlier data collections; it gives feedback about the cost function residuals minimization, which can serve to detect possible local minima, and to make sure that the optimization procedure is converging. Fig. 7 shows the three main visualization features of ATOM. Our calibration framework provides a simultaneous visualization of all the data collections, as well as immediate feedback of the alignment between ground-truth points and labeled points for optimization, images with the reprojection, robot meshes, the position of the reference frames, etc. Also, optimization graphics are provided with residuals values and the total error for each iteration. This configuration is similar to the standard one which is used during the initial parameter estimation, the data labeling and collection, but contains a couple of key distinctions. As mentioned above, the calibration procedure uses a dataset file which contains information about each of the stored collections. These collections contain data gathered in a a set of sequential instants in time. The ROS calibration configuration publishes data from all collections simultaneously, as if those time instants were packed together and processed as if they had occurred all at the same time. Collisions in topic names and reference frames are avoided by adding a collection related prefix to each designation. Also, the original transformation tree is replicated for each collection. A video with an example of a calibration execution for AgRob V16 is provided 4 Results To test and validate the performance of the proposed approach, an extensive evaluation procedure was developed. Our calibration framework, ATOM, was used to calibrate three configurations of the AgRob V16 sensing system. Two of them were pairwise calibrations between two cameras of a stereo system, and a single camera and a 3D LiDAR, which we denote as ATOM pairwise. The third was a calibration between all three sensors (two cameras and 3D LiDAR), which we call ATOM full, where results for particular pairs of sensors are obtained using a full calibration. In this procedure, three datasets were used, as represented in Table 1 . Two of them (train-1, train-2) were used for training, i.e., to perform the calibrations, and the third one (test-3) was used to test the calibration with specific metrics that will be detailed later on. The datasets contain incomplete and partial collections, i.e., collections where the pattern is not detected for all the sensors, and collections where the pattern is only partially visible, respectively. This section is divided in two parts: the evaluation of ATOM’s performance against state-of-the-art approaches, such as OpenCV stereo calibration (Bradski, 2000), and ICP point cloud alignment (Besl & McKay, 1992); the characterization of ATOM w.r.t. several characteristics of the datasets, such as the number of incomplete/partial collections, and the accuracy of the initial guess. Table 2 makes a summary of the calibration experiments that were carried out. OpenCV and the stereo camera factory calibration were used to get the camera-to-camera extrinsic calibration, and ICP was explored to get the camera-to-LiDAR calibration. This last calibration was obtained by the alignment of the 3D LiDAR point cloud, and the 3D point cloud provided by the stereo camera software development kit. Two versions of ICP were used as camera-to-LiDAR extrinsic calibration: the one corresponding to the collection where the fitting of point clouds was more accurate, and the average of the transformations obtained in all collections. Finally, as referenced before, ATOM was calibrated both in pairwise and full modes, and both approaches are evaluated. 4.1 Methodology One of the key characteristics of our evaluation procedure is the use of separate datasets to perform the calibration and generate the results. As discussed in Section 3, ATOM provides a data collection procedure, where datasets are generated and visualized on RVIZ. Datasets are composed of several collections, each one containing data of all the sensors, initial atomic transformations, pattern labelled points, and other information. To evaluate our calibration framework, two datasets where initially collected - train-1 and train-2. Then, three calibrations were executed over each one of the datasets, two pairwise and one using all the sensors to be calibrated in AgRob V16 system. These calibrations generate a json file similar to the one generated at the end of the data collection procedure, but with the calibrated atomic transformations. After obtaining all these calibration configurations, a third dataset was recorded - test-3. It was used to evaluate the accuracy of the calibrations obtained. Using the metrics that will be described later on, the chain of transformations of each calibration was loaded and used to compute errors using the labelled data of the test dataset. In this way, possible influence of using the same data to calibrate and test is eliminated, and a more rigorous evaluation is achieved. Unlike ATOM, both OpenCV and ICP perform sensor-to-sensor calibration, instead of calibrating atomic transformations without changing the topology of the chain of transformations. Thus, to evaluate these methods in the same way ATOM is evaluated, the atomic transformations set to be calibrated had to be recovered from the sensor-to-sensor calibrations. This problem is formulated in Fig. 8 . Let link 2 T base be the entire chain of transformations from the base link to the data link of the anchored sensor, T ̂ be the sensor to sensor calibration obtained, parent 1 T base be the chain of transformations from the base link to the parent link of the atomic transformation to be calibrated child 1 T parent 1 , and link 1 T child 1 the chain of transformation from the atomic transformation child link, to the non-anchored sensor data link. The entire chain of transformations relationships can be formulated as follows: (15) T ̂ · link 2 T base = link 1 T child 1 · child 1 T parent 1 · parent 1 T base . So, from (15), we can extract the atomic transformation of the non-anchored sensor as follows: (16) child 1 T parent 1 = ( link 1 T child 1 ) - 1 · T ̂ · link 2 T base · ( parent 1 T base ) - 1 . The advantage of this approach is that it is generic. For example, from OpenCV, a camera to camera metric is obtained. Thus, the procedure consists in anchoring one of the cameras, and use the obtained transformation to recover the atomic transformation marked for calibration in the original ATOM configuration. In the same way, ICP provides a camera to LiDAR calibration. Once again, we anchor one of these sensors, and apply the exact same routine to extract the non-anchored sensor atomic transformation to be calibrated. In this way, we are able to obtain a calibrated system without changing the initial chain topology, which allows the direct comparison of these state-of-the-art approaches with ATOM, using exactly the same metrics. These metrics are described in the next section. 4.2 Metrics To evaluate the camera to camera calibration performance, the methodology used is based on three different metrics: the mean rotation error (rad), the mean translation error (m), and the reprojection error (px). To compute the reprojection error, the idea is to use the calibration result to project the detected pattern corners of one camera image into the image of the second calibrated camera image and compare the projected pixel coordinates with the ground truth pattern corners coordinates in the image of the anchored camera. To transform pixels from one camera into another, we start from projecting the 3D world coordinates of the pattern corners into the image of a camera, using (6)–(7). Since the 3D pattern corners are defined in the local pattern reference frame, they all lie in the plane z = 0 . Thus, (6)–(7) can be simplified to the following: (17) p cam = K · cam T p ′ · p p ′ , where cam T p ′ is a portion of the matrix cam T w · w T p , without the component z of the rotation, as follows: (18) cam T p ′ = r 11 r 12 t x r 21 r 22 t y r 31 r 32 t z , and p p ′ is the pattern corner, represented as a vector in its homogeneous form, without the z component, i.e., p p ′ = x y 1 T . Using the fact that the 3D coordinates of the pattern’s corners are the same for both cameras, (17) can be applied to the two of them, so that we can find a relation between both expressions. This resulted in the following formulation: (19) p cam 2 = K cam 2 · cam 2 T p ′ · cam 1 T p ′ - 1 · K cam 1 - 1 · p cam 1 , where cam 1 and cam 2 refer to the cameras that were calibrated. This formulation provides the relationship between pixel coordinates of the pattern corners in both camera images. However, (19) requires the camera to pattern transformation matrix for both cameras. This can be a problem since, some approaches, unlike ATOM, do not estimate the camera to pattern transformation while performing the camera to camera calibration. In addition to this, ATOM estimates these transformations for a training dataset. So, the usage of a test dataset to evaluate all the frameworks, denies the use of the estimated pattern pose from ATOM. To overcome this, the pattern pose w.r.t. one of the cameras cam 1 T p is computed using the PnP algorithm. Then, using the output json file from each calibration, we recover the camera to camera transformation cam 1 T cam 2 , through the chain of transformations. In this manner, it is possible to determine the transformation of the other camera to the pattern, as follows: (20) cam 2 T p = cam 1 T cam 2 - 1 · cam 1 T p . From this expression, we can derive cam 2 T p ′ and cam 1 T p ′ , and successfully project pixels from one image into the other. With this information, the reprojection error is computed as follows: (21) e xy = p projected - p expected . From (21), the error can be decomposed in its x and y components. Also, considering the reprojection error for all the N collections, the root mean square error is calculted as follows: (22) e rms = 1 N ∑ e xy 2 . Fig. 9 illustrates a resulting corner reprojection from one camera into the other using the ATOM full calibration. For the calculation of the mean rotation and translation errors to evaluate the camera to camera calibration, we consider the following observation: the chain of transformations from the base link to the pattern reference frame that passes from each one of the calibrated cameras should be equal. This happens since the calibration pattern pose in reference with the base link is fixed, and any chain of transformations that link these two referentials should represent the same spatial relationship. Thus, the difference in rotation and translation can be quantified, assessing the inequality between the two chains of transformation. Once again, this formulation requires the pattern pose w.r.t. each one of the cameras, that is again extracted solving the PnP problem. With this information, we can state that (23) base R cam 1 base t cam 1 0 1 cam 1 R p cam 1 t p 0 1 = base R cam 2 base t cam 2 0 1 cam 2 R p cam 2 t p 0 1 . Now, we can define the rotation and translation difference as (24) Δ R = base R cam 1 · cam 1 R p - 1 · base R cam 2 · cam 2 R p (25) Δ t = base R cam 1 · cam 1 t p + base t cam 1 - base R cam 2 · cam 2 t p - base t cam 2 Finally, we can define the mean rotation error as (26) e R = 1 N ∑ i | | angle ( Δ R i ) | | , where angle is the angle-axis representation of the rotation, and the mean translation error as (27) e t = 1 N ∑ i | | Δ t i | | . To evaluate the 3D LiDAR to camera calibration, we used the reprojection error (px) and its corresponding root mean square error (px) considering all the test collections N. In this case, the mean rotation and translation errors were not used due to the difficulty of estimating the pattern pose w.r.t. the LiDAR sensor with precision. The process of calculating the reprojection error to evaluate the camera to LiDAR calibration consists in three main steps: 1. Label the pixels that belong to the boundaries of the pattern in the image. 2. Reproject the pattern boundary points in the LiDAR’s referential frame (detailed in Section 2.1) p boardlimit into the image. 3. Compute root mean square between labeled and projected points. Fig. 10 shows these steps. An annotation tool was developed to perform the labelling. This tool allows the user to manually annotate individual points corresponding to four classes, each one representing one side of the pattern in each image. Then, in order to account for the image distortion, we approximate each one of the pattern sides by a polynomial, fitting the labelled points. In this step, a simple linear regression would not suffice because images have distortion which transforms straight ines into curves. So, a polynomial is more suitable for modeling this phenomenon. Figs. 10a and 10b show these two steps. After having the annotations for all the images of the camera to be calibrated in the test dataset (test-3), the reprojection error is calculated. To do so, the 3D LiDAR labelled points that belong to the pattern boundaries are reprojected into the image using (6)–(9). So, for each collection, the error between each projected point and the closest ground truth point belonging to one of the polynomial curves is calculated as in (21). Fig. 10c shows an example of the reprojection result and the corresponding error for each point. Considering the reprojection errors calculated for each one of the N collections, the root mean square error is also calculated using (22). It should be emphasised that, all of these metrics are publicly available, and are integrated in the ATOM software framework. A specific package called ATOM evaluation was created and can be easily used by the user to evaluate the calibrations performed. 4.3 Evaluation The evaluation procedure applies the previously described metrics to compare ATOM with state-of-the-art calibration methods. Note that these state-of-art methods are pairwise and as such not able to calibrate the entire system simultaneously. The comparison with ATOM, which is a general, full calibration approach, against specialized pairwise methods is not entirely fair. However, since the nature of all the metrics used in the evaluation is also pairwise, it is ATOM that is at a disadvantage in comparison with the other methods. For the camera-to-camera calibration scenario, two versions of ATOM were calibrated in two diffeernt training datasets, and evaluated in the same test dataset. The first is a pairwise calibration between both cameras, and the second a full calibration of the entire AgRob V16 system, with the same two cameras and a 3D LiDAR. It is worth noting that, the calibrations performed consider each camera intrinsic parameters, as well as the pairwise extrinsic calibration between them. To compare ATOM with the state of the art, the OpenCV stereo calibration toolbox (Bradski, 2000) was used to calibrate exactly the same configuration. Additionally, the factory’s intrinsic and extrinsic calibrations were evaluated. Table 3 summarizes all these experiments. Starting by the analysis of ATOM pairwise and ATOM full, we can see that both versions present similar performances, with marginal differences with respect to all the metrics calculated. For example, for the train-1 dataset, we can see a root mean square error difference of 0.075 px and for train-2 0.046 px. Thus, we can conclude that, ATOM allows to optimize an entire robotic system without a significant loss of performance, when comparing with a specific pairwise calibration between the two sensors of interest. In what concerns OpenCV, Table 3 shows that, for the train-1 dataset, it is not able to calibrate. This happens since this framework requires collections where all the pattern corners are detected, i.e., non partial detections. So, since the majority of the collections present in this dataset are partial, OpenCV is not able to calibrate. This is a limitation since, to accomplish a dataset without partial collections, its variety can be limited due to the impossibility of collecting, e.g., collections with the pattern far away from the cameras. On the other hand, for the train-2 dataset, OpenCV achieves the smaller reprojection root mean square error. In this, the number of partial collections is low, and OpenCV, a specialized pairwise calibrator for cameras, performs an accurate intrinsic and extrinsic calibration. Even so, ATOM full shows errors only slightly higher than OpenCV for this dataset, showing that it is capable of achieving a state-of-the-art performance, even considering a non pairwise approach. Concerning the camera factory calibration, it is clear that it presents the less accurate calibration. This can be explained since the calibration the same for all the equipments. Finally, to analyse the impact of the ATOM’s calibration, Fig. 11 shows the dispersion of the reprejection error per collection, before and after calibrating with ATOM full. As expected, the dispersion of the error before calibrating is higher in almost all collections. On the contrary, after calibrating the cameras with ATOM full, the dispersion drastically reduces, with the exception of one collection (represented in brown). This collection can represent a degenerate of data collection, due to, for example, de-synchronization of the data from from the sensors. In order to evaluate the camera to LiDAR calibration, ATOM was used to calibrate both modalities in three different manners: two ATOM pairwise versions, one between the LiDAR and each camera, and the ATOM full version that comprises all three sensors. To compare our approach with the state-of-the-art, ICP (Besl & McKay, 1992) was used to calibrate the left camera and the LiDAR in two different ways: one considering the average of the calibration obtained in all the collections, and other considering only the collection that presents the best alignment between the two point clouds. Note that, ICP only calibrates the left camera w.r.t. the LiDAR since the stereo camera point cloud extracted directly using the manufacture’s API is defined in this camera referential. Table 4 summarizes all this information. Similarly to the camera-to-camera case, here we can verify that ATOM pairwise and ATOM full result in a similar calibration performance, with marginal reprojection error differences. So, once again, this leads to the conclusion that ATOM full can be used to calibrate all the robotic system without any significant loss of performance while evaluating calibrations between pairs of sensors. In this set of tests, a consistent decrease of performance of all the calibration configurations from train-1 to train-2 dataset is observed. This consistency can be caused by synchronization errors between sensors while collecting the calibration data. Looking for the ICP performance on train-1 dataset, firstly, we can conclude that the ICP average is highly affected by outliers, i.e., collections where the calibration fails. This can be inferred by the high standard deviations present in the x and y reprojection error compoenents. ICP best, despite being significantly less accurate than ATOM, presents a better performance than ICP average. The overall bad performance of ICP can be explained by the difficulty of aligning a dense point cloud (provided by the stereo camera), and a sparse one (provided by the laser). It is worth noting that, the train-2 dataset does not contain the stereo camera point cloud, so here ICP can not be used for calibration. To have a visual perception of the ATOM full performance on the calibration of the left camera and the LiDAR, Fig. 12 shows the reprojection of the LiDAR 3D points in the left camera image. In this Fig., color represents the points depth. Here, the transitions of the objects can be sharply observed, which is a good indicator for the calibration performance. 4.4 Impact of the number of collections used for training One of the major questions in general for calibration procedures is the minimum amount of data required to calibrate sensors with precision. In this section we propose an evaluation of ATOM full calibration using different numbers of training collections. The calibration of the three combinations of sensors is evaluated for five different levels of collections used. Table 5 presents the results obtained for each configuration. Starting by the analysis of the camera-to-camera calibration, here we can see that the increase of the number of collections leads to a increase in performance. Using a single collection, as expected, results in higher reprojection, rotation and translation errors. While increasing the number of training collections, the performance increases, with the best performance being observed with the maximum number of collections. Looking at the performance of the calibration of the LiDAR with both cameras, we can see that, as expected using a single collection also results in a higher reprojection error. However, in this case, this difference is not significant, and while increasing the number of collections, the performance saturates. Thus, we can conclude that, the increase of the number of collections has a positive impact in the final performance of all the calibration configurations. However, the camera-to-camera calibration is more sensible to the lower number of collections than the camera-to-LiDAR calibration. 4.5 Impact of the number of incomplete collections used for training The proposed calibration framework, ATOM, supports collections where the pattern is not detected by all the sensors in the calibration system. For example, suppose that the pattern is, for a specific collection, is viewed by the right camera and the LiDAR but not by the left camera. The current section intends to evaluate the impact of this type of collections, and conclude if the presence of incomplete collections has, or not, correlation with changes on ATOM’s performance. Table 6 summarizes the results obtained for three sensor configurations with four different values of incomplete collections, maintaining the same number of training collections. ATOM can deal with incomplete collections, as long as the pattern is detected by at least one sensor. If not, the calibration system can not compute any residual, and that collection must be discarded. On the other hand, if, a single sensor does not detect the pattern for a specific collection, this leads to a reduction of the number of residuals used on the optimization procedure. This being said, results do not show any correlation between the increase of the number of incomplete collections and the performance of ATOM. So, this leads to the conclusion that ATOM can deal with the decrease of the number of residuals (as long as sufficient number of collections are provided). This conclusion consistent with the one taken from Table 5, where it was shown that, ATOM can calibrate accurately for a reasonable low number of collections. 4.6 Impact of the accuracy of the initial estimate The initial estimate (or initial guess) of the calibration parameters has an impact in the outcome of the optimization. A good initial estimate provides a sufficient approximation that allows the optimization process to find the optimal solution that best represents the real calibration of the system. In turn, a inaccurate estimate may lead the optimization process to an unrecoverable state where it is not possible to achieve the optimal solution. This is known as the problem of local minima. In this section, we are interested in assessing the robustness of ATOM to the accuracy of the initial estimate. More specifically, the angle and distance error to the optimal state that our method can handle and the additional execution times that result from said errors. To characterize the robustness to the angle error, we start with the optimal angle and then add the angle error to the Euler components of the rotation. The sign of the error is provided by a fair binomial sampling (Girshick, Mosteller, & Savage, 2006). The tolerance to the distance error is found by sampling an uniform offset from the optimal positions that sits on a sphere with a radius equal to the distance error. Because random sampling is used, we run the experiment for each error 10 times and the reported values are the mean of the runs. By increasing the error in several steps, we can pinpoint the error at which our optimization process will fail. Note that the error is applied to all pose parameters that are being estimated. The results are presented in Fig. 13 . The obtained results show that our optimization process can handle an angle error of approximate 20 degrees and a distance error of 0.5 meters, for each sensor. In our opinion, these errors provide a sufficient margin of tolerance for a practical usage of our manual procedure for initial parameter estimation, described in Section 3.2. The execution times are strictly related to the convergence of the optimization. Inside the margin of tolerance, the execution times are mostly constant (with some fluctuations). This means the optimization process adequately handles the imposed error with a proper convergence. 5 Conclusions This paper solves the problem of camera-to-LiDAR calibration using the optimization of atomic transformations. To do so, this work formulates the calibration as a Bundle Adjustment problem, minimizing the reprojection error of sensors that can have different modalities. Our approach, ATOM, provides several advantages when compared with the current state-of-the-art: (i) it offers a framework to simultaneously calibrate any number of sensors; (ii) it improves the optimization of different sensor modalities by introducing the multi-modal normalization. (iii) it maintains the topology of the input transformation tree; (iv) it supports incomplete and partial collections of data, which makes the detection procedure more flexible and robust; (v) it uses a common calibration pattern, which generalizes the approach; (vi) it has seamless integration with ROS, setting a complete framework for camera to LIDAR calibration. Results show that the proposed approach presents similar performance in comparison with the state-of-the-art, even calibrating the entire robotic system simultaneously. These results demonstrate that ATOM can achieve the same performance of specialized methods in pairwise calibration between specific sensors, while running a complete calibration with multiple sensors of different modalities. Furthermore, our framework proved to be robust to inaccurate initial guesses and small number of collections. Finally, the use of a generic calibration pattern constitutes a major advance since, in the current state-of-the-art, many approaches use built in–house patterns. Future work aims to test ATOM in more advanced robotic systems, with multiple 3D LiDARs. Additionally, the problem of data synchronization while collecting data for calibration will be addressed thought the use of simple concepts such as data interpolation, and more advanced ones, such as generative adversarial networks to generate synchronized sensor data. CRediT authorship contribution statement André Silva Pinto de Aguiar: Conceptualization, Methodology, Software, Writing - original draft. Miguel Armando Riem de Oliveira: Conceptualization, Methodology, Software, Writing - review & editing. Eurico Farinha Pedrosa: Conceptualization, Methodology, Software, Writing - review & editing. Filipe Baptista Neves dos Santos: Writing - review & editing, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements André Silva Pinto de Aguiar thanks the FCT-Foundation for Science and Technology, Portugal for the Ph.D. Grant DFA/BD/5318/2020. References Agarwal et al., 2010 S. Agarwal N. Snavely S.M. Seitz R. Szeliski Bundle adjustment in the large K. Daniilidis P. Maragos N. Paragios Computer Vision – ECCV 2010 2010 Springer, Berlin Heidelberg Berlin, Heidelberg 29 42 Agarwal, S., Snavely, N., Seitz, S.M., & Szeliski, R. (2010). Bundle adjustment in the large. In K. Daniilidis, P. Maragos, & N. Paragios (Eds.), Computer Vision – ECCV 2010 (pp. 29–42). Berlin, Heidelberg: Springer Berlin Heidelberg. de Aguiar et al., 2020 A.S.P. de Aguiar F.B.N. dos Santos L.C.F. dos Santos V.M. de Jesus Filipe A.J.M. de Sousa Vineyard trunk detection using deep learning – An experimental device benchmark Computers and Electronics in Agriculture 175 2020 105535 de Aguiar, A.S.P., dos Santos, F.B.N., dos Santos, L.C.F., de Jesus Filipe, V.M., & de Sousa, A.J.M. (2020). Vineyard trunk detection using deep learning – an experimental device benchmark. Computers and Electronics in Agriculture, 175, 105535. Álvarez et al., 2014 S. Álvarez D. Llorca M. Sotelo Hierarchical camera auto-calibration for traffic surveillance systems Expert Systems with Applications 41 2014 1532 1542 Álvarez, S., Llorca, D., & Sotelo, M. (2014). Hierarchical camera auto-calibration for traffic surveillance systems. Expert Systems with Applications, 41, 1532–1542. Badue et al., 2021 C. Badue R. Guidolini R.V. Carneiro P. Azevedo V.B. Cardoso A. Forechi L. Jesus R. Berriel T.M. Paixão F. Mutz L. de Paula Veronese T. Oliveira-Santos A.F. De Souza Self-driving cars: A survey Expert Systems with Applications 165 2021 113816 Badue, C., Guidolini, R., Carneiro, R.V., Azevedo, P., Cardoso, V.B., Forechi, A., Jesus, L., Berriel, R., Paixão, T.M., Mutz, F., de Paula Veronese, L., Oliveira-Santos, T., & De Souza, A.F. (2021). Self-driving cars: A survey. Expert Systems with Applications, 165, 113816. Besl and McKay, 1992 P. Besl N.D. McKay A method for registration of 3-d shapes IEEE Transactions on Pattern Analysis and Machine Intelligence 14 1992 239 256 Besl, P., & McKay, N.D. (1992). A method for registration of 3-d shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14, 239–256. Bradski, 2000 G. Bradski The opencv library Dr Dobb’s J. Software Tools 25 2000 120 125 Bradski, G. (2000). The opencv library. Dr Dobb’s J. Software Tools, 25, 120–125. Czyzewski, 2017 Czyzewski, M. A. (2017). An extremely efficient chess-board detection for non-trivial photos. ArXiv, abs/1708.03898. Czyzewski, M.A. (2017). An extremely efficient chess-board detection for non-trivial photos. ArXiv, abs/1708.03898. Dhall et al., 2017 Dhall, A., Chelani, K., Radhakrishnan, V., & Krishna, K.M. (2017). Lidar-camera calibration using 3d–3d point correspondences. Dhall, A., Chelani, K., Radhakrishnan, V., & Krishna, K.M. (2017). Lidar-camera calibration using 3d-3d point correspondences. Durrant-Whyte and Bailey, 2006 H. Durrant-Whyte T. Bailey Simultaneous localization and mapping: part i IEEE Robotics & Automation Magazine 13 2006 99 110 Durrant-Whyte, H., & Bailey, T. (2006). Simultaneous localization and mapping: part i. IEEE Robotics & Automation Magazine, 13, 99–110. Fabbri et al., 2020 R. Fabbri P. Giblin B. Kimia Camera pose estimation using first-order curve differential geometry IEEE Transactions on Pattern Analysis and Machine Intelligence 2020 1 Fabbri, R., Giblin, P., & Kimia, B. (2020). Camera pose estimation using first-order curve differential geometry. IEEE Transactions on Pattern Analysis and Machine Intelligence, (pp. 1–1). Fischler and Bolles, 1981 M.A. Fischler R.C. Bolles Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography Communications of the ACM 24 1981 381 395 Fischler, M.A., & Bolles, R.C. (1981). Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24, 381–395. Foote, 2013 T. Foote tf: The transform library 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA) 2013 IEEE Foote, T. (2013). tf: The transform library. In 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA). IEEE. Fremont and Bonnifait, 2008 V. Fremont P. Bonnifait Extrinsic calibration between a multi-layer lidar and a camera 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 IEEE Fremont, V., & Bonnifait, P. (2008). Extrinsic calibration between a multi-layer lidar and a camera. In 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE. Furgale et al., 2013 P. Furgale J. Rehder R. Siegwart Unified temporal and spatial calibration for multi-sensor systems 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems 2013 IEEE Furgale, P., Rehder, J., & Siegwart, R. (2013). Unified temporal and spatial calibration for multi-sensor systems. In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. Gao et al., 2003 X.-S. Gao X.-R. Hou J. Tang H.-F. Cheng Complete solution classification for the perspective-three-point problem IEEE Transactions on Pattern Analysis and Machine Intelligence 25 2003 930 943 Gao, X.-S., Hou, X.-R., Tang, J., & Cheng, H.-F. (2003). Complete solution classification for the perspective-three-point problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25, 930–943. Garrido-Jurado et al., 2016 S. Garrido-Jurado R. Muñoz-Salinas F. Madrid-Cuevas R. Medina-Carnicer Generation of fiducial marker dictionaries using mixed integer linear programming Pattern Recognition 51 2016 481 491 Garrido-Jurado, S., Muñoz-Salinas, R., Madrid-Cuevas, F., & Medina-Carnicer, R. (2016). Generation of fiducial marker dictionaries using mixed integer linear programming. Pattern Recognition, 51, 481–491. Girshick et al., 2006 M.A. Girshick F. Mosteller L.J. Savage Unbiased estimates for certain binomial sampling problems with applications S.E. Fienberg D.C. Hoaglin Selected Papers of Frederick Mosteller 2006 Springer, New York New York, NY 57 68 Girshick, M.A., Mosteller, F., & Savage, L.J. (2006). Unbiased estimates for certain binomial sampling problems with applications. In S.E. Fienberg, & D.C. Hoaglin (Eds.), Selected Papers of Frederick Mosteller (pp. 57–68). New York, NY: Springer New York. Guindel et al., 2017 C. Guindel J. Beltran D. Martin F. Garcia Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 IEEE Guindel, C., Beltran, J., Martin, D., & Garcia, F. (2017a). Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE. Guindel et al., 2017 C. Guindel J. Beltran D. Martin F. Garcia Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 IEEE Guindel, C., Beltran, J., Martin, D., & Garcia, F. (2017b). Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE. Hornegger and Tomasi, 1999 J. Hornegger C. Tomasi Representation issues in the ML estimation of camera motion Proceedings of the Seventh IEEE International Conference on Computer Vision 1999 IEEE Hornegger, J., & Tomasi, C. (1999). Representation issues in the ML estimation of camera motion. In Proceedings of the Seventh IEEE International Conference on Computer Vision. IEEE. Hu et al., 2019 D. Hu D. DeTone T. Malisiewicz Deep ChArUco: Dark ChArUco marker pose estimation 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019 IEEE Hu, D., DeTone, D., & Malisiewicz, T. (2019). Deep ChArUco: Dark ChArUco marker pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. Huang and Grizzle, 2020 J.-K. Huang J.W. Grizzle Improvements to target-based 3d LiDAR to camera calibration IEEE Access 8 2020 134101 134110 Huang, J.-K., & Grizzle, J.W. (2020). Improvements to target-based 3d LiDAR to camera calibration. IEEE Access, 8, 134101–134110. Huang and Barth, 2009 L. Huang M. Barth A novel multi-planar LIDAR and computer vision calibration procedure using 2d patterns for automated navigation 2009 IEEE Intelligent Vehicles Symposium 2009 IEEE Huang, L., & Barth, M. (2009). A novel multi-planar LIDAR and computer vision calibration procedure using 2d patterns for automated navigation. In 2009 IEEE Intelligent Vehicles Symposium. IEEE. Kim and Park, 2019 E. su Kim E. S.-Y. Extrinsic calibration between camera and LiDAR sensors by matching multiple 3d planes Sensors 20 2019 52 su Kim, E., & Park, S.-Y. (2019). Extrinsic calibration between camera and LiDAR sensors by matching multiple 3d planes. Sensors, 20, 52. Liao et al., 2017 Y. Liao G. Li Z. Ju H. Liu D. Jiang Joint kinect and multiple external cameras simultaneous calibration 2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM) 2017 IEEE Liao, Y., Li, G., Ju, Z., Liu, H., & Jiang, D. (2017). Joint kinect and multiple external cameras simultaneous calibration. In 2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM). IEEE. Majumder and Pratihar, 2018 S. Majumder D.K. Pratihar Multi-sensors data fusion through fuzzy clustering and predictive tools Expert Systems with Applications 107 2018 165 172 Majumder, S., & Pratihar, D.K. (2018). Multi-sensors data fusion through fuzzy clustering and predictive tools. Expert Systems with Applications, 107, 165 – 172. Melendez-Pastor et al., 2017 C. Melendez-Pastor R. Ruiz-Gonzalez J. Gomez-Gil A data fusion system of gnss data and on-vehicle sensors data for improving car positioning precision in urban environments Expert Systems with Applications 80 2017 28 38 Melendez-Pastor, C., Ruiz-Gonzalez, R., & Gomez-Gil, J. (2017). A data fusion system of gnss data and on-vehicle sensors data for improving car positioning precision in urban environments. Expert Systems with Applications, 80, 28 – 38. Mirzaei et al., 2012 F.M. Mirzaei D.G. Kottas S.I. Roumeliotis 3d LIDAR–camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization The International Journal of Robotics Research 31 2012 452 467 Mirzaei, F.M., Kottas, D.G., & Roumeliotis, S.I. (2012). 3d LIDAR–camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization. The International Journal of Robotics Research, 31, 452–467. Oliveira et al., 2019 Oliveira, M., Castro, A., Madeira, T., Dias, P., & Santos, V. (2020). A general approach to the extrinsic calibration of intelligent vehicles using ros. In M.F. Silva, J. Luís Lima, L.P. Reis, A. Sanfeliu, & D. Tardioli (Eds.), Robot 2019: Fourth Iberian Robotics Conference (pp. 203–215). Cham: Springer International Publishing. Oliveira, M., Castro, A., Madeira, T., Dias, P., & Santos, V. (2020a). A general approach to the extrinsic calibration of intelligent vehicles using ros. In M.F. Silva, J. Luís Lima, L.P. Reis, A. Sanfeliu, & D. Tardioli (Eds.), Robot 2019: Fourth Iberian Robotics Conference (pp. 203–215). Cham: Springer International Publishing. Oliveira et al., 2020 M. Oliveira A. Castro T. Madeira E. Pedrosa P. Dias V. Santos A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Robotics and Autonomous Systems 131 2020 103558 Oliveira, M., Castro, A., Madeira, T., Pedrosa, E., Dias, P., & Santos, V. (2020b). A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach. Robotics and Autonomous Systems, 131, 103558. Pandey et al., 2010 G. Pandey J. McBride S. Savarese R. Eustice Extrinsic calibration of a 3d laser scanner and an omnidirectional camera IFAC Proceedings Volumes 43 2010 336 341 Pandey, G., McBride, J., Savarese, S., & Eustice, R. (2010). Extrinsic calibration of a 3d laser scanner and an omnidirectional camera. IFAC Proceedings Volumes, 43, 336–341. de Paula et al., 2014 M. de Paula C. Jung L. da Silveira Automatic on-the-fly extrinsic camera calibration of onboard vehicular cameras Expert Systems with Applications 41 2014 1997 2007 de Paula, M., Jung, C., & da Silveira, L. (2014). Automatic on-the-fly extrinsic camera calibration of onboard vehicular cameras. Expert Systems with Applications, 41, 1997–2007. Penate-Sanchez et al., 2013 A. Penate-Sanchez J. Andrade-Cetto F. Moreno-Noguer Exhaustive linearization for robust camera pose and focal length estimation IEEE Transactions on Pattern Analysis and Machine Intelligence 35 2013 2387 2400 Penate-Sanchez, A., Andrade-Cetto, J., & Moreno-Noguer, F. (2013). Exhaustive linearization for robust camera pose and focal length estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 2387–2400. Pradeep et al., 2014 V. Pradeep K. Konolige E. Berger Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach Experimental Robotics 2014 Springer Berlin Heidelberg 211 225 Pradeep, V., Konolige, K., & Berger, E. (2014). Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach. In Experimental Robotics (pp. 211–225). Springer Berlin Heidelberg. Quigley et al., 2009 Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., & Ng, A. Y. (2009). Ros: an open-source robot operating system. In ICRA workshop on open source software (p. 5). Kobe, Japan volume 3. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., & Ng, A.Y. (2009). Ros: an open-source robot operating system. In ICRA workshop on open source software (p. 5). Kobe, Japan volume 3. Rehder et al., 2016 J. Rehder R. Siegwart P. Furgale A general approach to spatiotemporal calibration in multisensor systems IEEE Transactions on Robotics 32 2016 383 398 Rehder, J., Siegwart, R., & Furgale, P. (2016). A general approach to spatiotemporal calibration in multisensor systems. IEEE Transactions on Robotics, 32, 383–398. Romero-Ramirez et al., 2018 F.J. Romero-Ramirez R. Muñoz-Salinas R. Medina-Carnicer Speeded up detection of squared fiducial markers Image and Vision Computing 76 2018 38 47 Romero-Ramirez, F.J., Muñoz-Salinas, R., & Medina-Carnicer, R. (2018). Speeded up detection of squared fiducial markers. Image and Vision Computing, 76, 38–47. Santos et al., 2016 F.N. dos Santos H. Sobreira D. Campos R. Morais A.P. Moreira O. Contente Towards a reliable robot for steep slope vineyards monitoring Journal of Intelligent & Robotic Systems 83 2016 429 444 dos Santos, F.N., Sobreira, H., Campos, D., Morais, R., Moreira, A.P., & Contente, O. (2016). Towards a reliable robot for steep slope vineyards monitoring. Journal of Intelligent & Robotic Systems, 83, 429–444. Santos et al., 2019 L. Santos F. Santos J. Mendes P. Costa J. Lima R. Reis P. Shinde Path planning aware of robot’s center of mass for steep slope vineyards Robotica 38 2019 684 698 Santos, L., Santos, F., Mendes, J., Costa, P., Lima, J., Reis, R., & Shinde, P. (2019). Path planning aware of robot’s center of mass for steep slope vineyards. Robotica, 38, 684–698. Sariff and Buniyamin, 2006 N. Sariff N. Buniyamin An overview of autonomous mobile robot path planning algorithms 2006 4th Student Conference on Research and Development 2006 IEEE Sariff, N., & Buniyamin, N. (2006). An overview of autonomous mobile robot path planning algorithms. In 2006 4th Student Conference on Research and Development. IEEE. Sturm, 2014 P. Sturm Pinhole camera model K. Ikeuchi Computer Vision: A Reference Guide 2014 Springer, US Boston, MA 610 613 Sturm, P. (2014). Pinhole camera model. In K. Ikeuchi (Ed.), Computer Vision: A Reference Guide (pp. 610–613). Boston, MA: Springer US. Verma et al., 2019 S. Verma J.S. Berrio S. Worrall E. Nebot Automatic extrinsic calibration between a camera and a 3D lidar using 3D point and plane correspondences 2019 IEEE Intelligent Transportation Systems Conference (ITSC) 2019 IEEE Auckland, New Zealand 3906 3912 Verma, S., Berrio, J.S., Worrall, S., & Nebot, E. (2019). Automatic extrinsic calibration between a camera and a 3D lidar using 3D point and plane correspondences. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 3906–3912). Auckland, New Zealand: IEEE. Wang et al., 2017 W. Wang K. Sakurada N. Kawaguchi Reflectance intensity assisted automatic and accurate extrinsic calibration of 3d LiDAR and panoramic camera using a printed chessboard Remote Sensing 9 2017 851 Wang, W., Sakurada, K., & Kawaguchi, N. (2017). Reflectance intensity assisted automatic and accurate extrinsic calibration of 3d LiDAR and panoramic camera using a printed chessboard. Remote Sensing, 9, 851. Zhou et al., 2018 L. Zhou Z. Li M. Kaess Automatic extrinsic calibration of a camera and a 3d LiDAR using line and plane correspondences 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2018 IEEE Zhou, L., Li, Z., & Kaess, M. (2018). Automatic extrinsic calibration of a camera and a 3d LiDAR using line and plane correspondences. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. Zuniga-Noel et al., 2019 D. Zuniga-Noel J.-R. Ruiz-Sarmiento R. Gomez-Ojeda J. Gonzalez-Jimenez Automatic multi-sensor extrinsic calibration for mobile robots IEEE Robotics and Automation Letters 4 2019 2862 2869 Zuniga-Noel, D., Ruiz-Sarmiento, J.-R., Gomez-Ojeda, R., & Gonzalez-Jimenez, J. (2019). Automatic multi-sensor extrinsic calibration for mobile robots. IEEE Robotics and Automation Letters, 4, 2862–2869. "
    },
    {
        "doc_title": "Robust texture mapping using rgb-d cameras",
        "doc_scopus_id": "85105402165",
        "doc_doi": "10.3390/s21093248",
        "doc_eid": "2-s2.0-85105402165",
        "doc_date": "2021-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D meshes",
            "Camera misalignments",
            "Camera pose estimation",
            "Estimation methodologies",
            "Pose estimation errors",
            "Rgb-d cameras",
            "Texture mapping",
            "Visual artifacts"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.The creation of a textured 3D mesh from a set of RGD-D images often results in textured meshes that yield unappealing visual artifacts. The main cause is the misalignments between the RGB-D images due to inaccurate camera pose estimations. While there are many works that focus on improving those estimates, the fact is that this is a cumbersome problem, in particular due to the accumulation of pose estimation errors. In this work, we conjecture that camera poses estimation methodologies will always display non-neglectable errors. Hence, the need for more robust texture mapping methodologies, capable of producing quality textures even in considerable camera misalignments scenarios. To this end, we argue that use of the depth data from RGB-D images can be an invaluable help to confer such robustness to the texture mapping process. Results show that the complete texture mapping procedure proposed in this paper is able to significantly improve the quality of the produced textured 3D meshes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Tomato Detection Using Deep Learning for Robotics Application",
        "doc_scopus_id": "85115446811",
        "doc_doi": "10.1007/978-3-030-86230-5_3",
        "doc_eid": "2-s2.0-85115446811",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Agricultural environments",
            "Agricultural robotics",
            "Augmented images",
            "Fruit and vegetables",
            "Fruit detection",
            "Fruit harvesting",
            "Harvesting robotic",
            "Image datasets",
            "Performance",
            "Robotics applications"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.The importance of agriculture and the production of fruits and vegetables has stood out mainly over the past few years, especially for the benefits for our health. In 2021, in the international year of fruit and vegetables, it is important to encourage innovation and evolution in this area, with the needs surrounding the different processes of the different cultures. This paper compares the performance between two datasets for robotics fruit harvesting using four deep learning object detection models: YOLOv4, SSD ResNet 50, SSD Inception v2, SSD MobileNet v2. This work aims to benchmark the Open Images Dataset v6 (OIDv6) against an acquired dataset inside a tomatoes greenhouse for tomato detection in agricultural environments, using a test dataset with acquired non augmented images. The results highlight the benefit of using self-acquired datasets for the detection of tomatoes because the state-of-the-art datasets, as OIDv6, lack some relevant characteristics of the fruits in the agricultural environment, as the shape and the color. Detections in greenhouses environments differ greatly from the data inside the OIDv6, which has fewer annotations per image and the tomato is generally riped (reddish). Standing out in the use of our tomato dataset, YOLOv4 stood out with a precision of 91%. The tomato dataset was augmented and is publicly available (See https://rdm.inesctec.pt/ and https://rdm.inesctec.pt/dataset/ii-2021-001 ).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-sensor extrinsic calibration using an extended set of pairwise geometric transformations",
        "doc_scopus_id": "85096588033",
        "doc_doi": "10.3390/s20236717",
        "doc_eid": "2-s2.0-85096588033",
        "doc_date": "2020-11-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Automatic method",
            "Calibration procedure",
            "Calibration process",
            "Experimental methods",
            "Extrinsic calibration",
            "Geometric transformations",
            "Human intervention",
            "Multiple sensors"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Systems composed of multiple sensors for exteroceptive perception are becoming increasingly common, such as mobile robots or highly monitored spaces. However, to combine and fuse those sensors to create a larger and more robust representation of the perceived scene, the sensors need to be properly registered among them, that is, all relative geometric transformations must be known. This calibration procedure is challenging as, traditionally, human intervention is required in variate extents. This paper proposes a nearly automatic method where the best set of geometric transformations among any number of sensors is obtained by processing and combining the individual pairwise transformations obtained from an experimental method. Besides eliminating some experimental outliers with a standard criterion, the method exploits the possibility of obtaining better geometric transformations between all pairs of sensors by combining them within some restrictions to obtain a more precise transformation, and thus a better calibration. Although other data sources are possible, in this approach, 3D point clouds are obtained by each sensor, which correspond to the successive centers of a moving ball its field of view. The method can be applied to any sensors able to detect the ball and the 3D position of its center, namely, LIDARs, mono cameras (visual or infrared), stereo cameras, and TOF cameras. Results demonstrate that calibration is improved when compared to methods in previous works that do not address the outliers problem and, depending on the context, as explained in the results section, the multi-pairwise technique can be used in two different methodologies to reduce uncertainty in the calibration process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach",
        "doc_scopus_id": "85085374137",
        "doc_doi": "10.1016/j.robot.2020.103558",
        "doc_eid": "2-s2.0-85085374137",
        "doc_date": "2020-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Extrinsic calibration",
            "Multi-modal approach",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization",
            "State-of-the-art approach"
        ],
        "doc_abstract": "© 2020 Elsevier B.V.This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2020-05-19 2020-05-19 2020-05-28 2020-05-28 2020-08-03T02:56:00 S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 S300 S300.1 FULL-TEXT 2020-08-03T02:04:37.606251Z 0 0 20200901 20200930 2020 2020-05-19T15:45:05.788726Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref vitae 0921-8890 09218890 true 131 131 C Volume 131 7 103558 103558 103558 202009 September 2020 2020-09-01 2020-09-30 2020 article fla © 2020 Elsevier B.V. All rights reserved. AROSFRAMEWORKFOREXTRINSICCALIBRATIONINTELLIGENTVEHICLESAMULTISENSORMULTIMODALAPPROACH OLIVEIRA M 1 Introduction 2 ROS based calibration setup 2.1 Configuration of the calibration procedure 2.2 Interactive positioning of sensors 2.3 Interactive data labelling 2.4 Collecting data 2.5 Sensor poses from partial transformations 3 Calibration procedure 3.1 Optimization parameters 3.2 Objective function 3.2.1 Camera sub-function 3.2.2 Laser sub-function 3.3 Sensors pose calibration: Optimization 4 Results 4.1 Camera to camera 4.2 Complete system calibration 5 Conclusions and future work Acknowledgements References MUELLER 2017 1 6 G 2017IEEE20THINTCONFINTELLIGENTTRANSPORTATIONSYSTEMSITSC CONTINUOUSSTEREOCAMERACALIBRATIONINURBANSCENARIOS WU 2015 2638 2642 L 2015IEEEINTCONFMECHATRONICSAUTOMATIONICMA BINOCULARSTEREOVISIONCAMERACALIBRATION ROUSU 2016 896 900 L 2016IEEEADVANCEDINFORMATIONMANAGEMENTCOMMUNICATESELECTRONICAUTOMATIONCONTROLCONFIMCEC AUTOMATICCALIBRATIONSYSTEMFORBINOCULARSTEREOIMAGING LING 2016 1771 1778 Y 2016IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROS HIGHPRECISIONONLINEMARKERLESSSTEREOEXTRINSICCALIBRATION DINH 2019 815 826 V VASCONCELOS 2012 2097 2107 F PEREIRA 2016 326 337 M ALMEIDA 2012 312 319 M IMAGEANALYSISRECOGNITION 3D2DLASERRANGEFINDERCALIBRATIONUSINGACONICBASEDGEOMETRYSHAPE GUINDEL 2017 1 6 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS KWON 2018 1451 1454 Y 201818THINTCONFCONTROLAUTOMATIONSYSTEMSICCAS AUTOMATICSPHEREDETECTIONFOREXTRINSICCALIBRATIONMULTIPLERGBDCAMERAS KHAN 2016 1960 1965 A 2016IEEEINTCONFROBOTICSBIOMIMETICSROBIO CALIBRATIONACTIVEBINOCULARRGBDVISIONSYSTEMSFORDUALARMROBOTS BASSO 2018 1315 1332 F QIAO 2013 253 256 Y 2013INTCONFCOMPUTATIONALPROBLEMSOLVINGICCP ANEWAPPROACHSELFCALIBRATIONHANDEYEVISIONSYSTEMS ZHANG 2011 1 6 C 2011IEEEINTCONFMULTIMEDIAEXPO CALIBRATIONBETWEENDEPTHCOLORSENSORSFORCOMMODITYDEPTHCAMERAS CHEN 2019 2685 2694 G QILONGZHANG 2004 2301 2306 G 2004IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROSIEEECATNO04CH37566VOL3 EXTRINSICCALIBRATIONACAMERALASERRANGEFINDERIMPROVESCAMERACALIBRATION HASELICH 2012 25 28 M 2012IEEEINTCONFEMERGINGSIGNALPROCESSINGAPPLICATIONS CALIBRATIONMULTIPLECAMERASA3DLASERRANGEFINDER CHEN 2016 448 453 Z 20169THINTCONGRESSIMAGESIGNALPROCESSINGBIOMEDICALENGINEERINGINFORMATICSCISPBMEI EXTRINSICCALIBRATIONALASERRANGEFINDERACAMERABASEDAUTOMATICDETECTIONLINEFEATURE VELAS 2014 M CALIBRATIONRGBCAMERAVELODYNELIDAR LEE 2017 64 69 G 2017IEEEINTCONFMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI CALIBRATIONVLP16LIDARMULTIVIEWCAMERASUSINGABALLFOR360DEGREE3DCOLORMAPACQUISITION LEVINSON 2013 J ROBOTICSSCIENCESYSTEMS AUTOMATICONLINECALIBRATIONCAMERASLASERS DEZHIGAO 2010 6211 6215 J 20108THWORLDCONGRESSINTELLIGENTCONTROLAUTOMATION AMETHODSPATIALCALIBRATIONFORCAMERARADAR SANTOS 2010 1421 1427 V 13THINTIEEECONFINTELLIGENTTRANSPORTATIONSYSTEMS ATLASCARTECHNOLOGIESFORACOMPUTERASSISTEDDRIVINGSYSTEMBOARDACOMMONAUTOMOBILE LIAO 2017 305 310 Y 20172NDINTCONFADVANCEDROBOTICSMECHATRONICSICARM JOINTKINECTMULTIPLEEXTERNALCAMERASSIMULTANEOUSCALIBRATION REHDER 2016 383 398 J PRADEEP 2014 211 225 V EXPERIMENTALROBOTICS12THINTSYMPOSIUMEXPERIMENTALROBOTICS CALIBRATINGAMULTIARMMULTISENSORROBOTABUNDLEADJUSTMENTAPPROACH OLIVEIRA 2020 203 215 M ROBOT2019FOURTHIBERIANROBOTICSCONFERENCE AGENERALAPPROACHEXTRINSICCALIBRATIONINTELLIGENTVEHICLESUSINGROS BRADSKI 2000 G QUIGLEY 2009 M ICRAWORKSHOPOPENSOURCESOFTWARE ROSOPENSOURCEROBOTOPERATINGSYSTEM FOOTE 2013 1 6 T 2013IEEECONFERENCETECHNOLOGIESFORPRACTICALROBOTAPPLICATIONSTEPRA TFTRANSFORMLIBRARY HORNEGGER 1999 640 647 J PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISIONVOL1 REPRESENTATIONISSUESINMLESTIMATIONCAMERAMOTION AGARWAL 2010 29 42 S BUNDLEADJUSTMENTINLARGE OLIVEIRAX2020X103558 OLIVEIRAX2020X103558XM 2022-05-28T00:00:00.000Z 2022-05-28T00:00:00.000Z © 2020 Elsevier B.V. All rights reserved. 2020-05-03T22:04:14.426Z FCT Fundação para a Ciência e a Tecnologia CYTED CYTED Ciencia y Tecnología para el Desarrollo item S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 271599 2020-08-03T02:04:37.606251Z 2020-09-01 2020-09-30 true 2168899 MAIN 11 62586 849 656 IMAGE-WEB-PDF 1 gr3 35928 345 552 gr9 19670 275 339 pic2 6890 131 112 gr1 33542 289 490 gr12 23770 202 495 gr11 36248 266 489 gr7 37441 309 371 fx1002 8338 79 360 gr4 28117 314 378 gr6 18379 143 385 fx1001 8242 73 367 gr5 45234 242 376 gr2 42926 236 553 gr8 52449 600 489 gr10 49550 264 489 fx1003 6813 77 283 gr3 8344 137 219 gr9 6226 164 202 pic2 19499 164 140 gr1 5132 129 219 gr12 5757 89 219 gr11 7781 119 219 gr7 20274 163 196 fx1002 2821 48 219 gr4 6100 163 197 gr6 10154 81 219 fx1001 2829 43 219 gr5 25359 141 219 gr2 10115 94 219 gr8 4814 163 133 gr10 11178 118 219 fx1003 3134 60 219 gr3 277021 1527 2446 gr9 136675 1219 1500 pic2 61746 583 499 gr1 243828 1282 2171 gr12 231425 895 2191 gr11 268102 1178 2168 gr7 362638 1370 1643 fx1002 30951 210 958 gr4 207176 1390 1675 gr6 147160 632 1706 fx1001 30843 193 975 gr5 416481 1072 1667 gr2 344316 1047 2451 gr8 390098 2658 2167 gr10 424428 1169 2168 fx1003 24651 206 753 si115 6585 si97 32421 si36 1420 si95 6165 si111 9455 si44 6967 si7 7552 si94 1102 si42 7085 si121 31424 si5 7072 si74 5953 si89 2139 si119 9249 si71 1806 si10 1146 si30 8639 si12 235 si35 7139 si8 12045 si37 8297 si54 6126 si16 25662 si41 1679 si117 27686 si80 14084 si88 1722 si51 1990 si91 10025 si85 7503 si6 5529 si110 14513 si21 22352 si26 1589 si32 5724 si45 21948 si47 2672 si24 1842 si27 1665 si40 5395 si112 8789 si118 8959 si18 5806 si55 7274 si59 21073 si1 1359 si87 1814 si70 2168 si38 3203 si52 22498 si83 5598 si53 5544 si77 23977 si4 916 si49 2727 si17 4071 si90 10327 si3 3897 si50 3168 si29 1813 si39 21584 si22 37437 si48 1268 si114 16710 si34 1274 si23 2199 si28 1675 si92 3705 si25 1781 si75 36143 si31 1407 am false 3012923 ROBOT 103558 103558 S0921-8890(20)30398-5 10.1016/j.robot.2020.103558 Elsevier B.V. Fig. 1 Two methodologies for solving the calibration of complex systems using pair-wise approaches: (a) sequential pairwise; (b) one level pyramid using a reference sensor. The estimated transformations use the arrangements shown in solid colour arrows. Other possible arrangements are presented in dashed grey lines. Note that in both cases only a subset of the available transformations is used. Fig. 2 The proposed calibration procedure: (a) initialization from xacro files and interactive first guess; (b) data labelling and collecting. Fig. 3 Interactive labelling of 2D LiDAR data: (a) creation of interactive marker on the sensor body, (b) dragging and dropping the marker on top of the data cluster containing the chessboard plane, (c) and (d) subsequent automatic tracking of the chessboard plane. Fig. 4 Conceptual transformation graph for a complex robotic system. Each sensor has a respective calibration partial transformation, denoted by the solid edges. Dashed edges contain transformations which are not optimized (they may be static or dynamic). Each sensor has a corresponding link to which the data it collects is attached, denoted in the figure by the solid thin ellipses. Very few approaches in the literature are capable of calibrating such a system while preserving the initial structure of the graph of transformations. Fig. 5 Example of reprojection of chessboard corners during the optimization procedure: squares denote the position of the detected chessboard corners (ground truth points); crosses denote the initial position of each projected corner; points denote the current position of the projected corners. Fig. 6 Chessboard: graphics visualization of the created grid and boundary correspondent to the real chessboard (a); image of the real chessboard (b). Fig. 7 ATLASCAR2: Autonomous vehicle from the Department of Mechanical Engineering of the University of Aveiro; the sensors are indicated by the Ellipses. Fig. 8 Pixel coordinates errors between projected (expected) chessboard corners and the ground truth indexes, from top left camera to top right camera, for each collection, before the optimization procedure (a) and after the optimization procedure (b). Fig. 9 Flowchart representing the results comparison structure. Each ellipse represents a JSON file and each rectangle identifies a programmed application. Fig. 10 Pixel coordinate errors between projected and expected chessboard corners. The kalibr results are not visible because of the selection of the axes range. Fig. 11 Average error per sensor during a full system calibration procedure. Errors for cameras in pixels, for LiDARs in metres. Fig. 12 Left laser (dots surrounded by red circles) and right laser (dots surrounded by green circles) data overlaid onto a representation of the chessboard, taking in consideration the pose of the chessboard and the LiDARs as estimated by the calibration for one particular collection: (a) and (b) the start of the optimization (initial guess); (c) and (d) the end of the optimization (calibration results). Table 1 Average errors and standard deviations along both directions, before and after the optimization. Values in pixels. Values Average error Standard deviation Initial Final Initial Final x error 2.25 1.64 2.68 1.69 y error 17.09 0.53 3.32 0.62 Both 17.34 1.83 8.71 1.51 Table 2 Average errors and standard deviations, in pixels, for the distances in x axis and y axis, for the proposed approach, the OpenCV stereo calibration and the kalibr calibration method. For kalibr, two datasets were used for training: the train dataset, which was also used to train all other approaches, and the test dataset, which was used to evaluate all approaches. Values in pixels. Calibration method Average error Standard deviation x y x y Proposed approach (left) 2.218 1.633 1.223 0.584 Proposed approach (right) 2.080 1.797 1.253 0.608 OpenCV stereo calibrate 1.251 0.903 1.509 0.767 Kalibr (train) [26] 67.383 8.887 0.832 1.722 Kalibr (test) [26] 1.187 17.999 1.369 2.225 A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Miguel Oliveira a b Afonso Castro b ⁎ Tiago Madeira a Eurico Pedrosa a Paulo Dias a c Vítor Santos a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal, Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro b Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal Department of Mechanical Engineering, University of Aveiro c Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro ⁎ Corresponding author. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups. Keywords Extrinsic calibration ROS Optimization Bundle adjustment Intelligent vehicles OpenCV 1 Introduction Intelligent vehicles require a considerable amount of on-board sensors, often of multiple modalities (e.g. camera, Light Detection And Ranging (LiDAR), etc.) in order to operate consistently. The combination of the data collected by these sensors requires a transformation or projection of data from one sensor coordinate frame to another. The process of estimating these transformations between sensor coordinate systems is called extrinsic calibration. An extrinsic calibration between two sensors requires an association of data from one sensor to the data of another. By knowing these data associations, an optimization procedure can be formulated to estimate the parameters of the transformation between those sensors that minimizes the distance between associations. Most calibration approaches make use of calibration patterns, i.e., objects that are robustly and accurately detected by distinct sensor modalities. Although there have been many solutions available in the literature, on the topic of calibration, there is no straightforward solution for the calibration of multiple sensors in intelligent vehicles, or robots in general. There are multiple factors that contribute to this lack of solutions. The majority of works on calibration focus on sensor to sensor pairwise calibrations: between only cameras [1–5] or between cameras and LiDARs [6–10]. When considering pairwise combinations of sensors, there are several possibilities, according to the modality of each of the sensors in the pair. Most of them have been addressed in the literature: RGB to RGB camera calibration [1–5,7]; RGB to depth camera (RGB-D cameras) calibration [11–16]; camera to 2D LiDAR [6,7,10,15,17–20]; 2D LiDAR to 3D LiDAR [8]; camera to 3D LiDAR [10,21,22]; and camera to radar [23]. Nonetheless, all these approaches have the obvious shortcoming of operating only with a single pair of sensors, which is not directly applicable to the case of intelligent vehicles, or more complex robotic systems in general. To be applicable in those cases, pairwise approaches must be arranged in a graph-like sequential procedure, in which one sensor calibrates with another, that then relates to a third sensor, and so forth. Another option is to establish one sensor as the reference sensor and link all other sensors to it. In this case, the graph of transformations between sensors results in a one level pyramid, which contains the reference sensor on top and all other sensors at the base. One example is [7], in which a methodology for calibrating the ATLASCAR2 autonomous vehicle [24] is proposed, wherein all sensors are paired with a reference sensor. Sequential pairwise approaches have three major shortcomings: (i) transformations are estimated using only data provided from the selected sensor tandem, despite the fact that data from additional sensors could be available and prove relevant to the overall accuracy of the calibration procedure; (ii) sensitivity to cumulative errors, since the transformations are computed in a sequence (in fact, this does not occur in [7], since the pose of a specific sensor only depends on the reference sensor pose); (iii) structure of transformation graph is enforced by the nature of the calibration procedure, rather than being defined by the preference of the programmer, which could compromise some robot functionalities. Fig. 1 shows a conceptual example in which these problems are visible. There are a few works which address the problem of calibration from a multi-sensor, simultaneous optimization, perspective. In [25], a joint objective function is proposed to simultaneously calibrate three RGB cameras with respect to an RGB-D camera. Authors report a significant improvement in the accuracy of the calibration. In [26], an approach for joint estimation of both temporal offsets and spatial transformations between sensors is presented. This approach is one of few that is not designed for a particular set of sensors, since its methodology does not rely on unique properties of specific sensors. It is able to calibrate systems containing both cameras and LiDARs. Moreover, the approach does not require the usage of calibration patterns for the LiDARs, using the planes present in the scene for that purpose. In [27], a joint calibration of the joint offsets and the sensors locations for a PR2 robot is proposed. This method takes sensor uncertainty into account and is modelled in a similar way to the bundle adjustment problem. Our approach is similar to [27], in the sense that we also employ a bundle adjustment-like optimization procedure. However, our approach is not focused on a single robotic platform, rather it is a general approach that is applicable to any robotic system, which also relates it with [26]. This paper is an extension of [28], where the general approach was originally proposed. This extension focuses on the comparison of this work against state of the art calibration approaches, i.e. methodologies provided by Open Source Computer Vision Library (OpenCV) [29] as well as the calibration method from [26]. Robot Operating System ROS [30] based architectures are the standard when developing robots. There are several ROS based calibration packages available. 1 1 2 2 3 3 In addition, some approaches are well integrated with ROS since the input data for the calibration is provided as a rosbag file. Despite this, no approach provides a complete solution for the calibration of intelligent vehicles. Thus, the seamless integration with ROS became a core component of the proposed approach. To that end, the proposed calibration procedure is self-configured using the standard ROS robot description files, the Unified Robot Description Format (URDF), and provide several tools for sensor positioning and data labelling based on RVIZ interactive markers. The remainder of this paper is organized as follows: Section 2 describes the methodologies used to set up an optimization procedure which calibrates the system. In this section, several auxiliary tools for labelling data and positioning sensors are described; Section 3 details the optimization procedure and how it is cast as a bundle adjustment problem; Section 4 provides comparisons with established OpenCV calibration methodologies; finally, Section 5 provides conclusions and future work. 2 ROS based calibration setup A schematic of the proposed calibration procedure is displayed in Fig. 2. It consists of five components: configuration; interactive positioning of sensors; interactive labelling of data; collection of data; and finally, the optimization procedure. Each component will be described in detail in the following subsections. 2.1 Configuration of the calibration procedure Robotic platforms are described in ROS using a xml file called URDF. We propose to extend the URDF description files of a robot in order to provide information necessary for configuring how the calibration should be carried out. A new URDF element, named calibration, is introduced specifically for the purpose of calibrating. Each calibration element describes a sensor to be calibrated. The element contains information about the calibration parent and child links, which define the partial transformation that is optimized. 2.2 Interactive positioning of sensors Optimization procedures suffer from the known problem of local minima. This problem tends to occur when the initial solution is far from the optimal parameter configuration. Thus, it is expected that, by ensuring an accurate first guess for the sensor poses, there is less likelihood of falling into local minima. We propose to solve this problem in an interactive fashion: the system parses the URDF robot description and creates an rviz interactive marker associated with each sensor. It is then possible to move and rotate the interactive markers. This provides a simple, interactive method to manually calibrate the system or, alternatively, to easily generate plausible first guesses for the poses of the sensors. Real time visual feedback is provided by the observation of the bodies of the robot model (e.g. where a LiDAR is placed w.r.t. the vehicle), and also by the data measured by the sensors (e.g. how well the measurements from two LiDARs match). An example of this procedure can be watched at 2.3 Interactive data labelling Since the goal is to propose a calibration procedure that operates on multi-modal data, a calibration pattern adequate to all available sensor modalities must be selected. A chessboard pattern is a common calibration pattern, in particular for RGB and RGB-D cameras. To label image data, one of the many available image-based chessboards detectors is used (Find Chessboard Corners OpenCV function 4 4 ). In the case of 2D LiDAR data, it is not possible to robustly detect the chessboard, since there are often multiple planes in the scene derived from other structures, such as walls and doors. To solve this, we propose an interactive approach which requires minimal user intervention: rviz interactive markers are positioned along the LiDAR measurement planes and the user drags the marker to indicate where in the data the chessboard is observed. This is done by clustering the LiDAR data, and selecting the cluster which is closer to the marker. This interactive procedure is done only once, since it is then possible to track the chessboard robustly. Fig. 3 shows an example of the labelling of 2D LiDAR data. This interactive data labelling procedure is showcased in 2.4 Collecting data Usually, different sensors stream data at different frequencies. However, to compute the associations between the data of multiple sensors, temporal synchronization is required. While some approaches require hardware synchronization to operate [26], in the current method this is solved trivially by collecting data (and the corresponding labels) at user defined moments in which the scene has remained static for a certain period of time. In static scenes, the problem of data desynchronization is not observable, which warrants the assumption that for each captured collection the sensor data is synchronized. We refer to these snapshot recordings of multi-sensor data as data collections. This information is stored in a JSON file that will be read by the optimization procedure afterwards. The JSON file contains abstract information about the sensors, such as the sensor transformation chain, among others, and specific information about each collection, i.e., sensor data, partial transformations, and data labels. It is important to note that the set of collections should contain as many different poses as possible. As such, collections should preferably have different distances and orientations w.r.t. the chessboard so that the calibration becomes more reliable. This concern is common to the majority of calibration procedures. 2.5 Sensor poses from partial transformations The representation of a complex, multi-sensor system requires the creation of a transformation graph. For this purpose, ROS uses a graph tree referred to as tf tree [31]. One critical factor for any calibration procedure is that it should not change the structure of that existing tf tree. The reason for this is that the tf tree, derived from the URDF files by the robot state publisher, 5 5 also supports additional functionalities, such as robot visualization or collision detection. If the tf tree changes due to the calibration, those functionalities may be compromised or require some redesigning. To accomplish this, we propose to compute the pose of any particular sensor (i.e., the transformation from the Reference Link, also known as World, to that Sensor) as an aggregate transformation A , obtained after the chain of transformations for that particular sensor, extracted from the topology of the tf tree: (1) where i T i + 1 represents the partial transformation from the ith to the i + 1 link, and p a r e n t and c h i l d are the indexes of the calibration parent and calibration child links in the sensor chain, respectively. Our approach preserves the predefined structure of the tf tree, since, during optimization, only one partial transformation contained in the chain is altered (the one in blue in Eq. (1)). This computation is performed within the optimization’s cost function. Therefore, a change in one partial transformation affects the global sensor pose, and consequently, the error to minimize. The optimization may target multiple links of each chain, and is agnostic to whether the remaining links are static or dynamic, since all existing partial transformations are stored for each data collection. To the best of our knowledge, our approach is one of few which maintains the structure of the transformation graph before and after optimization. This is a feature that is often overlooked, yet it is of critical practical importance for the selection of a calibration framework. Taking the example of Fig. 4, consider that Sensor 1 is mounted on top of a pan and tilt unit, where LinkA T Link C corresponds to the pan movement, and LinkC T Sensor 1 represents the tilt motion. For this particular case, Eq. (1) becomes: (2) where I is the identity matrix (since there are no prior links), and the pan and tilt motions are coloured in red to denote that these transformations are dynamic and, as a consequence, may also change from collection to collection. Another example is the one of Sensor 2: it contains an aggregate transformation that also includes the partial transformation optimized w.r.t. Sensor 1, resulting in the following aggregate transformation: (3) which is also directly derived from Eq. (1). Such complex arrangements are seldom tackled by a single calibration approach, even less in a transparent way by the same general formalism. The proposed optimization of partial transformations achieves this goal. We consider the ability to preserve the structure of the tf tree as a key feature of the proposed framework: from a practical standpoint, since it facilitates the integration into ROS, both before and after the optimization; and, moreover, from a conceptual perspective, since this formulation is general and adequate to handle most calibration scenarios. 3 Calibration procedure The goal of general optimization procedures is to find the parameter configuration that results in the smallest function value. This function, which depends on the optimization parameters Φ is known as the objective function. For the purpose of calibrating the multi-modal sensors of a robotic platform, like the ATLASCAR2 intelligent vehicle, the objective of this optimization is to estimate the pose of each sensor relatively to a reference link (base link for ATLASCAR2 case). 3.1 Optimization parameters An extrinsic calibration translates into a pose estimation. Thus, the set of parameters to optimize, defined as Φ , must contain parameters that together define the pose of each sensor. As discussed in the beginning of Section 2, we propose to maintain the initial structure of the transformation graph, and thus only optimize one partial transformation per sensor. In the example of Fig. 4, these partial transformations are denoted by solid arrows. Since the usage of camera sensors is considered, it is also possible to introduce the intrinsic parameters of each camera in the set Φ . Our goal is to define an objective function that is able to characterize sensors of different modalities. Pairwise methodology for devising the cost function results in complex graphs of exhaustive definition of relationships. For every existing pair of sensors, these relationships must be established according to the modality of each of the sensors, and, although most cases have been addressed in literature, as discussed in Section 1, a problem of scalability remains inherent to such a solution. To address this issue, we propose to structure the cost function in a sensor to calibration pattern paradigm, similar to what is done in bundle adjustment. That is, the positions of 3D points in the scene are jointly refined with the poses of the sensors. These 3D points correspond to the corners of the calibration chessboard. What is optimized is actually the transformation that takes these corners from the frame of reference of the chessboard to the world, for every collection. All variables must have some initial value, so that the optimizer may compute the first error, and start to refine the values in order to obtain the minimum of the cost function. The first guess for each chessboard is obtained by computing the pose of a chessboard detection in one of the cameras available. The output is a transformation from the chessboard reference frame to the camera’s reference frame. Since we already have the first guess for the poses of each sensor, calculated as an aggregate transformation A (see Eq. (1)), to obtain the transformation from the chessboard reference frame to the world (an external and absolute frame), the following calculation is applied: (4) chess T world = camera A world ︷ Eq. (1) ⋅ chess T camera ︷ chess detection , where chess and camera refer to chessboard and camera coordinate frames, respectively. Thus, the set of parameters to be optimized Φ , contains the transformation represented in Eq. (4), for each collection, along with the poses of each sensor: (5) Φ = [ x m = 1 , r m = 1 , i m = 1 , d m = 1 , … , x m = M , r m = M , i m = M , d m = M , ︷ Cameras x n = 1 , r n = 1 , … , x n = N , r n = N , ︷ LiDARs … , ︷ Other modalities x k = 1 , r k = 1 , … , x k = K , r k = K ︷ Calibration object ] where m refers to the mth camera, of the set of M cameras, n refers to the nth LiDAR, of the set of N LiDARs, k refers to the chessboard detection of the kth collection, contained in the set of K collections, x is a translation vector [ t x , t y , t z ] , r is a rotation represented through the axis/angle parameterization [ r 1 , r 2 , r 3 ] (where the vector [ r 1 , r 2 , r 3 ] is used to represent the axis and its norm the angle), i is a vector of a camera’s intrinsic parameters [ f x , f y , c x , c y ], and d is a vector of camera’s distortion coefficients [ d 0 , d 1 , d 2 , d 3 , d 4 ]. The initial estimate for the intrinsic parameters is obtained using any intrinsic camera calibration tool. The axis/angle parameterization was chosen because it has 3 components and 3 degrees of freedom, making it a fair parameterization, since it does not introduce more numerical sensitivity than the one inherent to the problem itself [32]. At this point, there are six parameters per sensor, related to the pose of each one, to be enhanced. These values compose the geometric transformation that will be calibrated. The cost function will compute the residuals based on an error (in pixels for RGB cameras and in millimetres for LiDARs) between the re-projected position of the chessboard, estimated by all transformations, and the position of the calibration pattern detected by each sensor. 3.2 Objective function The cost function for this optimization, F ( Φ ) , can be thought of as the sum of several sub-functions that compose a vector function, where, for every modality of sensor added to the calibration, a new sub-function is defined accordingly, which allows for the minimization of the error associated with the pose of sensors of that modality. Thus, the optimization procedure can be defined as: (6) arg min Φ F ( Φ ) = 1 2 ∑ i f i ( Φ i 1 , … , Φ i k ) 2 where f i ( ⋅ ) is the objective sub-function for the i th sensor with the respective parameters block { Φ i 1 , … , Φ i k } , being k the parameters number of each objective sub-function. In other words, the scalar cost function of this optimization is the sum of the squares of the returned values from a vector function, divided by two. Each different sensor has an inherent sub-function, that depends on the sensor modality. The value of all these sub-functions is a vector with the errors (residuals) associated to the re-projection of the calibration pattern points. Since for the ATLASCAR2 intelligent vehicle we are considering four sensors (two cameras and two 2D LiDARs), the objective function is composed by the vector values of four sub-functions, two of each type. Each different sub-function is detailed in the next sub-sections. 3.2.1 Camera sub-function When the sensors are cameras, their calibration is performed as a bundle adjustment [33], and as such, the sub-function created is based on the average geometric error corresponding to the image distance between a projected point and a detected one. The 3D points corresponding to the corners of the calibration chessboard are captured by one or more cameras in each collection. Each camera is defined by its pose relative to a reference link and intrinsic parameters. After the desired acquisitions are completed, the 3D points are projected from the world into the images and the 2D coordinates are compared to the ones obtained by detection of the calibration pattern in the corresponding images. The positions of the 3D points in the world are obtained by applying the transformation described in Eq. (4) to the chessboard corner points defined in the chessboard detection’s reference frame. The goal of this cost sub-function is to adjust the initial estimation of the camera parameters and the position of the points, in order to minimize the average reprojection error f camera , given by: (7) f camera = ℓ 2 ( x c = 1 , x ˆ c = 1 ) ℓ 2 ( x c = 2 , x ˆ c = 2 ) ⋯ ℓ 2 ( x c = C , x ˆ c = C ) ⊺ where ℓ 2 is the Euclidean distance between two vectors, c denotes the index of the chessboard corners, x c denotes the pixels coordinates of the measured points (given by chessboard detection), and x ˆ c are the projected points, given by the relationship between a 3D point in the world and its projection on an image plane. By knowing the real size of the chessboard squares, the 3D coordinates of all corners relatively to the chess frame can be inferred. Note that the z value will be, for every point, zero, since the chessboard is in the XoY plane. After obtaining the 3D coordinates of all corners in reference to the chessboard frame, the objective function computes the coordinates of the points relatively to the camera link through multiplying by the geometric transformation between the base link (reference frame for the ATLASCAR2 example) and the calibration pattern frame and by the transform between the camera link and the base link: (8) p c a m e r a = camera T world ⋅ world T chess ⋅ p c h e s s where p c h e s s refers to the x , y , z coordinates of a chessboard corner, defined in the local chessboard coordinate frame, and p c a m e r a refers to the x , y , z coordinates of the same chessboard corner, defined in the camera link. In fact, both p c h e s s and p c a m e r a are the homogenized matrices of the coordinates so that Eq. (8) is mathematically correct. Note that the parameters to be optimized define the chessboard to world transformation, and that the world to camera transformation is computed from an aggregate of several partial transformations, one of which is defined by other parameters being optimized; furthermore, the intrinsic matrix is dependent on parameters which are accounted for in the optimization. As is expected, the re-projected points become closer to the ground truth corners during the optimization procedure. Fig. 5 shows the difference between the initial position of the chessboard corners, projected from the 3D world to the camera image, and the final position of these same projected points, after the optimization has been completed. It is possible to observe that the pixels corresponding to the projection of the final position of the points (dots in Fig. 5) almost perfectly match the ground truth points (squares in Fig. 5). 3.2.2 Laser sub-function Finally, for the case of 2D LiDARs, the sub-function only considers the two border points, among all the measurements that are related to the chessboard plane, to compute the error associated to the pose of the LiDAR and the chessboard. In order to calculate the residuals that this cost sub-function should return, the detected points’ 3D coordinates from the chessboard frame are required. During the calibration setup stage, when the information of a time stamp is saved, the ranges of all measurements that the LiDAR is detecting are stored, as well as the information about this same LiDAR and the indexes of the ranges that correspond to the plane where the chessboard is. With the optimization parameters of the chessboard pose and the LiDAR pose (computed accordingly to Eq. (1)), both relative to the base link, the 3D coordinates of each labelled measurement of the point cloud in the chessboard frame are known: (9) x y z 1 chess = chess T world ⋅ world T lidar ⋅ x y z 1 lidar . Finally, with the coordinates from the chessboard frame, of both the first and the last points of the cluster extracted in the labelling stage, it is possible to compute the error evaluated by this cost sub-function. The error is based on the distance between each one of the limit points (the first and the last index) of the selected ranges and the chessboard surface boundaries. There are two computed distances for each point: orthogonal and longitudinal. The orthogonal distance is the z absolute value of the coordinates, in the calibration pattern frame, of the LiDAR data measurement. In an ideal setting, the z value should be zero, since the chessboard plane is on the XoY plane. This is why any value different from zero means that the optimization parameters (sensor pose and chess pose) are not yet correct. The longitudinal distance is the Euclidean distance between the x and y coordinates, in the calibration pattern frame, of the LiDAR data measurement and the x and y coordinates of the closest point that belong to the limit of the physical board that is being detected. In order to compute this distance, it is essential to create a group of points that represent the boundaries of the chessboard. By knowing the size of the board, the size of each chess square, and that the chess frame origin matches with the first (top left) chess corner, the coordinates were calculated and the points of the board boundaries were manually defined. The size of the border between the chess corner grid and the end of the physical chessboard had to be measured so that this step could be implemented. In Fig. 6, we can see the grid of the chess corners and a line around it: that line marks the limit of the board. This solid line has some points within it, which are going to be compared to the LiDAR data measured ones. Again, the optimizer will search for the closest limit point to each one of the studied LiDAR data measurement coordinates and then compute the longitudinal distance. Thus, the LiDAR sub-function f lidar is defined as: (10) f lidar = | z 1 chess | ℓ 2 ( p 1 b o a r d l i m i t , p 1 c h e s s ) | z 2 chess | ℓ 2 ( p 2 b o a r d l i m i t , p 2 c h e s s ) ⊺ where (11) p b o a r d l i m i t = x y boardlimit , (12) p c h e s s = x y chess , and z chess is the third coordinate value of the range measurement points transformed to the chessboard’s coordinate frame. 3.3 Sensors pose calibration: Optimization The cost function F ( Φ ) from Eq. (6) is minimized using a least-squares approach. 6 6 In this work we used the least-squares solver provided by SciPy: Least-squares finds a local minimum of a scalar cost function, with bounds on the variables, by having an m-dimensional real residual function of n real variables. As such, we choose this minimization approach as it is the best fit for our problem. 4 Results To assess the performance of the proposed calibration approach, we used an intelligent vehicle as test bed. The ATLASCAR2 [24] is an electric vehicle (Mitsubishi i-MiEV) with several sensors onboard. In this work four sensors were considered: two 2D LiDARs and two RGB cameras. Thus, two different modalities of sensors are used. The sensors are designated as follows: left laser, right laser, top left camera and top right camera. Fig. 7 shows the ATLASCAR2 vehicle. The proposed approach is used to calibrate the four selected sensors simultaneously. Nonetheless, as discussed above, there are no approaches which provide an off-the-shelf multi-sensor multi-modal calibration. As such, in order to evaluate this approach, we provide comparisons against other pairwise methodologies, which are abundant in the field, as was mentioned in Section 1. Note that, in the following comparisons, the results given by the proposed approach for a particular pair of sensors are obtained using a complete system calibration. On the other hand, the alternative methodologies calibrate a single pair of sensors. In this sense, the comparison methodology is not favourable to the proposed approach, since the other approaches are specialized in the case being evaluated. In the following lines, two tests are detailed: the first is a camera-to-camera evaluation which compares several calibration methods in a pairwise fashion, while the second characterizes the proposed joint optimization over time providing global metrics. 4.1 Camera to camera The methodology used to compute the error of the calibrated poses of the top right camera and the top left camera is based on the distance between pixel coordinates. These coordinates are, on the one hand, the detected chessboard corners (ground truth) of the top right camera and, on other hand, the coordinates of the projections of those corners, to the top left camera, using the transformation between the cameras which is the output of the calibration. To transform pixels from one camera to the other, we start from the projection of the 3D world coordinates to the image of a camera: (13) p = K ⋅ R t ⋅ P where P refers to the 3D homogeneous coordinates of the corners as viewed in the chessboard frame; p is a vector composed by the u , v and w values, in which: x pixel = u ∕ w and y pixel = v ∕ w , allowing for the direct extraction of image coordinates from this vector; R t is the non-homogeneous geometric transformation matrix from the camera frame to the chessboard frame, K represents the camera’s intrinsic matrix. Eq. (13) can be applied to each camera separately. Since the 3D chessboard corner coordinates are defined in the chessboard frame, the value of Z will be 0 for all corners, because they all lie on the XoY plane: Z chess = 0 . As a result, Eq. (13) may be simplified as follows: (14) u v w = f x 0 c x 0 f y c y 0 0 1 ⋅ r 11 r 12 t x r 21 r 22 t y r 31 r 32 t z ⋅ X Y 1 chess corners , which is equivalent to: (15) p camera = K ⋅ camera T chess ′ ⋅ P chess , where the geometric transformation matrix camera T chess ′ is a portion of the camera T chess matrix, as detailed in Eq. (15). We use (15) for both cameras, and relate both expressions by the 3D coordinates of the chessboard corners (which are the same for both cameras), resulting in: (16) p cam2 = K cam2 ⋅ cam2 T chess ′ ⋅ cam1 T chess ′ -1 ⋅ K cam1 -1 ⋅ p cam1 where cam1 and cam2 refer to the top left and top right cameras, respectively. This formulation provides the relation between image coordinates of the chessboard corners for both camera images of each collection. Notice, however, that calibration methods output the transformation between sensors, in this case between cameras, while Eq. (16) requires transformations from the cameras to the chessboard. Some approaches, as for example the proposed approach, also estimate the pose of the chessboards (see parameters of the calibration objects in Eq. (5)). Thus, at first glance, one could think of using these transformations directly in Eq. (16). However, these chessboard poses are estimated for a given training dataset, and cannot be accurately used for other datasets. Moreover, as said before, not all calibration approaches output the pose of the chessboards (e.g. OpenCV stereo calibrate). Instead, calibration approaches provide the transformation between cameras. By arbitrarily selecting one camera from which the chessboard pose is determined through the solvePNP function (we have used cam1, but tests have shown that the alternative provided similar results) and using the transformation cam1 T cam2 estimated by the calibration approaches, it is possible to determine the transformation of the other camera to the chess, as follows: (17) cam2 T chess = cam1 T cam2 -1 ︷ calibration ⋅ cam1 T chess ︷ s o l v e P n P . From this expression the partial matrices cam1 T chess ′ and cam2 T chess ′ are derived. Then, we apply Eq. (16) to compute the corner coordinates on the top right camera image, as projected from the detection of the top left camera image. The error is computed by measuring the difference between expected and projected corner coordinates on the top right camera image: (18) x error y error top right camera = x y projected − x y e x p e c t e d Fig. 8 shows the errors related to the projection of the chessboard corners from the top left camera to the top right camera before and after the optimization of the position and orientation parameters of the cameras. These results can be better evaluated through the calculated mean error and standard deviation values, as shown in Table 1: Next, the proposed approach was compared with other calibration methodologies: stereo calibrate function 7 7 provided by OpenCV and the kalibr calibration method [26]. The kalibr method requires hardware synchronization and receives a bag file as input, unlike the other approaches, which make use of the datasets collected as described in Section 2. Because of this, two different calibrations are provided for kalibr: the first in which the training dataset is used, and a second which uses the test dataset, i.e. the dataset which is used to evaluate all approaches. The results for the proposed approach are presented for two different scenarios, taking into account the camera (top left or top right) which was used for creating the initial values of the chessboard poses (see Eq. (4)). These two variants are used to assess the impact of the selection of the camera for providing initial estimates on the final calibration estimates. In this experiment, calibration of a pair of sensors composed by the top left camera (cam1) and the top right camera (cam2) is evaluated. The dataset used for running the calibration procedures, i.e. the training dataset, is composed of 27 collections (27 images per camera). The test dataset to be used to evaluate the estimated sensor-to-sensor transformations has 15 collections. Images from the train and test datasets are similar. In order to make this comparison fair, the three distinct calibration procedures are given the exact same information. Moreover, the procedures were implemented in such a way that the returned estimated parameters, and remaining data, are organized similarly to the proposed approach. This means that each distinct approach will output a final JSON file with the estimated position and orientation of the sensors. Taking all this into account, a specific tool was created for visualizing the results of the different calibration procedures named Results Visualization, which imports the JSON files outputted by each one of the several approaches. Fig. 9 shows a flowchart of this framework, built specifically to compare the proposed methodology with standard pairwise approaches. Fig. 10 shows the pixel errors of the three distinct calibration approaches. Note that the methodology described above is computed separately for each collection. The performance of the kalibr method is clearly below the other two. We suspect there are several factors contributing to this. The first is that this method requires hardware synchronization, not ensured in the used datasets. Another is that the kalibr method reads data from a bag file and thus we have no control over the images which are selected to run the calibration. In an attempt to address the problem, we ran a kalibr calibration using the test dataset as input (kalibr test in Table 2). It may be that the selection of images is not working well, which in turn causes a poor calibration performance. Also, due to the limited duration of the bag file of the experiment, only around 20 to 30 images have been selected, a total similar to the datasets we have used. It could be that kalibr requires a larger number of images. In any case, we believe these results are not representative of kalibr. The proposed approach and the stereo calibration display similar errors, which means that the proposed approach is on par with a state of the art calibration approach. Moreover, the largest error of each of the compared methodologies occurs for the same collection (in this case, for collection 4, the dark green). This also shows a high degree of consistency between the proposed approach and the stereo calibration. Table 2 shows the average error and the standard deviation of all tested calibration approaches. These results exhibit reprojection errors in the order of some pixels, which is the normal range of values for these methods and experimental setups. Moreover, the obtained values are very similar between the proposed approach and the stereo calibrate. As such, results show that the proposed approach is able to calibrate all sensors on-board the ATLASCAR2 using a single optimization procedure. Furthermore, the accuracy of this joint calibration framework we propose is the same as when using state of the art pairwise calibration methods. 4.2 Complete system calibration This section will provide results concerning a full system calibration. Note that, in Section 4.1, the results focus only on the evaluation of the camera sensors, despite the fact that the complete system was also calibrated. In this section, the goal is to characterize all the sensors and not just the cameras. Because of this, it is not possible to compare the full system calibration (taking into account all the sensors) with other approaches since, as described in Section 1, there is no calibration framework available, in particular a multi-sensor and multi-modal one. Fig. 11 shows the average error per sensor over the cost function evaluations, for a full system calibration test. The average error per sensor is estimated after the several error measurements computed for each particular sensor. For example, a camera cost sub-function returns as many residuals as chessboard corners (see Eq. (7)), while the LiDAR sub sub-function returns four measurements (see Eq. (10)). The average error for camera sensors is provided in pixels, while for LiDAR sensors the error is in metres. The first takeaway is that the optimization is working as intended, since the minimization of the errors of all sensors can be observed. This shows that the multi-sensor, multi-modal optimization (the joint minimization of all the sensor’s parameters) is in fact possible. Furthermore, the final errors values (after the optimization is finished) are around a few pixels for camera sensors (2.8 and 3.3 pixels for the top left camera and the top right camera, respectively), and around a few centimetres for the LiDARs (0.017 and 0.033 metres for the left laser and right laser, respectively). These values are on par with the state of the art, even when considering calibration results for pairwise approaches. Another important insight is the reason why the top left camera residual starts with a low error: Section 3.1, in particular Eq. (4), described how the initial poses of the chessboards were estimated using one camera sensor, which is arbitrarily selected. In this test, the top left camera was selected to produce the initial chessboard pose estimates. Thus, since the corner detection in the top left camera images are used to compute the initial chessboard poses, the reverse procedure of projecting the chessboard corners back to the image results in corner coordinates that are naturally very close to the detections at the beginning of the optimization. Fig. 12 shows the data from the LiDARs along with a representation of the chessboard. For a better visualization, a single collection is displayed. The four images correspond to different stages of the optimization process. It is possible to see an improvement during the calibration (i.e. from Fig. 12(a) and (b), the beginning of the optimization, to (c) and (d), the end of the optimization, since the data from both LiDARs is much closer to the chessboard plane (c) and (d) when compared to (a) and (b). This shows that the proposed approach is also capable of calibrating LiDARs within a joint optimization framework. 5 Conclusions and future work This paper proposes an extrinsic calibration methodology that is general, in the sense that the number of sensors and their modalities are not restricted. The approach is compliant with the ROS framework, having also the advantage of not altering the tf tree. To accomplish this, the problem is formalized as an optimization procedure of a set of partial transformations, which accounts for specific links in the transformation chains of the sensors. Additionally, the work contributes with a set of interactive tools for the positioning of the sensors and labelling of data, which facilitate the creation of a first guess and significantly ease the calibration procedure. Results show that the proposed approach is able to achieve similar accuracy when compared to state of the art methodologies, implemented in OpenCV. Moreover, these results are obtained by performing a complete calibration of the system, rather than one of a single pair of sensors. In other words, the proposed approach calibrates all sensors at once, with similar performance as the pairwise approaches. This confirms that the proposed approach is adequate for the calibration of complex robotic systems, as are most intelligent vehicles. Future work will focus on the extension to additional sensor modalities, e.g., 3D LiDARs, RGB-D cameras, Radio Detection And Ranging (RaDAR), etc. Given the scalability of the proposed framework, it is expected that this should be more or less straightforward. Finally, the ultimate goal is to produce a multi-sensor, multi-modal calibration package that may be released to the community. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This Research was funded by National Funds through the FCT — Foundation for Science and Technology, in the context of the project UIDB/00127/2020, as well as CYTED/TICs4CI — Aplicaciones TICS para Ciudades Inteligentes . References [1] Mueller G.R. Wuensche H. Continuous stereo camera calibration in urban scenarios 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC) 2017 1 6 10.1109/ITSC.2017.8317675 G. R. Mueller, H. Wuensche, Continuous stereo camera calibration in urban scenarios, in: 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC), 2017, 1–6. DOI: 10.1109/ITSC.2017.8317675. [2] Wu L. Zhu B. Binocular stereovision camera calibration 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA) 2015 2638 2642 10.1109/ICMA.2015.7237903 L. Wu, B. Zhu, Binocular stereovision camera calibration, in: 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA), 2015, 2638–2642. DOI: 10.1109/ICMA.2015.7237903. [3] Rou Su L. JingLiang Zhong B. QiaoLiang Li SuWen Qi HuiSheng Zhang TianFu Wang An automatic calibration system for binocular stereo imaging 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC) 2016 896 900 10.1109/IMCEC.2016.7867340 Rou Su, JingLiang Zhong, QiaoLiang Li, SuWen Qi, HuiSheng Zhang, TianFu Wang, An automatic calibration system for binocular stereo imaging, in: 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC), 2016, 896–900. DOI: 10.1109/IMCEC.2016.7867340. [4] Ling Y. Shen S. High-precision online markerless stereo extrinsic calibration 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) 2016 1771 1778 10.1109/IROS.2016.7759283 Y. Ling, S. Shen, High-precision online markerless stereo extrinsic calibration, in: 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2016, 1771–1778. DOI: 10.1109/IROS.2016.7759283. [5] Dinh V.Q. Nguyen T.P. Jeon J.W. Rectification using different types of cameras attached to a vehicle IEEE Trans. Image Process. 28 2 2019 815 826 10.1109/TIP.2018.2870930 V. Q. Dinh, T. P. Nguyen, J. W. Jeon, Rectification using different types of cameras attached to a vehicle, IEEE Trans. on Image Processing 28 (2) (2019) 815–826. DOI: 10.1109/TIP.2018.2870930. [6] Vasconcelos F. Barreto J.P. Nunes U. A minimal solution for the extrinsic Calibration of a Camera and a laser-rangefinder IEEE Trans. Pattern Anal. Mach. Intell. 34 11 2012 2097 2107 F. Vasconcelos, J. P. Barreto, U. Nunes, A minimal solution for the extrinsic calibration of a camera and a laser-rangefinder, IEEE Trans. on Pattern Analysis and Machine Intelligence 34 (11) (2012) 2097–2107. [7] Pereira M. Silva D. Santos V. Dias P. Self calibration of multiple lidars and cameras on autonomous vehicles Robot. Auton. Syst. 83 2016 326 337 M. Pereira, D. Silva, V. Santos, P. Dias, Self calibration of multiple lidars and cameras on autonomous vehicles, Robotics and Autonomous Systems 83 (2016) 326–337. [8] Almeida M. Dias P. Oliveira M. Santos V. 3d-2d laser range finder calibration using a conic based geometry shape Image Analysis and Recognition 2012 312 319 M. Almeida, P. Dias, M. Oliveira, V. Santos, 3d-2d laser range finder calibration using a conic based geometry shape, in: Image Analysis and Recognition, 2012, 312–319. [9] A. Geiger, F. Moosmann, O. Car, B. Schuster, Automatic camera and range sensor calibration using a single shot, in: Proc. IEEE Int. Conf. Robot. Autom., 2012, pp. 3936–3943. [10] Guindel C. Beltrán J. Martín D. García F. Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 1 6 C. Guindel, J. Beltrán, D. Martín, F. García, Automatic extrinsic calibration for lidar-stereo vehicle sensor setups, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) (2017) 1–6. [11] Kwon Y.C. Jang J.W. Choi O. Automatic sphere detection for extrinsic calibration of multiple rgbd cameras 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS) 2018 1451 1454 Y. C. Kwon, J. W. Jang, O. Choi, Automatic sphere detection for extrinsic calibration of multiple rgbd cameras, in: 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS), 2018, 1451–1454. [12] Khan A. Aragon-Camarasa G. Sun L. Siebert J.P. On the calibration of active binocular and rgbd vision systems for dual-arm robots 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO) 2016 1960 1965 10.1109/ROBIO.2016.7866616 A. Khan, G. Aragon-Camarasa, L. Sun, J. P. Siebert, On the calibration of active binocular and rgbd vision systems for dual-arm robots, in: 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), 2016, 1960–1965. DOI: 10.1109/ROBIO.2016.7866616. [13] Basso F. Menegatti E. Pretto A. Robust intrinsic and extrinsic calibration of rgb-d cameras IEEE Trans. Robot. 34 5 2018 1315 1332 10.1109/TRO.2018.2853742 F. Basso, E. Menegatti, A. Pretto, Robust intrinsic and extrinsic calibration of rgb-d cameras, IEEE Trans. on Robotics 34 (5) (2018) 1315–1332. DOI: 10.1109/TRO.2018.2853742. [14] Qiao Y. Tang B. Wang Y. Peng L. A new approach to self-calibration of hand-eye vision systems 2013 Int. Conf. on Computational Problem-Solving (ICCP) 2013 253 256 10.1109/ICCPS.2013.6893596 Y. Qiao, B. Tang, Y. Wang, L. Peng, A new approach to self-calibration of hand-eye vision systems, in: 2013 Int. Conf. on Computational Problem-Solving (ICCP), 2013, 253–256. DOI: 10.1109/ICCPS.2013.6893596. [15] Zhang C. Zhang Z. Calibration between depth and color sensors for commodity depth cameras 2011 IEEE Int. Conf. on Multimedia and Expo 2011 1 6 10.1109/ICME.2011.6012191 C. Zhang, Z. Zhang, Calibration between depth and color sensors for commodity depth cameras, in: 2011 IEEE Int. Conf. on Multimedia and Expo, 2011, 1–6. DOI: 10.1109/ICME.2011.6012191. [16] Chen G. Cui G. Jin Z. Wu F. Chen X. Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction IEEE Sens. J. 19 7 2019 2685 2694 10.1109/JSEN.2018.2889805 G. Chen, G. Cui, Z. Jin, F. Wu, X. Chen, Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction, IEEE Sensors Journal 19 (7) (2019) 2685–2694. DOI: 10.1109/JSEN.2018.2889805. [17] Qilong Zhang G. Pless R. Extrinsic calibration of a camera and laser range finder (improves camera calibration) 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), Vol. 3 2004 2301 2306 10.1109/IROS.2004.1389752 Qilong Zhang, R. Pless, Extrinsic calibration of a camera and laser range finder (improves camera calibration), in: 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), 3, 2004, 2301–2306 3. DOI: 10.1109/IROS.2004.1389752. [18] Häselich M. Bing R. Paulus D. Calibration of multiple cameras to a 3d laser range finder 2012 IEEE Int. Conf. on Emerging Signal Processing Applications 2012 25 28 M. Häselich, R. Bing, D. Paulus, Calibration of multiple cameras to a 3d laser range finder, in: 2012 IEEE Int. Conf. on Emerging Signal Processing Applications, 2012, 25–28. [19] Chen Z. Yang X. Zhang C. Jiang S. Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) 2016 448 453 Z. Chen, X. Yang, C. Zhang, S. Jiang, Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature, in: 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2016, 448–453. [20] Velas M. Spanel M. Materna Z. Herout A. Calibration of rgb camera with velodyne lidar 2014 M. Velas, M. Spanel, Z. Materna, A. Herout, Calibration of rgb camera with velodyne lidar, 2014. [21] Lee G. Lee J. Park S. Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 64 69 10.1109/MFI.2017.8170408 G. Lee, J. Lee, S. Park, Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition, in: 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI), 2017, 64–69. DOI: 10.1109/MFI.2017.8170408. [22] Levinson J. Thrun S. Automatic online calibration of cameras and lasers Robotics: Science and Systems 2013 J. Levinson, S. Thrun, Automatic online calibration of cameras and lasers, in: Robotics: Science and Systems, 2013. [23] Dezhi Gao J. Duan J. Xining Yang S. Zheng B. A method of spatial calibration for camera and radar 2010 8th World Congress on Intelligent Control and Automation 2010 6211 6215 10.1109/WCICA.2010.5554411 Dezhi Gao, J. Duan, Xining Yang, B. Zheng, A method of spatial calibration for camera and radar, in: 2010 8th World Congress on Intelligent Control and Automation, 2010, 6211–6215. DOI: 10.1109/WCICA.2010.5554411. [24] Santos V. Almeida J. Ávila E. Gameiro D. Oliveira M. Pascoal R. Sabino R. Stein P. Atlascar - technologies for a computer assisted driving system, on board a common automobile 13th Int. IEEE Conf. on Intelligent Transpor Tation Systems 2010 1421 1427 10.1109/ITSC.2010.5625031 V. Santos, J. Almeida, E. vila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, Atlascar - technologies for a computer assisted driving system, on board a common automobile, in: 13th Int. IEEE Conf. on Intelligent Transpor tation Systems, 2010, 1421–1427. DOI: 10.1109/ITSC.2010.5625031. [25] Liao Y. Li G. Ju Z. Liu H. Jiang D. Joint kinect and multiple external cameras simultaneous calibration 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM) 2017 305 310 10.1109/ICARM.2017.8273179 Y. Liao, G. Li, Z. Ju, H. Liu, D. Jiang, Joint kinect and multiple external cameras simultaneous calibration, in: 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM), 2017, 305–310. DOI: 10.1109/ICARM.2017.8273179. [26] Rehder J. Siegwart R. Furgale P. A general approach to spatiotemporal calibration in multisensor systems IEEE Trans. Robot. 32 2 2016 383 398 10.1109/TRO.2016.2529645 J. Rehder, R. Siegwart, P. Furgale, A general approach to spatiotemporal calibration in multisensor systems, IEEE Trans. on Robotics 32 (2) (2016) 383–398. DOI: 10.1109/TRO.2016.2529645. [27] Pradeep V. Konolige K. Berger E. Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach Experimental Robotics: The 12th Int. Symposium on Experimental Robotics 2014 Springer Berlin Heidelberg Berlin, Heidelberg 211 225 10.1007/978-3-642-28572-1˙15 V. Pradeep, K. Konolige, E. Berger, Calibrating a Multi-arm Multi-sensor Robot: A Bundle Adjustment Approach, Springer Berlin Heidelberg, Berlin, Heidelberg, 2014, 211–225. DOI: 10.1007/978-3-642-28572-1˙15. [28] Oliveira M. Castro A. Madeira T. Dias P. Santos V. A general approach to the extrinsic calibration of intelligent vehicles using ros Robot 2019: Fourth Iberian Robotics Conference 2020 Springer International Publishing Cham 203 215 M. Oliveira, A. Castro, T. Madeira, P. Dias, V. Santos, A general approach to the extrinsic calibration of intelligent vehicles using ros, in: Robot 2019: Fourth Iberian Robotics Conference, Springer International Publishing, Cham, 2020, 203–215. [29] Bradski G. The OpenCV Library, Dr. Dobb’s J. Softw. Tools 2000 G. Bradski, The OpenCV Library, Dr. Dobb’s Journal of Software Tools. [30] Quigley M. Conley K. Gerkey B.P. Faust J. Foote T. Leibs J. Wheeler R. Ng A.Y. Ros: an open-source robot operating system ICRA Workshop on Open Source Software 2009 M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, A. Y. Ng, Ros: an open-source robot operating system, in: ICRA Workshop on Open Source Software, 2009. [31] Foote T. Tf: The transform library 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA) 2013 1 6 10.1109/TePRA.2013.6556373 T. Foote, tf: The transform library, in: 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA), 2013, 1–6. DOI: 10.1109/TePRA.2013.6556373. [32] Hornegger J. Tomasi C. Representation issues in the ML estimation of camera motion Proceedings of the Seventh IEEE International Conference on Computer Vision, Vol. 1 1999 640 647 10.1109/ICCV.1999.791285 J. Hornegger, C. Tomasi, Representation issues in the ml estimation of camera motion, 1, 1999, 640–647 1. DOI: 10.1109/ICCV.1999.791285. [33] Agarwal S. Snavely N. M. Seitz S. Szeliski R. Bundle Adjustment in the Large 2010 29 42 10.1007/978-3-642-15552-9˙3 S. Agarwal, N. Snavely, S. M. Seitz, R. Szeliski, Bundle adjustment in the large, 2010, 29–42. DOI: 10.1007/978-3-642-15552-9˙3. Afonso Castro is a junior web developer. He has an M.Sc. Degree in Mechanical Engineering from the Department of Mechanical Engineering of the University of Aveiro (2019). His master specialization was in robotics and, more precisely, sensor calibration. During his M.Sc. Dissertation development, Afonso Castro has published a research article entitled “A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS” for ROBOT2019: Fourth Iberian Robotics Conference, where he has participated and presented the mentioned work. "
    },
    {
        "doc_title": "2D lidar to kinematic chain calibration using planar features of indoor scenes",
        "doc_scopus_id": "85085949613",
        "doc_doi": "10.1108/IR-09-2019-0201",
        "doc_eid": "2-s2.0-85085949613",
        "doc_date": "2020-08-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Calibration techniques",
            "Design/methodology/approach",
            "Geometric accuracy",
            "High angular resolutions",
            "Quantitative result",
            "Range measurements",
            "Reconstruction procedure"
        ],
        "doc_abstract": "© 2020, Emerald Publishing Limited.Purpose: 2D laser rangefinders (LRFs) are commonly used sensors in the field of robotics, as they provide accurate range measurements with high angular resolution. These sensors can be coupled with mechanical units which, by granting an additional degree of freedom to the movement of the LRF, enable the 3D perception of a scene. To be successful, this reconstruction procedure requires to evaluate with high accuracy the extrinsic transformation between the LRF and the motorized system. Design/methodology/approach: In this work, a calibration procedure is proposed to evaluate this transformation. The method does not require a predefined marker (commonly used despite its numerous disadvantages), as it uses planar features in the point acquired clouds. Findings: Qualitative inspections show that the proposed method reduces artifacts significantly, which typically appear in point clouds because of inaccurate calibrations. Furthermore, quantitative results and comparisons with a high-resolution 3D scanner demonstrate that the calibrated point cloud represents the geometries present in the scene with much higher accuracy than with the un-calibrated point cloud. Practical implications: The last key point of this work is the comparison of two laser scanners: the lemonbot (authors’) and a commercial FARO scanner. Despite being almost ten times cheaper, the laser scanner was able to achieve similar results in terms of geometric accuracy. Originality/value: This work describes a novel calibration technique that is easy to implement and is able to achieve accurate results. One of its key features is the use of planes to calibrate the extrinsic transformation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Autonomous scene exploration for robotics: A conditional random view-sampling and evaluation using a voxel-sorting mechanism for efficient ray casting",
        "doc_scopus_id": "85089166267",
        "doc_doi": "10.3390/s20154331",
        "doc_eid": "2-s2.0-85089166267",
        "doc_date": "2020-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Autonomous exploration",
            "Efficient point",
            "Explorer agents",
            "Next best view",
            "Novel information",
            "Octree representation",
            "Robot operating systems (ROS)",
            "Robotic manipulators"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Carrying out the task of the exploration of a scene by an autonomous robot entails a set of complex skills, such as the ability to create and update a representation of the scene, the knowledge of the regions of the scene which are yet unexplored, the ability to estimate the most efficient point of view from the perspective of an explorer agent and, finally, the ability to physically move the system to the selected Next Best View (NBV). This paper proposes an autonomous exploration system that makes use of a dual OcTree representation to encode the regions in the scene which are occupied, free, and unknown. The NBV is estimated through a discrete approach that samples and evaluates a set of view hypotheses that are created by a conditioned random process which ensures that the views have some chance of adding novel information to the scene. The algorithm uses ray-casting defined according to the characteristics of the RGB-D sensor, and a mechanism that sorts the voxels to be tested in a way that considerably speeds up the assessment. The sampled view that is estimated to provide the largest amount of novel information is selected, and the system moves to that location, where a new exploration step begins. The exploration session is terminated when there are no more unknown regions in the scene or when those that exist cannot be observed by the system. The experimental setup consisted of a robotic manipulator with an RGB-D sensor assembled on its end-effector, all managed by a Robot Operating System (ROS) based architecture. The manipulator provides movement, while the sensor collects information about the scene. Experimental results span over three test scenarios designed to evaluate the performance of the proposed system. In particular, the exploration performance of the proposed system is compared against that of human subjects. Results show that the proposed approach is able to carry out the exploration of a scene, even when it starts from scratch, building up knowledge as the exploration progresses. Furthermore, in these experiments, the system was able to complete the exploration of the scene in less time when compared to human subjects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Performance analysis on deep learning semantic segmentation with multivariate training procedures",
        "doc_scopus_id": "85085956029",
        "doc_doi": "10.1109/ICARSC49921.2020.9096145",
        "doc_eid": "2-s2.0-85085956029",
        "doc_date": "2020-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Learning approach",
            "Learning semantics",
            "Performance analysis",
            "Performance assessment",
            "Semantic segmentation",
            "Training parameters",
            "Training procedures"
        ],
        "doc_abstract": "© 2020 IEEE.Deep Learning approaches are becoming ubiquitous in many fields of computation, especially for tasks of object detection and classification in images. However, the diversity of architectures, the number and range of training parameters, and the dimension and representativeness of datasets make it very complex to define a unified method or set of techniques to address diverse problems, and it is quite common for the literature to skip valuable implementation details. One of these fields is the Semantic Segmentation with many applications like autonomous driving. In that line, this paper carried out a study using an ENet architecture for semantic segmentation on road images for intelligent vehicles and multiple experiments of deep learning training with a wealth of combinations of training procedures, including multiple loss functions and datasets for out-of-domain performance assessment. Besides some expected outcomes, the results allow identifying a set of combinations with performances that stand out and are advisable for researchers to tackle more efficiently problems of a similar nature.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancement of RGB-D image alignment using fiducial markers",
        "doc_scopus_id": "85081256441",
        "doc_doi": "10.3390/s20051497",
        "doc_eid": "2-s2.0-85081256441",
        "doc_date": "2020-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D reconstruction",
            "Camera calibration",
            "Fiducial marker",
            "Geometric optimization",
            "Inpainting",
            "Point cloud",
            "Projection of 3D points"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Three-dimensional (3D) reconstruction methods generate a 3D textured model from the combination of data from several captures. As such, the geometrical transformations between these captures are required. The process of computing or refining these transformations is referred to as alignment. It is often a difficult problem to handle, in particular due to a lack of accuracy in the matching of features. We propose an optimization framework that takes advantage of fiducial markers placed in the scene. Since these markers are robustly detected, the problem of incorrect matching of features is overcome. The proposed procedure is capable of enhancing the 3D models created using consumer level RGB-D hand-held cameras, reducing visual artefacts caused by misalignments. One problem inherent to this solution is that the scene is polluted by the markers. Therefore, a tool was developed to allow their removal from the texture of the scene. Results show that our optimization framework is able to significantly reduce alignment errors between captures, which results in visually appealing reconstructions. Furthermore, the markers used to enhance the alignment are seamlessly removed from the final model texture.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS",
        "doc_scopus_id": "85082115002",
        "doc_doi": "10.1007/978-3-030-35990-4_17",
        "doc_eid": "2-s2.0-85082115002",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bundle adjustments",
            "Calibration problems",
            "Calibration process",
            "Extrinsic calibration",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Intelligent vehicles are complex systems which often accommodate several sensors of different modalities. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration process. The calibration problem is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Editorial: Special issue on autonomous driving and driver assistance systems",
        "doc_scopus_id": "85071537701",
        "doc_doi": "10.1016/j.robot.2019.103266",
        "doc_eid": "2-s2.0-85071537701",
        "doc_date": "2019-11-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2019-08-22 2019-08-22 2019-09-03 2019-09-03 2019-10-04T06:55:49 S0921-8890(19)30668-2 S0921889019306682 10.1016/j.robot.2019.103266 S300 S300.1 FULL-TEXT 2019-10-04T06:04:27.220993Z 0 0 20191101 20191130 2019 2019-08-22T15:12:59.971894Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav body acknowledge affil articletitle auth authfirstini authfull authlast grantnumber grantsponsor grantsponsorid ref 0921-8890 09218890 true 121 121 C Volume 121 7 103266 103266 103266 201911 November 2019 2019-11-01 2019-11-30 2019 simple-article edi © 2019 Published by Elsevier B.V. EDITORIALSPECIALISSUEAUTONOMOUSDRIVINGDRIVERASSISTANCESYSTEMS SANTOS V Acknowledgments References HIRABAYASHI 2019 62 72 M GUINDEL 2019 109 122 C PERSIC 2019 217 230 J PRAKASH 2019 172 186 C LEE 2019 178 189 E NEMEC 2019 168 177 D LI 2019 201 210 S GASPAR 2018 59 67 A OKAMOTO 2019 155 171 K AHN 2018 1 12 B AMANATIADIS 2019 282 290 A ANTUNES 2019 56 62 J WEN 2019 28 39 S SAJADIALAMDARI 2019 291 303 S ANTUNES 2019 83 89 A MARINPLAZA 2019 P YOU 2019 1 18 C TALAMINO 2019 93 105 J TONUTTI 2019 162 173 M SANTOSX2019X103266 SANTOSX2019X103266XV 2021-09-03T00:00:00.000Z 2021-09-03T00:00:00.000Z © 2019 Published by Elsevier B.V. 2019-09-07T05:20:54.793Z S0921889019306682 Foundation for Science and Technology UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia FCT FCT Fuel Cell Technologies Program Spanish Government, Spain TIN2017-89723-P Generalitat de Catalunya FIEC-09-2015 Generalitat de Catalunya Cities REF-518RT0559 Quad Cities Community Foundation This work has been supported by: the Spanish Government, Spain under Project TIN2017-89723-P , the “CERCA Programme / Generalitat de Catalunya”, Spain , the ESPOL project PRAIM (FIEC-09-2015), Ecuador and FCT — Foundation for Science and Technology, Portugal , in the context of project UID/CEC/00127/2019 . The authors gratefully acknowledge the support of the CYTED, Spain , Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” ( REF-518RT0559 ). item S0921-8890(19)30668-2 S0921889019306682 10.1016/j.robot.2019.103266 271599 2019-10-04T06:04:27.220993Z 2019-11-01 2019-11-30 true 235162 MAIN 3 67849 849 656 IMAGE-WEB-PDF 1 ROBOT 103266 103266 S0921-8890(19)30668-2 10.1016/j.robot.2019.103266 Editorial: Special issue on autonomous driving and driver assistance systems Vitor Santos IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Angel D. Sappa ⁎ Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación Campus Gustavo Galindo Km 30.5, Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain Computer Vision Center, Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain ⁎ Corresponding editor. Miguel Oliveira IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitrio de Santiago, 3810-193 Aveiro, Portugal Arturo de la Escalera Universidad Carlos III de Madrid, Escuela Politécnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid), Spain Universidad Carlos III de Madrid, Escuela Politécnica Superior C/ Butarque, 15 - 28911 Leganes (Madrid) Spain Universidad Carlos III de Madrid, Escuela Politcnica Superior, C/ Butarque, 15 - 28911 Leganes (Madrid) Spain The research on Autonomous Driving and Driver Assistance Systems (ADDAS) has been increasing continuously for several decades, but the last years have exhibited unprecedented attention, both by the many dedicate books published in this field, but especially for the numerous papers in several conferences, workshops and special sessions worldwide. The contexts of these events cover a large spectrum of communities, ranging from robotics up to intelligent transportation, intelligent vehicles or vehicular technology, among others. The scope and potential of the technological applications in ADDAS is so huge that it can serve as test bed for an almost unbounded set of sciences and technologies that go from the classic navigation issues and system control, up to the more recent and daunting challenges in deep learning contexts. This special issue is not an exception to this scenario, and its 19 peer reviewed papers cover a wide range of problems that can be categorized into the following topics: 1. Scene Perception [1–5] 2. Localization, Visual Odometry and SLAM [6–8] 3. Cabin Concerns with Drivers and Passengers [9–11] 4. Vehicle Systems Control [12–15] 5. High Level Navigation [16–18] 6. Deep Learning [2,10,17,19] The previous categorization is however not very strict, since it is natural for a single paper to cover both a known specific problem along with some more generic tool applied to solve that particular problem. So, some times there is a fusion of focus and interest on the specific problem and the tool used to solve it, where the now ubiquitous Deep Learning paradigm is a clear example, or also the usage of Robot Operating System (ROS) as the supporting architecture for many implementations. The main contents of the papers published in the special issue are described next with more details in the following lines. Scene Perception Perception of vehicle’s surrounding is a topic of interest and a key component for both autonomous driving and driver assistance systems. Several papers have been focused on this topic covering problems that go from traffic lights recognition, scene awareness till obstacle detection and distance estimation. In [1] the authors propose an innovative reliable method to recognize the state of traffic lights in images using accurate 3D maps and a self-localization technique in it. Quantitative evaluations indicate that the method achieved over 97% average precision for each state and approximately 90% recall as far as 90 meters under preferable condition. In a more general way, [4] proposes a robust method for generic obstacle detection and collision warning. The proposed approach is able to detect all obstacles without prior knowledge and detect partially occluded obstacles. The approach is robust to variations in illumination and to a wide variety of vehicles and obstacles. Improvements on true positive detection in comparison with state of the art are shown. On the contrary to previous approaches, the usage of a stereovision based scene perception is proposed in [5] and [2]. In [5] the authors propose a novel framework for vehicle detection and localization with partial appearance using stereo vision and geometry. The proposed approach is based on the widely used v-disparity map representation to detect candidates vehicles. Then, a deep learning-based verification completes vehicle detection. A more general approach is presented in [2], where an efficient method to perform recognition and 3D localization of dynamic objects on images from a stereo camera is described. The proposed approach relies on a deep learning framework able to simultaneously identify a broad range of entities, such as vehicles, pedestrians or cyclists, with a frame rate compatible with the strict requirements of onboard automotive applications. The results presented in the paper show the capabilities of the perception system for a wide variety of traffic situations. Finally, and also related with scene perception, the authors in [3] tackle the challenging problem of extrinsic calibration of a radar-LiDAR-camera sensor system. The authors propose a novel calibration method that involves a special target design and a two-step optimization procedure. The proposed calibration method has been tested on a variety of sensor configuration showing that it is able to reliably estimate all the six parameters of the extrinsic calibration. Localization, Visual Odometry and SLAM The ability of autonomous vehicles to obtain their position and orientation within the environment where they are driving in is crucial to several tasks as planning and piloting. Several sensors are available for this, the GPS (Global Positioning System) and IMU (Inertial Measurement Unit) being two of the most used. Both have many advantages but have also several shortcomings for autonomous vehicles applications due to accuracy or their limitations in urban scenarios. That is why in [6] the authors propose sensor fusion for the localization (both translation and attitude) of a mobile wheeled robot. Several sensors are used: odometers, gyroscope, accelerometer, magnetometer and a camera for visual landmark localization. The algorithm is able to deal with the asynchronous nature of the sensors and the failures of many of them and runs in real-time. SLAM (Simultaneous Localization And Mapping) algorithm has been applied to robotics for some years and in [7] the authors proposed a system able to maintain the fast performance of a direct method and the high precision and loop closure capability of a feature-based method. A key-frame is used for global or local optimization and loop closure, whereas a non-key-frame is used for fast tracking and localization. Besides that, the system fuses the computer vision data with inertial measurements. Thanks to this an equilibrium between speed and accuracy is achieved. The public availability of data-sets is crucial for research teams do not have the needed sensors for start developing new ideas and for testing and comparing different approaches. In the last article of this subsection [8], the authors propose a benchmark of visual odometry and SLAM techniques. The Urban@CRAS data-set shows several scenarios presenting different conditions and urban situations: vehicle-to-vehicle and vehicle-to-human interactions, cross-sides, turn-around, roundabouts and different traffic conditions. The sensor data comes from a 3D LIDAR, color cameras, a high-precision IMU and a GPS navigation system. Besides the data, the authors propose a bench marking process for visual odometry and SLAM where qualitative and quantitative performance indicators are obtained so different approaches can be compared. Cabin Concerns with Drivers and Passengers Attention to car occupants, be it the driver or a passenger, is of course a central topic in Driving Assistance. Driver monitoring, namely the posture, can give indication of the driver focus and attention level. Paper [10] proposes a method to estimate head pose after a monocular camera performed by a deep neural network using a small gray scale image. Additionally, the authors released a new dataset of head poses for further studies, despite the fact that the solutions presented outperform current state-of-the-art techniques. Predicting human-driver reactions is a concern addressed by paper [9]. The authors perform a survey on several algorithms to predict the lateral control actions of human drivers. A comparison of the algorithms is made in terms of their suitability to develop haptic-shared ADAS, which share the control force with the human driver. The driver steering torque is considered a central point to establish a proper model, but as low-cost driver simulators only monitor steering angle and not steering torque, the authors propose a methodology to estimate the steering-wheel torque. Using the estimated steering torque, they train several machine learning driver control models and compare the performance using both simulated and real human-driving data sets. Paper [11] focuses on detecting and counting the passengers of nearby vehicles as seen from the ego-vehicle using monocular vision. The on-road Vehicle PassengEr Detection (ViPED) system is proposed and is based on the human perception model in terms of spatio-temporal reasoning, namely the slight movements of passenger shape silhouettes inside the cabin of the preceding car seen through the windshield. A Convolutional Neural Network is used to infer the number and position of passengers. Vehicle Systems Control Some of the proposed works have focused on specific systems of the vehicle control, be it the low level control or the fusion of data from multiple sensors: in [12], a dynamic test model used for the design and tuning of low level PID and LQR controllers are presented; in [15], the sidesliping of vehicles is estimated using a self-calibrating architecture which fuses several sensors such as an inertial measurement unit and a global positioning system; in [14], a nonlinear model predictive controller is proposed to produce an online estimate for a cost-effective cruising velocity; in [13], a study on the effect of medium access control protocol and the unreliable measurements on acceleration information for the cooperative control of vehicle platoons is presented. High Level Navigation The planning and execution of the movement of intelligent vehicles continues to be a relevant topic of research, in particular in complex, dynamic environments. As such, this special issue contains several works on the topics of navigation and trajectory planning: in [17], an approach is proposed that models the interaction between the autonomous vehicle and the environment, with the goal of determining the optimal driving strategy for the autonomous vehicle; in [18], an anticipatory kinodynamic motion planner that considers dynamic complex environments containing both static and dynamic obstacles is proposed. Finally, in [16], a novel software architecture for intelligent vehicles is proposed, focusing on the flexibility and scalability as a means to effectively evaluate novel algorithms. The implementation of the architecture is shown for two real platforms, the iCab (Intelligent Campus Automobile) and IvvI 1.0 vehicle (Intelligent Vehicle based on Visual Information). Deep Learning Deep learning based approaches are becoming the dominant paradigm in almost every basic and applied research topics. In the current special issue several works were based on the usage of such a framework. In this section just the most representatives are summarized. In [2] the authors propose an efficient approach to perform recognition and 3D localization of dynamic objects on images from a stereo camera, with the goal of gaining insight into traffic scenes in urban and road environments. The usage of a deep learning framework allows to identify a broad range of entities, at a frame rate compatible with the strict requirements of onboard automotive applications. Stereo information is later introduced to enrich the knowledge about the objects with geometrical information. A deep learning based framework has been also used in [17], but in this case it is intended to target the planning problem of autonomous vehicles. The system learns the driving style of an expert driver using reinforcement learning strategies. Simulated results demonstrate the system is able to reach the desired driving behaviors for an autonomous vehicle. Focusing also on the driving behavior, in [19] a deep learning based technique is proposed to accurately predict driving manoeuvres in a few seconds in advance. The authors propose a domain adaptation based technique, which is able to adapt a learned model to new drivers and different vehicles. The proposed approach has been evaluated in several datasets yielding an average increase in performance of 30% and 114% respectively compared to no adaptation based techniques. Finally, focusing on driver monitoring, [10] proposes a novel method to estimate a head pose from a monocular camera. The proposed algorithm is based on multi-task learning deep neural network that uses a small grayscale image. The network jointly detects multi-view faces and estimates head pose even under poor environment conditions such as illumination change, vibration, large pose change, and occlusion. The authors also release a new dataset for head pose estimation. The proposed framework outperforms state-of-the-art approaches quantitatively and qualitatively with an average head pose mean error of less than 4°in real-time. Conclusions Autonomous Driving and Driving Assistance Systems are now highly versatile and broad contexts which support the test and usage of both classic and modern techniques in domains ranging from perception, planning, control until classification and deep learning. Being a concern of daily routines for humans, and now also machines, it is becoming irrefutable that many quests can be tried and experimented in this attractive field of technology and science. This special issue clearly corroborates that with its wide range of topics and techniques as described in the earlier sections. But, what is now striking fiercely is the almost unavoidable imposition of the soft computation tool of the moment: deep learning. For example, we still see classic oriented trends such as Model Predictive Control (MPC) to predict and control machine motion, but often there are also works which tackle similar problems with deep learning approaches. Many authors are surrendering to this almost sinful and irresistible framework; more and more are migrating, and right now we can only guess what future editions of this special issue will bring to the community. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work has been supported by: the Spanish Government, Spain under Project TIN2017-89723-P, the “CERCA Programme / Generalitat de Catalunya”, Spain, the ESPOL project PRAIM (FIEC-09-2015), Ecuador and FCT — Foundation for Science and Technology, Portugal, in the context of project UID/CEC/00127/2019. The authors gratefully acknowledge the support of the CYTED, Spain , Network: “Ibero-American Thematic Network on ICT Applications for Smart Cities” (REF-518RT0559). References [1] Hirabayashi M. Sujiwo A. Monrroy A. Kato S. Edahiro M. Traffic light recognition using high-definition map features Robot. Auton. Syst. 111 2019 62 72 M. Hirabayashi, A. Sujiwo, A. Monrroy, S. Kato, M. Edahiro, Traffic light recognition using high-definition map features, Robotics and Autonomous Systems 111 (2019) 62 – 72 [2] Guindel C. Martín D. Armingol J.M. Traffic scene awareness for intelligent vehicles using convnets and stereo vision Robot. Auton. Syst. 112 2019 109 122 C. Guindel, D. Martn, J. M. Armingol, Traffic scene awareness for intelligent vehicles using convnets and stereo vision, Robotics and Autonomous Systems 112 (2019) 109 – 122 [3] Peršić J. Marković I. Petrović I. Extrinsic 6dof calibration of a radar–lidar–camera system enhanced by radar cross section estimates evaluation Robot. Auton. Syst. 114 2019 217 230 J. Peri, I. Markovi, I. Petrovi, Extrinsic 6dof calibration of a radarlidarcamera system enhanced by radar cross section estimates evaluation, Robotics and Autonomous Systems 114 (2019) 217 – 230 [4] Prakash C.D. Akhbari F. Karam L.J. Robust obstacle detection for advanced driver assistance systems using distortions of inverse perspective mapping of a monocular camera Robot. Auton. Syst. 114 2019 172 186 C. D. Prakash, F. Akhbari, L. J. Karam, Robust obstacle detection for advanced driver assistance systems using distortions of inverse perspective mapping of a monocular camera, Robotics and Autonomous Systems 114 (2019) 172 – 186 [5] Lee E.S. Choi W. Kum D. Bird’s eye view localization of surrounding vehicles: Longitudinal and lateral distance estimation with partial appearance Robot. Auton. Syst. 112 2019 178 189 E. S. Lee, W. Choi, D. Kum, Birds eye view localization of surrounding vehicles: Longitudinal and lateral distance estimation with partial appearance, Robotics and Autonomous Systems 112 (2019) 178 – 189 [6] Nemec D. imák V. Janota A. Hruboš M. Bubeníková E. Precise localization of the mobile wheeled robot using sensor fusion of odometry, visual artificial landmarks and inertial sensors Robot. Auton. Syst. 112 2019 168 177 D. Nemec, V. imk, A. Janota, M. Hrubo, E. Bubenkov, Precise localization of the mobile wheeled robot using sensor fusion of odometry, visual artificial landmarks and inertial sensors, Robotics and Autonomous Systems 112 (2019) 168 – 177 [7] Li S.-P. Zhang T. Gao X. Wang D. Xian Y. Semi-direct monocular visual and visual-inertial SLAM with loop closure detection Robot. Auton. Syst. 112 2019 201 210 S. peng Li, T. Zhang, X. Gao, D. Wang, Y. Xian, Semi-direct monocular visual and visual-inertial slam with loop closure detection, Robotics and Autonomous Systems 112 (2019) 201 – 210 [8] Gaspar A.R. Nunes A. Pinto A.M. Matos A. Urban@CRAS dataset: Benchmarking of visual odometry and SLAM techniques Robot. Auton. Syst. 109 2018 59 67 A. R. Gaspar, A. Nunes, A. M. Pinto, A. Matos, Urban@cras dataset: Benchmarking of visual odometry and slam techniques, Robotics and Autonomous Systems 109 (2018) 59 – 67 [9] Okamoto K. Tsiotras P. Data-driven human driver lateral control models for developing haptic-shared control advanced driver assist systems Robot. Auton. Syst. 114 2019 155 171 K. Okamoto, P. Tsiotras, Data-driven human driver lateral control models for developing haptic-shared control advanced driver assist systems, Robotics and Autonomous Systems 114 (2019) 155 – 171 [10] Ahn B. Choi D.-G. Park J. Kweon I.S. Real-time head pose estimation using multi-task deep neural network Robot. Auton. Syst. 103 2018 1 12 B. Ahn, D.-G. Choi, J. Park, I. S. Kweon, Real-time head pose estimation using multi-task deep neural network, Robotics and Autonomous Systems 103 (2018) 1 – 12 [11] Amanatiadis A. Karakasis E. Bampis L. Ploumpis S. Gasteratos A. ViPED: On-road vehicle passenger detection for autonomous vehicles Robot. Auton. Syst. 112 2019 282 290 A. Amanatiadis, E. Karakasis, L. Bampis, S. Ploumpis, A. Gasteratos, Viped: On-road vehicle passenger detection for autonomous vehicles, Robotics and Autonomous Systems 112 (2019) 282 – 290 [12] Antunes J. Antunes A. Outeiro P. Cardeira C. Oliveira P. Testing of a torque vectoring controller for a formula student prototype Robot. Auton. Syst. 113 2019 56 62 J. Antunes, A. Antunes, P. Outeiro, C. Cardeira, P. Oliveira, Testing of a torque vectoring controller for a formula student prototype, Robotics and Autonomous Systems 113 (2019) 56 – 62 [13] Wen S. Guo G. Observer-based control of vehicle platoons with random network access Robot. Auton. Syst. 115 2019 28 39 S. Wen, G. Guo, Observer-based control of vehicle platoons with random network access, Robotics and Autonomous Systems 115 (2019) 28 – 39 [14] Sajadi-Alamdari S.A. Voos H. Darouach M. Nonlinear model predictive control for ecological driver assistance systems in electric vehicles Robot. Auton. Syst. 112 2019 291 303 S. A. Sajadi-Alamdari, H. Voos, M. Darouach, Nonlinear model predictive control for ecological driver assistance systems in electric vehicles, Robotics and Autonomous Systems 112 (2019) 291 – 303 [15] Antunes A. Outeiro P. Cardeira C. Oliveira P. Implementation and testing of a sideslip estimation for a formula student prototype Robot. Auton. Syst. 115 2019 83 89 A. Antunes, P. Outeiro, C. Cardeira, P. Oliveira, Implementation and testing of a sideslip estimation for a formula student prototype, Robotics and Autonomous Systems 115 (2019) 83 – 89 [16] Marin-Plaza P. Hussein A. Martin D. de la Escalera A. Icab use case for ROS-based architecture Robot. Auton. Syst. 2019 P. Marin-Plaza, A. Hussein, D. Martin, A. de la Escalera, icab use case for ros-based architecture, Robotics and Autonomous Systems (2019) [17] You C. Lu J. Filev D. Tsiotras P. Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning Robot. Auton. Syst. 114 2019 1 18 C. You, J. Lu, D. Filev, P. Tsiotras, Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning, Robotics and Autonomous Systems 114 (2019) 1 – 18 [18] Talamino J.P. Sanfeliu A. Anticipatory kinodynamic motion planner for computing the best path and velocity trajectory in autonomous driving Robot. Auton. Syst. 114 2019 93 105 J. P. Talamino, A. Sanfeliu, Anticipatory kinodynamic motion planner for computing the best path and velocity trajectory in autonomous driving, Robotics and Autonomous Systems 114 (2019) 93 – 105 [19] Tonutti M. Ruffaldi E. Cattaneo A. Avizzano C.A. Robust and subject-independent driving manoeuvre anticipation through domain-adversarial recurrent neural networks Robot. Auton. Syst. 115 2019 162 173 M. Tonutti, E. Ruffaldi, A. Cattaneo, C. A. Avizzano, Robust and subject-independent driving manoeuvre anticipation through domain-adversarial recurrent neural networks, Robotics and Autonomous Systems 115 (2019) 162 – 173 "
    },
    {
        "doc_title": "Industry focused in data collection: How industry 4.0 is handled by big data",
        "doc_scopus_id": "85072797441",
        "doc_doi": "10.1145/3352411.3352414",
        "doc_eid": "2-s2.0-85072797441",
        "doc_date": "2019-07-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Data collection",
            "Implemented strategy",
            "Industrial scale",
            "Interpretation of data",
            "Knowledge extraction",
            "Maintenance process",
            "Production data",
            "Production process"
        ],
        "doc_abstract": "© 2019 Association for Computing Machinery.The paper aims to organize and structure data collected and associated to technologies that powers the abroad concept of Industry 4.0. It starts with the historic evolution of industry, separated by date landmarks and approaches the last transition between 3.0 to 4.0. Apart from the differences between industry models, production data stats show a huge and important transformation in the amount of data related to manufacturing and how that knowledge is processed. The paper also aims to put on debate the lack of solutions regarding the knowledge extraction of data from machines and systems, needed for data analytics. Approaches with cyber-physical systems, machine learning, virtual environments, Industrial IoT 1 and augmented reality, in an industrial scale, are some of the strategies to power the reading and interpretation of data, in order to promote industrial efficiency. Real context industrial applications are taken into account in order to state the importance of collected data in the efficiency of a production process. Exploring technologies and concepts to improve digital twins systems, perception and perceived systems as well as maintenance processes are some of the explored implemented strategies that make Industry 4.0. Some possible strategies are presented, as well as the transition for Industry 5.0.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extrinsic Calibration of 2D Laser Range Finders using Planar Features",
        "doc_scopus_id": "85068445859",
        "doc_doi": "10.1109/ICARSC.2019.8733654",
        "doc_eid": "2-s2.0-85068445859",
        "doc_date": "2019-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3D reconstruction",
            "Calibration procedure",
            "Extrinsic calibration",
            "High angular resolutions",
            "Point cloud",
            "Quantitative result",
            "Range measurements",
            "Reconstruction procedure"
        ],
        "doc_abstract": "© 2019 IEEE.2D Laser Range Finders, or 2D-LRFs, are essential sensors in the field of robotics, providing accurate range measurements with high angular resolution. These sensors can be assembled on top of systems which, by granting additional degrees of freedom to the movement of the LRF, enable the 3D reconstruction of a scene. The reconstruction procedure consists of the concatenation of each scan in a single point cloud representation. To do so, the extrinsic transformation between the LRF and the motorized system, in this case, a Pan-tilt unit, must be known with high accuracy, otherwise, the quality of the 3D reconstructed point clouds is insufficient. In this work, a calibration procedure which determines this transformation is proposed. The method does not require a dedicated marker, which is commonly necessary and has numerous disadvantages. Qualitative inspections show that the proposed method is able to significantly reduce artifacts which typically appear on uncalibrated point clouds. Furthermore, quantitative results demonstrate that the calibrated point cloud represents the geometries present in the scene with much higher accuracy when compared with the uncalibrated point cloud.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Monocular Visual Odometry Benchmarking and Turn Performance Optimization",
        "doc_scopus_id": "85068441000",
        "doc_doi": "10.1109/ICARSC.2019.8733633",
        "doc_eid": "2-s2.0-85068441000",
        "doc_date": "2019-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Appearance based",
            "Crop monitoring",
            "Global Navigation Satellite Systems",
            "Localization accuracy",
            "Localization system",
            "Noise variations",
            "Performance optimizations",
            "Scale estimation"
        ],
        "doc_abstract": "© 2019 IEEE.Developing ground robots for crop monitoring and harvesting in steep slope vineyards is a complex challenge due to two main reasons: harsh condition of the terrain and unstable localization accuracy obtained with Global Navigation Satellite System. In this context, a reliable localization system requires an accurate and redundant information to Global Navigation Satellite System and wheel odometry based system. To pursue this goal we benchmark 3 well known Visual Odometry methods with 2 datasets. Two of these are feature-based Visual Odometry algorithms: Libviso2 and SVO 2.0. The third is an appearance-based Visual Odometry algorithm called DSO. In monocular Visual Odometry, two main problems appear: pure rotations and scale estimation. In this paper, we focus on the first issue. To do so, we propose a Kalman Filter to fuse a single gyroscope with the output pose of monocular Visual Odometry, while estimating gyroscope's bias continuously. In this approach we propose a non-linear noise variation that ensures that bias estimation is not affected by Visual Odometry resultant rotations. We compare and discuss the three unchanged methods and the three methods with the proposed additional Kalman Filter. For tests, two public datasets are used: the Kitti dataset and another built in-house. Results show that our additional Kalman Filter highly improves Visual Odometry performance in rotation movements.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing Spatial and Mobile Augmented Reality for Guiding Assembling Procedures with Task Validation",
        "doc_scopus_id": "85068430962",
        "doc_doi": "10.1109/ICARSC.2019.8733642",
        "doc_eid": "2-s2.0-85068430962",
        "doc_date": "2019-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly sequence",
            "Augmented reality systems",
            "Computer vision techniques",
            "Controlled experiment",
            "Mobile augmented reality",
            "Real-time validation",
            "Spatial augmented realities",
            "Validation process"
        ],
        "doc_abstract": "© 2019 IEEE.Assembly tasks are a common situation in many industrial applications. These tasks are often presented on paper or digital manuals containing instructions, photos or diagrams to guide an assembly sequence. While some Augmented Reality (AR) systems have also been proposed to support these processes, only a few track the state of the assembling procedure, validating the process in real-time. In this work, we propose two different AR-based (mobile and spatial AR) methods with real-time validation to provide assistance to users during the execution of an assembly process. The validation process uses computer vision techniques to keep track of the state of the assembly sequence, verifying the completion of each stage and providing information at the end of the assembly. A controlled experiment was used to compare the performance, ease of use, and acceptance of the two AR-based methods proposed. Participants were significantly faster and made fewer errors using the Spatial AR condition. Besides, participants also preferred this condition. In addition, Nasa TLX rating showed that the Spatial AR condition had a slightly lower cognitive load on the participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluating and enhancing google tango localization in indoor environments using fiducial markers",
        "doc_scopus_id": "85048857208",
        "doc_doi": "10.1109/ICARSC.2018.8374174",
        "doc_eid": "2-s2.0-85048857208",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.Recent advances in 3D sensing technologies, as well as in inertial measurement technologies, have resulted in significant improvements in the accuracy of the localization of systems that combine all these sensors. Project Tango is one of the most successful examples of such systems. Developed by Google, it integrates in an Android mobile device a set of sensors and software required to provide accurate real-time 3D information when moving the equipment freely in hand. This is making mapping and navigation accessible to the general public, with evident applications in robotics, augmented reality, computer vision and others. The contribution of this paper is towfold: first, we present a thorough evaluation of the localization accuracy of the Tango platform in different conditions; second, we present a fiducial marker-based extension of the Tango localization system, which improves the localization estimates in certain conditions. The paper presents a set of experiments performed to evaluate the position and orientation errors in indoor environments, using Augmented Reality for visualization purposes, with and without area learning, e.g. using a priori information acquired from the environment. In addition, we propose a solution based on the use of additional visual markers, which allows the re-calibration of augmented content in specific locations, to improve tracking accuracy in dynamic environments where spatial and/or illumination changes may occur. A statistical analysis of the results shows that the Tango with area learning and the proposed solution provide a level of accuracy significantly better that the Tango without area learning. Moreover, the proposed solution can overcome some limitations of Tango with area learning when used in dynamic environments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards lifelong assistive robotics: A tight coupling between object perception and manipulation",
        "doc_scopus_id": "85043759771",
        "doc_doi": "10.1016/j.neucom.2018.02.066",
        "doc_eid": "2-s2.0-85043759771",
        "doc_date": "2018-05-24",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Assistive robots",
            "Interactive learning",
            "Object manipulation",
            "Open-ended learning"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.This paper presents an artificial cognitive system tightly integrating object perception and manipulation for assistive robotics. This is necessary for assistive robots, not only to perform manipulation tasks in a reasonable amount of time and in an appropriate manner, but also to robustly adapt to new environments by handling new objects. In particular, this system includes perception capabilities that allow robots to incrementally learn object categories from the set of accumulated experiences and reason about how to perform complex tasks. To achieve these goals, it is critical to detect, track and recognize objects in the environment as well as to conceptualize experiences and learn novel object categories in an open-ended manner, based on human–robot interaction. Interaction capabilities were developed to enable human users to teach new object categories and instruct the robot to perform complex tasks. A naive Bayes learning approach with a Bag-of-Words object representation are used to acquire and refine object category models. Perceptual memory is used to store object experiences, feature dictionary and object category models. Working memory is employed to support communication purposes between the different modules of the architecture. A reactive planning approach is used to carry out complex tasks. To examine the performance of the proposed architecture, a quantitative evaluation and a qualitative analysis are carried out. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform complex tasks.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2018-03-07 2018-03-07 2018-03-22 2018-03-22 2018-03-22T15:20:58 S0925-2312(18)30232-7 S0925231218302327 10.1016/j.neucom.2018.02.066 S300 S300.1 FULL-TEXT 2018-03-22T16:18:06.833994Z 0 0 20180524 2018 2018-03-07T17:38:33.811543Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid misctext orcid primabst ref vitae 0925-2312 09252312 true 291 291 C Volume 291 14 151 166 151 166 20180524 24 May 2018 2018-05-24 2018 article fla © 2018 Elsevier B.V. All rights reserved. TOWARDSLIFELONGASSISTIVEROBOTICSATIGHTCOUPLINGBETWEENOBJECTPERCEPTIONMANIPULATION HAMIDREZAKASAEI S 1 Introduction 2 Related work 2.1 Assistive and service robots 2.2 Object manipulation 2.3 Object perception and learning 3 Overall system architecture 4 Environment exploration and dictionary construction 5 Object detection and representation 5.1 Object detection and tracking 5.2 Feature extraction and object representation 6 Interactive object category learning and recognition 6.1 User interaction 6.2 Object conceptualizer 6.3 Object category recognition 7 Planning and execution 8 Experimental results 8.1 Off-line evaluation of the perceptual learning approach 8.2 Open-ended evaluation 8.3 A real life use-case: clear table 9 Conclusions Acknowledgments References CIOCARLIE 2014 241 252 M EXPERIMENTALROBOTICS TOWARDSRELIABLEGRASPINGMANIPULATIONINHOUSEHOLDENVIRONMENTS KIM 2012 2 14 D JAIN 2010 45 64 A LEROUX 2013 101 107 C BEETZ 2011 529 536 M 11THIEEERASINTERNATIONALCONFERENCEHUMANOIDROBOTSHUMANOIDS2011 ROBOTICROOMMATESMAKINGPANCAKES VAHRENKAMP 2010 2883 2888 N 2010IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA INTEGRATEDGRASPMOTIONPLANNING JEONG 2012 130 140 S SMITH 2005 13 29 L CHAUHAN 2011 341 354 A HE 2008 1727 1738 H KASAEI 2015 537 553 S OLIVEIRA 2014 2216 2223 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS OLIVEIRA 2015 2488 2495 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS CONCURRENTLEARNINGVISUALCODEBOOKSOBJECTCATEGORIESINOPENENDEDDOMAINS OLIVEIRA 2016 614 626 M SRINIVASA 2008 2155 2162 S INTERNATIONALCONFERENCEINTELLIGENTAUTONOMOUSSYSTEMS ROBOTICBUSBOYSTEPSTOWARDSDEVELOPINGAMOBILEROBOTICHOMEASSISTANT HERTZBERG 2014 297 304 J ROCKEL 2013 52 57 S DESIGNINGINTELLIGENTROBOTSREINTEGRATINGAIIIAAAISPRINGSYMPOSIUMSTANFORDUSA ONTOLOGYBASEDMULTILEVELROBOTARCHITECTUREFORLEARNINGEXPERIENCES MOKHTARI 2016 509 517 V TWENTYSIXTHINTERNATIONALCONFERENCEAUTOMATEDPLANNINGSCHEDULING EXPERIENCEBASEDROBOTTASKLEARNINGPLANNINGGOALINFERENCE SRINIVASA 2010 5 20 S BOHG 2014 289 309 J SAHBANI 2012 326 336 A CHINELLATO 2009 223 254 E MONACO 2015 454 465 S CASTIELLO 2005 726 736 U CULHAM 2006 2668 2684 J SHAFII 2016 2895 2900 N 2016IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS LEARNINGGRASPFAMILIAROBJECTSUSINGOBJECTVIEWRECOGNITIONTEMPLATEMATCHING STUCKLER 2013 1106 1115 J ALDOMA 2012 80 91 A MARTINEZTORRES 2010 2043 2049 M IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA2010 MOPEDASCALABLELOWLATENCYOBJECTRECOGNITIONPOSEESTIMATIONSYSTEM ISLAM 2011 1398 1401 M 8THASIANCONTROLCONFERENCEASCC2011 OBJECTCLASSIFICATIONBASEDVISUALEXTENDEDFEATURESFORVIDEOSURVEILLANCEAPPLICATION YEH 2009 280 287 T IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION2009CVPR2009 FASTCONCURRENTOBJECTLOCALIZATIONRECOGNITION YEH 2008 1 8 T IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR2008 DYNAMICVISUALCATEGORYLEARNING KIRSTEIN 2012 90 105 S COLLET 2015 3 25 A QUIGLEY 2009 5 11 M ICRAWORKSHOPOPENSOURCESOFTWARE ROSOPENSOURCEROBOTOPERATINGSYSTEM HAMIDREZAKASAEI 2014 47 52 S 2014IEEEINTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC INTERACTIVEOPENENDEDLEARNINGAPPROACHFOR3DOBJECTRECOGNITION LIM 2014 153 160 G 23RDIEEEINTERNATIONALSYMPOSIUMROBOTHUMANINTERACTIVECOMMUNICATION2014ROMAN INTERACTIVETEACHINGEXPERIENCEEXTRACTIONFORLEARNINGABOUTOBJECTSROBOTACTIVITIES FISCHLER 1981 381 395 M HARTIGAN 1979 100 108 J SEABRALOPES 2007 53 81 L LAI 2011 1817 1824 K 2011IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA ALARGESCALEHIERARCHICALMULTIVIEWRGBDOBJECTDATASET BADDELEY 1997 A HUMANMEMORYTHEORYPRACTICE KASAEI 2016 312 320 S RUSU 2010 2155 2162 R 2010IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS FAST3DRECOGNITIONPOSEUSINGVIEWPOINTFEATUREHISTOGRAM KASAEI 2016 1948 1956 S ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMSNIPS HIERARCHICALOBJECTREPRESENTATIONFOROPENENDEDOBJECTCATEGORYLEARNINGRECOGNITION HAMIDREZAKASAEIX2018X151 HAMIDREZAKASAEIX2018X151X166 HAMIDREZAKASAEIX2018X151XS HAMIDREZAKASAEIX2018X151X166XS 2020-03-22T00:00:00.000Z UnderEmbargo © 2018 Elsevier B.V. All rights reserved. item S0925-2312(18)30232-7 S0925231218302327 10.1016/j.neucom.2018.02.066 271597 2018-03-22T15:36:32.186592Z 2018-05-24 true 4881073 MAIN 16 54620 849 656 IMAGE-WEB-PDF 1 gr1 10651 115 219 gr10 11599 164 154 gr11 9351 82 219 gr12 26962 155 219 gr13 10036 87 219 gr14 30351 164 192 gr2 16717 128 219 gr3 17641 163 189 gr4 7005 67 219 gr5 15703 164 190 gr6 10047 120 219 gr7 26213 145 219 gr8 12701 164 211 gr9 9512 121 219 fx1 16407 163 121 fx2 20096 164 123 fx3 18022 164 123 fx4 20147 164 123 fx5 13277 164 123 gr1 47886 271 518 gr10 175841 860 809 gr11 81706 304 809 gr12 35822 274 386 gr13 50601 301 756 gr14 123104 564 662 gr2 124400 457 784 gr3 67649 456 527 gr4 24552 185 602 gr5 64347 520 602 gr6 30416 211 386 gr7 77632 397 602 gr8 155709 628 809 gr9 78168 445 809 fx1 8590 150 111 fx2 8116 151 113 fx3 8034 151 113 fx4 8724 151 113 fx5 9747 151 113 gr1 448813 1441 2752 gr10 1668752 4569 4300 gr11 693215 1614 4300 gr12 373823 1454 2050 gr13 374749 1333 3346 gr14 1106550 2500 2933 gr2 1452536 2428 4164 gr3 683503 2017 2333 gr4 215965 821 2667 gr5 583371 2303 2668 gr6 254415 1122 2053 gr7 709493 1760 2667 gr8 1791270 3340 4300 gr9 673821 2367 4300 fx1 54703 667 494 fx2 57038 667 500 fx3 55795 667 500 fx4 82756 667 500 fx5 86154 667 500 si10 526 16 143 si20 496 16 106 si30 610 18 149 si5 312 17 72 si6 284 13 63 si7 170 11 19 si8 1704 18 510 si9 184 16 20 si11 129 13 13 si12 2064 46 390 si13 443 16 73 si14 396 16 66 si15 633 16 114 si16 1203 16 360 si17 3195 110 354 si18 418 16 71 si19 1128 49 220 si2 396 17 62 si21 185 12 24 si22 394 14 111 si23 611 39 102 si24 511 16 143 si25 1983 43 432 si26 1588 52 298 si27 1391 52 265 si28 166 13 16 si29 1436 31 261 si3 1671 19 527 si4 456 16 95 si1 396 17 63 NEUCOM 19370 S0925-2312(18)30232-7 10.1016/j.neucom.2018.02.066 Elsevier B.V. Fig. 1 Overall architecture of the proposed system. Fig. 1 Fig. 2 Dictionary construction: (left) the robot moves through an office to extract tabletop objects; (center) the captured scenes are processed to produce a pool of object candidates; (right) a pool of local shape features is obtained by computing spin-images from the pool of object candidates; the dictionary is subsequently constructed by clustering the features using the k-means algorithm; finally, a dictionary with 20 visual words is built. Fig. 2 Fig. 3 An example of preprocessing and Object Detection: (a) experiment setup; the JACO robotic arm performs manipulation tasks to clear the table; (b) distance filtering; (c) result of the second preprocessing step and table detection; (d) the position of the arm joints are used to filter out the points corresponding to robot’s body from the original point cloud. The object candidates are shown by different bounding boxes and colors. The red, green and blue lines represent the local reference frame of the objects. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article). Fig. 3 Fig. 4 Object representation for a flask: (a) keypoint extraction; (b) surface normal estimation for the keypoints; (c) a schematic of how spin-image is computed for a keypoint p; (d) histogram of visual words that represents the object view. Fig. 4 Fig. 5 A 3D visualization of an object labelling event: (a) pointing to object by the instructor; (b) associating a label to the object that is currently being pointed; (c) labelling object categories by associating a label to a TrackID; (d) instructing the robot to perform the clear_table task. Fig. 5 Fig. 6 Schematic representation of task planning, grasp planning and execution manager. Fig. 6 Fig. 7 Sequence of snapshots showing the JACO robotic arm performing a constrained pick and place task to clean the table; In this task, the orientation of the grasped object must be kept consistent throughout the plan; (a) the JACO robotic arm goes to the initial pose and extracts object (i.e. ’PlasticCup’) pose and shape properties; (b) a side grasp is selected and the robot goes to pre-grasp position; (c) the robot approaches and grasps the PlasticCup; (d) picking up the PlasticCup and moving it to the side; (e) placing the object and (f) going back to the initial position. Fig. 7 Fig. 8 Object recognition performance for different values of four parameters of the system; the system parameters are represented as a tuple (VS, DS, IW, SL). Fig. 8 Fig. 9 (top) Evolution of teaching protocol accuracy versus number of question/correction iterations in the first 200 iterations of the simulated teacher experiment 1 with the protocol accuracy threshold set to 0.67; (bottom) protocol accuracy versus the number of learned categories, for the same experiment. Fig. 9 Fig. 10 Evolution of teaching protocol accuracy versus number of question/correction iterations in simulated teacher experiments #3, 5, 7 and 9 with the protocol accuracy threshold set to 0.67. Fig. 10 Fig. 11 System performance during simulated user experiments: (left) global accuracy versus number of learned categories, a measure of how well the system learns; (right) number of learned categories versus number of question/correction iterations, represents how fast the system learned object categories. Fig. 11 Fig. 12 Our experimental setup consists of a computer for human–robot interaction purposes, a Kinect sensor and a JACO robotic-arm as the primary sensory-motor embodiments for perceiving and acting upon its environment. Fig. 12 Fig. 13 System performance during the clear_table use-case; (left): Initially, the system starts with no knowledge of any object. The position of the arm joints are retrieved from Working Memory and visualized by grey spheres and black lines. The table is then detected as shown by the green rectangle. Afterwards, the object candidates are detected and highlighted by different colors. The grey bounding boxes and the local reference frames represent the pose of the objects as estimated by object tracking module. (center): A user then teaches all the active objects to the system and all objects are correctly recognized, i.e., the output of object recognition is shown in blue on top of each object. (right): When grasping and manipulating an object, the shape of the object is partially changed and, as a consequence, a misclassification might happen. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article). Fig. 13 Fig. 14 The sequence of snapshots showing the JACO robotic arm performing a clear_table task; (First row): PlasticCup is the closest object to the arm’s base. Therefore, the robot picks it up first from the table, transports it into the first predefined area and then, places the PlasticCup down. (Second row): CoffeeJug is selected as the second closest object. The robot goes to the pre-grasp area and then grasps the CoffeeJug. The robot moves the object into the second placing area and places it down. (Third row): Similarly, Bottle object is picked-up, moved and placed. Fig. 14 Table 1 List of used constraints with a short description for each one. Table 1 Constraints Description Section C table: “is this candidate on a table?” The target object candidate is placed on top of a table. 4 C track: “is this candidate being tracked?” Storing all object views while the object is static would lead to unnecessary accumulation of highly redundant data. This constraint is used to infer that the segmented object is already being tracked or not. 4 C size: “is this candidate manipulatable?” Reject large object candidate 5 C instructor: “is this candidate part of the instructor’s body? Reject candidates that are belong to the user’s body 4 C robot: “is this candidate part of the robot’s body?” Reject candidates that are belong to the robot’s body 4 C edge: “is this candidate near to the edge of the table?” Reject candidates that are near to the edge of the table 5 C key _ view : “is this candidate a key view?” For representing an object, only object views that are marked as key-views are stored in the database. An object view is selected as a key view whenever the tracking of an object is initialized, or when it becomes static again after being moved. In case the hands are detected near the object, storing key views is postponed until the hands are withdrawn. 5 Table 2 Average object recognition performance for different parameters. Table 2 Parameters VS DS IW SL Values 0.01 0.02 0.03 50 60 70 80 90 4 8 0.02 0.03 0.04 0.05 Average accuracy 0.76 0.74 0.71 0.72 0.73 0.74 0.74 0.75 0.75 0.72 0.63 0.74 0.78 0.79 Table 3 Summary of experiments(1). Table 3 EXP# #QCI #LC #AIC GCA (%) APA (%) 1 1257 49 8.16 79 83 2 1228 49 7.83 80 84 3 1227 49 7.65 81 84 4 1240 49 9.08 75 78 5 1236 49 7.95 80 83 6 1346 49 9.46 76 79 7 1293 49 9.02 77 81 8 1330 49 9.79 74 79 9 1336 49 9.55 75 78 10 1225 49 8.30 78 82 EXP#: experiment number; QCI: Question/Correction Iterations; LC: Learned Categories; AIC: Average Instances per Category; GCA: Global Classification Accuracy; APA: Average Protocol Accuracy. Towards lifelong assistive robotics: A tight coupling between object perception and manipulation S. Hamidreza Kasaei ⁎ a Miguel Oliveira a b Gi Hyun Lim a Luís Seabra Lopes a c Ana Maria Tomé a c a IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro Universidade de Aveiro Portugal b Instituto de Engenharia de Sistemas e Computadores, Tecnologia Ciência, R. Dr. Roberto Frias, 465, Porto 4200, Portugal Instituto de Engenharia de Sistemas e Computadores, Tecnologia Ciência R. Dr. Roberto Frias, 465 Porto 4200 Portugal c Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática Universidade de Aveiro Portugal ⁎ Corresponding author. Communicated by Prof. Zidong Wang This paper presents an artificial cognitive system tightly integrating object perception and manipulation for assistive robotics. This is necessary for assistive robots, not only to perform manipulation tasks in a reasonable amount of time and in an appropriate manner, but also to robustly adapt to new environments by handling new objects. In particular, this system includes perception capabilities that allow robots to incrementally learn object categories from the set of accumulated experiences and reason about how to perform complex tasks. To achieve these goals, it is critical to detect, track and recognize objects in the environment as well as to conceptualize experiences and learn novel object categories in an open-ended manner, based on human–robot interaction. Interaction capabilities were developed to enable human users to teach new object categories and instruct the robot to perform complex tasks. A naive Bayes learning approach with a Bag-of-Words object representation are used to acquire and refine object category models. Perceptual memory is used to store object experiences, feature dictionary and object category models. Working memory is employed to support communication purposes between the different modules of the architecture. A reactive planning approach is used to carry out complex tasks. To examine the performance of the proposed architecture, a quantitative evaluation and a qualitative analysis are carried out. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform complex tasks. Keywords Assistive robots 3D object perception Open-ended learning Interactive learning Object manipulation 1 Introduction Assistive robots are extremely useful because they can help elders or people with motor impairments to achieve independence in everyday tasks [1,2]. Elderly, injured, and disabled people have consistently attributed a high priority to object manipulation tasks [3]. Object manipulation tasks consist of two phases: the first is the perception of the object and the second is the planning and execution of arm or body motions which grasp the object and carry out the manipulation task. These two phases are closely related: object perception provides information to update the model of the environment, while planning uses this world model information to generate sequences of arm movements and grasp actions for the robot. In addition, assistive robots must perform the tasks in reasonable time. It is also expected that the competence of the robot increases over time, that is, robots must robustly adapt to new environments by being capable of handling new objects. However, it is not reasonable to assume that one can pre-program all necessary object categories for assistive robots. Instead, robots should learn autonomously from novel experiences, supported in the feedback from human teachers. In order to incrementally adapt to new environments, an autonomous assistive robot must have the ability to process visual information and conduct learning and recognition tasks in a concurrent and interleaved fashion. Several state-of-the-art assistive robots use traditional object category learning and recognition approaches [4–6]. These classical approaches are often designed for static environments in which it is viable to separate the training (off-line) and testing (on-line) phases. In these cases, the world model is static, in the sense that the representation of the known categories does not change after the training stage. Therefore, these robots are unable to adapt to dynamic environments [7]. This leads to several shortcomings such as the inability to detect/recognize new or unknown categories. To cope with these issues, several cognitive robotics groups have started to explore how robots could learn incrementally from their own experiences as well as from interaction with humans [8–10]. In this paper, a cognitive framework for assistive robots is presented which provides a tight coupling between object perception and manipulation. The approach is designed to be used by an assistive robot working in a domestic environment. In particular, we present an adaptive object perception system based on environment exploration and Bayesian learning. The objective is that the robotic system is capable of continuously learning new object categories while carrying out manipulation tasks in the environment. This work focuses on learning, recognizing and manipulating table-top objects. The contributions proposed in this work are the following: (i) an integrated framework for object manipulation incorporating perception and planning capabilities for manipulation tasks; (ii) unsupervised object exploration methodology that produces a dictionary of visual words used for representing objects (Bag-of-Words model); (iii) interactive categorization (labelling) of physical objects, in which a human user playing the role of tutor provides category labels for objects under shared attention; (iv) open-ended learning of object category models from experiences. The fourth contribution follows our previous works on open-ended learning for object recognition [11–14]. These previous approaches are instance-based, i.e. a set of features is stored for each object view. In contrast, the present work uses a Naive Bayes learning method to compute category models from the observed views of instances of the categories. Furthermore, manipulation experiments are carried out for validating the approach. The remainder of the paper is organized as follows: Section 2 describes the related work; an overview of the developed system is presented in Section 3; Sections 4–7 describe in detail the proposed methodologies. Finally, results are presented and discussed in Section 8 and conclusions are presented in Section 9. 2 Related work Although an exhaustive survey of assistive robotics as well as object perception and manipulation techniques is beyond the scope of this paper, representative works will be reviewed in this section. 2.1 Assistive and service robots Daily tasks such as setting a table for a meal or cleaning a table are difficult for disabled or elder people [2]. Over the past decade, several researches have been conducted to develop robots to assist those people in order to enable them to maintain an active life less dependent on others [1]. In the ARMEN project, Leroux et al. [4] proposed a mobile assistive robotics approach providing advanced functions to help maintaining elderly or disabled people at home. Similar to our system, this project involves object manipulation, knowledge representation and object recognition. The authors also developed an interface to facilitate the communication between the user and the robot. Jain et al. [3] presented an assistive mobile manipulator named EL-E that can autonomously pick objects from a flat surface and deliver them to the users. They used a multi-step control policy that is not suitable to achieve real time performance. In our approach we can achieve real-time performance through the use of ROS nodelets and multiplexing mechanisms [12]. Furthermore, in [3], the user provides the location of the object to be grasped by the robot by briefly illuminating a location with a laser pointer. In this work, objects are detected and recognized autonomously. Therefore it is enough for the user to specify the category of the object to be picked up. In another work [15], a multi-robot assistive system, consisting of a Segway mobile robot with a tray and a stationary Barrett WAM robotic arm, was developed. The Segway robot navigates through the environment and collects empty mugs from people. Then, it delivers the mugs to a predefined position near the Barrett arm. Afterwards, the arm detects and manipulates the mugs from the tray and loads them into a dishwasher rack. This work is similar to ours in that it integrates perception and motion planning for pick and place operations. However there are some differences: their vision system is designed for detecting a single object type (mugs), while our perception system not only tracks the pose of different types of objects but also recognizes their categories. Furthermore, because there is a single object type (i. e. mug), they computed the set of grasp points off-line. In our approach, grasping must handle a variety of objects never seen before. In the RACE project (Robustness by Autonomous Competence Enhancement), a PR2 robot demonstrated effective capabilities in a restaurant scenario including the ability to serve a coffee, set a table for a meal and clear a table [16–18]. The aim of RACE was to develop a cognitive system, embodied by a service robot, which enabled the robot to build a high-level understanding of the world by storing and exploiting appropriate memories of its experiences. Other examples of assistive robot platforms that have demonstrated perception and action coupling include TUM Rosie robot [5], HERB [19] and ARMAR-III [6]. 2.2 Object manipulation In most cases, prior works on object manipulation requires a complete geometric description of the objects [20,21]. However, in real scenarios, it is not possible to have complete knowledge of the geometric properties of all possible objects in advance. That information has to be extracted online from the experiences of the robot. In neuroscience and neurocomputing literature, it has been demonstrated that visual processing in the ventral and dorsal pathways is based on classifying the grasped objects into three groups: known, familiar and unknown objects [1,22–25]. This classification has been adopted in robotics [20]. The underlying reason for this classification is that prior knowledge about objects determines how grasp candidates are generated and ranked. For known objects, i.e., when there is complete knowledge of the geometric properties of objects, grasping is limited to solving the problems of recognition and pose estimation. In the case of familiar objects, an object comparison procedure may be used to compare the given object with known objects, and to define grasping strategies based on that [26]. For unknown objects, heuristic methods are used to extract grasps in run-time from 3D sensor data. Commonly, the heuristic methods work based on both the overall shape of the object and its features. For more details on grasp synthesis, we refer the reader to the surveys of Bohg et al. [20] and Sahbani [21]. Similar to our grasping approach, Ciocarlie et al. [1] and Stuckler et al. [27] have considered grasps on objects either from above or from the side based on the overall shape of the object and the global characteristics such as center of mass and bounding box obtained from RGBD data. The intuition behind this approach is that many domestic objects are graspable by aligning the grippers with the (estimated) principal axes of the object. They follow a standard train and test procedure for object recognition, while our approach can incrementally update its knowledge based on new observations. 2.3 Object perception and learning Interactive open-ended object category learning and recognition are key capabilities in assistive and service robotics. This means that a robot should be capable of continuously learning new objects in order to perform different tasks in domestic domains. Aldoma et al. [28] reviewed properties, advantages and disadvantages of several state-of-the-art 3D shape descriptors available from the Point Cloud Library (PCL) to develop 3D object recognition and pose estimation system. They also proposed two pipelines for object recognition systems using local and global 3D shape descriptors from PCL. Martinez et al. [29] described a fast and scalable perception system for object recognition and pose estimation. The authors employed the RANSAC and Levenberg Marquardt algorithms to segment objects and represented them based on SIFT descriptors. In [30], an object classification approach was proposed, in which the object representation was based on SIFT, SURF and color histograms. All these features were compacted into a histogram of visual words for optimizing the recognition process, as well as memory usage. In this case, authors used a naive Bayes classifier in the recognition stage. Yeh et al. [31] integrated the bag-of-words methodology to propose an efficient method for concurrent object localization and recognition. In most of the proposed systems described above, training and testing are separate processes, i.e., they do not occur simultaneously. However, in open-ended applications, data is continuously available and the target object categories are not known in advance. In these cases, traditional object recognition approaches are not well suited, because those systems are limited to using off-line data for training and are therefore unable to adapt to new environments / objects. There are some approaches which support incremental learning of object categories. In these approaches, the set of classes is predefined and the models of known object categories are enhanced (e.g., augmented, improved) over time, while in open-ended approaches the set of categories is also continuously growing. Haibo et al. [10] proposed an incremental multiple-object recognition and localization (IMORL) framework using a multilayer perceptron (MLP) structure as the base learning model. The authors claimed that the proposed framework can incrementally learn from accumulated experiences and use such knowledge for object recognition. Yeh and Darrell [32] developed novel methods for efficient incremental learning of SVM-based visual category classifiers, and showed that, using their framework, it is possible to adapt the classifiers incrementally. Kirstein et al. [33] proposed a lifelong learning approach for interactive learning of multiple categories based on vector quantization and a user interface. Collet et al. [34] proposed a graph-based approach for lifelong robotic object discovery. Similar to our approach, they used a set of constraints to explore the environment and to detect object candidates from raw RGB-D data streams. In contrast, their system does not interactively acquire more data to learn and recognize the object. Seabra Lopes and Chauhan [9] approached the problem of object experience gathering and category learning with a focus on open-ended learning and human-robot interaction. In their approach, learning is based on multiple representations as well as combinations of classifiers. They showed a system that starts with an empty vocabulary and can incrementally acquire object categories through the interaction with a human user. They used RGB data whereas we used depth data. Moreover, their object detection, learning and recognition approaches are completely different from our approach. 3 Overall system architecture The overall system architecture is depicted in Fig. 1 . It is a reusable framework, with all modules developed in Robot Operating System (ROS) [35]. The current architecture is an evolution of the architecture developed in previous work for object perception and open-ended perceptual learning [14,36]. Information exchange is performed using standard ROS mechanisms (i.e. either publish / subscribe or server/client). Therefore, any new module can be easily added to the system. The architecture includes two memory systems, namely the Working Memory and the Perceptual Memory. Both memory systems have been implemented using a lightweight NoSQL database called LevelDB 1 1 LevelDB has been developed by Google: . LevelDB is a fast key-value storage database that provides an ordered mapping from string keys to string values. The Working Memory is used for temporarily storing information as well as for communication among different modules. It keeps track of the evolution of both the internal state of the robot and the events observed in the environment (i.e. world model). The object features, dictionary of visual words, object representation data and object category models are stored into the Perceptual Memory. The goal of Grasp Planning is to extract a grasp pose (i.e. a gripper pose relative to the object) either from above or from the side of the object, using global characteristics of the object. The Execution Manager works based on a Finite-State-Machine (FSM) paradigm. It retrieves the task plan and the world model information from Working Memory and computes the next action (i.e. a primitive operator) based on the current context. Then, it dispatches the action to the robot platform as well as records success or failure information in the Working Memory. Whenever the robot captures a scene, the first step is preprocessing which includes three filtering procedures, namely distance filtering, a filter to remove the robot’s body from sensor data, and a downsampling filter for reducing the size of the data. Object Detection, responsible for detecting objects in the scene, launches a new perception pipeline for each detected object. Each pipeline includes Object Tracking, Feature Extraction, Object Representation and Object Recognition modules. The Object Tracking module estimates the current pose of the object based on a particle filter, which uses shape and color data [12]. The Feature Extraction module extracts features of the current object view and stores them in the Perceptual Memory. Based on the extracted features and on a visual dictionary, the Object Representation module describes objects as histograms of visual words and stores them into the Perceptual Memory. A user can provide category labels for these objects via the User Interaction module [37]. User Interaction is essential for supervised experience gathering. A graphical user interface has been developed to teach the robot new object categories or to instruct the robot to perform a complex task. The developed architecture, shown in Fig. 1, includes two perceptual learning modules. One of them, the Dictionary Builder, is concerned with building a dictionary of visual words for object representation. The dictionary plays a prominent role because it is used for category learning as well as recognition. The second learning module is the Object Conceptualizer. Whenever the instructor provides a category label for an object, the Conceptualizer retrieves the probabilistic models of the current object categories as well as the representation of the labeled object in order to improve an existing object category model or to create a new category model. In recognition situations, a probabilistic classification rule is used to assign a category label to the detected object. The system is run in two stages. The first stage is dedicated to environment exploration. In this stage, unsupervised object discovery is carried out in the environment while the robot operates. The robot seeks to segment the world into “object” and “non-object”. Afterwards, a pool of shape features is created by computing local shape features for the extracted objects. The pool of features is then clustered by the Dictionary Builder leading to a set of visual words (dictionary). Only the modules directly involved in object discovery and dictionary building are active in this stage. The second stage corresponds to the normal operation of the robot, with object category learning, recognition, planning and execution. In the following sections, the characteristics of each module are explained in detail. 4 Environment exploration and dictionary construction Comparing 3D objects by their local features would be computationally expensive. To address this problem, a Bag-of-Word (BoW) approach is adopted for object representation, i.e. objects are described by histograms of local shape features. This approach requires a dictionary of visual words. Usually, this dictionary is created off-line through clustering of a given training set. In open-ended learning scenarios, there is no predefined set of training data available at the beginning of the learning process. To cope with this limitation, we look at human cognition, in particular at the fact that human babies explore their environment in a playful (arbitrary) way [8]. Therefore, we propose that the robot freely explores several scenes and collects several object experiences. Gathering object experiences by exploration has the advantage of not requiring any human annotation of individual objects. This (non goal-directed) exploration provides chances to discover new objects. In general, object exploration is a challenging task because of the dynamic nature of the world and ill-definition of the objects [34]. Since a system of boolean equations can represent any expression or any algorithm, it is particularly well suited for encoding the world and object candidates. Similar to Collet’s work [34], we use boolean algebra 2 2 , using three logical operators, namely AND (∧), OR (∨) and NOT (¬). A set of boolean constraints, C, was then defined based on which boolean expressions, ψ, were established to encode object candidates for the process of constructing the dictionary of visual words as well as for interactive object category learning and recognition (see Table 1 ). The definition of “object” in the exploration stage is more general than in the normal operation stage (see Eqs. 1 and 4). In both cases, we assume that interesting objects are on tables and the robot seeks to detect tabletop objects (i.e. C table). Due to memory size concerns, a representation of an object should only contain distinctive views. A view which is different from the current view may appear after the object is moved (i.e. the pose of the object relative to the sensor changes). An object view is selected as a key view (i.e. C k e y _ v i e w ) whenever the tracking of an object is initialized (C track), or when it becomes static again after being moved. Therefore, the C k e y _ v i e w constraint is used to optimize memory usage and computation while keeping potentially relevant and distinctive information. Moreover, C instructor and C robot are used to filter out object candidates which are part of the instructor’s body or robot’s body. Accordingly, the resulting object candidates are less noisy and include only data corresponding to the environment: (1) ψ exploration = C table ∧ C track ∧ C key _ view ∧ ¬ ( C instructor ∨ C robot ) , In our current setup, a table is detected by finding the dominant plane in the point cloud. This is done using the RANSAC algorithm [38]. Extraction of polygonal prisms is used for collecting the points which lie directly above the table. Afterwards, an Euclidean Cluster Extraction 3 3 algorithm is used to segment each scene into individual clusters. Every cluster that satisfies the exploration expression, ψ exploration, is selected. The output of object exploration is a pool of object candidates. It should be noted that to balance computational efficiency and robustness, a downsampling filter is applied to obtain a smaller set of points distributed over the surface of the object. Subsequently, to construct a pool of features, spin-images 4 4 The default spin-image parameters are the following: S L = 5 mm , A = π / 2 and I W = 4 . are computed for the selected points extracted from the pool of object candidates. We use a PCL function to compute spin-images 5 5 In this work, we computed around 32,000 spin-images from the point cloud of the 194 objects. . These capabilities are implemented in the Object Detection, Object Representation and Feature Extraction modules (see Fig. 1). Finally, the dictionary is constructed by clustering the features using the k-means algorithm [39]. The centers of the N generated clusters are treated as visual words, w i (1 ≤ i ≤ N). Fig. 2 shows a dictionary containing 20 words. In the implementation, we tested different dictionary sizes (see Section 8.1). In the context of the RACE project [16], the University of Osnabruck provided us with a rosbag collected by one of their robots while exploring an office environment. A video of this exploration is available at: The exploration stage was run on this rosbag. 5 Object detection and representation This section presents the Object Detection, Feature Extraction and Object Representation modules as they are used in the normal operation stage. 5.1 Object detection and tracking A common way for fast processing of massive point clouds is to use some mechanisms for removing unnecessary or irrelevant data. For this purpose, two filters are used that discard large quantities of 3D points from the original point cloud. The first step is to define a cubic volume in 3D (distance filtering), which defines the region of interest. The second filter reduces the spatial resolution of points (downsampling) using a voxelized grid approach 6 6 . Furthermore, the points corresponding to the body of the robot are filtered out from the original point cloud by retrieving the knowledge of the positions of the arm joints relative to the camera pose from the working memory. After preprocessing, the next step is to find objects in the scene using the preprocessed point cloud. The object detection module implements the following specification: (2) ψ detection = C table ∧ C track ∧ C size ∧ ¬ ( C instructor ∨ C robot ∨ C edge ) , The object detection uses a size constraint, C size, to detect objects which can be manipulated by the robot. Moreover, a C edge constraint is considered to filter out the segmented point clouds that are too close to the edge of the table. The Object Detection module then assigns a new TrackID to each newly detected object and launches an object perception pipeline for the object. Finally, the object detection module pushes the segmented object candidate into the respective pipeline for subsequent processing steps. An example of the proposed detection approach is shown in Fig. 3 . The Object Tracking module is responsible for keeping track of the target object over time while it remains visible. It receives the point cloud of the detected object and computes an oriented bounding box aligned with the point cloud’s principal axes. The center of the bounding box is considered as the pose of the object. The module sends out the tracked object information to the Feature Extraction module. 5.2 Feature extraction and object representation Object representation is critical to any object recognition system. In the present work, we adopt an approach to object representation in which object views (instances) are described by histograms of frequencies of visual words. The input is the set of features of an object candidate, O , computed by the Feature Extraction module. The Feature Extraction module involves keypoint extraction and computation of a spin image for each keypoint. Finally, the Object Representation module represents these features as a histogram of visual words. For keypoint extraction, first a voxelized grid approach is used to obtain a smaller set of points. The nearest neighbor point to each voxel center is selected as a keypoint [11]. Afterwards, the spin-image descriptor is used to encode the surrounding shape in each keypoint using the original point cloud. By searching for the nearest neighbor in the dictionary, each spin image is assigned to a visual word. Finally, each object is represented as a histogram of occurrences of visual words: (3) h = [ h 1 h 2 . . . h n ] , where the ith element of h is the count of the number of features assigned to a visual word, w i and n is the size of the dictionary. Fig. 4 illustrates the Feature Extraction and Object Representation processes for an object. The obtained histogram is dispatched to the Object Recognition module and is recorded in Perceptual Memory. To optimize the Perceptual Memory, some object views are marked as key views and only these are recorded into the memory. Key object views are selected by the Object Tracking module when the object is not moving and the user’s hands are far away from the object [37]. In other words, key views are defined as follows: (4) ψ key _ view = C table ∧ C track ∧ C size ∧ C key _ view ∧ ¬ ( C instructor ∨ C robot ∨ C edge ) . 6 Interactive object category learning and recognition The key idea for fast 3D object recognition is to use mechanisms for representing objects in a uniform and compact format. Estimating a robust model for each object category is more promising than template matching. In this section, first, a user interface for supervised experience gathering is presented. The interface is used not only for teaching new object categories in situations where the robot encounters with new objects but also for providing corrective feedback in the case there is a misclassification. The Bag-of-Words representation combined with the Naive Bayes approach are used to incrementally learn probabilistic models of object categories. 6.1 User interaction Human–robot interaction is essential for supervised experience gathering i.e. for instructing the robot how to perform different tasks. Particularly, an open-ended object category learning and recognition system will be more flexible if it is able to learn new categories using the feedback of a human user. The User Interaction module provides a graphical menu to facilitate the collection of supervised object experiences and to instruct the robot to perform a task. In the case of supervised object experiences, two alternative interactions with an instructor are supported: gesture recognition or the usage of a graphical menu interface. In the first case, the instructor points to an object and then selects the desired label from a menu. In the second case the instructor can select the category label for an object based on its TrackID. Further details on supervised object experience gathering are available in [37]. An example of object labelling is depicted in Fig. 5 . The instructor puts a ‘Vase’ on the table. Tracking is initialized with TrackID 1. The gray bounding box signals the pose of the object as estimated by the tracker. TrackID 1 is classified as ‘Unknown’ because vases are not yet known to the system; the instructor points at TrackID 1. The system recognizes the pointing gesture and the corresponding menu is activated. The instructor labels the object as ‘Vase’. The Object Conceptualizer (category learning) module is activated when the instructor provides a category label for the object. In addition, the User Interaction module provides a menu to request the robot to perform a task or to abort the current task. 6.2 Object conceptualizer Learning methods used in most of the classical object recognition systems are not designed for open-ended domain, since those methods do not support an incremental update of the internal robot’s knowledge based on new experiences. On the contrary, open-ended learning approaches can incrementally update the acquired knowledge (category models) and extend the set of categories over time, which is suitable for real-world scenarios. For example, if the robot does not know how a ‘Mug’ looks like, it may ask the user to show one. Such situation provides an opportunity to collect training instances from actual experiences of the robot and the system can incrementally update it’s knowledge rather than retraining from scratch when a new instance is added or a new category is defined. In this section, we propose an open-ended 3D object category learning approach, which considers category learning as a process of updating a probabilistic model for each object category using the Naive Bayes approach. There are two reasons why Bayesian learning is useful for open-ended learning. One of them is the computational efficiency of the Naive Bayes approach. In fact, this model can be easily updated when new information is available, rather than retrained from scratch. Second, instance-based open-ended systems have continuously growing memory since they are constantly storing new object view representations (instances). Therefore, these systems must resort to experience management methodologies to discard some instances and thus prevent the accumulation of a too large set of experiences. In Bayesian learning, new experiences are used to update category models and then the experiences are forgotten immediately. The category model encodes the information collected so far. Therefore, this approach consumes a much smaller amount of memory when compared to any instance-based approach. The probabilistic category model requires calculating the likelihoods of the object given the category k, P ( O | C k ) , and it is also parametrized by the prior probabilities P(Ck ). It should be noted that the parameters of the likelihood are the probabilities of each visual word given the object category P ( w t | C ) . In this work, we consider the probability of each visual word occurring in the object independently, regardless of any possible correlations with the other visual words (Naive Bayes approach). The P ( C k ) P ( O | C k ) is equivalent to the joint probability model P ( C k , w 1 , ⋯ , w n ) = P ( C k ) P ( w 1 , ⋯ , w n | C k ) . The joint model can be rewritten using conditional independence assumptions: (5) P ( C k | w 1 , ⋯ , w n ) ∝ P ( C k , w 1 , ⋯ , w n ) ∝ p ( C k ) P ( w 1 | C k ) P ( w 2 | C k ) ⋯ P ( w n | C k ) ∝ P ( C k ) ∏ i = 1 n P ( w i | C k ) , where n is the size of the dictionary and P ( w i | C k ) is the probability of the visual word w i occurring in an object of category k. (6) P ( w i | C k ) = s i k + 1 ∑ j = 1 n ( s j k + 1 ) , where sik is the number of times that word w i was seen in objects from category Ck . Note, the probabilities are estimated with Laplace smoothing, by adding one to every counter, in order to prevent P ( w i | C k ) = 0 . On each newly seen object of this category with xi features of type w i , the following update is carried out: (7) s i k ← s i k + x i . The prior probability of category Ck is estimated as follows: (8) P ( C k ) = N k N , where N is the total number of seen objects of all categories and Nk is the number of seen objects from category k. 6.3 Object category recognition The last step in object perception is object category recognition. To classify an object O, which is represented as a histogram of occurrences of visual words h = [ h 1 h 2 . . . h n ] , the posterior probability for each object category is approximated using the Bayes theorem as: (9) P ( C k | O ) = P ( C k | h ) = P ( h | C k ) P ( C k ) P ( h ) ≈ P ( h | C k ) P ( C k ) . Because the denominator does not depend on Ck , and the values of the features are given as a histogram of occurrences of visual words, the denominator is constant. Eq. 9 is re-expressed based on Eq. 5 and multinomial distribution assumption: (10) P ( h | C k ) P ( C k ) ≈ P ( C k ) ∏ i = 1 n P ( w i | C k ) h i , In addition, to avoid underflow problems, the logarithm of the likelihood is computed: (11) ≈ log P ( C k ) + ∑ i = 1 n h i log P ( w i | C k ) , The category of the target object O is the one with highest likelihood: (12) C a t e g o r y ( O ) = argmax C k ∈ C P ( C k | O ) . 7 Planning and execution Fig. 6 shows a schematic representation of the planning and execution framework. In this framework, task planning is triggered when a user instructs the robot to achieve a task (e.x. clear_table). This is handled by the User Interaction module. The current state of the system, including world model information, global characteristics of the object of interest (i.e. overall shape, main axis, center of bounding box) and robot pose is retrieved from the working memory. Then, a task plan would be generated. A plan is a sequence of primitive operators to be performed to achieve the given goal. It should be noted that Task Planning is not in the scope of this paper. Previously, we showed how to conceptualize successfully executed task plans and how to use these conceptualized experiences for task planning [18]. In the present work, a predefined task plan is used. In order to be executed, a task plan must be complemented with end-effector poses. A pose is represented as a tuple G = (x , y , z , roll , pitch , yaw), specified relative to the base reference frame of the robot. The Grasp Planning module receives the task plan and chooses a grasp point either from above or from the side as well as a pre-grasp pose using the world model information and global characteristics of the object. In the current setup, the pre-grasp pose is placed at a fixed distance ( d p r e − g r a s p = 0.15 m ) behind or above the center of bounding box of the object. The intuition behind this assumption is that many domestic objects are graspable by aligning grippers with the principal axes of the object [1,27]. In another work, we proposed an advanced grasping approach to learn how to grasp familiar objects using interactive object view labeling and kinesthetic grasp teaching [26]. Afterwards, the Execution Manager retrieves the plan and grasp information from the Working Memory. The Execution Manager uses a Fine State Machine to reactively execute the plan. The actions are dispatched to the Robot Capabilities module. Inverse kinematics and safe controller, integrated from the JACO arm driver 7 7 , are used to transform a given end-effector pose goal into joint-space goals. Whenever the object is grasped, the height of the robot’s end-effector relative to the robot’s base is recorded into Working Memory and it is used as the desired height for placing the grasped object. The Execution Manager computes a new trajectory to navigate the robot’s end-effector to the placing area and sends out the action. After executing each action, the current state of the robot is updated in the Working Memory. Since world model information is updated by different modules (i.e. Object Detection, Execution Manager and etc.), the Execution Manager can abort execution when an unpredictable situation happens along expected execution path such as new obstacles move into the planned path of the robot arm. It should be noted that an orientation constraint on the end-effector is used to grasp and move an object parallel to the support plane. In addition, objects outside of the arm’s workspace are not considered. Fig. 7 illustrates the result of a constrained pick and place plan executed on the robot. 8 Experimental results Three types of experiments were performed to evaluate the proposed approach. First, an off-line quantitative evaluation for the object recognition system is presented (Section 8.1). Second, in Section 8.2, a “simulated teacher” was developed to assess the performance and scalability of the proposed object perception system. Finally, a qualitative analysis of the complete interactive open-ended object recognition system is shown in the context of a real-life use case (Section 8.3). In this case, a seven-minute demonstration session is described, where a user interacts with the system by teaching several objects to the robot and instructing the robot to perform a “clear_table” task. 8.1 Off-line evaluation of the perceptual learning approach An object dataset has been acquired for off-line evaluations, which contains 339 views of 10 categories of objects [11]. The system has four different parameters that must be tunned to provide a good balance between recognition performance, memory usage and computation time. To examine the accuracy of different configurations of the proposed approach, 10-fold cross validation was carried out. A total of 120 experiments were performed for different values of the four system parameters namely the voxel size (VS), which is related to number of keypoints extracted from each object view, the dictionary size (DS), the image width (IW) and support length (SL) of spin images. Results are presented in Table 2 . The object recognition performance for each system configuration is depicted in Fig. 8 where the system parameters are represented as a tuple (VS, DS, IW, SL). The parameters that obtained the best average accuracy were selected as the default system parameters. They are the following: VS = 0.01, DS = 90, IW = 4 and SL = 0.05. The accuracy of the system with the default parameters was 79%. Results show that the overall performance of the recognition system is promising. Spin images are capable of collecting distinctive traits of the local surface patches of each object. The results presented in Sections 8.3 and 8.2 are computed using this configuration. 8.2 Open-ended evaluation The off-line evaluation methodologies are not well suited to evaluate open-ended learning systems, because they do not abide to the simultaneous nature of learning and recognition and also those methodologies imply that the set of categories must be predefined. Therefore, an open-ended teaching protocol [9,11,40] is adopted in this evaluation. A simulated teacher was developed to assess the performance and scalability of the proposed object perception system by following the teaching protocol. The simulated teacher autonomously interacts with the learning system using teach, ask and correct actions. For each newly taught category, the simulated teacher repeatedly picks unseen object views of the currently known categories from a dataset and presents them to the system for checking whether the system can recognize them. The simulated teacher also provides corrective feedback in case of misclassification. Experiments were run on the largest publicly available 3D object dataset namely Washington RGB-D Object Dataset consisting of 250,000 views of 300 common household objects [41]. In the experiments that will be presented, the system begins with zero knowledge and the training instances become gradually available according to the teaching protocol. Therefore, the system learns new object categories as well as incrementally updates the existing object category models. Average Protocol Accuracy (APA) is computed using a sliding window of size 3n, where n is the number of categories that have already been introduced. If the number of iterations k, since the last time a new category was introduced, is less than 3n, all results are used. APA is used to determine if a new category can be taught. According to the protocol, the system is ready to learn a new object category when APA is higher than a certain threshold (marked by the horizontal line in Fig. 9 ), and at least one instance of every known category has been tested (k ≥ n). When an experiment is carried out, learning performance is evaluated using several measures, including: • The number of learned categories at the end of an experiment (LC), an indicator of How much does it learn?. • The number of question / correction iterations (QCI) required to learn those categories and the average number of stored instances per category (AIC), indicators of time and memory resources required for learning; i.e. How fast does it learn? • Global classification accuracy (GCA), computed using all predictions in a complete experiment, and the Average Protocol Accuracy (APA), indicators of How well does it learn?. Since the order of introduction of new categories may have an effect on the performance of the system, ten experiments were carried out in which categories were introduced in random sequences. Fig. 9 (top) shows the performance of the system in the initial 200 iterations of the first experiment. The introduced categories are signaled by vertical red lines and category labels in the plot. In the additional nine experiments, these categories were used again with different introduction sequences, the results of which are reported in Table 3 . By comparing all experiments, it is visible that in the third experiment, the system learned all categories faster than other experiments. In the case of experiment 9, the number of iterations required to learn 49 object categories was greater than other experiments. The underlying reason for different performances of these experiments is that categories were introduced to the system in a different order, which has a significant influence on the evolution of the learning performance. Figs. 9(top) and 10 show the evolution of the teaching protocol accuracy in experiments 1, 3, 5, 7 and 9. Fig. 9 (bottom) shows the protocol accuracy as a function of the number of learned categories. Fig. 11 (left) shows the global classification accuracy (i.e. the accuracy since the beginning of the experiment) as a function of the number of learned categories. In this figure we can see that the global classification accuracy decreases as more categories are learned. This is expected since the number of categories known by the system makes the classification task more difficult. To cope with this issue, memory management mechanisms [42], including salience and forgetting, can be considered. Finally, Fig. 11 (right) shows the number of learned categories as a function of the protocol iterations. This gives a measure of how fast the learning occurred in each of the experiments. 8.3 A real life use-case: clear table In this section, we present and discuss a “Clear Table” use-case to show all the functionalities of the system. In this use-case, the system works in a scenario where a table is in front of the robot, and a user interacts with the system. In this task, the robot must be able to detect and recognize different objects and transport all objects except decorative table-top objects (e.g., Vase) to predefined areas. The experimental setup is shown in Fig. 12 . It consists of a computer for human–robot interactions, a Kinect sensor for perceiving users and environment and a JACO robotic arm. The JACO arm has six degrees of freedom and a three fingers gripper. Since the JACO arm can carry up to 1.5 kg 8 8 , it is ideal for manipulating everyday objects. Moreover, infinite rotation around the wrist joints allows for flexible and effective interaction in a domestic environment. At the beginning of the session, there is a Vase object on top of the table. Later, a user places three more objects including Bottle, CoffeeJug and PlasticCup on the table. Note that, at the start of the experiment, the set of categories known to the system is empty and therefore, the system recognizes all table-top objects as Unknown (see Fig. 13 left). Afterwards, the user labels TrackID1 as a Vase. The system conceptualizes the Vase category and the category of TrackID1 is correctly recognized. Similarly, the user teaches all the other objects to the robot by providing the respective category labels. As depicted in Fig. 13 (center), the system could recognize all objects properly. Afterwards, the user instructs the robot to perform a clear_table task (i.e. puts the table back into a clear state). While there are active objects on the table, the robot retrieves the world model information from the Working Memory, including label and position of all active objects. The robot then selects the object closer to the arm’s base and clears it from the table (see Fig. 14 ). As it is shown in Fig. 13 (right), whenever the robot grasps an object, the shape of the object is partially changed and therefore a misclassification might happen. This real life use-case shows that the developed system is capable of detecting new objects, tracking and recognizing them, as well as manipulating objects in various positions. In other words, it shows the important role of robust object recognition and manipulation in performing tasks in human environments. Moreover, it shows how human–robot interaction is currently supported. A video of this session is available online at: 9 Conclusions In this paper, we have presented a cognitive architecture designed to support a tight coupling between perception and manipulation for assistive robots. In particular, an interactive open-ended learning approach for grounding 3D object categories has been presented, which enables robots to adapt to different environments and reason out how to behave in response to the request of a complex task such as clear_table. Unsupervised object exploration is used to construct a feature dictionary based on which objects are represented and object categories are learned. A Bayesian approach to category learning is proposed. We have assumed that the set of object categories to be learned is not known in advance and the training instances are extracted from actual experiences of a robot rather than being available at the beginning of the learning process. The proposed approach starts with the construction of a local 3D shape dictionary (visual words); each object is represented as a histogram of visual words and then the system creates or updates the probabilistic object category models based on Bayesian learning. For recognition, a probabilistic classification rule was used to assign a category label to the detected object. Results showed that the system can incrementally learn new object categories and perform manipulation tasks in reasonable time and appropriate manner. We have also tried to make the proposed architecture easy to integrate on other robotic systems. Our approach to object perception has been successfully tested on a JACO arm, showing the importance of having a tight coupling between perception and manipulation. In the continuation of this work, we are investigating the possibility of improving performance by topic modelling based on Latent Dirichlet Allocation (LDA) and also using other 3D shape descriptors (e.g. GOOD [43] and VFH [44]). Some results obtained with LDA have already been published [45]. Moreover, we would like to integrate compliance into the arm to provide comfortable interaction with the arm. Acknowledgments This work was funded by the EC 7th FP theme FP7-ICT-2011-7, grant agreement no. 287752 (project RACE - Robustness by Autonomous Competence Enhancement) and by National Funds through FCT project PEst-OE/EEI/UI0127/2014 and FCT scholarship SFRH/BD/94183/2013. We would like to thank the other RACE project partners for their efforts in the integration and the demonstrations, and especially to the Knowledge-Based Systems Group, Institute of Computer Science, University of Osnabruck for providing the ROS bag used for dictionary building in the exploration stage. References [1] M. Ciocarlie K. Hsiao E.G. Jones S. Chitta R.B. Rusu I.A. Şucan Towards reliable grasping and manipulation in household environments Experimental Robotics 2014 Springer 241 252 [2] Kim D.-J. R. Hazlett-Knudsen H. Culver-Godfrey G. Rucks T. Cunningham D. Portee J. Bricout Wang Z. A. Behal How autonomy impacts performance and satisfaction: results from a study with spinal cord injured subjects using an assistive robot IEEE Trans. Syst., Man Cybern., Part A: Syst. Hum. 42 1 2012 2 14 [3] A. Jain C.C. Kemp El-e: an assistive mobile manipulator that autonomously fetches objects from flat surfaces Auton. Robots 28 1 2010 45 64 [4] C. Leroux O. Lebec M.B. Ghezala Y. Mezouar L. Devillers C. Chastagnol J.-C. Martin V. Leynaert C. Fattal Armen: Assistive robotics to maintain elderly people in natural environment IRBM 34 2 2013 101 107 [5] M. Beetz U. Klank I. Kresse A. Maldonado L. Mosenlechner D. Pangercic T. Ruhr M. Tenorth Robotic roommates making pancakes 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2011 2011 IEEE 529 536 [6] N. Vahrenkamp M. Do T. Asfour R. Dillmann Integrated grasp and motion planning 2010 IEEE International Conference on Robotics and Automation (ICRA) 2010 IEEE 2883 2888 [7] S. Jeong M. Lee Adaptive object recognition model using incremental feature representation and hierarchical classification Neural Netw. 25 0 2012 130 140 [8] L. Smith M. Gasser The development of embodied cognition: six lessons from babies Artif. Life 11 1–2 2005 13 29 [9] A. Chauhan L. Seabra Lopes Using spoken words to guide open-ended category formation Cogn. Process. 12 4 2011 341 354 [10] He H. Chen S. Imorl: Incremental multiple-object recognition and localization IEEE Trans. Neural Netw. 19 10 2008 1727 1738 [11] S.H. Kasaei M. Oliveira Lim G.H. L. Seabra Lopes A.M. Tomé Interactive open-ended learning for 3D object recognition: an approach and experiments J. Intell. Robot. Syst. 80 3 2015 537 553 [12] M. Oliveira Lim G.H. L. Seabra Lopes H. Kasaei A. Tomé A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2014 IEEE 2216 2223 [13] M. Oliveira L. Seabra Lopes Lim G.H. H. Kasaei A. Sappa A. Tomé Concurrent learning of visual codebooks and object categories in open-ended domains Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2015 IEEE 2488 2495 [14] M. Oliveira L. Seabra Lopes Lim G.H. S.H. Kasaei A.M. Tomé A. Chauhan 3D object perception and perceptual learning in the RACE project Robot. Auton. Syst. 75, Part B 2016 614 626 [15] S. Srinivasa D.I. Ferguson M. Vande Weghe R. Diankov D. Berenson C. Helfrich H. Strasdat The robotic busboy: steps towards developing a mobile robotic home assistant International Conference on Intelligent Autonomous Systems 2008 2155 2162 [16] J. Hertzberg Zhang J. Zhang L. S. Rockel B. Neumann J. Lehmann K. Dubba A. Cohn A. Saffiotti F. Pecora M. Mansouri Š. Konĕcný M. Günther S. Stock L. Seabra Lopes M. Oliveira Lim G. H. Kasaei V. Mokhtari L. Hotz W. Bohlken The race project KI - Künstliche Intelligenz 28 4 2014 297 304 [17] S. Rockel et al. An ontology-based multi-level robot architecture for learning from experiences Designing Intelligent Robots: Reintegrating AI II, AAAI Spring Symposium, Stanford (USA) 2013 52 57 [18] V. Mokhtari L. Seabra Lopes A.J. Pinho Experience-based robot task learning and planning with goal inference Twenty-Sixth International Conference on Automated Planning and Scheduling 2016 509 517 [19] S.S. Srinivasa D. Ferguson C.J. Helfrich D. Berenson A. Collet R. Diankov G. Gallagher G. Hollinger J. Kuffner M.V. Weghe Herb: a home exploring robotic butler Auton. Robots 28 1 2010 5 20 [20] J. Bohg A. Morales T. Asfour D. Kragic Data-driven grasp synthesis-a survey IEEE Trans. Robot. 30 2 2014 289 309 [21] A. Sahbani S. El-Khoury P. Bidaud An overview of 3D object grasp synthesis algorithms Robot. Auton. Syst. 60 3 2012 326 336 [22] E. Chinellato A.P. Del Pobil The neuroscience of vision-based grasping: a functional review for computational modeling and bio-inspired robotics J. Integr. Neurosci. 8 02 2009 223 254 [23] S. Monaco A. Sedda C. Cavina-Pratesi J.C. Culham Neural correlates of object size and object location during grasping actions Eur. J. Neurosci. 41 4 2015 454 465 [24] U. Castiello The neuroscience of grasping Nat. Rev. Neurosci. 6 9 2005 726 736 [25] J.C. Culham C. Cavina-Pratesi A. Singhal The role of parietal cortex in visuomotor control: what have we learned from neuroimaging? Neuropsychologia 44 13 2006 2668 2684 [26] N. Shafii S.H. Kasaei L. Seabra Lopes Learning to grasp familiar objects using object view recognition and template matching 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2016 IEEE 2895 2900 [27] J. Stückler R. Steffens D. Holz S. Behnke Efficient 3D object perception and grasp planning for mobile manipulation in domestic environments Robot. Auton. Syst. 61 10 2013 1106 1115 [28] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library: three-dimensional object recognition and 6 DoF pose estimation IEEE Robot. Autom. Mag. 19 3 2012 80 91 [29] M. Martinez Torres A. Collet Romea S. Srinivasa Moped: a scalable and low latency object recognition and pose estimation system IEEE International Conference on Robotics and Automation, (ICRA 2010) 2010 2043 2049 [30] M. Islam F. Jahan J.-H. Min J. hwan Baek Object classification based on visual and extended features for video surveillance application 8th Asian Control Conference (ASCC 2011) 2011 1398 1401 [31] Yeh T. Lee J.J. T. Darrell Fast concurrent object localization and recognition IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 2009 IEEE 280 287 [32] Yeh T. T. Darrell Dynamic visual category learning IEEE Conference on Computer Vision and Pattern Recognition, (CVPR 2008) 2008 1 8 [33] S. Kirstein H. Wersing H.-M. Gross E. Körner A life-long learning vector quantization approach for interactive learning of multiple categories Neural Netw. 28 2012 90 105 [34] A. Collet Xiong B. C. Gurau M. Hebert S.S. Srinivasa Herbdisc: towards lifelong robotic object discovery Int. J. Robot. Res. 34 1 2015 3 25 [35] M. Quigley K. Conley B. Gerkey J. Faust T. Foote J. Leibs R. Wheeler A.Y. Ng ROS: an open-source robot operating system ICRA Workshop on Open Source Software vol. 3 2009 5 11 [36] S. Hamidreza Kasaei M. Oliveira Lim G.H. L. Seabra Lopes A. Tomé An interactive open-ended learning approach for 3D object recognition 2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC) 2014 47 52 [37] Lim G.H. M. Oliveira V. Mokhtari S. Hamidreza Kasaei A. Chauhan L. Seabra Lopes A. Tomé Interactive teaching and experience extraction for learning about objects and robot activities The 23rd IEEE International Symposium on Robot and Human Interactive Communication, 2014 RO-MAN 2014 153 160 [38] M.A. Fischler R.C. Bolles Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [39] J.A. Hartigan Wong M.A. Algorithm AS 136: A k-means clustering algorithm J. Royal Stat. Soc. Series C (Appl. Stat.) 28 1 1979 100 108 [40] L. Seabra Lopes A. Chauhan How many words can my robot learn?: an approach and experiments with one-class learning Interact. Stud. 8 1 2007 53 81 [41] Lai K. Bo L. X. Ren D. Fox A large-scale hierarchical multi-view rgb-d object dataset 2011 IEEE International Conference on Robotics and Automation (ICRA) 2011 1817 1824 [42] A.D. Baddeley Human Memory: Theory and Practice 1997 Psychology Press [43] S.H. Kasaei A.M. Tomé L. Seabra Lopes M. Oliveira Good: a global orthographic object descriptor for 3d object recognition and manipulation Pattern Recognit. Lett. 83 2016 312 320 [44] R.B. Rusu G. Bradski R. Thibaux Hsu J. Fast 3d recognition and pose using the viewpoint feature histogram 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2010 IEEE 2155 2162 [45] S.H. Kasaei A.M. Tomé L. Seabra Lopes Hierarchical object representation for open-ended object category learning and recognition Advances in Neural Information Processing Systems (NIPS) 2016 1948 1956 Hamidreza Kasaei is a Ph.D. student at the University of Porto (MAP-i), Portugal. Currently, he is a researcher at the IEETA, University of Aveiro, Portugal, where he works on 3D object category learning and recognition in open-ended domains. His main research interests focus on the intersection of robotics, machine learning, and machine vision. He is interested in developing algorithms for an adaptive perception system based on interactive environment exploration and open-ended learning, which enables robots to learn from past experiences and interact with human users. He investigates active perception, where robots use their mobility and manipulation capabilities not only to gain the most useful perceptual information to model the world, also to predict the next best view for improving object detection and manipulation performances. Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a post-doctoral researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal. His research interests lie in the area of intelligence and learning for robots, including perception and semantics. Luís Seabra Lopes is Associate Professor of Informatics in the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, Portugal. He received a PhD in Robotics and Integrated Manufacturing from the New University of Lisbon, Portugal, in 1998. Lus Seabra Lopes has long standing interests in robot learning, cognitive robotic architectures, and human-robot interaction. Ana Maria Tomé is an Associate Professor of electrical engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as machine learning applications. "
    },
    {
        "doc_title": "Special Issue on Autonomous Driving and Driver Assistance Systems",
        "doc_scopus_id": "85015102047",
        "doc_doi": "10.1016/j.robot.2017.01.011",
        "doc_eid": "2-s2.0-85015102047",
        "doc_date": "2017-05-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2017-01-27 2017-01-27 2017-02-09 2017-02-09 2017-12-23T23:01:50 S0921-8890(17)30056-8 S0921889017300568 10.1016/j.robot.2017.01.011 S300 S300.3 FULL-TEXT 2017-12-23T23:43:56.219361Z 0 0 20170501 20170531 2017 2017-01-27T16:56:31.823447Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav body acknowledge affil articletitle auth authfirstini authfull authlast pubtype 0921-8890 09218890 true 91 91 C Volume 91 18 208 209 208 209 201705 May 2017 2017-05-01 2017-05-31 2017 simple-article edi © 2017 Elsevier B.V. All rights reserved. SPECIALISSUEAUTONOMOUSDRIVINGDRIVERASSISTANCESYSTEMS SANTOS V Acknowledgments SANTOSX2017X208 SANTOSX2017X208X209 SANTOSX2017X208XV SANTOSX2017X208X209XV 2019-02-09T00:00:00.000Z UnderEmbargo © 2017 Elsevier B.V. All rights reserved. item S0921-8890(17)30056-8 S0921889017300568 10.1016/j.robot.2017.01.011 271599 2017-12-23T23:43:56.219361Z 2017-05-01 2017-05-31 true 294479 MAIN 2 67248 849 656 IMAGE-WEB-PDF 1 ROBOT 2786 S0921-8890(17)30056-8 10.1016/j.robot.2017.01.011 Elsevier B.V. Editorial Special Issue on Autonomous Driving and Driver Assistance Systems Vitor Santos IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal Angel D. Sappa ⁎ Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain Computer Vision Center, Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador ⁎ Corresponding editor. Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador Miguel Oliveira INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal Research on Autonomous Driving and Driver Assistance Systems has increased and became an active research field during the last decades. This increase in research is motivated by the need to reduce road accidents, which represent the 6th cause of dead in high-income countries according to the World Health Organization, and 11th worldwide. Vehicles are becoming more and more intelligent, promising fully autonomous vehicles for the near future. Such a big challenge opens a wide range of research topics that need to be tackled and efficiently solved by the community. This Special Issue on Autonomous Driving and Advanced Driver Assistance Systems (ADDAS) published by the Robotics and Autonomous Systems Journal contains extended versions of a set of selected papers from a special session on the same topic, which took place in the framework of the Second Iberian Robotics Conference, November 19th–21st 2015, in Lisbon, Portugal. The papers included in this special issue address several subjects of relevance to the ADDAS community, from scene representations using multi-modal data to infrared imaging or multiple sensor calibration algorithms. From the papers accepted for publication, two or three main interest lines were observed, which could be interpreted as current dominant trends for this community. They cover mainly 3D perception, multimodal sensors and sensor fusion. The importance of 3D perception is now undeniable in the ADDAS context, be it using lidar or stereo based sensors. As such, several papers in this special issue address the problem in several fronts, ranging from the automatic calibration in lidar and cameras multisensorial setups (Pereira et al.), up to efficient modelling and detection of features for navigation, namely on the ground, as shown both in 3D lidar approaches (Asvadi et al.) and stereo approaches (de la Escalera et al.). The ultimate challenge in processing 3D data is naturally environment and scene reconstruction, and the works by (Oliveira et al.) include relevant contributions in that field as well. Multimodality also appears as a relevant issue, and it is no longer uncommon to see on the same setups crossed combinations of lidar, vision, infrared and even radar; this richness of exteroceptive modality naturally implies the need for sensorial calibration and the subsequent data fusion procedures. The selected papers include contributions on the following topics: Evaluations on the usage of images resulting from the fusion of cross-spectral inputs, applied to the monocular visual odometry estimation, have been reported (Sappa et al.); such a fused image based approach has shown advantages when compared with the state of the art. The classical traffic signs detection and classification have been tackled with a highly optimized and accurate convolutional neural network architecture in Aghdam et al.; the stability of proposed approach has been evaluated on noisy images. In de la Escalera et al. a stereo odometry algorithm that detects and tracks features on the surface of the ground is proposed. In order to ensure a uniform distribution of feature keypoints, an inverse perspective image is used. Results show that this visual odometry is accurately estimated. In order to overcome the fact that particle filters are computationally demanding, a hardware implementation on FPGA (field programmable gate arrays) has been proposed and validated to estimate the localization and mapping (SLAM) of a robot (Sileshi et al.). In Ćesić et al., a multisensor setup consisting of a radar and a stereo camera mounted on top of a vehicle is used. Authors model the sensors uncertainty on the product of two special Euclidean groups, and show that the model is more accurate than classical approaches. Autonomous vehicles and ADAS systems often resort to multi-modal setups in order to improve the efficiency of the systems. This, however, raise the problem of geometric registration between the sensors. The work described in Pereira et al. presents a solution for the automatic calibration of multiple sensors of different natures, e.g. LIDARs and cameras, using a simple spherical calibration object. In Asvadi et al. a framework for the detection of static and moving obstacles is proposed. The algorithm is based on a piecewise planar surface fitting and a 3D voxel representation. Results have been tested on diversified driving environments. The online reconstruction of scenarios observed by autonomous vehicles is not trivial. The work presented in Oliveira et al. (a) proposes an algorithm designed to create a geometric representation of such a scenario. Furthermore, this work proposes a way to update these representations online. Results show that the approach is capable of producing accurate descriptions of the scene. Finally, the work presented in Oliveira et al. (b) focuses on the photometric reconstruction of the geometric scenario reconstructions produced in Oliveira et al. (a). The algorithm uses a constrained Delaunay triangulation to produce a mesh that is updated using a specially devised sequence of operations that ensure the quality of the final texture. Results show that the approach is capable of producing fine quality textures. We hope that these works may contribute to the development of novel and more efficient ADDAS algorithms and systems. Acknowledgments This Special Issue has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R; and by the “Fundação para a Ciência e Tecnologia” (Portuguese Foundation for Science and Technology) under grant agreement SFRH/BPD/109651/2015 and National Funds within project UID/EEA/50014/2013. This work was also financed by the ERDF - European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020 Programme within project “POCI-01-0145-FEDER-006961”. "
    },
    {
        "doc_title": "A mobile robot based sensing approach for assessing spatial inconsistencies of a logistic system",
        "doc_scopus_id": "85015636264",
        "doc_doi": "10.1016/j.jmsy.2017.02.016",
        "doc_eid": "2-s2.0-85015636264",
        "doc_date": "2017-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Continuous assessment",
            "Enterprise information system",
            "Geometric locations",
            "Logistic systems",
            "Potential benefits",
            "Real time",
            "Spatial representations",
            "World model"
        ],
        "doc_abstract": "© 2017 The Society of Manufacturing EngineersThis paper demonstrates the potential benefits of the integration of robot based sensing and Enterprise Information Systems extended with information about the geometric location and volumetric information of the parts contained in logistic supermarkets. The comparison of this extended world model with hierarchical spatial representations produced by a fleet of robots traversing the logistic supermarket corridors enables the continuous assessment of inconsistencies between reality, i.e., the spatial representations collected from online 3D data, and the modelled information, i.e., the world model. Results show that it is possible to detect inconsistencies reliably and in real time. The proposed approach contributes to the development of more robust and effective Enterprise Information Systems.",
        "available": true,
        "clean_text": "serial JL 277340 291210 291883 291885 31 Journal of Manufacturing Systems JOURNALMANUFACTURINGSYSTEMS 2017-03-21 2017-03-21 2017-03-21 2017-03-21 2019-10-29T04:28:50 S0278-6125(17)30029-8 S0278612517300298 10.1016/j.jmsy.2017.02.016 S300 S300.1 FULL-TEXT 2019-10-30T16:18:18.084821Z 0 0 20170401 20170430 2017 2017-03-21T06:38:53.245437Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst primabst pubtype ref 0278-6125 02786125 true 43 43 P1 Volume 43, Part 1 11 129 138 129 138 201704 April 2017 2017-04-01 2017-04-30 2017 article fla © 2017 The Society of Manufacturing Engineers. Published by Elsevier Ltd. All rights reserved. AMOBILEROBOTBASEDSENSINGAPPROACHFORASSESSINGSPATIALINCONSISTENCIESALOGISTICSYSTEM ARRAIS R 1 Introduction 2 Related work 3 High level system architecture 4 Logistic world model 5 Spatial change detection 5.1 Hierarchical spatial representations 5.2 Octree based mapping 5.3 Spatial change detection using an unsupervised model 5.4 Spatial change detection using a supervised model 6 Results 6.1 Small-scale scenario using a robotic manipulator 6.2 Large-scale scenario using a mobile robot 7 Conclusions Acknowledgements References LEE 2008 363 369 E 200811THIEEEINTERNATIONALSYMPOSIUMOBJECTORIENTEDREALTIMEDISTRIBUTEDCOMPUTINGISORC CYBERPHYSICALSYSTEMSDESIGNCHALLENGES LEITAO 2013 2360 2372 P XU 2011 630 640 L HE 2014 35 42 W HERREROPEREZ 2010 166 180 D METZGER 2011 570 581 M ESPINA 2011 183 209 M INNOVATIONSINDEFENCESUPPORTSYSTEMS3 MULTIROBOTTEAMSFORENVIRONMENTALMONITORING VIEIRA 2013 1510 1521 M SENTHILKUMAR 2012 123 132 K MONOSTORI 2014 9 13 L WANG 2015 517 527 L KRUEGER 2016 1114 1127 V COLOMBO 2014 A INDUSTRIALCLOUDBASEDCYBERPHYSICALSYSTEMSIMCAESOPAPPROACH RIEDL 2014 123 133 M IEC 2007 ENTERPRISECONTROLSYSTEMINTEGRATIONPART3ACTIVITYMODELSMANUFACTURINGOPERATIONSMANAGEMENT GIRBEA 2014 185 196 A VEIGA 2009 746 755 G HUHNS 2005 75 81 M BIANCO 2011 P ARCHITECTINGSERVICEORIENTEDSYSTEMS TANEJA 2011 2336 2343 A 2011INTERNATIONALCONFERENCECOMPUTERVISION IMAGEBASEDDETECTIONGEOMETRICCHANGESINURBANENVIRONMENTS CHEN 2015 4126 4130 B 2015IEEEINTERNATIONALCONFERENCEIMAGEPROCESSINGICIP BUILDINGCHANGEDETECTIONBASED3DRECONSTRUCTION BRUNNER 2010 3210 3213 D 2010IEEEINTERNATIONALGEOSCIENCEREMOTESENSINGSYMPOSIUM CHANGEDETECTIONFOREARTHQUAKEDAMAGEASSESSMENTINBUILTUPAREASUSINGHIGHRESOLUTIONOPTICALSARIMAGERY VIEIRA 2014 766 774 A MARINELLI 2016 3595 3598 D 2016IEEEINTERNATIONALGEOSCIENCEREMOTESENSINGSYMPOSIUMIGARSS FUSIONHIGHHIGHDENSITYLIDARDATAFOR3DFORESTCHANGEDETECTION HORNUNG 2013 189 206 A ELSEBERG 2011 1 7 J 2011XXIIIINTERNATIONALSYMPOSIUMINFORMATIONCOMMUNICATIONAUTOMATIONTECHNOLOGIESICAT EFFICIENTPROCESSINGLARGE3DPOINTCLOUDS RUSU 2011 1 4 R 2011IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA 3DPOINTCLOUDLIBRARYPCL SIM 2005 956 959 J IEEEINTERNATIONALSYMPOSIUMCIRCUITSSYSTEMS2005ISCAS2005 CONSTRUCTIONREGULAR3DPOINTCLOUDSUSINGOCTREEPARTITIONINGRESAMPLING NIU 2013 2147 2154 N CROSBY 2014 M WORKSHOPUKPLANNINGSCHEDULINGSPECIALINTERESTGROUPPLANSIG2014 CENTRALISEDHIGHLEVELPLANNINGFORAROBOTFLEET CROSBY 2014 16 24 M PROCEEDINGSICAPS2014WORKSHOPDISTRIBUTEDMULTIAGENTPLANNINGDMAP TEMPORALMULTIAGENTPLANNINGCONCURRENTACTIONCONSTRAINTS PEDERSEN 2016 282 291 M ROVIDA 2015 3288 3295 F 2015IEEEINTERNATIONALCONFERENCEINDUSTRIALTECHNOLOGYICIT DESIGNDEVELOPMENTASOFTWAREARCHITECTUREFORAUTONOMOUSMOBILEMANIPULATORSININDUSTRIALENVIRONMENTS PEDERSEN 2014 4523 4530 M 2014IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS2014 INTUITIVESKILLLEVELPROGRAMMINGINDUSTRIALHANDLINGTASKSAMOBILEMANIPULATOR KOHLBRECHER 2011 155 160 S 2011IEEEINTERNATIONALSYMPOSIUMSAFETYSECURITYRESCUEROBOTICS AFLEXIBLESCALABLESLAMSYSTEMFULL3DMOTIONESTIMATION ARRAISX2017X129 ARRAISX2017X129X138 ARRAISX2017X129XR ARRAISX2017X129X138XR 2019-03-21T00:00:00.000Z © 2017 The Society of Manufacturing Engineers. Published by Elsevier Ltd. All rights reserved. 2019-03-06T19:05:54.670Z North Portugal Regional Operational Programme Portuguese Foundation for Science and Technology SFRH/BPD/109651/2015 European Regional Development Fund European Regional Development Fund ERDF ? COMPETE 2020 Programme within NORTE POCI-01-0145-FEDER-006961 SAICTPAC/0034/2015- POCI-01-0145-FEDER-016418 EU FP7 FP7-ICT-2013-10 610917 FP7 Ideas: European Research Council PORTUGAL Fundação Portugal Telecom This work has been supported by the ?Funda??o para a Ci?ncia e Tecnologia? (Portuguese Foundation for Science and Technology) under grant agreements SFRH/BPD/109651/2015. This work was also financed by the ERDF ? European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation ? COMPETE 2020 Programme within project POCI-01-0145-FEDER-006961, SAICTPAC/0034/2015- POCI-01-0145-FEDER-016418 and project ?NORTE -01 -0145 -FEDER- 000020?, financed by the North Portugal Regional Operational Programme (NORTE 2020, under the PORTUGAL 2020 Partnership Agreement). This work was partially funded by the EU FP7 project STAMINA, FP7-ICT-2013-10, Grant number 610917. item S0278-6125(17)30029-8 S0278612517300298 10.1016/j.jmsy.2017.02.016 277340 2019-10-30T16:18:18.084821Z 2017-04-01 2017-04-30 true 2768820 MAIN 10 58561 849 656 IMAGE-WEB-PDF 1 gr1 19501 129 219 gr2 6773 164 160 gr3 17413 164 204 gr4 11871 163 122 gr5 17813 145 219 gr6 18651 150 219 gr7 18801 164 143 gr8 4087 108 219 gr9 14307 164 167 gr1 47425 289 491 gr2 37542 395 386 gr3 60925 425 528 gr4 46085 447 334 gr5 22322 220 331 gr6 23262 234 340 gr7 41093 393 343 gr8 38683 296 602 gr9 55477 494 503 gr1 490247 1278 2174 gr2 304462 1750 1708 gr3 641307 1882 2340 gr4 684923 1979 1479 gr5 321543 972 1465 gr6 329772 1035 1507 gr7 408636 1743 1521 gr8 305025 1313 2667 gr9 560818 2188 2229 si1 211 18 43 si2 550 23 154 si3 187 17 20 JMSY 541 S0278-6125(17)30029-8 10.1016/j.jmsy.2017.02.016 The Society of Manufacturing Engineers Fig. 1 Mobile manipulator to perform kitting operations: (a) virtual model, (b) real robot. Fig. 2 STAMINA high level system architecture and major information flow. Fig. 3 Definition of object instances in a 2D CAD image (a), in a 2D SLAM based image (b) and in a 3D point cloud based image (c). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of the article.) Fig. 4 Octrees created using octomap: (a) point cloud of the model scene, (b) octree of the model scene, (c) point cloud of the target scene, (d) octree of the target scene. Fig. 5 Spatial change detection comparing octrees of model and target scenes: (a) detected spatial inconsistencies of type missing (green) and exceeding (red), (b) spatial inconsistencies filtered and clustered. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of the article.) Fig. 6 Spatial change detection comparing an octree with the logistic volumes: (a) logistic volumes marked as cube wireframes (red, exceeding, green missing), (b) detected inconsistencies marked as filled volumes (red, exceeding, green missing). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of the article.) Fig. 7 Experimental setups for spatial change detection using supervised models: (a) robotic manipulator with 3D camera, (b) and (c) mobile robot with 3D camera traversing a 65m2 area. Fig. 8 ROC curves for the spatial inconsistencies detection using supervised models: (a) exceeding type inconsistencies classifier, (b) missing type inconsistencies classifier. Fig. 9 Spatial change detection comparing an octree of a scene with offline defined logistic regions: (a) logistic regions, (b) octree, (c) detected spatial inconsistencies of type missing (green) and exceeding (red). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of the article.) Technical Paper A mobile robot based sensing approach for assessing spatial inconsistencies of a logistic system Rafael Arrais ⁎ Miguel Oliveira César Toscano Germano Veiga INESC TEC – INESC Technology and Science, R. Dr. Roberto Frias, 465, 4200 Porto, Portugal INESC TEC – INESC Technology and Science R. Dr. Roberto Frias, 465 Porto 4200 Portugal ⁎ Corresponding author. This paper demonstrates the potential benefits of the integration of robot based sensing and Enterprise Information Systems extended with information about the geometric location and volumetric information of the parts contained in logistic supermarkets. The comparison of this extended world model with hierarchical spatial representations produced by a fleet of robots traversing the logistic supermarket corridors enables the continuous assessment of inconsistencies between reality, i.e., the spatial representations collected from online 3D data, and the modelled information, i.e., the world model. Results show that it is possible to detect inconsistencies reliably and in real time. The proposed approach contributes to the development of more robust and effective Enterprise Information Systems. Keywords Advanced logistics Hierarchical spatial representation and detection Service oriented architectures World model 1 Introduction With robots becoming increasingly flexible and capable of sensing and interacting with complex environments, great challenges and opportunities appear in terms of its vertical integration as flexible and adaptive Cyber-Physical Systems (CPS) in large industrial setups [1,2]. In fact, the development of Enterprise Information Systems (EIS) shows potential for more complex interaction with CPS deployed in industrial applications [3–5]. The objectives of this paper are twofold. First, it presents the development of a service oriented logistic system, based on a world model concept that enhances traditional logistic systems with dynamic geometric and semantic data. Second, the paper demonstrates how the world model can be integrated with mobile robot based sensing to assess inconsistencies in a logistic supermarket [6]. The motivation for this work comes in the context of the European project FP7 – STAMINA, 1 1 where a mobile manipulator (see Fig. 1 ) is being developed to perform picking operations in logistic supermarkets. The STAMINA robot is a mobile manipulator capable of navigating in the logistic supermarket composing a kit, i.e., picking parts available in shelves and boxes placed on the ground. When close to a box the robot relies on local sensors to achieve the required accuracy to perform the picking of a part, but the search for a part has to be limited to a small space for the robot to perform efficiently. To support the operations of the robot there was the need to develop a world model based logistic system, which identifies all the physical objects available in the logistic supermarket. All the parts and containers (shelves and boxes) to be handled are located in the logistic supermarket and identified in terms of their geometrical dimensions and their internal structure (e.g. a shelf has several levels containing small boxes). This logistic world model is encapsulated in a service oriented logistic planner, which acts as the central node for the overall logistic system providing adequate interfaces to different systems, such as the robots, the software component managing the robot fleet, the Manufacturing Execution System (MES)/Enterprise Resource Planning (ERP), and the local operator responsible for the logistic supermarket. This paper demonstrates the potential and interoperability of the world model based logistic system, by developing techniques that allow a fleet of robots to continuously check the consistency of the logistic world model while traversing the logistic supermarket corridors. Those techniques are based on the use of octree-based algorithms to either perform octree with octree comparisons or to check if the relevant volumes of the supermarket space are consistent with the information present in the logistic world model. Octrees are hierarchical structures, which can be dynamically constructed and updated using a probabilistic approach to combine multiple measurements over time, optimizing spatial search and discretizing space, and thus enhancing the proposed approach outcome. Since this evaluation of consistency is carried out using volumetric information exclusively, it is possible that the system is tricked if an object with a similar volume is presented to the system. However, we believe that a simple volumetric evaluation is sufficient to detect the majority of the errors in logistic systems. Furthermore, this can be used as a preprocessing step to dedicated object recognition systems which will, in turn, be able to complement the volumetric consistency detection in cases where it fails. The remainder of the paper is organized as follows: Section 2 describes related work, Section 3 details the software architecture of the proposed system, Section 4 presents the world model concept and Section 5 describes the spatial change detection mechanisms used to find inconsistencies. Finally, Section 6 presents the results, and Section 7 draws the conclusions. 2 Related work The use of robots as mobile nodes in sensing systems has been explored in several contexts, namely for environmental monitoring [7], mobile mesh networks [8] or terrain exploration [9]. In the industrial context, however, the research on robot sensing is more focused on the robot needs for localization and interacting with the environment. In this context it is particularly interesting the integration of the robot as a CPS that is deeply interconnected with others sensors and the network infrastructures of modern industrial plants. Cyber-Physical Systems may be seen as systems of collaborating computational entities which are in intensive connection with the surrounding physical world and its on-going processes, providing and using, at the same time, data-accessing and data-processing services available on the internet [10]. More simply, they are integrations of networked and/or distributed feedback systems between computation and physical processes, through which physical processes affect computations and vice versa [11,12]. They exhibit adaptive, predictive and intelligent behaviour and strong relations are established with other concepts and technologies like systems of systems, IoT, big data and analytics, and cloud technology [10–12]. The feedback nature of a CPS naturally requires the existing physical objects and processes to be translated into cyber space as virtual models so that they can be monitored and controlled as well as improving the virtual model through feedback data provided by the physical space. This paper addresses logistic supermarkets comprised of different types of automotive components (parts) organized in different types of containers (large boxes on the ground and small boxes contained within shelves of different sizes). This physical environment is in constant transformation as parts are picked from a box, placed inside a kit and as boxes get empty they are replaced by new ones. This dynamic is increased as automotive parts evolve constantly in the life cycle of a vehicle, and parts are added, suppressed or replaced each period of time (from one to several months), causing possible changes in the way the supermarket is organized in order to optimize the access to the parts. The introduction of mobile manipulators in the supermarket for picking parts and placing them in a kitting box requires the physical objects located in the supermarket to be represented in a virtual model, describing all their structural geometrical aspects, manipulation behaviour and location in space. If cognitive capabilities are incorporated in the mobile manipulator so that it is able to sense its environment and, based on the sensory input reason, plan and act accordingly, high levels of flexibility and adaptation are achieved. However, the dynamics of the supermarket model (i.e., the logistic world model) must consider inputs from the technician(s) responsible for maintaining the supermarket and automatic consistency checks to ensure that the model is reliable and accurate. This is very different from modern manufacturing robots that work mostly blindly and are based on a large set of specific assumptions, e.g., part locations and timing and are unable to deal with uncertainties (e.g. physical objects are not located where they are expected, physical objects are detected in places expected to be empty). According to many authors [10,13,14], the development of Cyber-Physical Production System (CPPS) breaks the traditional automation pyramid, well represented by a five-level hierarchical model in ISA-95 2 2 /IEC 62264 standard [15] into a distributed set of co-operating and collaborating entities, presenting advanced behaviour such as intelligence, proactivity, fault-tolerance and reusability [13,16,17]. As these entities need to interoperate both at cyber and physical levels, the service-oriented approach [18,19] appears as a promising solution: the functionality implemented by each entity is made accessible through a defined service interfaces and messaging protocols; new functionality may be discovered in the service ecosystem thus enabling the construction through composition of new cross-layer services residing at the enterprise system, at the network itself and/or at device level [13]. As mentioned, the usage of mobile manipulators as sensing systems for assessing the coherency of virtual representations of an industrial environment presents an interesting opportunity. In this paper, we focus on the development of a mechanism that allows a fleet of mobile manipulators to perform spatial change detection in a logistic supermarket, in parallel to its normal operation. Spatial change detection can be defined as the search for incoherences between two spatial representations. This is useful for assessing if, for example, an object was removed from a previously known environment, or, similarly, if an object was introduced in the environment. Some approaches for spatial change detection have been proposed for many applications, such as urban environments [20], city monitoring [21], disaster assessment [22], surveillance [23] and forestry [24]. In the industrial context, we propose to use this concept to the assessment of the spatial consistency of a virtual representation of a logistic supermarket. Given the size of an industrial logistic supermarket, and since memory consumption is often the main bottleneck of 3D mapping systems, it is essential that mapping is as efficient as possible [25,26]. Therefore, spatial representations of the environment are central components of most autonomous systems. Processing of raw point cloud datasets [27] is not memory efficient, as memory consumption increases with the amount of measurements, without an upper bound. The lack of connectivity information between points to define local neighbours [28] and the inability to model free and unknown space makes point cloud processing unsuitable for large-scale 3D mapping systems. To meet requirements associated with data querying, spatial decomposition techniques such as k-d trees and octrees are employed as access structures. While the first have a data-driven approach, enabling a faster data access by requiring less memory for the tree structure, the latter is based on a space-driven partitioning approach, supporting a faster data update [25]. On the purpose of real-time mapping of dynamic environments, the usage of octree data structures presents a significant advantage, since they can support dynamic point insertion and deletion, and thus, are easier to update. 3 High level system architecture This section aims at providing a high level overview on STAMINA architecture, as to connect the proposed approach component with the overall logistic system. The STAMINA logistic system is comprised by several logistic components, as depicted in Fig. 2 . Each robot (Fig. 2 shows only one robot) is managed by two elements: a task manager, responsible for the assignment and execution of tasks in the robot in the form of a sequence of skills, and a skill manager which controls the execution of skills by the robot. At a higher level, the Logistic Planner is responsible for building the model of the logistic supermarket (i.e., the logistic world model) and to integrate the logistic activities with the organization's EIS [29]. The creation and assignment of a mission to individual robots are achieved by the Mission Planner. The Logistic Planner provides the integration mechanism to the organizations’ EIS, which include its MES and a collection of complementary systems that manage data about the logistic supermarket. In this context, conversion logic to higher-level planning functions defined in MES-type systems is provided. The semantics and format of data originating in the EIS are converted to a format understandable by the Logistic Planner and, inversely, actions performed by these components are identified and converted back to a form understandable by the EIS, thus hiding these integration-related issues from the remaining components in the system. Moreover, the Logistic Planner mediates the planning and execution functions performed by the Mission Planner and by the robot fleet, providing them with data about the working environment, i.e. the world model. Inconsistency checks are performed by the Spatial Change Detection component (Section 5). The mission planner is responsible for generating mission assignments for individual robots. Missions are created based on the status of the robots and the set of available kitting orders, and are defined as a goal assignment to a specific robot for completing a single trip around the kitting zone. This is achieved by using general-purpose automated planning techniques, by invoking a novel multi-agent planning algorithm [30,31]. As a result, the mission specifies the optimized sequence of skills that satisfy the goal and that will be subsequently executed by the selected robot. Mission assignments are forwarded by the Logistic Planner to the Task Manager running in the selected robot, which calls for the execution of skills (via the skill manager), and which keeps track of the robot's progress on its mission. Details of the Task Manager and the Skill based robot operation can be found here [32–34]. Given the above partitioning of the system in components, the information flow between them was designed (see Fig. 2) and used to define interfaces and, consequently, well-defined services implementing the correspondent functionality. A service-oriented approach [19] was followed. Services are dynamically discovered and designed as stateless so as to maximize the loose coupling of the logistic components. Two types of interaction patterns were selected: the request–response interaction, based on asynchronous messages where the service business logic requires the provider to send back one or multiple response messages to the consumer's request message; and the publish-subscribe, where service consumers and providers act as publishers and subscribers of a given “topic”. Messages published on a given topic are forwarded asynchronously to all subscribers of that topic. The interaction between the Logistic Planner and the Mission Planner is an example of the former interaction, where the Logistic Planner requests from the Mission Planner the creation of a mission to fulfil a given kitting order. On the other hand, the dynamic consistency checking of the world model, is more appropriately supported by the publish-subscribe pattern. 4 Logistic world model A crucial function of the logistic planner is to act as the major information provider to the planning and execution elements in the logistic system: the Mission Planner and the Task/Skill Manager inside each robot. The type of information communicated among these components mainly concerns the physical objects located in the logistic supermarket, namely shelves, small and large boxes, conveyors, kits, parts, and packaging elements. For each object, its characteristics (e.g., internal organization, geometry, and spatial location) are modelled in the logistic planner so as to provide this information to the planning and execution components. As such, the logistic world model specifies the position, orientation, and geometry (in the form of a logistic volume) of all object instances in the supermarket. This is represented by a hierarchical graph, where a single node may be linked to many children nodes thus representing a containment relationship between a physical object (modelled by the parent node) and other physical objects. The node on the top of the hierarchy is the root node and models the entire space of the logistic supermarket. This region is further decomposed in specific regions, dedicated to contain the different categories of objects (e.g. shelves, large boxes and conveyors) or to achieve specific purposes (e.g. navigation of the robot). Below these specific regions a set of nodes are used to model kits (boxes with compartments), large boxes (containers located in the ground and organized in layers with compartments on each layer), shelves (also structured in layers containing small boxes), and conveyors. Parts are not normally represented as nodes in order to avoid excessive information in the graph (a large box containing four layers may have 80 parts) and also because there is no need to identify each part instance. Common characteristics of the part are modelled in the logistic world model so as to support the robot operations (e.g. the 3D model of the part). The initial construction of the logistic world model is accomplished in two phases: first, the representation of these object categories is gathered from the external EIS system or through the logistic planner's user interface; second, actual instances of the objects are created and positioned in the 3D space occupied by the logistic supermarket. This is achieved by the technician responsible for the logistic supermarket by using a selection of tools which allows the world model to be created or edited (Logistic World Model Editor in Fig. 2). Three different approaches and corresponding interfaces are proposed for the creation of the logistic world model (see Fig. 3 ): (1) A 2D bitmap image exported from a CAD application is used by the technician to visually specify the location and orientation of each object in the logistic supermarket. The red rectangle represents the object being implanted in the supermarket while the remaining rectangles represent objects already implanted. (2) A 2D bitmap image created by the robot through its Simultaneous Localization and Mapping (SLAM) functionality is used by the technician to visually specify the location and orientation of objects in the logistic supermarket. A video illustrating the process can be seen on footnote 3 3 . (3) A 3D point cloud created by the robot while scanning the logistic supermarket is used by the technician to visually specify the location and orientation of an object in the logistic supermarket. There is a video 4 4 that illustrates the process of creating the logistic world model using the 3D approach. 5 Spatial change detection Spatial change detection is defined as the search for inconsistencies between two scenarios. When comparing two scenarios, e.g., a model and a target scenario, two types of inconsistencies may occur: exceeding inconsistency, when the target scenario contains an object which was not present in the model scenario; and missing inconsistency, when the target scenario does not contain an object which was present in the model scenario. In several industrial applications, such as the logistic supermarket of an automotive manufacturer, the task of detecting the absence of a given object or identifying the introduction of an unknown object is an essential contribution for the consistency of the logistic system. As such, this paper is focused on using volumetric information to identify spatial inconsistencies. We propose two modes of operation for the spatial change detection module. These modes differ in the way the world model is created. In an industrial scenario, the model is defined by the logistic information employed to manage the corresponding physical space (i.e. the logistic supermarket). Several interfaces have been developed to update and edit the logistic world model (see Section 4 for details). All these configurations are supervised, in the sense that the logistic world model is defined by human intervention, i.e. the logistic technician creates a virtual representation of the physical logistic supermarket, which is then used as an input to the spatial detection mechanism. A set of logistic volumes, defined within the logistic world model, will be used as an input to the supervised spatial change detection mechanism presented in Section 5.4. In other situations, it could happen that there is no information concerning the contents of the logistic supermarket. It could also be difficult to define where the storage volumes are spatially located. In these cases, we propose to conduct an unsupervised acquisition of the model scenario, where the robot relies on an octree representation of the model scenario, created offline. This mechanism is described in Section 5.3. 5.1 Hierarchical spatial representations An octree is a tree-based hierarchical data structure for managing sparse 3D data based on the principle of recursive decomposition of space [25]. The root node of an octree represents a cubic bounding box which encapsulates all points in space. The construction of an octree is obtained by recursively subdividing the cubical space represented by the root node into eight congruent disjoint cubes, called octants. Each node on the tree spans a particular volume, expressed as an axis-aligned bounding box. The recursive subdivision process finishes when blocks of uniform value are obtained, or when a defined level of decomposition is reached. If all children of a node have the same state, i.e., occupied, free or unknown, they can be grouped, allowing a compact representation of the tree and consequently reducing memory consumption. In [25] it was shown that an octree based scene reconstruction is much more compact when compared to a 3D grid-based representation of equivalent resolution. 5.2 Octree based mapping Octomap 5 5 To conduct octree based mapping, we used the implementation from the OctoMap toolbox is a framework based on octrees for the representation of 3D environments. Space is modelled by cells that can be defined as occupied, free or unknown (implicitly) [25]. A cell is defined as a cubic volume in 3D space containing the occupancy probability for that volume. The occupancy probability for a cell is updated whenever a sensor range measurement gives novel information concerning the occupancy status of that cell. This occurs in two situations: (a) when a measured 3D point is inside the volume defined by the cell, the probability that the cell is occupied is increased; (b) when the ray casted from the sensor's origin to a measured 3D point intersects the volume of the cell, the probability that the cell is occupied is decreased. Unobserved volumes are implicitly modelled as unknown space [25]. The octomap framework is capable of creating and updating a dynamic octree, by combining in a probabilistic manner multiple measurements from a single sensor over time. As a consequence, the level of confidence in the representation of the environment is enhanced with the increase of the number of measurements taken. If the robot travels through the environment at a high speed, few measurements are collected, resulting in a scene representation which is less accurate. Nonetheless, the representation of the scene is still created regardless of the number of measurements collected for each scene. The reliability of the spatial change detection mechanisms is therefore increased with the number of measurements taken. 5.3 Spatial change detection using an unsupervised model In the case of the spatial change detection using an unsupervised model, we propose the offline creation of an octree representation of the model scenario. We refer to this as a model octree. Afterwards, the robot will again travel through the same scenario. However, this time, the scenario should contain some differences with respect to the original version. This is called the target scenario. The target octree is generated while the robot is travelling the target scenario. Thus, the detection of spatial changes is obtained through a comparison between the model octree and the target octree. To allow the comparison between scenes, both octrees are captured according to the same reference frame. The advantages of comparing octrees as opposed to comparing raw point cloud data are that octrees are hierarchical structures, which optimizes spatial search. In addition, octrees also discretize space (the size of the smallest cubes is defined by the octree resolution and depth), which also eases comparison operations. Algorithm 1 Octree to octree spatial change detection. Input: α , the model octree Input: β , the target octree Input: v, a search volume Input: T e , occupation ratio threshold for exceeding inconsistencies Input: T m , occupation ratio threshold for missing inconsistencies Output: Ie , list of α cells with inconsistency of type exceeding Output: Im , list of α cells with inconsistency of type missing 1: Ie ←{} 2: Im ←{} 3: for i =1← length( α ), ∀ α i inside v do 4: Initialize the number of cells inside v, N ←0 5: Initialize the number of occupied cells, O ←0 6: for j =1← length( β ), ∀ β j inside α i do 7: N ← N +1 8: if occupied( β j ) then 9: O ← O +1 10: end if 11: end for 12: Define the occupation ratio for cell α i , r ← O N 13: if occupied( α i ) and r ≤ T m then 14: Mark α i with type missing inconsistency, Im ←{Im , α i } 15: els if occupied( α i ) and r ≥ T e then 16: Mark α i with type exceeding inconsistency, Ie ←{Ie , α i } 17: end if 18: end for Algorithm 1 shows the procedure designed to determine spatial inconsistencies of types exceeding and missing. For each cell in the model octree, that is, inside a predefined search volume, an occupation ratio is computed from all the target octree cells that lie inside the model cell. Inconsistencies of type exceeding are detected when the model cell is defined as free, and the occupation ratio is too high, i.e., above the occupation ratio threshold for type exceeding. Conversely, inconsistencies of type missing are identified when the model cell is occupied and the occupation ratio is too small. Let the model scene be the one displayed in Fig. 4 (a), and the target scene the one shown in Fig. 4(c). The octrees of both the model and target scenes can be seen in Fig. 4(b) and (d), respectively. Note that the target scene (Fig. 4(c)) contains two additional boxes placed on the right side of the upper shelf, and in the centre of the second shelf counting from the top. These objects are not present in the model and should, therefore, trigger inconsistencies of type exceeding. On the other hand, the object on the left side of the bottom shelf is present in the model but missing in the target. Therefore, this should constitute an inconsistency of type missing. Fig. 5 (a) shows the results of the spatial change detection using an unsupervised model. The three objects that differentiate the model and the target scene are clearly identified by the algorithm, i.e., the two extra boxes and the missing object on the bottom shelf. In addition to that, several smaller size false detections are also present. To address this we propose to cluster the lists of inconsistent octree cells (I e and I m in Algorithm 1). Clustering is important because it will be able to generate macro level inconsistencies, rather than a list of octree inconsistent cells. In addition to that, it will also help to filter smaller size clusters. Algorithm 2 displays the algorithm used to create a list of clusters. After running the clustering, it is possible to filter out small size clusters. To do so, we compute the total volume of each cluster, defined as: (1) V k = ∑ V α i , ∀ i ∈ C k where V k is the volume of cluster k, and V α i the volume of cell α i . Then, clusters are removed if their volume is smaller than a predefined threshold. Fig. 5(b) shows the objects (clusters) after the filtering is performed. This time, only the three objects are detected, and their inconsistency type correctly reported. Algorithm 2 Clustering a list of inconsistent octree cells. Input: I, list of inconsistent octree cells Output: C, list of lists (clusters) of octree cells 1: C ←{} 2: Initialize a queue list of cells that need to be assigned to a cluster, Q ← I 3: while length(Q) ≠0 do 4: Add first element of Q to seed cells list S, S ←{Q 1} 5: Remove the first element of Q, Q ← Q \\ Q 1 6: Initialize flooded cells list F, F ←{} 7: while length(S) ≠0 do 8: for i =1← length(Q) do 9: if neighbours(S, Q i ) then 10: S ←{S, Q i } 11: Remove element i from Q, Q ← Q \\ Q i 12: end if 13: end for 14: Append first element of S to F, F ←{F, S 1} 15: Remove first element of S, S ←{S \\ S 1} 16: end while 17: Append F to the list of clusters C, C ←{C, F} 18: end while 5.4 Spatial change detection using a supervised model Section 5.3 described the algorithm designed to perform spatial change detection using an unsupervised model. The procedure consists in comparing an octree created offline for the model scene against another octree created online from the target scene. In this section, we address the spatial change detection using a supervised model. In this case, rather than having to create the model octree from the model scene, we expect that there is a list of cuboids, which we refer to as logistic volumes, that specify, unambiguously, portions of the space that must be occupied or free (by definition). This list may be defined by a human operator through one of the interfaces we described earlier, e.g., 2D CAD and SLAM based, or 3D point cloud based (i.e. in the logistic world model, see Section 4), or it may be extracted from the information used to manage the logistic supermarket. During runtime operation, while the robot is moving around the target scene, a target octree is created. Thus, the problem is formulated as a comparison between a list of logistic volumes and a target octree. This is detailed in Algorithm 3. Algorithm 3 Octree to logistic volume spatial change detection. Input: V, list of logistic volumes Input: β , the target octree Input: T e , occupation ration threshold for exceeding inconsistencies Input: T m , occupation ration threshold for missing inconsistencies Output: V e , list of inconsistent logistic volumes of type exceeding Output: V m , list of inconsistent logistic volumes of type missing 1: V e ←{} 2: V m ←{} 3: for i =1← length(V) do 4: Initialize the number of cells inside V i , N ←0 5: Initialize the number of occupied cells, O ←0 6: for j =1← length( β j ), ∀ β j inside V i do 7: N ← N +1 8: if occupied( β j ) then 9: O ← O +1 10: end if 11: Define the occupation ratio for cell β j , r ← O N 12: if defined_as_occupied(V i ) and r ≤ T m then 13: Mark β j with type missing inconsistency, V m ←{V m , β j } 14: els if defined_as_occupied(V i ) and r ≥ T e then 15: Mark β j with type exceeding inconsistency, V e ←{V e , β j } 16: end if 17: end for 18: end for Fig. 6 shows the result of the spatial change detection using logistic volumes, as defined in the logistic world model. The logistic volumes are shown in Fig. 6(a): volumes defined as free are denoted by a green colour wireframe, while volumes marked as occupied are denoted by a red colour wireframe. Two volumes per shelf have been defined, including those containing the objects that are inconsistent between both scenes. When volumes are deemed inconsistent, they are filled with the opposite colour to the wireframe, i.e., if a volume is marked as free (green colour wireframe), and its occupation ratio is high, then the volume is filled with red colour signalizing the fact that the observation of the target scene indicates that the volume is occupied. Fig. 6(b) shows the result of the detection of inconsistent volumes. Three inconsistencies are detected, two of exceeding type (red colour) and one of missing type (green colour), which is the correct result (see the scenes in Fig. 4(a) and (c)). In general, detected inconsistencies are sent back to the logistic planner through the spatial inconsistencies service defined in Fig. 2. Within the Logistic Planner, alerts are raised to inform the logistic technician about the detected inconsistencies. In order to deal with potential false alarm situations, an alert is only generated after the detection of a user defined number of congruent inconsistencies in a short span of time. A 3D view is then provided showing the logistic volumes and the related inconsistencies. It is the responsibility of the logistic technician to decide how to deal with the situation at hand: (a) if the reported inconsistency is of type exceeding and is accepted by the logistic technician, this means that there is a new physical object in the logistic volume (e.g. small box) and this is reflected in the logistic world model (e.g. by adding a new small box to a given rack); (b) if the reported inconsistency is of type missing and is accepted by logistic technician, this means that a physical object was removed from the logistic volume and accordingly, it is removed from the logistic world model (e.g. by removing a small box from a given rack); (c) if a inconsistency is reported but it is not validated by the logistic technician, the logistic world model is not updated. 6 Results Fig. 5, in Section 5.3, demonstrates the spatial change detection using models created in an unsupervised fashion. In this section, we report both quantitative and qualitative results for the spatial change detection using models created in a supervised fashion (see Section 5.4). Section 6.1 presents quantitative results using a controlled small-scale environment, where the scenario was captured from a camera mounted on a robotic manipulator. In this controlled scenario, it is possible to hand label the ground truth, therefore, we can present the receiver operating characteristic (ROC) curves depicted in Fig. 8 . In Section 6.2, we changed the scenario to a more complex environment. We also used a different robot, in this case an automated guided vehicle. This scenario is used to display the potential application of the supervised spatial change detection technique to large-scale logistic supermarkets. In this case, due to the complexity of the scenario, it was not possible to hand label the ground truth. 6.1 Small-scale scenario using a robotic manipulator To evaluate the exceeding and missing classifiers, five scenes were collected (similar to those of Fig. 4(a) and (c). Each scene has shelves with objects in different positions. To collect the scenes we used a 3D camera mounted on a robotic manipulator, as shown in Fig. 7 (a). The manipulator scans the shelf by moving the camera to different viewpoints. The 3D data is used to create an octree in real time of the complete scene using multiple viewpoints as input (similar to those of Fig. 4(b) and (d). There is a video 6 6 illustrating the process of capturing a target scene. For each scene, we define the logistic volumes as shown in Fig. 3(c). Fig. 6(a) presents an example of a scene with the logistic volumes defined. To evaluate the performance of the algorithm, we combined two of the five scenes described above. One of these scenes is defined as the model and the other as the target. The differences in the position of objects from the model scene to the target scene should generate inconsistencies by the proposed algorithm. We marked these inconsistencies manually, and used this as ground truth for evaluation the performance of the algorithm. We repeat the process for all pairwise combinations of the five scenes collected, resulting in 20 model target scene pairs. The algorithm compares the octree with the logistic volumes and produces a set of inconsistencies which are matched with the ground truth created, as described above. We then compute the precision and recall for a given classifier. For each classifier (exceeding or missing), two parameters are used: the occupation ratio threshold (see Algorithm 3) and the volume threshold (see Section 5.3). Occupation ratio threshold values of 0, 10 and 20%, and volume threshold values of 0.1×10−3, 0.5×10−3, 1×10−3, 2×10−3, 4×10−3 m3 were used. All combinations of those values result in several configurations for the classifiers. To analyse the trade-off between the successful detection of inconsistencies and the false alarm rate for the set of parameters chosen, a ROC curve was employed. Fig. 8 shows the ROC curves for each of the classifiers. As expected, the trade-off between true positive rate (the proportion of inconsistencies that are successfully signalized) and false positive rate (the ratio of occurrences of inconsistencies that are wrongly signalized) implies that as the rate of true detections rises, the rate of false alarms rises accordingly. In the case of the exceeding and missing classifiers, designing such classifiers with maximum true positive rate entails a false positive rate of around 0.6, i.e. having an exceeding or missing inconsistency being always detected implies that 60% of detections are false alarms. Fig. 8 shows that a good compromise would be to have an exceeding and missing classifiers with 0.9 precision and false positive rate under 0.4. With a set of parameters, for a 90% rate of successful inconsistencies detections, 40% would be false alarms. 6.2 Large-scale scenario using a mobile robot To evaluate the proposed spatial change detection approach using a supervised model on a large-scale environment, a mobile robot navigating around a room with about 65m2 was used, as shown in Fig. 7(b) and (c). The robot uses SLAM based localization, which provides a transformation from the pose of all the views captured by the 3D camera to a global coordinate frame. Since the localization problem is out of the scope of the current paper, we used a standard SLAM approach [35] to obtain the poses of the camera views. Note that the methodology is very similar to the experimental setup described in Section 6.1 (Fig. 7(a)), but while in that case the pose of the camera is given as a function of the direct kinematics of the robot, in the case of the mobile robot, it is the SLAM based localization that is used to estimate the pose of the robot and consequently of the camera. Fig. 9 (a) shows a fused point cloud obtained from merging multiple point clouds of different views of the model scene. In this section, we evaluate the methodology proposed to compare an octree against a list of logistic volumes defined offline using a qualitative approach. The logistic volumes defined for this experiment are displayed in Fig. 9(a) by the red (volumes defined as occupied) and green (volumes defined as free) wireframes. The octree used in the comparison is computed online while the robot is moving around the scene. In Fig. 9(b) the final state of the octree is shown, i.e., the octree obtained after the robot has navigated through the entire scenario. Note that this octree is captured from the target scene, and therefore there are differences with respect to the model scene displayed in Fig. 9(a). For example, the object that is inside the large red wireframe in the bottom of Fig. 9(a) (the model scene), has moved to the left in Fig. 9(b) (the target scene). This should be recognized as an inconsistency. Fig. 9(c) shows the detected inconsistencies marked as filled boxes. There is a video 7 7 demonstrating the process of acquiring the 3D data and detecting the spatial inconsistencies in real time, using the proposed supervised approach. The case discussed above is correctly identified with the reporting of two inconsistencies: one of type missing, the green filled box with the red wireframe on the bottom of Fig. 9(c) (i.e., the logistic volume is defined as occupied but there is nothing inside it in the target scene), and another of type exceeding, the red filled box with green wireframe (i.e., the logistic volume is marked as free, but there is an object inside the volume in the target scene). This shows that the algorithm is capable of accurately and efficiently performing spatial change detection in this scenario. 7 Conclusions The creation of a logistic world model representing the reality of a logistic supermarket by defining the geometric and volumetric information about their physical objects (shelves, small and large boxes, conveyors, kits, and packaging elements) enables robots to move and act in the space in an effective and controlled way. This paper proposed a system capable of continuously monitoring and ensuring the consistency of this information by having the robots create hierarchical spatial representations of the physical volumes that are detected while the robots move in the scenario and execute kitting operation missions. The comparison of this real time model with the one defined in the logistic world model contributes to the increase of the accuracy of the information contained in the logistic world model and to the detection of faults and errors in the model. This work is integrated in the EU project STAMINA, where mobile manipulators are being developed to conduct automatic pick and place operations in a logistic supermarket. Future work includes testing the approach in one of the logistic supermarkets of the PSA – Peugeot Citroën Automobiles S.A., in Rennes, France. This test will assess the scalability of the proposed approach to large-scale industrial scenarios. Acknowledgements This work has been supported by the “Fundação para a Ciência e Tecnologia” (Portuguese Foundation for Science and Technology) under grant agreements SFRH/BPD/109651/2015. This work was also financed by the ERDF – European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation – COMPETE 2020 Programme within project POCI-01-0145-FEDER-006961, SAICTPAC/0034/2015- POCI-01-0145-FEDER-016418 and project “NORTE -01 -0145 -FEDER- 000020”, financed by the North Portugal Regional Operational Programme (NORTE 2020, under the PORTUGAL 2020 Partnership Agreement). This work was partially funded by the EU FP7 project STAMINA, FP7-ICT-2013-10, Grant number 610917. References [1] E.A. Lee Cyber physical systems: design challenges 2008 11th IEEE international symposium on object oriented real-time distributed computing (ISORC) 2008 IEEE 363 369 [2] P. Leit ao V. Marik P. Vrba Past, present, and future of industrial agent applications IEEE Trans Ind Inform 9 4 2013 2360 2372 [3] L.D. Xu Enterprise systems: state-of-the-art and future trends IEEE Trans Ind Inform 7 4 2011 630 640 [4] W. He L. Da Xu Integration of distributed enterprise applications: a survey IEEE Trans Ind Inform 10 1 2014 35 42 [5] D. Herrero-Perez H. Martinez-Barbera Modeling distributed transportation systems composed of flexible automated guided vehicles in flexible manufacturing systems IEEE Trans Ind Inform 6 2 2010 166 180 [6] M. Metzger G. Polakow A survey on applications of agent technology in industrial process control IEEE Trans Ind Inform 7 4 2011 570 581 [7] M.V. Espina R. Grech D. De Jager P. Remagnino L. Iocchi L. Marchetti D. Nardi D. Monekosso M. Nicolescu C. King Multi-robot teams for environmental monitoring Innovations in defence support systems-3 2011 Springer 183 209 [8] M.A. Vieira M.E. Taylor P. Tandon M. Jain R. Govindan G.S. Sukhatme M. Tambe Mitigating multi-path fading in a mobile mesh network Ad Hoc Netw 11 4 2013 1510 1521 [9] K. Senthilkumar K. Bharadwaj Multi-robot exploration and terrain coverage in an unknown environment Robot Autonom Syst 60 1 2012 123 132 [10] L. Monostori Cyber-physical production systems: roots, expectations and R&D challenges Procedia CIRP 17 2014 9 13 [11] L. Wang M. Törngren M. Onori Current status and advancement of cyber-physical systems in manufacturing J Manuf Syst 37 Part 2 2015 517 527 [12] V. Krueger A. Chazoule M. Crosby A. Lasnier M.R. Pedersen F. Rovida L. Nalpantidis R. Petrick C. Toscano G. Veiga A vertical and cyber-physical integration of cognitive robots in manufacturing Proc IEEE 104 5 2016 1114 1127 [13] A.W. Colombo T. Bangemann S. Karnouskos J. Delsing P. Stluka R. Harrison F. Jammes J.L. Lastra Industrial cloud-based cyber-physical systems: the IMC-AESOP approach 2014 [14] M. Riedl H. Zipper M. Meier C. Diedrich Cyber-physical systems alter automation architectures Annu Rev Control 38 1 2014 123 133 10.1016/j.arcontrol.2014.03.012 [15] IEC Enterprise-control system integration-part 3: activity models of manufacturing operations management 2007 Tech. rep., IEC 62264-3 [16] A. Girbea C. Suciu S. Nechifor F. Sisak Design and implementation of a service-oriented architecture for the optimization of industrial applications IEEE Trans Ind Inform 1 2014 185 196 [17] G. Veiga J.N. Pires K. Nilsson Experiments with service-oriented architectures for industrial robotic cells programming Robot Comput Integr Manuf 25 4–5 2009 746 755 [18] M.N. Huhns M.P. Singh Service-oriented computing: key concepts and principles IEEE Internet Comput 9 1 2005 75 81 [19] P. Bianco G.A. Lewis P. Merson S. Simanta Architecting service-oriented systems 2011 Tech. rep., DTIC Document [20] A. Taneja L. Ballan M. Pollefeys Image based detection of geometric changes in urban environments 2011 international conference on computer vision 2011 2336 2343 10.1109/ICCV.2011.6126515 [21] B. Chen L. Deng Y. Duan S. Huang J. Zhou Building change detection based on 3d reconstruction 2015 IEEE international conference on image processing (ICIP) 2015 4126 4130 10.1109/ICIP.2015.7351582 [22] D. Brunner L. Bruzzone G. Lemoine Change detection for earthquake damage assessment in built-up areas using very high resolution optical and SAR imagery 2010 IEEE international geoscience and remote sensing symposium 2010 3210 3213 10.1109/IGARSS.2010.5651416 [23] A.W. Vieira P.L.J. Drews M.F.M. Campos Spatial density patterns for efficient change detection in 3d environment for autonomous surveillance robots IEEE Trans Autom Sci Eng 11 3 2014 766 774 10.1109/TASE.2013.2294851 [24] D. Marinelli C. Paris L. Bruzzone Fusion of high and very high density LiDAR data for 3d forest change detection 2016 IEEE international geoscience and remote sensing symposium (IGARSS) 2016 3595 3598 10.1109/IGARSS.2016.7729931 [25] A. Hornung K.M. Wurm M. Bennewitz C. Stachniss W. Burgard Octomap: an efficient probabilistic 3d mapping framework based on octrees Autonom Robots 34 3 2013 189 206 [26] J. Elseberg D. Borrmann A. Nüchter Efficient processing of large 3d point clouds 2011 XXIII international symposium on information, communication and automation technologies (ICAT) 2011 IEEE 1 7 [27] R.B. Rusu S. Cousins 3D is here: Point cloud library (PCL) 2011 IEEE international conference on robotics and automation (ICRA) 2011 IEEE 1 4 [28] J.-Y. Sim S.-U. Lee C.-S. Kim Construction of regular 3d point clouds using octree partitioning and resampling IEEE International symposium on circuits and systems, 2005. ISCAS 2005 2005 IEEE 956 959 [29] N. Niu L. Da Xu Z. Bi Enterprise information systems architecture-analysis and evaluation IEEE Trans Ind Inform 9 4 2013 2147 2154 [30] M. Crosby R.P. Petrick Centralised high-level planning for a robot fleet Workshop of the UK Planning and Scheduling Special Interest Group (PlanSIG 2014) UK 2014 [31] M. Crosby R.P. Petrick Temporal multiagent planning with concurrent action constraints Proceedings of the ICAPS 2014 Workshop on distributed and multi-agent planning (DMAP) 2014 16 24 [32] M.R. Pedersen L. Nalpantidis R.S. Andersen C. Schou S. Bøgh V. Krüger O. Madsen Robot skills for manufacturing: from concept to industrial deployment Robot Comput Integ Manuf 37 2016 282 291 [33] F. Rovida V. Kruger Design and development of a software architecture for autonomous mobile manipulators in industrial environments 2015 IEEE international conference on industrial technology (ICIT) 2015 IEEE 3288 3295 [34] M.R. Pedersen D.L. Herzog V. Kruger Intuitive skill-level programming of industrial handling tasks on a mobile manipulator 2014 IEEE/RSJ international conference on intelligent robots and systems (IROS 2014) 2014 IEEE 4523 4530 [35] S. Kohlbrecher O. Von Stryk J. Meyer U. Klingauf A flexible and scalable slam system with full 3d motion estimation 2011 IEEE international symposium on safety, security, and rescue robotics 2011 IEEE 155 160 "
    },
    {
        "doc_title": "Measuring and modeling vertical gradients in suspended sediments in the Solimões/Amazon River",
        "doc_scopus_id": "85006056643",
        "doc_doi": "10.1002/hyp.11059",
        "doc_eid": "2-s2.0-85006056643",
        "doc_date": "2017-01-30",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Water Science and Technology",
                "area_abbreviation": "ENVI",
                "area_code": "2312"
            }
        ],
        "doc_keywords": [
            "Concentration gradients",
            "Concentration profiles",
            "In-situ concentrations",
            "Monitoring strategy",
            "Rouse model",
            "Sampling procedures",
            "Suspended sediment concentrations",
            "Suspended sediment flux"
        ],
        "doc_abstract": "Copyright © 2016 John Wiley & Sons, Ltd.Accurately measuring sediment flux in large rivers remains a challenge due to the spatial and temporal cross-sectional variability of suspended sediment concentrations in conjunction with sampling procedures that fail to accurately quantify these differences. This study presents a field campaign methodology that can be used to improve the measurement of suspended sediment concentrations in the Amazon River or similarly large rivers. The turbidity signal and Rouse model are together used in this study to define the spatial distribution of suspended sediment concentrations in a river cross-section, taking into account the different size fractions of the sediment. With this methodology, suspended sediment fluxes corresponding to each sediment class are defined with less uncertainty than with manual samples. This paper presents an application of this methodology during a field campaign at different gauging stations along a 3,000-km stretch of the Solimões/Amazon River during low water and flood periods. Vertical concentration profiles and Rouse model applications for distinctive sediment sizes are explored to determine concentration gradients throughout a cross-section of the river. The results show that coupling both turbidity technology and the Rouse model may improve our understanding of the spatial distribution of different sediments fractions sizes in the Solimões/Amazon River. These data are very useful in defining a pertinent monitoring strategy for suspended sediment concentrations in the challenging context of large rivers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An orthographic descriptor for 3D object learning and recognition",
        "doc_scopus_id": "85006365227",
        "doc_doi": "10.1109/IROS.2016.7759612",
        "doc_eid": "2-s2.0-85006365227",
        "doc_date": "2016-11-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Classification performance",
            "Computation time",
            "Descriptors",
            "Object reference",
            "Object representations",
            "Real-time application",
            "State of the art"
        ],
        "doc_abstract": "© 2016 IEEE.Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure reliability, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. The performance of the proposed object descriptor is compared with the main state-of-the-art descriptors. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-oftheart descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Monocular visual odometry: A cross-spectral image fusion based approach",
        "doc_scopus_id": "85044113968",
        "doc_doi": "10.1016/j.robot.2016.08.005",
        "doc_eid": "2-s2.0-85044113968",
        "doc_date": "2016-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Evaluation metrics",
            "Fused images",
            "Monocular image",
            "Mutual informations",
            "Spectral image fusions",
            "Spectral images",
            "Spectral imaging",
            "Visual odometry"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.This manuscript evaluates the usage of fused cross-spectral images in a monocular visual odometry approach. Fused images are obtained through a Discrete Wavelet Transform (DWT) scheme, where the best setup is empirically obtained by means of a mutual information based evaluation metric. The objective is to have a flexible scheme where fusion parameters are adapted according to the characteristics of the given images. Visual odometry is computed from the fused monocular images using an off the shelf approach. Experimental results using data sets obtained with two different platforms are presented. Additionally, comparison with a previous approach as well as with monocular-visible/infrared spectra are also provided showing the advantages of the proposed scheme.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-08-26 2016-08-26 2016-09-11 2016-09-11 2017-11-02T19:32:34 S0921-8890(16)30100-2 S0921889016301002 10.1016/j.robot.2016.08.005 S300 S300.4 FULL-TEXT 2017-11-02T17:12:05.605018-04:00 0 0 20161101 20161130 2016 2016-08-26T01:30:35.735865Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 85 85 C Volume 85 4 26 36 26 36 201611 November 2016 2016-11-01 2016-11-30 2016 article fla © 2016 Elsevier B.V. All rights reserved. MONOCULARVISUALODOMETRYACROSSSPECTRALIMAGEFUSIONBASEDAPPROACH SAPPA A 1 Introduction 2 Proposed approach 2.1 Wavelet based image fusion 2.1.1 Discrete Wavelet Transform (DWT) 2.1.2 Fusion strategies 2.2 Performance evaluation 2.3 Monocular visual odometry 3 Experimental results 3.1 Acquisition platforms 3.2 Visual odometry results 3.2.1 CVC-Vid00 video sequence 3.2.2 CVC-Vid01 video sequence 3.2.3 CVC-Vid02 video sequence 3.2.4 KAIST-5AM video sequence 3.2.5 KAIST-10AM video sequence 3.2.6 KAIST-10PM video sequence 4 Conclusion Acknowledgments References BORRMANN 2014 425 440 D SHAH 2013 537 552 P BOURLAI 2010 1343 1347 T 201020THINTERNATIONALCONFERENCEPATTERNRECOGNITION CROSSSPECTRALFACEVERIFICATIONINSHORTWAVEINFRAREDSWIRBAND POUJOL 2015 517 528 J ROBOT2015SECONDIBERIANROBOTICSCONFERENCE VISIBLETHERMALFUSIONBASEDMONOCULARVISUALODOMETRY SCARAMUZZA 2011 80 92 D CHILIAN 2009 4571 4576 A IEEEINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS STEREOCAMERABASEDNAVIGATIONMOBILEROBOTSROUGHTERRAIN NILSSON 2011 12952 12957 E 18THIFACWORLDCONGRESSMILANOITALY28AUGUST2SEPTEMBER2011 VEHICLEMOTIONESTIMATIONUSINGINFRAREDCAMERA MOUATS 2015 1210 1224 T RICAURTE 2014 3690 3701 P AGUILERA 2012 12661 12672 C DONG 2015 268 274 L WANG 2005 1391 1402 Z LANG 1996 10 12 M CHANG 1993 429 441 T BARRI 2012 1303 1314 A AMOLINS 2007 249 263 K MEHRA 2015 153 160 I JAGALINGAM 2015 133 142 P ZHOU 2004 600 612 W YANG 2008 156 160 C HAGHIGHAT 2011 744 756 M BARRERA 2012 437 446 F SAPPAX2016X26 SAPPAX2016X26X36 SAPPAX2016X26XA SAPPAX2016X26X36XA 2018-09-11T00:00:00.000Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0921-8890(16)30100-2 S0921889016301002 10.1016/j.robot.2016.08.005 271599 2017-11-02T17:12:05.605018-04:00 2016-11-01 2016-11-30 true 4692911 MAIN 11 50526 849 656 IMAGE-WEB-PDF 1 gr1 41352 164 193 gr10 23312 164 197 gr2 13688 60 219 gr3 17563 109 219 gr4 32462 98 219 gr5 30627 162 219 gr6 31657 152 219 gr7 31297 164 215 gr8 25217 164 197 gr9 25525 164 194 pic1 38528 163 140 pic2 33608 164 141 pic3 29294 163 140 pic4 22992 163 140 pic5 27315 163 140 pic6 26991 163 140 pic7 30660 164 140 gr1 93088 319 376 gr10 116696 584 702 gr2 35406 98 357 gr3 43322 150 302 gr4 80659 235 528 gr5 134368 533 719 gr6 138249 498 717 gr7 148003 547 719 gr8 135011 601 721 gr9 146463 610 720 pic1 50829 132 113 pic2 42495 131 113 pic3 36394 132 113 pic4 36020 132 113 pic5 40284 132 113 pic6 38137 132 113 pic7 36789 131 112 gr1 848215 1413 1664 gr10 775222 2586 3107 gr2 81598 436 1584 gr3 110652 665 1340 gr4 521268 1042 2338 gr5 1003376 2357 3182 gr6 1295753 2204 3176 gr7 1222860 2420 3182 gr8 1078930 2661 3193 gr9 1187785 2698 3187 pic1 168291 583 500 pic2 90204 583 501 pic3 68374 583 500 pic4 63361 583 500 pic5 78112 583 500 pic6 74364 583 500 pic7 72047 583 499 si1 117 12 6 si10 265 11 36 si11 144 14 12 si13 136 11 12 si14 141 11 12 si15 187 11 21 si16 185 11 22 si17 179 11 22 si2 135 12 10 si22 199 12 32 si23 1232 17 368 si24 175 14 18 si25 461 15 98 si26 117 10 10 si27 124 10 11 si28 232 15 33 si3 146 11 13 si30 334 14 59 si31 434 12 91 si32 493 12 112 si6 176 14 17 si7 161 14 15 si8 216 8 43 si9 176 11 19 ROBOT 2673 S0921-8890(16)30100-2 10.1016/j.robot.2016.08.005 Elsevier B.V. Fig. 1 (left) Pair of images (VS-LWIR) to be fused. (right) DWT decompositions (one level) of the input images. Fig. 2 Illustration of a DWT based image fusion scheme. Fig. 3 Two dimensional wavelet decomposition scheme ( l : low pass filter; h : high pass filter; dec: decimation). Fig. 4 Acquisition systems (cross-spectral imaging on the top): (left) Electric vehicle from the Computer Vision Center (Barcelona, Spain) [5] (CVC video sequences); (right) Car from the Korea Advanced Institute of Science and Technology (Seoul, Korea) [29] (KAIST video sequences). Fig. 5 Estimated trajectories for the CVC-Vid00 video sequence: (a) Visible spectrum; (b) Infrared spectrum; (c) DWT fused images from [5]; and (d) DWT fused images, proposed approach. Fig. 6 Estimated trajectories for CVC-Vid01 sequence: (a) Visible spectrum; (b) Infrared spectrum; (c) DWT based fused image, result from [5]; and (d) DWT based fused image, proposed approach. Fig. 7 Estimated trajectories for CVC-Vid02 sequence: (a) Visible spectrum; (b) Infrared spectrum; (c) DWT based fused image, result from [5]; and (d) DWT based fused image, proposed approach. Fig. 8 Estimated trajectories for KAIST-5AM video sequence: (a) Visible spectrum; (b) Infrared spectrum; (c) DWT based fused image, result using [5]; and (d) Fused with the proposed DWT based approach, by selecting the best setup. Fig. 9 Estimated trajectories for KAIST-10AM video sequence: (a) Visible spectrum; (b) Infrared spectrum; (c) DWT based fused image, result using [5]; and (d) Fused with the proposed DWT based approach, by selecting the best setup. Fig. 10 Estimated trajectories for KAIST-10PM video sequence: (a) Visible spectrum; (b) Infrared spectrum; (c) DWT based fused image, result using [5]; and (d) Fused with the proposed DWT based approach, by selecting the best setup. Table 1 Wavelet families evaluated in the current work. Wavelet name Comments Setups Haar (haar) Orthogonal wavelet linear phase. haar Daubechies (dbN) Daubechies’ external phase wavelets. N refers to the number of vanishing moments. db1, db2, …, db8. Symlets (symN) Daubechies’ least asymmetric wavelets. N refers to the number of vanishing moments. sym2, sym3,…, sym8. Coiflets (coifN) In this family, N is the number of vanishing moments for both the wavelet and scaling function. coif1, coif2, …, coif5. Biorthogonal (biorNr.Nd) Biorthogonal wavelets with linear phase. Feature pair of scaling functions (with associated wavelet filters), one for decompositions and one for reconstruction, which can have different number of vanishing moments. Nr and Nd represent the number of vanishing moments. bior1.1, bior1.3, bior1.5, bior2.2, bior2.4, bior2.6, bior2.8, bior3.1, bior3.3, bior3.5, bior3.7, bior3.9, bior3.5, bior3.7, bior3.9, bior4.4, bior5.5, bior6.8 Reverse Biorthogonal (rbioNr.Nd) Reverse of the Biorthogonal wavelet explained above. rbio1.1, rbio1.3, rbio1.5, rbio2.2, rbio2.4, rbio2.6, rbio2.8, rbio3.1, rbio3.3, rbio3.5, rbio3.7, rbio3.9, rbio4.4, rbio5.5, rbio6.8 Discrete meyer approximation (dmey) Approximation of meyer wavelets leading to FIR filters that can be used in DWT. dmey Table 2 VO results in the CVC-Vid00 video sequence using images from different spectrum and fusion approaches (VS: visible spectrum; LWIR: Long Wavelength Infrared spectrum; DWT [5]: fusion using Discrete Wavelet Transform; DWT [Prop. App]: fusion using Discrete Wavelet Transform selecting the best setup). Results VS LWIR DWT [5] DWT [Prop. App.] Total traveled distance (m) (GPS traveled dist.: 235 m) 234.88 241.27 245 240.3 Final position error (m) 2.9 18 5.4 5.1 Average number of matches 2053 3588 4513 2123 Percentage of inliers 71.5 61.94 60 65.1 Table 3 VO results in the CVC-Vid01 video sequence using images from different spectrum and fusion approaches (VS: visible spectrum; LWIR: Long Wavelength Infrared spectrum; DWT [5]: fusion using Discrete Wavelet Transform; DWT [Prop. App]: fusion using Discrete Wavelet Transform selecting the best setup). Results VS LWIR DWT [5] DWT [Prop. App.] Total traveled distance (m) (GPS traveled dist.: 365 m) 371.8 424 386 379 Final position error (m) 32.6 84.7 44 38.2 Average number of matches 1965 1974 2137 2071 Percentage of inliers 72.6 67.8 61.5 66.3 Table 4 VO results in the CVC-Vid02 video sequence using images from different spectrum and fusion approaches (VS: visible spectrum; LWIR: Long Wavelength Infrared spectrum; DWT [5]: fusion using Discrete Wavelet Transform; DWT [Prop. App]: fusion using Discrete Wavelet Transform selecting the best setup). Results VS LWIR DWT [5] DWT [Prop. App.] Total traveled distance (m) (GPS traveled dist.: 370 m) 325.6 336.9 354.4 334.7 Final position error (m) 37.7 48.7 37.2 38.0 Average number of matches 1890 1028 1952 1719 Percentage of inliers 70 65.8 61 64.1 Table 5 VO results for the 5AM video sequence using different images (VS: visible spectrum; LWIR: Long Wavelength Infrared spectrum; DWT [5]: fused images using Discrete Wavelet Transform; DWT [Prop. App]: fused images using Discrete Wavelet Transform selecting the best setup). Results VS LWIR DWT [5] DWT [Prop. App.] Total traveled distance (m) (GPS traveled dist.: 287.9 m) 278.7 279.5 278.9 274.6 Final position error (m) 58.4 32.1 25.1 24.1 Average number of matches 3197 2659 2555 2584 Percentage of inliers 51.7 64.1 53.6 54.1 Table 6 VO results for the 10AM video sequence using different images (VS: visible spectrum; LWIR: Long Wavelength Infrared spectrum; DWT [5]: fused images using Discrete Wavelet Transform; DWT [Prop. App]: fused images using Discrete Wavelet Transform selecting the best setup). Results VS LWIR DWT [5] DWT [Prop. App.] Total traveled distance (m) (GPS traveled dist.: 351.6 m) 348.2 330.2 347.8 347.3 Final position error (m) 10.1 53.9 12.6 11.8 Average number of matches 12696 4278 11078 11119 Percentage of inliers 93.8 69.7 90.6 89.9 Table 7 VO results for the 10PM video sequence using different images (VS: visible spectrum; LWIR: Long Wavelength Infrared spectrum; DWT [5]: fused images using Discrete Wavelet Transform; DWT [Prop. App]: fused images using Discrete Wavelet Transform selecting the best setup). Results VS LWIR DWT [5] DWT [Prop. App.] Total traveled distance (m) (GPS traveled dist.: 222.3 m) 281.90 218.07 226.15 222.79 Final position error (m) 42.20 24.99 15.2 12.62 Average number of matches 271 2308 1065 2301 Percentage of inliers 44.89 61.71 52.29 61.73 Monocular visual odometry: A cross-spectral image fusion based approach Angel D. Sappa a b ⁎ Cristhian A. Aguilera a c Juan A. Carvajal Ayala b Miguel Oliveira d e Dennis Romero b Boris X. Vintimilla b Ricardo Toledo a c a Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain Computer Vision Center, Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain b Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Campus Gustavo Galindo Km 30.5, Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador c Computer Science Dept., Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain Computer Science Dept., Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain d INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n 4200-465 Porto Portugal e IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago 3810-193 Aveiro Portugal ⁎ Corresponding author at: Computer Vision Center, Universitat Autònoma de Barcelona, 08193-Bellaterra, Barcelona, Spain. Computer Vision Center, Universitat Autònoma de Barcelona Barcelona 08193-Bellaterra Spain This manuscript evaluates the usage of fused cross-spectral images in a monocular visual odometry approach. Fused images are obtained through a Discrete Wavelet Transform (DWT) scheme, where the best setup is empirically obtained by means of a mutual information based evaluation metric. The objective is to have a flexible scheme where fusion parameters are adapted according to the characteristics of the given images. Visual odometry is computed from the fused monocular images using an off the shelf approach. Experimental results using data sets obtained with two different platforms are presented. Additionally, comparison with a previous approach as well as with monocular-visible/infrared spectra are also provided showing the advantages of the proposed scheme. Keywords Monocular visual odometry LWIR-RGB cross-spectral imaging Image fusion 1 Introduction The usage of cross-spectral imaging has been increasing due to the drop in price of cameras working at different spectral bands. That increase is motivated by the possibility of developing robust solutions that cannot be obtained if a single band were used. These robust solutions can be found in domains such as thermal inspection [1], video surveillance [2], face detection [3], driving assistance [4] and visual odometry [5], which is the focus of the current work. Before tackling one of the problems mentioned above, the information provided by the cameras working at different spectral bands needs to be fused into a single and compact representation for further processing, assuming an early fusion scheme is followed. Visual Odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human or a robot) using only the input of a single or multiple cameras attached to it. This term has been proposed by Nister [6] in 2004, which has been chosen for its similarity to wheel odometry. In wheel odometry, the motion of a vehicle is obtained by integrating the number of turns of its wheels over time. Similarly, VO operates by incrementally estimating the pose of the vehicle by analyzing the changes induced by the motion in the images of the onboard vision system. State of the art VO approaches are based on monocular or stereo vision systems; most of them working with cameras in the visible spectrum (e.g., [7,8]). The approaches proposed in the literature can be coarsely classified into feature based methods, image based methods and hybrid methods. The feature based methods rely on visual features extracted from the given images (e.g., corners, edges) that are matched between consecutive frames to estimate the egomotion. On the contrary to feature based methods, the image based approaches directly estimate the motion by minimizing the intensity error between consecutive images. Finally, hybrid methods are based on a combination of the approaches mentioned before to reach a more robust solution. All the VO approaches based on visible spectrum imaging, in addition to their own intrinsic limitations, have additional ones related with the nature of the images (i.e., photometry). Having in mind these limitations (i.e., noise, sensitivity to lighting changes, etc.) monocular and stereo vision based VO approaches, using cameras in the infrared spectrum, have been proposed (e.g., [9,10]) and more recently cross-spectral stereo based approaches have been also introduced (e.g., [11,5]). The current work proposes a step further by tackling the monocular vision odometry problem with an image resulting from the fusion of a cross-spectral imaging device. The goal behind such an approach is to take advantage of the strengths of each band according to the characteristics of the scenario (e.g., daytime, nighttime, poor lighting conditions, etc.). A difference to a previous approach published in [5] is that in the current work fusion parameters are adapted to the characteristics of the given images. Image fusion is the process of combining information from two or more images of a given scene into a single representation. This process is intended for encoding information from source images into a single and more informative one, which could be suitable for further processing or visual perception. There are two different cases where image fusion takes place; firstly, the case of images obtained from different sensors (multisensory), which could also work at different spectral band (multispectral). Secondly, the case of images of the same scene but acquired at different times (multitemporal). The current work is focused on the first case, more specifically, fusing pair of images from visible and infrared spectra obtained at the same time by different sensors. It is assumed that the images to be fused are correctly registered [12]; otherwise a process of cross-spectral feature detection and description should be followed in order to find the correspondences between the images (e.g., [13,14]). During the last decades, the image fusion problem has been largely studied, mainly for remote sensing applications (e.g., [15,16]). Most of these methods have been proposed to produce a high-resolution multispectral representation from a low-resolution multispectral image fused with high-resolution panchromatic one. The difference in image resolution is generally tackled by means of multi-scale image decomposition schemes that preserve spectral characteristics but represented at a high spatial resolution. Among the different proposals, wavelet based approaches have shown some of the best performance by producing better results than standard methods such as intensity–hue–saturation (IHS) transform technique or principal component analysis (PCA) [17]. Wavelet based image fusion consists of two stages. Firstly, the given images are decomposed into two components (more details are given in Section 2.1.1); secondly, the components from the given images are fused in order to generate the final representation. Hence, the main challenge with wavelet based fusion schemes lies on finding the best setup for both the image decomposition approach (i.e., number of levels, wavelet family and its configurations) and the fusion strategy to merge the information from decomposed images into a single representation (e.g., min, max, mean, rand, etc., from the two approximations and details obtained from the given images at element-wise by taking, respectively, the minimum, the maximum, the mean value, or a random element). The selection of the right setup for fusing the given images will depend on the way the performance is evaluated. Hence a special care should be paid to the quantitative metric used to evaluate the obtained result, avoiding psychophysical experiments that will result in qualitative values [18]. The current paper addresses the problem of cross-spectral fused image visual odometry by using the algorithm proposed by Geiger et al. in [19], which is referred to as LibVISO2. The main novelty of the current approach is to take advantage of information obtained at different spectral bands when visual odometry is estimated. In this way, robust solutions are obtained independently of the scenario’s characteristics (e.g., daytime). Fused images are obtained by a Wavelet based scheme. Different fusion schemes are quantitatively evaluated looking for the best one, evaluations are performed by means of a quality metric based on Mutual Information. Once the best configuration is found, the fused image based visual odometry is computed and compared with a previous cross-spectral based approach [5] and classical visible/infrared based approaches. The manuscript is organized as follows. Section 2 presents the proposed approach detailing the discrete wavelet transform based image fusion and its setups together with the off the shelf monocular visual odometry algorithm used to compute the vehicle odometry. Experimental results and comparisons are presented in Section 3. Finally, conclusions are given in Section 4. 2 Proposed approach This section presents the Discrete Wavelet Transform image fusion scheme, the evaluation metric used to find the best setup and the monocular visual odometry approach used in the current work. 2.1 Wavelet based image fusion Wavelet theory has been largely studied in digital signal processing and applied to several subjects (from noise reduction [20] to texture classification [21], jut to mention a couple). At this section, the basic concepts and elements of Discrete Wavelet Transform (DWT) in the context of image fusion are introduced. Let I VS and I IR be the original images, of m × n pixels, in the visible ( VS ) and Long Wavelength Infrared ( LWIR ) spectra, respectively. We assume the given pair of images are already registered. Let I F be the image, also of m × n pixels, resulting from their fusion. In the wavelet based image fusion, the given images are decomposed at their corresponding approximation ( A ) and detail ( D ) components, which correspond to the low pass and high pass filtering for each decomposition level. These decompositions can be represented through sub-images. The detail representations correspond to the vertical details ( VD ), horizontal details ( HD ) and diagonal details ( DD ), respectively (see Fig. 3). Fig. 1 (right) depicts illustrations of one level DWT decompositions obtained from the original images Fig. 1(left) (different approaches used to decompose the given images are introduced in Section 2.1.1). Once the coefficients (approximations and details) from each decomposition level are obtained, a fusion scheme is applied to catch the most relevant information from each representation. The most widely used fusion schemes proposed in the literature to merge the information are reviewed in Section 2.1.2. Finally, the inverse DWT is applied to the result in order to obtain the sought fused image ( I F ), which is used in the current work as a monocular image to compute the visual odometry. Fig. 2 presents a classical DWT based image fusion pipeline. In order to cope with misalignments, extensions to this basic pipeline have been also proposed in the literature, such as for instance the dual-tree complex wavelet transform [22]. In the current work just DWT is considered since images to be fused are correctly registered. 2.1.1 Discrete Wavelet Transform (DWT) At this section, basic concepts of discrete wavelet transform are introduced. The DWT can be represented as a bank of filters, where at each level of decomposition the given signal is split up into high frequency and low frequency components. The low frequency components can be further decomposed until the desired resolution is reached. If multiple levels of decomposition are applied, it is referred to as multiresolution decomposition. Although there is no rule, in general, in the image fusion problem just one level of decomposition is considered. In the current work, the optimum number for the level of decomposition is found by evaluating different configurations. Several wavelet families have been proposed in the literature, each family has a wavelet function and a scaling function. These two functions can be represented by means of a high pass filter (the wavelet function) and a low pass filter (the scaling function). A wavelet family is normally represented by only its wavelet function [23]. Within each of these families some subclasses exist that depend on the number of vanishing moments in the wavelet function. This is just a mathematical property that can directly relate to the number of coefficients. Each of these wavelet functions and their subclasses represent a different way of decomposing a signal. In the current work, looking for the best visual odometry result under different scenarios (daytime), different wavelet families have been evaluated (see Table 1 ). Details about the evaluation metric approach are presented in Section 2.2. 2.1.2 Fusion strategies Once the given images are split up into the corresponding approximation images and detail images (i.e., horizontal details, vertical details and diagonal details) the fused image ( I F ) is obtained by using a merging scheme that takes into account the approximation and detail information from both images—a correct registration is assumed. Some of the most used merging schemes are summarized below [24]: Substitutive wavelet fusion: in this scheme, the information from one image is completely replaced with information from the other image. In other words, the approximation from one image is merged with the detail of the other image. Once the information is merged the inverse transform is computed to obtain I F . Additive wavelet fusion: as indicated by the name, at this scheme the approximations from one image are added to the other one. The same happens for the detail information. If multiple decompositions were applied, the details at each resolution level are added. Finally, after merging the information the inverse transform is performed resulting in the sough I F . In our implementation, this scheme is implemented by considering the mean value, instead of just the result from the addition. Weighted models: at this scheme a user tuned merging strategy is applied. Depending on the application and the kind of input images approximations and details are combined according to some statistic values ( μ , σ ) or according to some other relevant criteria. This scheme is not considered in the current work because input images are of the same resolution, and the performance of fusion based on DWT of infrared and visible images in a general way is evaluated. Other schemes have been proposed in the literature, which somehow can be considered as combinations of the ones presented above; for instance in this work a strategy that considers the minimum value from each image (approximation or detail images), the maximum value or a random selection was also considered. From the three approaches presented above, in the current work four different options have been considered as fusion strategies: mean (mean value between approximation coefficients and mean value between detail coefficients); max (the coefficients with maximum value are selected, in both cases approximation and details); min (the coefficients with minimum values are selected); rand (coefficients of approximation and details are randomly selected). 2.2 Performance evaluation The main challenge with wavelet based fusion schemes lies on finding the best setup for both, the image decomposition approach (Section 2.1.1) (i.e., number of levels, wavelet family and its configurations) and the fusion strategy (Section 2.1.2) used to merge the information from decomposed images into a single representation. Trying all the possible combinations presented above (combinations from Table 1 plus fusion strategies from 2.1.2), 2600 different configurations can be obtained. Since there is not a clear indication in the literature as to which should be the best configuration, all the possibilities are quantitatively evaluated looking for the best one. In the current work, before computing the visual odometry, a set of pairs of images are first selected and evaluated to find the best DWT based fusion configuration. It is expected that a different setup would be required at each daytime or weather condition. Once this set up is obtained, all the images from the data set are fused and the visual odometry estimated. Quantitative evaluation of fused images has been an active research topic in recent years (e.g., [25,26]). Proposed approaches can be classified into two categories depending on the existence or not of a reference image [25]. In the case a reference image is available, it can be used as a ground truth to evaluate results by means of quality metrics such as Root Mean Square Error (RMSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Mutual Information (MI) among others (e.g., [27,26]). On the other hand, when there is no reference image, the quality of the results is indirectly measured through some metrics such as Entropy (a high entropy value indicates the fused image as rich in information content), Standard Deviation (high values indicate high contrast) and Fusion Mutual Information (the larger the value the better quality fused images) (e.g., [28,29]). Although there is no reference image, in the current work an adaptation of a quality metric based on the Mutual Information (MI) approach recently presented in [28] is considered. This implementation is a fast version of the original one [29]. It evaluates the performance of the fusion algorithm by measuring the amount of information carried from the source images to the fused image by means of mutual information. Our adaptation consists in computing this metric twice, once assuming the visible image as a reference and once with the infrared image as a reference. Then, the average value is considered: (1) FMI VS − IR = ( MI F , VS / ( H F + H VS ) + MI F , IR / ( H F + H IR ) ) / 2 , where MI is the mutual information value and H k , with k = { VS , IR , F } , are the histogram based entropies of the visible, infrared and fused images, respectively, as presented in [29]. 2.3 Monocular visual odometry The fused images obtained with the best configuration as mentioned above are used in the monocular version of the well-known algorithm proposed by Geiger et al. in [19], which is referred to as LibVISO2. The algorithm is briefly presented below, for more details see [19]. Generally, results from monocular systems are up to a scale factor; in other words they lack of a real 3D measure. This problem affects most of monocular odometry approaches. In order to overcome this limitation, LibVISO2 assumes a fixed transformation from the ground plane to the camera (parameters given by the camera height and the camera pitch). These values are updated at each iteration by estimating the ground plane. Hence, features on the ground as well as features above the ground plane are needed for a good odometry estimation. Roughly speaking, the algorithm consists of the following steps: • Compute the fundamental matrix ( F ) from point correspondences using the 8-point algorithm. • Compute the essential matrix ( E ) using the camera calibration parameters. • Estimate the 3D coordinates and [ R | t ] . • Estimate the ground plane from the 3D points. • Scale the [ R | t ] using the values of camera height and pitch obtained in previous step. 3 Experimental results This section presents experimental results and comparisons with classical approaches based on visible spectrum or infrared images. Additionally, comparisons with the results presented in [5] are provided showing the improvements when a better setup is considered for the fusion algorithm. In all the cases GPS information is used as ground truth data to evaluate the performance of evaluated approaches. Below, the acquisition platforms are introduced and then experimental results are depicted. 3.1 Acquisition platforms The proposed approach has been evaluated using images obtained from two different platforms. Fig. 4 (left) shows the electric car with the cross-spectral stereo head used in [5]. The stereo head consists of a pair of cameras arranged in a non verged geometry. One of the camera works in the infrared spectrum, more precisely Long Wavelength Infrared (LWIR), detecting radiations in the range of 8 – 14 μ m . The other camera, which is referred to as Visible Spectrum (VS) responds to wavelengths from about 390 to 750 nm (visible spectrum). Both cameras are synchronized using an external hardware trigger. The images provided by the cross-spectral stereo head are calibrated and rectified using [12]; a process similar to the one presented in [30] is followed. It consists of a reflective metal plate with an overlain chessboard pattern. This chessboard can be visualized in both spectrums making possible the cameras’ calibration and image rectification. With this acquisition platform three video sequences have been obtained (see [5] for more details about them) and used in the current work. They will be referred to as CVC − VidNN 1 1 Data set available at: Fig. 4(right) depicts an illustration of the acquisition cross-spectral system from [29]. In this case, also LWIR and VS images are obtained, but the cameras are arranged differently. In this case a beam splitter (made of Zinc coated Silicon wafer) is used, so that both cameras capture images from the same point of view. Like in the previous case, both cameras are synchronized using an external hardware trigger. A calibration process similar to the one presented above is applied (in this case using squares milled onto a thin cooper board—a printed circuit board). Images are also calibrated and rectified using the toolkit from [12]. With this platform, a data set containing video sequences obtained at six different times of the day has been generated. In the current work, the following three videosequences have been used: (i) 5 AM; (ii) 12 AM; (iii) 10 PM just to evaluate results from scenes that contain different light (from early morning till dark night). These video sequences will be referred to as KAIST − VidNNN . The CVC video sequences were all obtained at day light time (about midday), hence it is difficult to appreciate the advantages of the proposed cross-spectral visual odometry approach. In other words, in the CVC video sequences just a visible spectrum based approach would be enough to compute visual odometry. In order to appreciate the advantages of using the proposed cross-spectral based approach three video sequences from KAIST [29] have been evaluated. These video sequences correspond to different times of day (from early morning till late evening). More precisely, they were obtained at 5 AM, 10 AM and 10 PM. In the three sequences, the car has traveled about 300 m at an average speed of 35 km/h. Results with these two data sets are presented below. 3.2 Visual odometry results In this section, experimental results and comparisons, with the video sequences from the two platforms introduced above, are presented. In order to have a fair comparison, the user defined parameters for the VO algorithm (LibVISO2) have been tuned accordingly to the image nature (visible, infrared, fused) and characteristics of the video sequence. These parameters were empirically obtained looking for the best performance in every image domain. In all the cases, ground truth data from GPS are used for comparisons. Additionally, the average number of matches and percentage of inliers per video sequence evaluated are provided as complementary information; there is no correlation between these values and the final position error. 3.2.1 CVC-Vid00 video sequence It consists of a large curve in a urban scenario. The car travels more than 200 m at an average speed of about 17 km/h. The VO algorithm (LibVISO2) has been tuned as presented in [5]. Fig. 5 depicts the plots corresponding to the four different cases (visible, infrared, fused [5] and fused (proposed approach)) when they are compared with ground truth data (GPS information). Quantitative results corresponding to these four trajectories are presented in Table 2 . VO computed with the visible spectrum video sequence gets the best result since the sequence has been obtained at day light; it can be appreciated that the DWT tuned with the proposed approach (selecting the best configuration using the FMI metric) gets better results than the one presented in [5]. The visual odometry computed with the infrared spectrum video sequence gets the worst results; this is mainly due to the lack of texture in the images. 3.2.2 CVC-Vid01 video sequence It is a simple straight line trajectory on a urban scenario consisting of about 350 m; the car travels at an average speed of about 25 km/h. The (LibVISO2) algorithm has been tuned as presented in [5]. Fig. 6 depicts the plots of the visual odometry computed over each of the four representations (VS, LWIR, DWT based fused images [5] and DWT based fused images with the best setup) together with the corresponding GPS data. Like in the previous case, the visual odometry computed with the infrared video sequence gets the worst result, as can be easily appreciated in Fig. 6 and confirmed by the final position error value presented in Table 3 . The results obtained with the other three representations (visible spectrum, DWT based image fusion [5] and proposed approach) are similar both qualitatively and quantitatively. Once again, like in the previous case, results from the proposed approach are considerably better than the ones obtained from [5], which suggests the need for selecting the best configuration of the fusion’s parameters. 3.2.3 CVC-Vid02 video sequence It is a “L” like shape trajectory on a sub-urban scenario. It is the longest trajectory (370 m) and the car has traveled faster than in the previous cases (about 30 km/h). The (LibVISO2) algorithm has been tuned as presented in [5]. In this particular video sequence, the visible spectrum and both fused based approaches get similar results (see Fig. 7 and Table 4 ). Although the DWT based approach from [5] gets the smallest final position error, the difference with respect to the results obtained in the visible spectrum and the proposed approach is smaller than one meter. On the contrary, it can be appreciated that the traveled distance in [5] is considerably higher, in other words, although [5] gets the smallest final position error, both, the visible spectrum and the proposed approach result in trajectories quite similar to the one obtained with the GPS. As a conclusion from the CVC-video sequences, it can be appreciated that, although they correspond to day light sequences, the usage of fused images results in quite stable solutions. The best setup of the DWT based image fusion corresponds to Reverse of the Biorthogonal wavelet family with vanishing moments (Nr = 2 and Nd = 8) for the DWT (rbio2.8 in Table 1); and (mean, max) for the fusion strategy (see Section 2.1.2). The same setup for the fusion configuration has been used in the three video sequences. This setup has been empirically obtained by evaluating quantitatively a set of frames of CVC video sequence using the FMI metric presented in Section 2.2. 3.2.4 KAIST-5AM video sequence This is the first video sequence where the advantages of cross-spectral based approaches can be easily appreciated. At this daytime, although there is already a little bit of light, results from visible spectrum are the worst. On the contrary, the visual odometry computed with the infrared video sequence gets better results. Finally, the best VO results are obtained with the images fused with the proposed approach (selecting the best configuration of fusion’s parameters). It should be noticed that the results (final position error) obtained with the proposed approach are two times better than the one from visible spectrum and almost 50% better than infrared spectrum (see quantitative values in Table 5 ). The estimated trajectories can be found in Fig. 8 ; it can be appreciated how the results from the proposed approach keep almost attached to the GPS trajectory. Regarding the fusion strategy, the best configuration corresponds to Biorthogonal wavelet family with vanishing moments (Nr = 3 and Nd = 5) for the DWT (bio3.5 in Table 1); and (max, min) for the fusion strategy (see Section 2.1.2). 3.2.5 KAIST-10AM video sequence On the contrary to the previous case, here the results are similar to the ones obtained with the CVC video sequences. It makes sense since the sequence has been obtained at day light time (10 AM). In other words, visible spectrum and cross-spectral based approaches reach to similar results. This can be quantitatively appreciated looking at Fig. 9 . Quantitative values are provided in Table 6 . As expected, infrared based visual odometry gets the worst results. Like in the CVC video sequence, in this case the best setup of the DWT based image fusion corresponds to Reverse of the Biorthogonal wavelet family with vanishing moments (Nr = 2 and Nd = 8) for the DWT (rbio2.8 in Table 1); and (mean, min) for the fusion strategy (see Section 2.1.2). 3.2.6 KAIST-10PM video sequence Finally, this last sequence corresponds to a quite dark night, which is a challenging scenario for visible spectrum video sequence. Like in the KAIST-5AM video sequence, the worst result is obtained using visible spectrum images. This worst result is due to the lack of light in the scenario, just car’s light is present. Visual odometry computed with infrared video sequence gets quite acceptable results. The best VO result was obtained with images fused with the proposed approach (by selecting the best configuration for the fusion’s parameters); in this case, VO from fused images, the final position error (see Table 7 ) is two times smaller than the obtained with infrared images and more than three times when compared with the result from visible spectrum video sequence. The resulting trajectories can be appreciated in Fig. 10 . 4 Conclusion The manuscript evaluates the performance of a classical monocular visual odometry when cross-spectral fused images are used. The best fusion strategy is selected by using a novel mutual information based metric. The obtained visual odometry results are compared with a previous approach as well as with classical ones (based on visible and infrared spectrum, respectively). While at day light time the performance of classical visible spectrum based approach is quite similar to the one obtained with proposed approach, results show that the proposed approach is the best option to tackle challenging scenarios, in particular those with dark or poor lighting conditions. As a future work, other challenging scenarios, including different weather conditions (fog and rain), will be evaluated. Acknowledgments This work has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R; the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”; Ecuador; the ESPOL project Pattern recognition: case study on agriculture and aquaculture (M1-D1-2015); and the “Secretaria d’ Universitats i Recerca del Departament d’ Economia i Coneixement de la Generalitat de Catalunya” (2014-SGR-1506). C. Aguilera has been supported by the Universitat Autònoma de Barcelona (09-2013,09-2017). M. Oliveira has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia (FTC)”, under grant agreement SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER- 006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. References [1] D. Borrmann A. Nüchter M. akulović I. Maurović I. Petrović D. Osmanković J. Velagić A mobile robot based system for fully automated thermal 3D mapping Adv. Eng. Inf. 28 4 2014 425 440 [2] P. Shah B.C.S. Reddy S.N. Merchant U.B. Desai Context enhancement to reveal a camouflaged target and to assist target localization by fusion of multispectral surveillance videos Signal Image Video Process. 7 3 2013 537 552 [3] T. Bourlai N. Kalka A. Ross B. Cukic L. Hornak Cross-spectral face verification in the short wave infrared (swir) band 2010 20th International Conference on Pattern Recognition (ICPR) 2010 IEEE 1343 1347 [4] Y. Choi, et al. All-day visual place recognition: Benchmark dataset and baseline, in: IEEE International Conference on Computer Vision and Pattern Recognition Workshops, CVPRWVPRICE, 2015. [5] J. Poujol C. Aguilera E. Danos B. Vintimilla R. Toledo A.D. Sappa Visible-thermal fusion based monocular visual odometry ROBOT’2015: Second Iberian Robotics Conference Advances in Intelligent Systems and Computing vol. 417 2015 Springer Verlag Lisbon, Portugal 517 528 [6] D. Nistér, O. Naroditsky, J. Bergen, Visual odometry, in: IEEE Intgernational Conference on Computer Vision and Pattern Recognition, vol. 1, 2004, pp. I–652. [7] D. Scaramuzza, F. Fraundorfer, R. Siegwart, Real-time monocular visual odometry for on-road vehicles with 1-point RANSAC, in: IEEE International Conference on Robotics and Automation, 2009, pp. 4293–4299. [8] D. Scaramuzza F. Fraundorfer Visual odometry [tutorial] IEEE Robot. Autom. Mag. 18 4 2011 80 92 [9] A. Chilian H. Hirschmüller Stereo camera based navigation of mobile robots on rough terrain IEEE International Conference on Intelligent Robots and Systems IROS 2009 IEEE 4571 4576 [10] E. Nilsson C. Lundquist T. Schön D. Forslund J. Roll Vehicle motion estimation using an infrared camera 18th IFAC World Congress, Milano, Italy, 28 August-2 September, 2011 2011 Elsevier 12952 12957 [11] T. Mouats N. Aouf A.D. Sappa C.A. Aguilera-Carrasco R. Toledo Multispectral stereo odometry IEEE Trans. Intell. Transp. Syst. 16 3 2015 1210 1224 [12] J.-Y. Bouguet, Camera calibration toolbox for matlab, July 2010. [13] P. Ricaurte C. Chilán C.A. Aguilera-Carrasco B.X. Vintimilla A.D. Sappa Feature point descriptors: Infrared and visible spectra Sensors 14 2 2014 3690 3701 [14] C. Aguilera F. Barrera F. Lumbreras A.D. Sappa R. Toledo Multispectral image feature points Sensors 12 9 2012 12661 12672 [15] L. Dong Q. Yang H. Wu H. Xiao M. Xu High quality multi-spectral and panchromatic image fusion technologies based on curvelet transform Neurocomputing 159 2015 268 274 [16] Z. Wang D. Ziou C. Armenakis D. Li Q. Li A comparative analysis of image fusion methods IEEE Trans. Geosci. Remote Sens. 43 6 2005 1391 1402 [17] R. Gharbia, A.T. Azar, A.H.E. Baz, A.E. Hassanien, Image fusion techniques in remote sensing, CoRR abs/1403.5473. [18] A.E. Hayes, G.D. Finlayson, R. Montagna, Rgb-nir color image fusion: metric and psychophysical experiments, in: Proc. SPIE, vol. 9396, 2015. [19] A. Geiger, J. Ziegler, C. Stiller, Stereoscan: Dense 3D reconstruction in real-time, in: Intelligent Vehicles Symposium (IV), 2011. [20] M. Lang H. Guo J.E. Odegard C.S. Burrus R. Wells Jr. Noise reduction using an undecimated discrete wavelet transform IEEE Signal Process. Lett. 3 1 1996 10 12 [21] T. Chang C.J. Kuo Texture analysis and classification with tree-structured wavelet transform IEEE Trans. Image Process. 2 4 1993 429 441 [22] A. Barri A. Dooms P. Schelkens The near shift-invariance of the dual-tree complex wavelet transform revisited J. Math. Anal. Appl. 389 2 2012 1303 1314 [23] K. Amolins Y. Zhang P. Dare Wavelet based image fusion techniques — An introduction, review and comparison ISPRS J. Photogramm. Remote Sens. 62 2007 249 263 [24] I. Mehra N.K. Nishchal Wavelet-based image fusion for securing multiple images through asymmetric keys Opt. Commun. 335 2015 153 160 [25] P. Jagalingam A.V. Hegde A review of quality metrics for fused image Aquat. Proc. 4 2015 133 142 International Conference on Water Resources, Coastal and Ocean Engineering (ICWRCOE’15) [26] W. Zhou A. Bovik H. Sheikh E. Simoncelli Image quality assessment: from error visibility to structural similarity IEEE Trans. Image Process. 13 4 2004 600 612 [27] C. Yang J.-Q. Zhang X.-R. Wang X. Liu A novel similarity based quality metric for image fusion Inf. Fusion 9 2 2008 156 160 [28] M. Haghighat, M. Razian, Fast-fmi: Non-reference image fusion metric, in: IEEE 8th International Conference on Application of Information and Communication Technologies, 2014, pp. 1–3. [29] M.B.A. Haghighat A. Aghagolzadeh H. Seyedarabi A non-reference image fusion metric based on mutual information of image features Comput. Electr. Eng. 37 5 2011 744 756 Special Issue on Image Processing [30] F. Barrera F. Lumbreras A.D. Sappa Multimodal stereo vision system: 3D data extraction and algorithm evaluation IEEE J. Sel. Top. Sign. Proces. 6 5 2012 437 446 Angel Domingo Sappa received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, Spain, where he is currently a Senior Researcher, member of the Advanced Driver Assistance Systems Group. Since 2016 he is also with the Electrical and Computer Science Engineering school of the ESPOL, Guayaquil, Ecuador, as an invited Full Professor. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereo image processing and analysis, 3D modeling, dense optical flow estimation and multispectral imaging. Cristhian Aguilera received the B.S. degree in automation engineer from the Universidad del Bío-Bío Concepción, Chile, in 2008 and the M.Sc. degree in computer vision from the Autonomous University of Barcelona, Barcelona, Spain, in 2014. He is currently working towards the Ph.D. degree in computer science from the Autonomous University of Barcelona. Since 2015, he is an editor assistant of the Electronic Letter on Computer Vision and Image Analysis journal. His current research focuses in cross-spectral image similarity, stereo vision and deep convolutional networks. Juan Carvajal Ayala received the Bachelor’s degree in Electronic Communications Engineering from the University of Navarra School of Engineering, San Sebastian, Spain, in 2014. He did a research internship at Fraunhofer IIS in Erlangen, Germany, and is currently a research assistant at Center for Research, Development and Innovation of Computer Systems (CIDIS), ESPOL. His research interests are image fusion, pattern recognition, and deep learning. Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, respectively. Later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of Autonomous Driving and Drivers Assistance Systems. Currently he is a researcher at both the Institute for Systems and Computer Engineering,Technology and Science in Porto, Portugal, as well as the Institute of Electronics and Telematics Engineering of Aveiro, Portugal. In addition, he is an assistant professor at the Department of Mechanical Engineering, University of Aveiro, Portugal, where he teaches computer vision courses. His research interests include visual object recognition in open-ended domains, scene reconstruction from multi-modal sensor data, image and 3D data processing, computer vision and robotics. Dennis G. Romero received the Computer Engineering degree from Escuela Superior Politécnica del Litoral, ESPOL, Guayaquil, Ecuador, in 2007, and the Ph.D. degree in Electrical Engineering from Universidade Federal do Espírito Santo, UFES, Vitória, Brazil, in 2014. In 2014, he joined the Center for Research, Development and Innovation of Computer Systems (CIDIS). He is a member of the Pattern recognition research group at ESPOL. His research interests center on improving the data understanding from sensor fusion, mainly through the application of machine learning and statistics for pattern recognition. His current research focuses on Pattern Recognition from microscope images of shrimps for identification of diseases. Boris X. Vintimilla received his degree in mechanical engineering in 1995 at the Escuela Superior Politécnica del Litora—ESPOL, Guayaquil, Ecuador, and his Ph.D. degree in industrial engineering in 2001 at the Polytechnic University of Catalonia, Barcelona, Spain. In May 2001, he joined the Department of Electrical and Computer Science Engineering of the ESPOL as associated professor and in 2008 became a full professor. Dr. Vintimilla has been the director of the Center of Vision and Robotics from 2005 to 2008. He did his post-doctorate research in the Digital Imaging Research Center at Kingston University (London, UK) from 2008 to 2009. Currently, he is director of the Center for Research, Development and Innovation of Computer Systems (CIDIS) at ESPOL. His research areas include topics related with image processing and analysis, and vision applied to mobile robotics. Dr. Vintimilla has been involved in several projects supported by international and national organizations, as result of these researches he has published more than 40 scientific articles and book chapters. Ricardo Toledo received the degree in Electronic Engineering from the Universidad Nacional de Rosario (Argentina) in 1986, the M.Sc. degree in image processing and artificial intelligence from the Universitat Autònoma de Barcelona (UAB) in 1992 and the Ph.D. in 2001. Since 1989 he was lecturer in the Computer Science Dept. of the UAB, and was involved in R + D projects. In 1996, he participated in the foundation of the Computer Vision Center (CVC) at the UAB. Currently, he is a full time associated professor at the Computer Science Dept., Cordinator of the Master in Informatics at the Escola d’Enginyeria (UAB) and member of the Computer Vision Centre. Ricardo has participated in several national and international/EU R + D projects, being the leader of some of them, is author/co-author of more than 40 papers, all these in the field of computer vision, robotics and medical imaging and has supervised several Master and Ph.D. thesis. "
    },
    {
        "doc_title": "GOOD: A global orthographic object descriptor for 3D object recognition and manipulation",
        "doc_scopus_id": "84994232513",
        "doc_doi": "10.1016/j.patrec.2016.07.006",
        "doc_eid": "2-s2.0-84994232513",
        "doc_date": "2016-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Classification performance",
            "Disambiguation method",
            "Distribution matrices",
            "Object perception",
            "Object representations",
            "Orthographic projections",
            "Real-time application"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure robustness, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. We propose a novel sign disambiguation method, for computing a unique reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the target object view captured by a 3D sensor. Three principal orthographic projections and their distribution matrices are computed by exploiting the object reference frame. The descriptor is finally obtained by concatenating the distribution matrices in a sequence determined by entropy and variance features of the projections. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications. The estimated object's pose is precise enough for real-time object manipulation tasks.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2016-07-20 2016-07-20 2016-11-03 2016-11-03 2017-03-08T23:40:18 S0167-8655(16)30168-4 S0167865516301684 10.1016/j.patrec.2016.07.006 S300 S300.3 FULL-TEXT 2017-03-08T21:25:26.072496-05:00 0 0 20161101 2016 2016-07-20T22:11:34.444637Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb vol volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0167-8655 01678655 true 83 83 P3 Volume 83, Part 3 11 312 320 312 320 20161101 1 November 2016 2016-11-01 2016 Efficient Shape Representation, Matching, Ranking, and its Applications Xiang Bai Michael Donoser Hairong Liu Longin Jan Latecki article sco © 2016 Elsevier B.V. All rights reserved. GOODAGLOBALORTHOGRAPHICOBJECTDESCRIPTORFOR3DOBJECTRECOGNITIONMANIPULATION KASAEI S 1 Introduction 2 Related work 3 Local reference frame 4 Object descriptor 5 Experimental results 5.1 Descriptiveness 5.2 Scalability 5.3 Robustness 5.3.1 Gaussian noise 5.3.2 Varying point cloud density 5.4 Efficiency 5.4.1 Memory footprint 5.4.2 Computation time 5.5 System demonstration 6 Conclusion Acknowledgments References ALDOMA 2012 A ANDREOPOULOS 2013 827 891 A BO 2011 1729 1736 L IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR2011 OBJECTRECOGNITIONHIERARCHICALKERNELDESCRIPTORS BRO 2008 135 140 R CHEN 2007 1252 1262 H COVER 2012 T ELEMENTSINFORMATIONTHEORY DENG 2009 248 255 J IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION2009CVPR2009 IMAGENETALARGESCALEHIERARCHICALIMAGEDATABASE DINH 2006 863 870 H IEEECOMPUTERSOCIETYCONFERENCECOMPUTERVISIONPATTERNRECOGNITION2006 MULTIRESOLUTIONSPINIMAGES FROME 2004 224 237 A COMPUTERVISIONECCV2004 RECOGNIZINGOBJECTSINRANGEDATAUSINGREGIONALPOINTDESCRIPTORS GUO 2013 86 93 Y GRAPPIVAPP TRISIADISTINCTIVELOCALSURFACEDESCRIPTORFOR3DMODELINGOBJECTRECOGNITION HORN 1984 1671 1686 B JOHNSON 1999 433 449 A KASAEI 2015 537 553 S LAI 2011 1817 1824 K ROBOTICSAUTOMATIONICRA2011IEEEINTERNATIONALCONFERENCE ALARGESCALEHIERARCHICALMULTIVIEWRGBDOBJECTDATASET MARTON 2010 365 370 Z 10THIEEERASINTERNATIONALCONFERENCEHUMANOIDROBOTSHUMANOIDS2010 HIERARCHICALOBJECTGEOMETRICCATEGORIZATIONAPPEARANCECLASSIFICATIONFORMOBILEMANIPULATION MIAN 2010 348 361 A MILLER 1995 39 41 G OLIVEIRA 2014 2216 2223 M IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS20142014 APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS OLIVEIRA 2015 M OSADA 2002 807 832 R PANG 2015 171 179 G INTERNATIONALCONFERENCE3DVISION3DV2015 FASTROBUSTMULTIVIEW3DOBJECTRECOGNITIONINPOINTCLOUDS PASQUALOTTO 2013 608 623 G REGAZZONI 2014 719 728 D RUSU 2009 3212 3217 R IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATION2009ICRA09 FASTPOINTFEATUREHISTOGRAMSFPFHFOR3DREGISTRATION RUSU 2010 2155 2162 R IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS2010 FAST3DRECOGNITIONPOSEUSINGVIEWPOINTFEATUREHISTOGRAM RUSU 2009 47 54 R IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS2009 DETECTINGSEGMENTINGOBJECTSFORMOBILEMANIPULATION RUSU 2008 927 941 R SU 2015 945 953 H PROCEEDINGSIEEEINTERNATIONALCONFERENCECOMPUTERVISION MULTIVIEWCONVOLUTIONALNEURALNETWORKSFOR3DSHAPERECOGNITION TOMBARI 2010 356 369 F COMPUTERVISIONECCV2010 UNIQUESIGNATURESHISTOGRAMSFORLOCALSURFACEDESCRIPTION WOHLKINGER 2011 2987 2992 W IEEEINTERNATIONALCONFERENCEROBOTICSBIOMIMETICSROBIO2011 ENSEMBLESHAPEFUNCTIONSFOR3DOBJECTCLASSIFICATION WU 2015 1912 1920 Z PROCEEDINGSIEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION 3DSHAPENETSADEEPREPRESENTATIONFORVOLUMETRICSHAPES ZHONG 2009 689 696 Y IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS2009 INTRINSICSHAPESIGNATURESASHAPEDESCRIPTORFOR3DOBJECTRECOGNITION KASAEIX2016X312 KASAEIX2016X312X320 KASAEIX2016X312XS KASAEIX2016X312X320XS 2018-11-03T00:00:00.000Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0167-8655(16)30168-4 S0167865516301684 10.1016/j.patrec.2016.07.006 271524 2017-03-08T21:25:26.072496-05:00 2016-11-01 true 2164112 MAIN 9 55193 849 656 IMAGE-WEB-PDF 1 gr1 15364 164 203 gr2 9775 83 219 gr3 14154 141 219 gr4 7518 52 219 gr5 12316 91 219 gr6 8243 54 219 gr7 11732 78 219 gr8 7807 76 219 gr9 15100 86 219 gr1 43088 311 385 gr2 59290 305 809 gr3 33529 246 383 gr4 57188 194 808 gr5 17361 131 316 gr6 66223 200 810 gr7 17689 119 334 gr8 20773 134 385 gr9 24998 150 382 gr1 307699 1377 1704 gr2 563764 1351 3583 gr3 301966 1090 1697 gr4 501105 857 3576 gr5 276100 581 1402 gr6 558076 885 3587 gr7 271243 527 1478 gr8 165674 595 1705 gr9 226874 667 1693 si1 224 14 46 si10 171 12 22 si11 174 12 22 si12 265 11 58 si13 184 16 20 si14 200 20 28 si15 186 17 28 si16 901 22 315 si17 409 13 90 si18 1178 56 198 si19 341 18 83 si2 507 17 143 si20 127 13 23 si21 141 15 24 si22 198 16 63 si23 601 16 145 si24 272 15 44 si25 562 20 132 si26 581 17 173 si27 580 17 173 si28 1151 53 244 si29 1183 53 245 si3 623 52 106 si30 149 24 10 si31 245 24 46 si32 272 24 46 si33 176 13 20 si34 1150 16 322 si35 962 52 206 si36 168 11 22 si37 460 17 93 si38 271 16 59 si39 972 52 207 si4 162 13 17 si40 189 12 26 si41 596 58 112 si42 340 19 57 si43 219 13 44 si44 853 16 261 si5 1066 52 228 si6 382 16 84 si7 461 16 122 si8 699 17 162 si9 146 11 16 PATREC 6593 S0167-8655(16)30168-4 10.1016/j.patrec.2016.07.006 Elsevier B.V. Fig. 1 Visualization of sign disambiguation procedure: (a) orthographic projection of the object on the XoZ and XoY planes; (b) XoY plane is used to determine the sign of Y axis; (c) XoZ plane is used to determine the sign of X axis. The red, green and blue lines represent the unambiguous X, Y, Z axes respectively. Fig. 1 Fig. 2 An illustrative example of the producing a GOOD shape description for a mug object (i.e. d = 5 ): (a) The mug object and its bounding box, reference frame and three projected views; the object’s points are then projected onto three planes; therefore, XoZ (b), YoZ (c) and XoY projections (d) are created. Each plane is partitioned into bins and the number of point falling into each bin is counted. Accordingly, three distribution matrices are obtained for the projections; afterwards, each distribution matrix is converted to a distribution vector, (i.e. (e), (f) and (g)) and two statistic features including entropy and variance are then calculated for each distribution vector; (h) the distribution vectors are consequently concatenated together using the statistics features, to form a single description for the given object. The ordering of the three distribution vectors is first by decreasing values of entropy. Afterwards the second and third vectors are sorted again by increasing values of variance. Fig. 2 Fig. 3 Example of how the projections used to build GOOD can also be used for extracting features relevant for object manipulation (see text): (a) Local reference frame and projections; (b) Projections in multi-view layout. Fig. 3 Fig. 4 Object recognition performance in descriptiveness and scalability experiments; (left) effect of number of bins on performance; (center) scalability of the selected descriptors with respect to varying numbers of categories in the dataset as a function of accuracy vs. Number of categories; (right) scalability experiment time vs. Number of categories. Fig. 4 Fig. 5 An illustration of a Vase object with different levels of Gaussian noise. Fig. 5 Fig. 6 The robustness of the selected descriptors to different level of Gaussian noise and varying point cloud density: (left) different levels of Gaussian noise applied to the test; (center) different levels of downsampling applied to the test data; (right) different levels of downsampling applied to the train data. Fig. 6 Fig. 7 An illustration of a Flask object with different levels of downsampling. Fig. 7 Fig. 8 Average computation time of the selected descriptors on 20 randomly selected objects from the RGB-D dataset. Fig. 8 Fig. 9 Two snapshots showing the object perception system performing object recognition and pose estimation using the GOOD descriptor; (left) the instructor puts a Mug and a Vase on the table. The gray bonding boxes and red, green and blue lines signal the pose of the object and the GOOD descriptions are visualized and computed; this frame shows that the system is able to compute the GOOD description and estimate pose of objects in the scene. Moreover, it demonstrates that the Vase and the Mug are properly recognized; (right) A Plate enters the scene. Its shape description and pose are computed and visualized. Because there is no prior knowledge about plates, it is classified as Unknown [13]. Fig. 9 Table 1 Summary of descriptiveness experiments. Table 1 Number of bins Descriptor size Memory (Kb) Accuracy 5 75 0.3 0.92 10 300 1.2 0.93 15 675 2.7 0.94 20 1200 4.8 0.93 25 1875 7.5 0.94 30 2700 10.8 0.93 35 3675 14.7 0.93 40 4800 19.2 0.93 45 6075 24.3 0.93 50 7500 30.0 0.93 Table 2 Summary of scalability experiments. Table 2 Number of categories Accuracy GOOD(5bins) GOOD(15bins) VFH ESF GFPFH GRSD 5 0.96 0.98 0.98 0.97 0.90 0.97 10 0.94 0.95 0.96 0.96 0.71 0.92 15 0.95 0.95 0.97 0.98 0.75 0.80 20 0.95 0.95 0.97 0.96 0.71 0.81 25 0.95 0.95 0.97 0.97 0.61 0.79 30 0.94 0.94 0.96 0.94 0.61 0.77 35 0.94 0.94 0.94 0.96 0.59 0.76 40 0.93 0.94 0.94 0.93 0.59 0.72 45 0.92 0.94 0.94 0.93 0.60 0.69 50 0.92 0.94 0.94 0.93 0.59 0.68 Table 3 Summary of robustness to Gaussian noise experiments. Table 3 Gaussian noise(mm) Accuracy GOOD 5bins VFH ESF GFPFH GRSD 1 0.94 0.95 0.90 0.40 0.66 2 0.93 0.93 0.74 0.17 0.61 3 0.92 0.93 0.49 0.10 0.51 4 0.91 0.94 0.32 0.09 0.35 5 0.89 0.91 0.24 0.09 0.26 6 0.85 0.85 0.23 0.09 0.19 7 0.83 0.78 0.23 0.09 0.12 8 0.78 0.57 0.22 0.09 0.08 9 0.69 0.42 0.23 0.09 0.10 10 0.67 0.36 0.22 0.09 0.09 Table 4 Length of selected 3D shape descriptors. Table 4 No. Descriptor Feature length (float) Adjustable length Implementation 1 GFPFH 16 No PCL 1.7 2 GRSD 21 No PCL 1.8 3 GOOD 75 Yes – 4 VFH 308 No PCL 1.7 5 ESF 640 No PCL 1.7 ☆ This paper has been recommended for acceptance by Gabriella Sanniti di Baja. GOOD: A global orthographic object descriptor for 3D object recognition and manipulation S. Hamidreza Kasaei ⁎ a Ana Maria Tomé a b Luís Seabra Lopes a b Miguel Oliveira c a IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Aveiro, 3810-193, Portugal IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro Universidade de Aveiro Aveiro 3810-193 Portugal b Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática Universidade de Aveiro Portugal c Instituto de Engenharia de Sistemas e Computadores, Tecnologia e Ciência R. Dr. Roberto Frias, 465, Porto 4200, Portugal Instituto de Engenharia de Sistemas e Computadores Tecnologia e Ciência R. Dr. Roberto Frias 465 Porto 4200 Portugal ⁎ Corresponding author. Tel.: +351 234 370 500; fax: +351 234 370 545. Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure robustness, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. We propose a novel sign disambiguation method, for computing a unique reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the target object view captured by a 3D sensor. Three principal orthographic projections and their distribution matrices are computed by exploiting the object reference frame. The descriptor is finally obtained by concatenating the distribution matrices in a sequence determined by entropy and variance features of the projections. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications. The estimated object’s pose is precise enough for real-time object manipulation tasks. Keywords 3D object recognition Object Perception Orthographic projection 1 Introduction Following the advent of inexpensive depth sensing devices such as Microsoft Kinect or the ASUS Xtion, which record RGB and depth information, the use of 3D data is becoming increasingly popular. One of the primary goals in service robotics is to develop reliable capabilities in the area of perception that will allow robots to robustly recognize objects and interact with the environment by manipulating those objects. For this purpose, a robot must reliably recognize the object. Furthermore, in order to interact with human users, this process of object recognition cannot take more than a fraction of a second. Although many object recognition methods for both 2D and 3D data have been proposed [2], recognizing 3D objects in the presence of noise and variable point cloud resolution is still a challenging task. However, 3D data contains more information about the spatial positioning of objects, which in turn eases the process of object segmentation. Moreover, depth data is more robust than RGB data to the effects of illumination and shadows [25]. Therefore, 3D data can be employed to describe the surface of the objects based on geometric properties 1 1 2D data can also be used to distinguish objects that have same geometric properties with different texture (e.x. a Coke can from a Diet Coke can). . A 3D object recognition system is composed of several software modules such as Object Detection, Object Representation, Object Recognition and Perceptual Memory. Object Detection is responsible for detecting all objects in a scene. Object representation is concerned with the calculation of a set of features for the detected object, which are send to the Object Recognition. Objects are recognized by comparing their description against the descriptions of known objects (stored in the Perceptual Memory). Object Representation plays a prominent role because the output of this module is used for learning as well as recognition. Existing 3D object representation approaches are based on either global or local descriptors. Global descriptors encode the entire 3D object, while local descriptors represent a small area of an object around a specific keypoint. Generally, global descriptors are increasingly used in the context of 3D object recognition, object manipulation, as well as geometric categorization [1]. These must be efficient in terms of computation time as well as memory, to facilitate real-time performance. For example, Ensemble of Shape Functions (ESF) [32], Global Fast Point Feature Histogram (GFPFH) [28], Viewpoint Feature Histogram (VFH) [27] and Global Radius-based Surface Descriptor (GRSD) [16], are global descriptors. Local descriptors tend to handle occlusion and clutter better when compared to global descriptor. However, comparing 3D object views based on their local features tends to be computationally more expensive [1]. Examples in this category include Spin Images (SI) [12], Signature of Histograms of Orientations (SHOT) [31], Fast Point Feature Histogram (FPFH) [26] and Hierarchical Kernel Descriptors [3]. Invariance to the pose of an object is a critical property of any 3D shape descriptor. A number of 3D shape descriptors achieve pose invariance using either a reference axis only (e.x. Spin-Images [12]) or a complete object reference frame (e.x. Intrinsic Shape Signatures [34]). In this paper, a new global 3D shape descriptor named GOOD (i.e. Global Orthographic Object Descriptor) is presented. GOOD provides an appropriate trade-off between descriptiveness, computation time and memory usage. The descriptor is designed to be scale and pose invariant, informative and stable, with the objective of supporting accurate 3D object recognition. A novel sign disambiguation method is proposed to compute a unique and repeatable reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the object. Using this reference frame, three three principal projections, namely XoZ, XoY and YoZ, are created based on orthographical projection. The space of each projection is partitioned into bins and the number of points falling into each bin is counted. From this, three distribution matrices are obtained for the projected views. Two statistic features, namely entropy and variance are then calculated for each distribution matrix. The distribution matrices are consequently concatenated together using the entropy and variance features to form a single description for the given object view. In this paper, we assume that an object has already been segmented from the point cloud of the scene, and we will focus on detailing the 3D object descriptor. This descriptor works directly on 3D point clouds and requires neither triangulation of the object’s points nor surface meshing. For additional details on the object detection and object recognition methodologies, we refer the reader to our previous works on interactive open-ended learning for 3D object recognitions [13,19,20]. The contributions presented in this paper are the following: (i) design a new sign disambiguation method to compute a unique and unambiguous complete local reference frame, from the eigenvectors obtained through Principal Component Analysis of the segmented point cloud of the object and (ii) a novel global object descriptor computed using that local reference frame, that provides a good trade-off between descriptiveness, computation time and memory usage. The remainder of this paper is organized as follows. In Section 2, we discuss related works. The methodology for computing the local reference frame is presented in Section 3. Section 4 describes the proposed global object descriptor. Evaluation of the proposed shape descriptor is presented in Section 5. Finally, in Section 6, conclusions are presented and future research is discussed. 2 Related work Three-dimensional shape description has been under investigation for a long time in various research fields, such as pattern recognition, computer graphics and robotics. Although an exhaustive survey of shape descriptor is beyond the scope of this paper, we will review a few recent efforts. As previously mentioned, some descriptors use LRF to compute a pose invariant description. Therefore, this property can be used to categorize 3D shape descriptors into two categories including (i) shape descriptors without LRF; (ii) shape descriptors with LRF. Most of the shape descriptors of the first category use certain statistic features or geometric properties of the points on the surface like depth value, curvature and surface normal to generate a description. For instance, Shape Distributions descriptor [21] represents an object as a shape distribution sampled from a shape function measuring global geometric properties of the object. Extended Gaussian Images (EGI) descriptor [11] is based on the distribution of surface normals on the Gaussian sphere. Since descriptiveness of the EGI depends on the shape of the object and it is not suitable for non-convex object. Chen and Bhanu [5] proposed a local surface patch (LSP) descriptor that encodes the shape of objects by accumulating points in particular bins along the two dimensions that are the shape index value and the cosine of the angle between the surface normals. Wohlkinger and Vincze [32] introduced a global shape descriptor called Ensemble of Shape Function (ESF) that does not require the use of normals to describe the object and the characteristic properties of an object is represented using an ensemble of ten 64-bin histograms of angle, point distance, and area shape functions. Point Feature Histogram (PFH) [29] can be used as local or global shape descriptor. The PFH represents the relative orientation of normals, as well as distances, between point pairs. For each point p, k-neighborhood points are selected based on a sphere centered at p with radius r. Afterwards, a surface normal for each point is estimated. Subsequently, four features are calculated for every pair of points using their surface normals, positions and angular variations. In a later work [26], in order to improve the robustness of PFH in case of point densities variations, the distance between point pairs is excluded from the histogram of PFH. The computation complexity of a PFH is O(n 2), where n is the number points in the point cloud. Fast Point Feature Histogram (FPFH) [26] is an extension version of PFH. The FPFH estimates the sets of values only between every point and its k nearest neighbors. This is different from PFH, where all pairs of points in the support region are considered. Therefore, the computational complexity is reduced to O(k.n). The FPFH is a scale and pose invariant descriptor which is not suitable for grasping. Viewpoint Feature Histogram (VFH) [27] is another extension of PFH descriptor. The VFH descriptor computes the same angular features as the PFH. Additionally, it computes another statistics between the central viewpoint direction and the normals estimated at each point. The VFH shape descriptor produces a single histogram that encodes the geometry of the whole object and its viewpoint. Because of the global nature of VFH, the computational complexity of VFH is O(n). The descriptiveness of the above shape descriptors are limited because the 3D spatial information either is not taken into account or it is discarded during the description process. Unlike the above approaches, some researchers have recently adopted deep learning algorithms for 3D object representation, learning and recognition [15,30,33]. These works use a collection of 2D images rendered from different view points to learn a shape representation that aggregates information from input views and provides a compact shape descriptor. As it was pointed out in [33], training a deep artificial neural network for 3D object representation requires a large collection of 3D objects to provide accurate representations and typically involves long training times. In contrast, the shape descriptors in the second category encode the spatial information of the objects’ points using a Local Reference Frame (LRF). Some descriptors provide a description using only a Reference Axis. For example, Spin-Images [12] uses surface normal of a vertex as a reference axis and proposed a spin image representation by projecting the surface points to the tangent plane of the vertex. Then, each projected point is represented by a pair (α, β), where α is the distance to the surface normal, i.e., the radius, and β is the perpendicular distance from the point to the tangent plane. Consequently, a histogram is formed by counting the occurrences of different discretized distance pairs. Spin images descriptor has been successfully used in many applications, but one limitation of this descriptor is that it is not scale invariant. Dinh and Kropac [8] proposed multi-resolution pyramids of spin images in order to improve the discrimination of the original spin image and speed up the matching process. Some variants of the spin image shape descriptor also presented such as Tri-Spin-Image descriptor (TriSI) [10] and color spin image [23]. Similar to the SI, 3D Shape Context (3DSC) [9] uses the surface normal of an basis point as its LRF. The 3DSC descriptor is calculated by counting the weighted number of points falling into each bin of an sphere grid centered on the basis point and its north pole oriented with the surface normal. The sphere grid is constructed based on dividing the support area into bins by logarithmically spaced boundaries along the radial dimension and equally spaced boundaries in the azimuth and elevation dimensions. Whenever only an axis is used as a reference frame, there is an uncertainty in the rotation around the axis that should be handled for generating a robust and repeatable description. In order to eliminate this issue, several descriptors (e.g. 3D Shape Context) proposed to compute multiple descriptions for different possible rotations of the object. Since this kind of solutions are caused increasing the computational cost in terms of both execution time as well as memory usage, they are not optimum and real solutions. Furthermore, the recognition process becomes not only significantly slow, but also more laborious. Differently, Zhong [34] proposed a shape descriptor namely Intrinsic Shape Signatures (ISS) using defining a LRF based on the eigenvectors of the scatter matrix of the point cloud of the object and describing the point distribution in the spherical angular space. Similar to Zhong work, Mian et al. [17] introduced LRF computed with eigenvectors of the covariance matrix of the object’s points. However, in both cases the eigenvectors define the principal directions of the data, their sign is not defined unambiguously. Accordingly, different descriptors can be generated for the object. As highlighted before, they are neither computationally efficient nor repeatable. [22] proposed a multi-view 3D object recognition approach. In this approach, each object is projected into 46 projection planes distributed on a sphere, whereas we just compute three principal orthographic projections. Their object representation is clearly not efficient for real time application like robotics. In order to achieve true rotation invariant descriptor, Tombari et al. [31] proposed a 3D shape descriptor namely Signature of Histograms of OrienTations (SHOT). To generate the description for the object, they first applied a sign disambiguation technique to the eigenvectors of the scatter matrix of the object and constructed a unique and unambiguous LRF. The object’s points are then aligned with the LRF. Consequently, similar to 3D Shape Context, a spherical coordinate based approach is used to generate a SHOT description for the given object. 3D object descriptors that use spherical coordinate system suffer from the singularity issue at the poles, because bins at the poles are significantly smaller than bins around the equator. Our shape descriptor differ from all of the listed descriptors above as it is simultaneously unique, unambiguous, and robust to noise and varying low-level point cloud density. Besides, our approach can be used not only for object recognition but also for object manipulation. 3 Local reference frame A Local Reference Frame (LRF), invariant to translations and rotations and robust to noise is important for object recognition as well as object manipulation. Since the repeatability of a LRF directly affects the descriptiveness of the object representation [17], the LRF should be as repeatable and robust as possible to improve the performance of object recognition. In this section, we propose a method to compute a LRF. For this purpose, the three principal axes of a given object are firstly determined based on Principal Component Analysis (PCA). Given a point cloud of an object that contains m points, O = { p 1 , ⋯ , p m } , the geometric center of the object is defined as: (1) c = 1 m ∑ i = 1 m p i , where p i is a three dimensional point in the object’s point cloud. The normalized covariance matrix, C, of the object is constructed: (2) C = 1 m ∑ i = 1 m ( p i − c ) ( p i − c ) T . Then, eigenvalue decomposition is performed on C: (3) C V = E V , where V = [ v 1 , v 2 , v 3 ] contains the three eigenvectors, E = d i a g ( λ 1 , λ 2 , λ 3 ) is a diagonal matrix of the corresponding eigenvalues and λ 1 ≥ λ 2 ≥ λ 3. Since the covariance matrix is symmetric positive, its eigenvalues are positive and the eigenvectors are orthogonal. Eigenvectors define directions which are not unique, i.e. not repeatable across different PCA trials. This is known as the sign ambiguity problem, for which there is no mathematical solution [4]. Since there are two possible directions for each eigenvector, a total of eight reference frames can be created from the same set of eigenvectors. A mechanism is needed to transform this reference frame into a unique object reference frame, which will be always the same across multiple trials. We start with a provisional reference frame, in which the first two axes, X and Y, are defined by the eigenvectors v 1 and v 2 , respectively. However, regarding the third axis, Z, instead of defining it based on v 3 , we define it based on the cross product v 1 × v 2 . This way, because the result of the cross product follows the right-hand rule, the number of alternatives is reduced to four. It is now enough to disambiguate the directions of the X and Y axes. So either the directions of X and Y are both changed or both remain unchanged. To complete the disambiguation, the object’s point cloud, O , is transformed to be placed in the provisional reference frame. Then, the number of points that have positive x, S x + , and the number of points that have negative x, S x − , are counted as follows: (4) S x + = { i : x p i > t } , S x − = { i : x p i < − t } , where t is a threshold (e.g. t = 0.015 m ) that is used to deal with the special case when a point is close to the YoZ plane, and therefore can change from negative to positive X in different trials. Afterwards, the variable Sx is defined as: (5) S x = { + 1 , | S x + | ≥ | S x − | − 1 , o t h e r w i s e , where |.| denotes the number of points of the argument. A similar indication, Sy , is computed for the Y axis. Finally, the sign of the axes is determined as: (6) s = S x . S y , where s can be either − 1 or + 1 . In case of s = − 1 , the directions of X and Y must be changed, otherwise not. Therefore, the final LRF (X, Y, Z) will be defined by ( s v 1 , s v 2 , v 1 × v 2 ) . An illustrative example of the sign disambiguation procedure is provided in Fig. 1 . 4 Object descriptor This section describes the computation of the proposed object descriptor, GOOD, in the obtained LRF centered in the geometric center of the object. The descriptor consists of a concatenation of the orthographic projections of the object on the three orthogonal planes, XoY, YoZ and XoZ. Each projection is described by a distribution matrix. To ensure correct comparison between different object shapes, the number of bins in the distribution matrices must be the same and the bins should be of equal size. Therefore, each distribution matrix must be computed from a square area in the projection plane centered on the object’s center, and this square area must have the same dimensions for the three projections. The side length of these square areas, l, is determined by the largest edge length of a tight-fitting axis-aligned bounding box (AABB) of the object. The dimensions of the AABB are obtained by computing the minimum and maximum coordinate values along each axis. With this setup, the number of bins, n, is the only parameter that must be specified to compute GOOD. For each projection, the l × l projection area is divided into n × n square bins. Finally, a distribution matrix M n × n is obtained by counting the number of points falling into each bin. For each projected point ρ = ( α , β ) ∈ R 2 , where α is the perpendicular distance to the horizontal axis and β is the perpendicular distance to the vertical axis, a row, r ( ρ ) ∈ { 0 , ⋯ , n − 1 } , and a column, c ( ρ ) ∈ { 0 , ⋯ , n − 1 } , are associated as follows: (7) r ( ρ ) = ⌊ α + l 2 l + ϵ n ⌋ = ⌊ n α + l 2 l + ϵ ⌋ , (8) c ( ρ ) = ⌊ β + l 2 l + ϵ n ⌋ = ⌊ n β + l 2 l + ϵ ⌋ , where ϵ is a very small value used to deal with the special cases when a point is projected onto the upper bound of the projection area, and ⌊x⌋ returns the largest integer not greater than x. Note that the projected view is shifted to right and top by l 2 (i.e. α + l 2 and β + l 2 ). Furthermore, to achieve invariance to point cloud density, M is normalized such that the sum of all bins is equal to one (see Fig. 2 ). The matrix M is called distribution matrix, because it represents the 2D spatial distribution of the object’s points. According to standard practice, this matrix is converted to a vector m 1 × n 2 = [ M ( 1 , 1 ) , M ( 1 , 2 ) , ⋯ , M ( n , n ) ] . The three projection vectors will be concatenated producing a vector of dimension 3 × n 2 which is the final object descriptor, GOOD. Statistical features are used to decide the order in which the projection vectors will be concatenated. For the first projection in the descriptor, the one with largest area is preferred. The number of points is not a good indicator of area because all points of the object are represented in the three projections. The number of occupied bins (the ones with a mass greater than 0) could be used as a measure of area. However, this measure tends to be brittle when the boundary of the object is close to boundaries between bins. Therefore, in this work the entropy of the projection is used. Entropy, a measure from Information Theory [6], nicely takes into account both the number of occupied bins and their density. In this work, the entropy of a projection is computed as follows: (9) H ( m ) = − ∑ i = 1 n m i log 2 m i , where m i is the mass in bin i. The logarithm is taken in base 2 and 0 log 2 0 = 0 . The projection with highest entropy is the one that will appear in the first n 2 positions of the descriptor. The next step is to select, from the remaining two projections, which one should appear in the second part of the descriptor (positions n 2 to 2 n 2 − 1 ). It is common that these two projections have similar areas, and therefore similar entropies, leading to instability of the decision if it is made based on entropy. Therefore, instead of entropy, we use variance to make this decision. Since the projection matrices are probability mass functions (pmf), the variance is defined as follows: (10) σ 2 ( m ) = ∑ i = 1 n ( i − μ m ) 2 m i , where μ m is the expected value (i.e. a weighted average of the possible values of i, corresponding to the geometric center of the projection), which is computed as follows: (11) μ m = ∑ i = 1 n 2 i m i , unlike the simple mean, which gives each projection equal weight, the mean of a projection weights each bin, i, according to its probability distribution, m i . The variance measure, σ 2 ( m ) , is used to measure the spread or variability of the spatial distribution of the object’s points in the projection vector. A small variance indicates that the projected points tend to be very close to each other and to the mean of the vector, i.e. the shape of distribution is small and compact. A high variance indicates that the data points in the projection vector are very spread out from the mean. An illustrative example of the proposed shape descriptor is depicted in Fig. 2. In this example, after determining the local reference frame, a mug is projected onto the three orthogonal planes. Based on the entropy criterion, the XoZ projection (Fig. 2b) is selected to appear in the first part of the descriptor. Based on the variance criterion, the YoZ projection (Fig. 2c) is selected to appear in the second part of the descriptor. The remaining projection, XoY (Fig. 2d), appears in the last part of the descriptor. In order to grasp an object, it is necessary to know true dimensions of different parts of the object. Such information is not adequately represented in most shape descriptors (e.g. Viewpoint Feature Histogram [27]). Because GOOD is composed of three orthogonal projections, it is especially rich in terms of information suited for manipulation tasks. In Fig. 3 , again we consider the projections of a mug. Here, we adopt a multi-view orthographic projection layout in which there is a central or front view, a top view and a side view. The central view is the one selected based on the entropy criterion and appearing in the first part of the descriptor. The top view contains the projection in the orthogonal plane formed by the horizontal axis of the central projection and the third axis. The side view contains the projection in the orthogonal plane formed by the vertical axis of the central projection and the third axis. The figure shows that projections can be further processed for object manipulation purposes. In the top view, the gray symbols C, W, D and T represent how the projection can be further processed and some features for manipulation task are extracted, namely inner radius (C), thickness (T), handle length (W) and handle thickness (D). 5 Experimental results Several experiments were carried out to evaluate the performance of the proposed object descriptor concerning descriptiveness, scalability, robustness and efficiency characteristics. The proposed descriptor has a parameter namely number of bins (i.e. d) that must be well selected to provide a good balance between recognition accuracy, memory usage and computation time. For this purpose, 10 experiments were performed for different values of the descriptor’s parameter. The performance of the shape descriptor and its scalability were examined on the Washington RGB-D Object Dataset [14]. Afterwards, various tests were executed to measure the robustness of the proposed shape descriptor concerning different levels of noise and varying mesh resolutions on the Restaurant Object Dataset [13]. Next, two efficiency evaluations relating to computational efficiency and memory usage were performed. Furthermore, a real demonstration was performed to show all the characteristics of the proposed descriptor. The largest publicly available dataset, namely Washington RGB-D Object Dataset [14], consisting of 250,000 views of 300 common household objects. The objects are categorized into 51 categories arranged using WordNet [18] hypernym-hyponym relationships (similar to ImageNet [7]). The Restaurant Object Dataset contains 241 views of one instance of each category (Bottle, Bowl, Flask, Fork, Knife, Mug, Plate, Spoon, Teapot, and Vase) [13]. In all experiments, an instance-based learning approach is used, i.e. object categories are represented by sets of known instances. The instance-based approach is a baseline method for evaluating representations. However, more advanced approaches like SVM and object Bayesian approaches can be easily adapted. Similarly, a simple baseline recognition mechanism in the form of a Euclidean nearest neighbor classifier is used. Moreover, the proposed descriptor was compared with four state-of-the-art object descriptors that are available in the Point-Cloud Library 2 2 (PCL version 1.7 and 1.8) including VFH [27], ESF [32], GFPFH [28] and GRSD [16]. The selected descriptors were evaluated based on a 10-fold cross validation algorithm in terms of Accuracy [24]. In each iteration, a single fold is used for testing, and the remaining data are used as training data. The cross-validation process is then repeated 10 times, which each of the 10 folds used exactly once as the test data. For all selected shape descriptors, the default parameters in the respective PCL implementations were used. All tests were performed with an i7, 2.40GHz processor and 16GB RAM. 5.1 Descriptiveness As mentioned above, GOOD has a parameter called number of bins that has effect on descriptiveness, efficiency and robustness. Therefore, it must be well selected to provide a good balance between recognition performance, memory usage and computation time. The descriptiveness of the proposed descriptor with respect to varying number of bins was evaluated using Washington dataset. Results are presented in Fig. 4 (left) and Table 1 . In these experiments, the configurations that obtained the best precision and recall figures were number 3 and 5. Although, a large number of bins provides more details about the point distribution, it increases computation time, memory usage and sensitivity to noise. Therefore, since the difference to other configurations is not very large, we prefer to use configuration number 1 (i.e. b = 5 ) which displays a good balance between recognition performance, memory usage, and processing speed. The accuracy of the proposed system with this configuration was 92%. It shows that the overall performance of the recognition system is promising and the proposed descriptor is capable of providing distinctive global feature for the given object. The following results are computed using this the default value, unless otherwise noted. 5.2 Scalability A set of experiments was carried out to evaluate the performance of the proposed descriptor on the Washington dataset, concerning its scalability with respect to varying numbers of categories. Results are depicted in Fig. 4 (center) and (right). One important observation is that the accuracy decreases in all approaches as more categories are used (Fig. 4 (center)). This is expected since the number of categories known by the system makes the classification task more difficult and the difference in performance between descriptors becomes smaller. Moreover, it can be concluded from Table 2 that when the number of object categories increases (i.e. more than 35 categories), VFH and GOOD descriptors achieve the best accuracy and stable performance regarding varying numbers of categories Table 3 . It is clear from Fig. 4 (right) that the computation time of our approach is significantly smaller than VFH, GRSD and GFPFH. However, GOOD, VFH and ESF descriptors obtain an acceptable scalability regarding varying numbers of categories, the scalability of GRSD and GFPFH are very low and their performance drops aggressively when the number of categories increases. Although ESF descriptor achieves better performance than our approach with 5 bins (i.e. GOOD 5bins), the length of ESF descriptor (i.e. compactness) is around 8.5 times more than our descriptor (see Table 4 ). It is notable that whenever the size of dataset is larger than 35 object categories, the difference between EFS performance and our approach with 5 bins, is equal or less than 1% and in the similar situation our approach with 15 bins (i.e. GOOD 15bins) works better than ESF descriptor. 5.3 Robustness The robustness of the proposed object descriptor with respect to different levels of Gaussian noise and varying mesh resolutions was evaluated and compared with other global object descriptors. These experiments were run on the mentioned Restaurant Object Dataset. 5.3.1 Gaussian noise Ten levels of Gaussian noise with standard deviations from 1 to 10 mm were added to the test data. For a given test object, Gaussian noise is independently added to the X, Y and Z-axes. As an example, a Vase object with three levels of standard deviation of Gaussian noise ( σ = 3 mm , σ = 6 mm , σ = 9 mm ) is depicted in Fig. 5 . The results are presented in Fig. 6 (left) and Table 3. An important observation can be made from Figs. 6 and 4. Although GOOD, ESF and VFH descriptors achieved a really good performance on noise free data, GOOD outperformed ESF, GFPFH and GRSD descriptors by a large margin under all levels of Gaussian noise. While the performance of VFH descriptor was similar to our approach under a low-level noise (i.e. σ ≤ 6 mm), our shape descriptor outperformed all descriptors under high levels of noise. It can be concluded from this observation that GOOD descriptor is robust to noise due to use an stable, unique and unambiguous object reference frame. In contrast, since the VFH and GFPFH descriptors are rely on surface normals to calculate their shape descriptions, they are highly sensitive to the noise. GRSD employs radial relationships to describe the geometry of points at each voxel cell and ESF uses distances and angels between randomly sampled points to generate a shape description; therefore, GRSD and ESF are also sensitive to the noise and its performance decrease rapidly when the standard deviation of the Gaussian noise increases. In addition, GOOD descriptor uses three distribution matrices that are constructed based on orthographical projection, therefore less affected by noise (i.e. in each orthographic projection one dimension is discarded). 5.3.2 Varying point cloud density Two sets of experiments were carried out to examine the robustness of the proposed descriptor with respect to varying point cloud density. In the first set of experiments, the original density of training objects was kept and the density of testing objects was reduced (downsampling) using a voxelized grid approach 3 3 In the second set of experiments, the original density was kept in testing objects and reduced in training objects. This is initiated with a root volume element (voxel) and the eight children voxels in which each internal node has exactly eight children nodes. These are recursively subdivided until all voxels contain at most one point or the minimum voxel size is reached (i.e. The cloud is divided in multiple voxels with the desired resolution). Afterwards all the points that fall into the same voxel will be downsampled with their centroid. In this evaluation, each object (either test or train) is downsampled using five different voxel sizes including {1, 5, 10, 15, 20} millimetre. An illustration example of a Flask object with four level of downsampling is depicted in Fig. 7 . The robustness results regarding varying point cloud density in test and train data are presented in Fig. 6. From experiments of reducing density of test data (i.e. Fig. 6(left)), it was found that our approach is more robust than the other descriptors concerning low-level downsampling (i.e. DS ≤ 3.5 mm) and works slightly better than the other in high-level downsampling resolution (i.e. DS ≥ 18 mm). In contrast, the performance of VFH, ESF and GRSD were better than GOOD descriptor in mid-level downsampling resolution (i.e. 3.5 mm < DS < 18). The performance of GFPFH was very low under all level levels of point cloud resolution. Besides, it can be concluded from Fig 6 (right) that when the level of up-sampling increases, VFH, ESF and GRSD descriptors achieve better performance than GOOD and GFPFH descriptors. 5.4 Efficiency In this subsection, evaluations regarding computational efficiency and memory footprint (i.e. the amount of main memory that a program uses or references while running) are presented and discussed. 5.4.1 Memory footprint The length or size of a descriptor, has direct influence on memory usage and computation time in object recognition process (see Fig. 4). The length of all descriptors used in this evaluation is given in Table 4. Although GFPFH and GRSD are the tow most compact descriptors in this evaluation (see Table 4), their computation time and are not good as depicted in Figs. 4 and 8 . Our approach is the third compact descriptor that provides good balance between computation time and descriptiveness with 75 floats. However, VFH and ESF descriptors achieve a good description power, their feature length is around 4.1 and 8.5 times larger than our approach and 20 and 40 times larger than GFPFH descriptor respectively. ESF is the lowest compact descriptor compared to all the other descriptors. 5.4.2 Computation time Several experiments were performed to measure computational time of all descriptors used in this evaluation. Since the number of object’s points directly affects the computational time, we calculate the average time required to generate a description for 20 randomly selected objects from the RGB-D dataset. Fig. 8 compares the average computation time of the selected object descriptors in which several observations can be made; first, GOOD descriptor is the most computation time efficient descriptor. In contrast, GFPFH descriptor is the computationally most expensive descriptor. ESF, VFH and GRSD descriptors achieve a medium performance in terms of computation time. Overall, GOOD descriptor achieves the best performance, which is around 10 times better performance than ESF and 44, 50 and 254 times better performance than VFH, GRSD and GFPFH descriptors. VFH, GRSD and GFPFH descriptors are extremely time consuming descriptors. The underlying reason is that GOOD descriptor works directly on 3D point clouds and requires neither triangulation of the object’s points nor surface meshing. According to the evaluations our approach is competent for robotic applications with strict limits on the memory footprint and computation time requirements. 5.5 System demonstration To show all the described functionalities and properties of the proposed GOOD descriptor, a real demonstration was performed. For this purpose, GOOD has been integrated in the object perception system presented in [13,20] and [19] (see Fig. 9 ). In this demonstration a table is in front of a robot and two users interact with the system. During the demonstration, users presented objects to the system and provided the respective category labels. Therefore, throughout this session, the system must be able to detect, conceptualize and recognize unknown (i.e. new) objects. It should be noted that a constraint has been set on the Z axis that the initial direction of Z axis of objects’ LRF should be similar to direction of Z axis of the table. It is assumed that there is no learned categories in the memory at the beginning of the demonstration. It was observed that the proposed object descriptor is capable to provide distinctive global feature for recognizing different type of objects. It also estimates pose of objects and build orthographic projections for object manipulation purposes. A video of this demonstration is available in: 6 Conclusion This paper presented a global object descriptor named GOOD (i.e. Global Orthographic Object Descriptor) that provides a good trade-off between descriptiveness, computation time and memory usage, allowing concurrent object recognition and pose estimation. For an object, GOOD is computed on a unique and repeatable local reference frame. It is calculated with the discretization of the three orthographic projections and their concatenation to form a single description for the given object. A set of experiments were carried out to assess the performance of GOOD and compare it with other state-of-art descriptors with respect to several characteristics including descriptiveness, scalability, robustness (Gaussian noise and varying low-level point cloud density) and efficiency (memory footprint and computation time). Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. GOOD outperformed the selected state-of-the-art descriptors (i.e. VFH, ESF, GRSD and GFPFH descriptors), achieving appropriate descriptiveness and significant robustness to Gaussian noise. GOOD was robust to varying low-level point cloud density too. The accuracy of VFH, ESF and GRSD was better than GOOD in the case of varying medium and high point cloud density. In addition, GOOD obtained the best computation time performance. Besides, GOOD demonstrates the capability of estimating objects’ poses and building orthographic projections for object manipulation purposes. We are currently working on integrating and using the GOOD descriptor for manipulation purposes and we would like to put the source code of the GOOD descriptor available to the research community in the ROS 4 4 repository and Point Cloud Library 5 5 in the near future. Acknowledgments This work was funded by National Funds through FCT project PEst-OE/EEI/UI0127/2014 and FCT scholarship SFRH/BD/94183/2013. References [1] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library IEEE Robot. Autom. Mag. 1070 9932/12 2012 [2] A. Andreopoulos J.K. Tsotsos 50 years of object recognition: Directions forward Comput. Vis. Image Underst. 117 8 2013 827 891 [3] Bo L. Lai K. Ren X. D. Fox Object recognition with hierarchical kernel descriptors IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011 2011 IEEE 1729 1736 [4] R. Bro E. Acar T.G. Kolda Resolving the sign ambiguity in the singular value decomposition J. Chemometr. 22 2 2008 135 140 [5] Chen H. B. Bhanu 3d free-form object recognition in range images using local surface patches Pattern Recogn. Lett. 28 10 2007 1252 1262 [6] T.M. Cover J.A. Thomas Elements of Information Theory 2012 John Wiley & Sons [7] Deng J. Dong W. R. Socher Li L.-J. Li K. Fei-Fei L. Imagenet: A large-scale hierarchical image database IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 2009 IEEE 248 255 [8] H.Q. Dinh S. Kropac Multi-resolution spin-images IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006 1 2006 IEEE 863 870 [9] A. Frome D. Huber R. Kolluri T. Bulow J. Malik Recognizing objects in range data using regional point descriptors Computer Vision-ECCV 2004 2004 Springer 224 237 [10] Guo Y. F.A. Sohel M. Bennamoun Lu M. Wan J. Trisi: A distinctive local surface descriptor for 3d modeling and object recognition. GRAPP-IVAPP 2013 86 93 [11] B.K. Horn Extended gaussian images Proc. IEEE 72 12 1984 1671 1686 [12] A.E. Johnson M. Hebert Using spin images for efficient object recognition in cluttered 3d scenes IEEE Trans. Pattern Anal. Mach. Intell. 21 5 1999 433 449 [13] S. Kasaei M. Oliveira G. Lim L. Seabra Lopes A.M. Tome Interactive open-ended learning for 3d object recognition: an approach and experiments J. Intell. Robot. Syst. 80 3–4 2015 537 553 [14] Lai K. Bo L. Ren X. D. Fox A large-scale hierarchical multi-view rgb-d object dataset Robotics and Automation (ICRA), 2011 IEEE International Conference on 2011 IEEE 1817 1824 [15] Y. Li, S. Pirk, H. Su, C.R. Qi, L.J. Guibas, Fpnn: field probing neural networks for 3d data, arXiv preprint arXiv:1605.06240 (2016). [16] Z.-C. Marton D. Pangercic R.B. Rusu A. Holzbach M. Beetz Hierarchical object geometric categorization and appearance classification for mobile manipulation 10th IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2010 2010 IEEE 365 370 [17] A. Mian M. Bennamoun R. Owens On the repeatability and quality of keypoints for local feature-based 3d object retrieval from cluttered scenes Int. J. Comput. Vis. 89 2–3 2010 348 361 [18] G.A. Miller Wordnet: a lexical database for english Commun. ACM 38 11 1995 39 41 [19] M. Oliveira Lim G.H. L. Seabra Lopes S. Hamidreza Kasaei A.M. Tome A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014), 2014 2014 IEEE 2216 2223 [20] M. Oliveira L.S. Lopes Lim G.H. S.H. Kasaei A.M. Tomé A. Chauhan 3d object perception and perceptual learning in the race project Robot. Auton. Syst. 2015 [21] R. Osada T. Funkhouser B. Chazelle D. Dobkin Shape distributions ACM Trans. Graph. 21 4 2002 807 832 [22] Pang G. U. Neumann Fast and robust multi-view 3d object recognition in point clouds International Conference on 3D Vision (3DV), 2015 2015 IEEE 171 179 [23] G. Pasqualotto P. Zanuttigh G.M. Cortelazzo Combining color and shape descriptors for 3d model retrieval Signal Process.: Image Commun. 28 6 2013 608 623 [24] D.M. Powers, Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation (2011). [25] D. Regazzoni G. de Vecchi C. Rizzi Rgb cams vs rgb-d sensors: low cost motion capture technologies performances and limitations J. Manuf. Syst. 33 4 2014 719 728 [26] R.B. Rusu N. Blodow M. Beetz Fast point feature histograms (fpfh) for 3d registration IEEE International Conference on Robotics and Automation, 2009. ICRA’09. 2009 IEEE 3212 3217 [27] R.B. Rusu G. Bradski R. Thibaux Hsu J. Fast 3d recognition and pose using the viewpoint feature histogram IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2010 2010 IEEE 2155 2162 [28] R.B. Rusu A. Holzbach M. Beetz G. Bradski Detecting and segmenting objects for mobile manipulation IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops), 2009 2009 IEEE 47 54 [29] R.B. Rusu Z.C. Marton N. Blodow M. Dolha M. Beetz Towards 3d point cloud based object maps for household environments Robot. Auton. Syst. 56 11 2008 927 941 [30] H. Su S. Maji E. Kalogerakis E. Learned-Miller Multi-view convolutional neural networks for 3d shape recognition Proceedings of the IEEE International Conference on Computer Vision 2015 945 953 [31] F. Tombari S. Salti L. Di Stefano Unique signatures of histograms for local surface description Computer Vision–ECCV 2010 2010 Springer 356 369 [32] W. Wohlkinger M. Vincze Ensemble of shape functions for 3d object classification IEEE International Conference on Robotics and Biomimetics (ROBIO), 2011 2011 IEEE 2987 2992 [33] Wu Z. Song S. A. Khosla Yu F. Zhang L. Tang X. Xiao J. 3d shapenets: a deep representation for volumetric shapes Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015 1912 1920 [34] Zhong Y. Intrinsic shape signatures: A shape descriptor for 3d object recognition IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops), 2009 2009 IEEE 689 696 "
    },
    {
        "doc_title": "Incremental texture mapping for autonomous driving",
        "doc_scopus_id": "84991738411",
        "doc_doi": "10.1016/j.robot.2016.06.009",
        "doc_eid": "2-s2.0-84991738411",
        "doc_date": "2016-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Autonomous Vehicles",
            "Constrained Delaunay triangulation",
            "Geometric description",
            "Partial configuration",
            "Scene reconstruction",
            "Texture mapping",
            "Vision-based sensors"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Autonomous vehicles have a large number of on-board sensors, not only for providing coverage all around the vehicle, but also to ensure multi-modality in the observation of the scene. Because of this, it is not trivial to come up with a single, unique representation that feeds from the data given by all these sensors. We propose an algorithm which is capable of mapping texture collected from vision based sensors onto a geometric description of the scenario constructed from data provided by 3D sensors. The algorithm uses a constrained Delaunay triangulation to produce a mesh which is updated using a specially devised sequence of operations. These enforce a partial configuration of the mesh that avoids bad quality textures and ensures that there are no gaps in the texture. Results show that this algorithm is capable of producing fine quality textures.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-07-05 2016-07-05 2016-08-26 2016-08-26 2016-08-26T17:58:13 S0921-8890(16)30081-1 S0921889016300811 10.1016/j.robot.2016.06.009 S300 S300.1 FULL-TEXT 2016-08-26T13:55:25.23236-04:00 0 0 20161001 20161031 2016 2016-07-05T15:19:52.486451Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 84 84 C Volume 84 10 113 128 113 128 201610 October 2016 2016-10-01 2016-10-31 2016 article fla © 2016 Elsevier B.V. All rights reserved. INCREMENTALTEXTUREMAPPINGFORAUTONOMOUSDRIVING OLIVEIRA M 1 Introduction 2 Related work 3 Proposed approach 3.1 One-shot texture mapping using data dependent triangulation 3.2 Incremental texture mapping 4 Results 4.1 One-shot texture mapping 4.2 Incremental texture mapping 4.3 Projection of a single camera onto the ground plane 4.4 Projection of multiple cameras onto the ground plane 5 Conclusions Acknowledgments References OLIVEIRA 2016 503 515 M ROBOT2015SECONDIBERIANROBOTICSCONFERENCEADVANCESINROBOTICSVOLUME1 SCENEREPRESENTATIONSFORAUTONOMOUSDRIVINGAPPROACHBASEDPOLYGONALPRIMITIVES OLIVEIRA 2016 M HUANG 2011 1595 1601 A SEGAL 1992 249 252 M DEBEVEC 1998 P EFFICIENTVIEWDEPENDENTIMAGEBASEDRENDERINGPROJECTIVETEXTUREMAPPINGTECHREP RIPPA 1992 257 270 S GARLAND 1995 M FASTPOLYGONALAPPROXIMATIONTERRAINSHEIGHTFIELDSTECHREPCMUCS95181 SCHATZL 2001 309 321 R GEOMETRICMODELLING DATADEPENDENTTRIANGULATIONINPLANEADAPTIVEKNOTPLACEMENT DEMARET 2006 1604 1616 L SAPPA 2007 23010 A DYN 1992 179 192 N SCHUMAKER 1993 329 345 L OPENGL 2005 OPENGLRPROGRAMMINGGUIDEOFFICIALGUIDELEARNINGOPENGLRVERSION2 BLYTHE 2006 724 734 D LEHNER 2007 178 187 B GILECTURENOTESININFORMATICSVISUALIZATIONLARGEUNSTRUCTUREDDATASETS SURVEYTECHNIQUESFORDATADEPENDENTTRIANGULATIONS CERVENANSKSY 2010 125 135 M SVALBE 1989 941 950 I YVINEC 2012 M CGALUSERREFERENCEMANUAL40EDITION 2DTRIANGULATIONS SHEWCHUK 2008 580 637 J MOLLER 1997 25 30 T CHANG 2009 235 240 J FOGEL 2012 E CGALUSERREFERENCEMANUAL40EDITION 2DREGULARIZEDBOOLEANSETOPERATIONS OLIVEIRAX2016X113 OLIVEIRAX2016X113X128 OLIVEIRAX2016X113XM OLIVEIRAX2016X113X128XM 2018-08-26T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0921-8890(16)30081-1 S0921889016300811 10.1016/j.robot.2016.06.009 271599 2016-08-26T13:11:12.895259-04:00 2016-10-01 2016-10-31 true 8641462 MAIN 16 49999 849 656 IMAGE-WEB-PDF 1 gr4 26663 164 127 gr7 30032 163 206 gr8 29537 164 203 pic5 33051 163 140 gr2 30967 164 169 pic2 25133 163 140 gr12 33037 164 197 gr1 29767 164 183 gr5 20010 163 140 gr13 41383 163 154 pic3 22237 163 140 gr6 26630 164 214 gr9 23491 52 219 pic4 22871 164 141 gr11 23784 163 122 pic1 23187 164 141 gr3 25322 113 219 gr10 38213 164 168 gr4 152384 649 502 gr7 192344 600 756 gr8 183020 594 736 pic5 39234 132 113 gr2 149230 587 606 pic2 38908 132 113 gr12 193022 529 637 gr1 129052 533 596 gr5 164313 882 756 gr13 293812 664 626 pic3 35322 132 113 gr6 155924 579 755 gr9 111172 156 656 pic4 35061 131 113 gr11 208963 834 624 pic1 35749 131 113 gr3 64157 312 606 gr10 203603 571 585 si123 161 13 16 si56 159 12 17 si21 144 13 13 si96 4249 133 353 si71 222 15 36 si107 724 39 134 si121 2455 64 382 si166 300 19 45 si81 578 15 150 si53 159 13 17 si17 188 13 42 si184 390 15 87 si75 112 11 6 si46 401 15 87 si147 201 14 29 si37 117 12 6 si168 265 19 37 si203 401 15 87 si134 149 13 14 si39 264 15 39 si68 1486 36 284 si88 399 15 64 si78 136 10 16 si115 127 10 11 si114 405 15 80 si201 400 15 87 si163 276 19 37 si10 178 12 23 si124 151 13 16 si181 346 19 51 si202 407 15 87 si152 332 21 61 si122 154 13 16 si63 170 11 16 si66 229 13 39 si199 391 15 87 si11 195 12 29 si126 115 7 10 si119 187 14 27 si8 134 10 11 si127 1424 16 390 si110 277 16 40 si99 1388 41 292 si196 168 12 35 si132 470 15 148 si9 160 12 17 si57 922 16 247 si73 151 14 14 si54 169 13 18 si159 183 11 41 si69 170 14 19 si83 330 15 62 si205 397 15 87 si2 126 11 11 si125 713 15 145 si61 566 15 183 si80 142 10 17 si148 200 14 29 si16 178 13 40 si197 388 15 87 si108 147 14 11 si145 230 13 41 si178 287 19 36 si183 496 15 136 si207 398 15 87 si176 324 19 45 si34 145 13 13 si18 188 13 41 si170 356 19 57 si1 202 12 40 si29 136 11 12 si174 324 19 45 si140 233 13 39 si30 131 11 10 si206 403 15 87 si164 256 19 36 si135 944 17 283 si133 794 15 171 si74 699 21 173 si62 1004 51 189 si175 330 19 45 si43 273 15 52 si67 1107 15 328 si154 358 23 60 si72 143 14 14 si100 1647 41 383 si200 183 12 36 si161 318 19 45 si120 255 14 43 si60 243 17 36 si153 351 23 61 si90 140 11 10 si59 286 11 72 si91 1560 31 389 si47 392 15 87 si41 187 12 36 si70 351 15 76 si65 155 10 20 si77 326 24 62 si40 192 12 36 si195 212 12 40 si20 134 13 12 si117 972 16 249 si187 395 15 87 si105 216 15 27 si186 386 15 87 si38 114 10 8 si162 272 19 36 si112 135 12 10 si3 141 11 12 si130 167 14 16 si44 326 15 65 si50 189 13 41 si129 163 14 16 si182 381 15 87 si137 223 13 40 si131 234 13 39 si104 210 13 41 si64 160 10 19 si102 113 10 10 si146 193 14 28 si22 141 13 13 si177 283 19 37 si103 2739 70 382 si55 149 12 16 si85 160 21 14 si98 577 17 148 si58 131 8 11 si45 379 15 94 si76 123 14 8 si128 248 14 39 si158 1670 44 346 si179 287 19 37 si82 146 11 13 si106 285 15 51 si97 500 15 141 si79 145 10 17 si139 221 13 41 si167 231 13 40 si198 395 15 87 ROBOT 2656 S0921-8890(16)30081-1 10.1016/j.robot.2016.06.009 Elsevier B.V. Fig. 1 An example from the MIT data-set: three projections are collected over a period of time and mapped to a wall panel (GPP k = 4 , in blue): (a) positions of the vehicle at the time each projection is collected; (b) image from front camera, at location C ; (c), front camera, intermediate location; (d) front camera, location D ; (e) left camera, location D . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Triangulated meshes (top) and textures (bottom) created separately for each of the projections shown in the example of Fig. 1: (a) front camera, location C ; (b) front camera, intermediate location; (c) front camera, location D ; (d) left camera, location D . Fig. 3 Textures obtained using different fusion strategies: (a) option (1), average textures from local meshes; (b) option (2), insert vertices from all local meshes; (c) option (3), insert triangles of better quality, removing overlapping triangles. Fig. 4 A diagram showing the main components of the proposed system. Fig. 5 Triangle overlap test: (a) intersection returns points, overlap true; (b) intersection returns line segments, overlap true; (c) intersection returns polygons, overlap true; (d) intersection returns empty, overlap false; (e) intersection returns points, overlap false; (f) intersection returns line segments, overlap false. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 The insertion operation: (a) candidate triangle and initial mesh; (b) insertion of candidate triangle’s vertices; (c) insertion of the candidate triangle’s vertices and constraints; (d) preparation of the mesh followed by the insertion of the candidate triangle’s vertices and constraints. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 The insertion operation, example 1: (a) candidate triangles and initial mesh M ; (b) first insertion, mesh M ∗ ; (c) second insertion, mesh M ∗ ∗ ; (d) third insertion, mesh M ∗ ∗ ∗ . Fig. 8 The projection parent status of each triangle (same example as in Fig. 7): (a) candidate triangles and initial mesh M ; (b) first insertion, mesh M ∗ ; (c) second insertion, mesh M ∗ ∗ ; (d) third insertion, mesh M ∗ ∗ ∗ . Fig. 9 One-shot texture mapping: (a) image with line segments detected (red lines); (b) arbitrary Delaunay triangulation; (c) proposed approach, using a constrained Delaunay triangulation. Constrained edges marked in red. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 The evolution of the global primitive mesh (top) as well as the texture (bottom): (a) time t = t 1 ; (b) time t = t 2 ; (c) time t = t 3 , insertion of front center camera; (d) time t = t 3 , insertion of front left camera. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Mapping of a single camera to the ground plane: (first row) front center camera; (second row) rear center camera; (third row) front left camera; (left column) global primitive meshes; (right column) contribution of each projection to the total number of triangles in the global mesh. Colors denote each of the projections, i.e., black is time t 1 , red is time t 2 and yellow is time t 3 . Blue triangles are orphan triangles. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Distribution of triangles according to projection, for an example with five cameras. Three time instants (15 projections in total) are considered; (a) t 1 ; (b) t 2 ; (c) t 3 ; (d) percentage of triangles by parent projection. Projections are colored with a black to yellow color coding, denoting oldest to newest projections. Blue color denotes orphan triangles. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Images and local triangulated meshes for all projections shown in Fig. 12: First row front center teleobjective camera; Second row front center camera; Third row, front right camera; Fourth row, front left camera; Fifth row, rear center camera; Left column: time t 1 , (Fig. 11(a)); Middle column: time t 2 , (Fig. 11(b)); Right column: time t 3 , (Fig. 11(c)). Incremental texture mapping for autonomous driving Miguel Oliveira a b ⁎ Vitor Santos b Angel D. Sappa c d Paulo Dias b A. Paulo Moreira a e a INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal b IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal c Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Campus Gustavo Galindo, Km 30.5 Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador d Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center, Campus UAB Bellaterra Barcelona 08193 Spain e FEUP - Faculty of Engineering, University of Porto, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal FEUP - Faculty of Engineering, University of Porto R. Dr. Roberto Frias s/n Porto 4200-465 Portugal ⁎ Corresponding author at: INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal. INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal Autonomous vehicles have a large number of on-board sensors, not only for providing coverage all around the vehicle, but also to ensure multi-modality in the observation of the scene. Because of this, it is not trivial to come up with a single, unique representation that feeds from the data given by all these sensors. We propose an algorithm which is capable of mapping texture collected from vision based sensors onto a geometric description of the scenario constructed from data provided by 3D sensors. The algorithm uses a constrained Delaunay triangulation to produce a mesh which is updated using a specially devised sequence of operations. These enforce a partial configuration of the mesh that avoids bad quality textures and ensures that there are no gaps in the texture. Results show that this algorithm is capable of producing fine quality textures. Keywords Scene reconstruction Autonomous driving Texture mapping 1 Introduction Autonomous vehicles often have a very large number of sensors mounted on-board. This is due to the need to observe the environment all around the vehicle, but also because vehicles must observe the scene with sensors of different nature. Mainly, sensors are divided into two groups: range sensors and vision based sensors. Sensors of the first group provide 3D measurements of the scene. On the other hand, vision based sensors collect photometric information of the scene. Due to the large number of sensors on-board these vehicles, it is not trivial to combine data from these sensors into a unique representation of the scene. Given that these sensors provide a continuous stream of data over time, and that they are displaced by the movement of the vehicle, then it follows that the representation of the scene must also be dynamic, in the sense that it must evolve to represent novel information collected at later stages of the mission. Note that, given a continuous throughput of images, the most recent image is not necessarily the best image to be used for texture mapping. For example, if the vehicle is moving away from an object, a camera on the rear side of the vehicle will produce images with decreasing quality. Rather, what is required is an algorithm that produces a scene representation at the early stages of a mission (because this might be immediately required for other tasks such as navigation, planning, etc.), but the later on is also capable of evaluating newly acquired images to assess whether or not these images are better than the previously used for mapping the texture. We refer to this as incremental texture mapping. In [1], an algorithm for creating and incrementally updating a geometrical representation of the scenario is presented. This work was later extended in [2]. The approach is based on Geometric Polygonal Primitives (GPP), and is shown to be capable of providing an accurate geometric description of the scenario. It uses data from range sensors only, and the geometric description changes to accommodate novel sensor data. In this paper we use the results given by the approach described in [2]. This means that we consider that there is, at all times, a geometric description of the scenario which is constantly evolving. In this paper, we focus on how the vision based sensors can be used to enrich the description of the scenario. In other words, we propose to use the images from the cameras on-board the vehicle to produce texture, which may be added to the 3D description of the environment. Note that, as in the case of the range sensors, the vision based sensors also produce a continuous stream of information which must be integrated in order to create a unique photometric description of the scenario. In this paper, we propose an approach which is capable of incrementally updating texture mapped onto GPPs. The following lines show an example in which the need for incremental texture mapping becomes clear. For testing and evaluation purposes, we use a data-set from the Massachusetts Institute of Technology (MIT) Team, taken from their participation in the DARPA Urban Challenge [3]. A small 40 s sequence was cropped from the MIT data-set. This sequence is referred to as the MIT sequence, and five key locations ( A through E where marked in the sequence (see [2] for details). The approach described in [2] produces a description of the geometric structure of the environment observed by the vehicle’s sensors. This description is given in the form of Geometric Polygonal Primitives (GPP), i.e., a list of polygons. Note that, as pointed out in [2] the geometric description of the scene is dynamic, since it may change whenever novel sensor information is collected. An example is presented in Fig. 1 where the vehicle travels from location C to location D of the MIT sequence. Images are collected at three locations: location C at mission time t 0 , location D at mission time t 2 and an intermediate location between those two at mission time t 1 (Fig. 1(a) shows the vehicle at each location). Consider a camera of index l , that produces an image which may virtually be projected to any GPP (i.e., to one of the polygons that constitute the geometric description of the scene), at any given mission time t . The term projection is defined as an image captured from a camera that can be used to map some texture to one of the polygons contained in the geometric description of the scene, and is denoted as C { k , l , t } . The data-set contains five color cameras (see [2,4] for details). Without loss of generality, in this example only images from two cameras are used: front center ( l = 0 ) and front left ( l = 3 ), and only a single GPP (index k = 4 ) is employed, which corresponds to the wall panel in front of the vehicle (in blue, left side of Fig. 1(a)). Note that, under the constraints defined above, i.e., k = { 4 } , l = { 0 , 3 } and t = { t 0 , t 1 , t 2 } , there are a total of six possible projections. However, two of these projections are empty, namely C { k = 4 , l = 3 , t = t 0 } and C { k = 4 , l = 3 , t = t 1 } . This is because the left camera ( l = 3 ) does not see the wall panel ( k = 4 ) in the first two locations ( t = t 0 and t = t 1 ). This can be observed in Fig. 1(a), which shows that the vehicle turns right at location D , and only then the left side camera is pointed in the direction of the wall panel. The images from the remaining four projections are shown in Fig. 1(b)–(e). As the vehicle approaches the wall panel, it collects images with higher resolution and better quality of that surface. Our goal is to study how a low resolution texture created when the vehicle was distant from the surface may evolve to a higher resolution texture once the vehicle comes closer to the panel. In other words, how can the texture of a surface be incrementally refined. Note that we assume that an accurate localization is available at all times. In the case of the MIT dataset, localization is provided by an Applanix POS-LV 220 system, 1 1 which includes a GPS, an inertial measurement unit and a wheel encoder. This is a very accurate system which publishes the 6 DOF pose of the vehicle at high frequencies (100 Hz). Thus, it is possible to gather the pose of each of the onboard cameras at any point in time. Obviously, a less accurate ego motion estimation should influence the mapping of texture. However, a detailed analysis of the impact of other ego motion estimation systems is out of the scope of the current paper. The remainder of the paper is organized as follows: related work is presented in Section 2; the proposed approach is described in Section 3 and, finally, results and conclusions are given in Sections 4 and 5. 2 Related work Texture mapping is a technique for mapping a 2D image onto a 3D surface by transforming color data so that it conforms to the surface plot. It allows the application of texture such as tiles or wood grain, to a surface without performing the geometric modeling necessary to create a surface with these features, or, in other words, without computing the projection of every pixel in the image onto the surface. The color data can also be any image, such as a picture taken by a camera. Texture mapping is performed over convex polygons, most commonly on triangles. Let X 1 , X 2 be the coordinates of the vertices 1 and 2 in 3D space. The coordinates u 1 , u 2 of the pixels that correspond to those vertices in the image plane can be obtained using direct projection: (1) u i = projection ( X i ) , ∀ i ∈ { 1 , 2 } . Let α be a parameter 0 < α < 1 , that indicates how a given vertex is positioned along the X 1 X 2 ¯ line segment. Texture mapping interpolates the color value for any vertices along the line segment as follows: (2) u α = ( 1 − α ) ⋅ u 0 + α ⋅ u 1 , which is of course a linear interpolation. When this kind of linear interpolation is used, the texture mapping is referred to as affine texture mapping. A linear interpolation works fine when the image plane and the projection plane are parallel. However, when this does not occur, the projection shows some artifacts that derive from the assumption that a linear interpolation can be used. This is a well documented problem, and is discussed in several works [5,6]. The solution to this problem is called view dependent texture mapping, and it consists of making texture mapping account for the position of the vertexes in 3D space, rather than simply interpolating a 2D triangle. This achieves the correct visual effect, but it is slower to calculate. Instead of interpolating the texture coordinates directly, the coordinates are divided by their depth (relative to the viewer), and the reciprocal of the depth value is also interpolated and used to recover the perspective corrected coordinate. This correction operates so that in parts of the polygon that are closer to the viewer, the difference from pixel to pixel between texture coordinates is smaller (stretching the texture wider), and in parts that are farther away this difference is larger (compressing the texture). View dependent texture mapping can be formulated as: (3) u α = ( 1 − α ) ⋅ u 0 w 0 + α ⋅ u 1 w 1 ( 1 − α ) ⋅ 1 w 0 + α ⋅ 1 w 1 . The solution proposed in Eq. (3) is capable of producing accurate mapping for texture. View dependent texture mapping is significantly slower when compared to affine texture mapping. Since triangles are the atomic entities of texture mapping, triangulation methodologies are an important part of the process. In this scope, Data Dependent Triangulation (DDT) algorithms are of particular interest since they can produce triangulated meshes which are ideal for texture mapping. The goal of a DDT is, on the one hand, to obtain the best approximation possible, and on the other to reduce the number of triangles and in turn the memory load. Consequently, the number of vertices should be kept as small as possible to speed up processing and reduce memory load. The two variables that should be tuned to achieve a good approximation are then the position of the vertices and the connections between them. Even if we decide to fix the number of triangles and vertices, the possible combinations of the connections between vertices are usually very large. Hence, an exhaustive search of all possible combinations is not possible. Also, no assumptions should be made on the optimal shape or size of the triangles. One might tend to assume long, thin triangles are not adequate but in fact that depends on the nature of the image [7]. If the image contains high gradient long feature such as poles or trees, such triangles could be well suited to represent these regions. DDT algorithms can be divided into refinement, decimation, or modification approaches. In refinement approaches, the starting point for the algorithm is a very coarse triangular mesh that is then refined. The mesh is refined by inserting new vertices. Since the number of possible positions where vertices can be inserted is very high, authors make use of heuristics to limit the number of options. The greedy refinement algorithm proposed in [8] works by inserting vertices into a triangulated mesh. In every step, a new vertex is inserted at the position of the largest distance between the approximation and the data provided in the image. In [9], the choice of which are the triangles to decimate is based on the high curvature of the data, and the positions where new vertices are to be inserted are locations with high proximity to the data. These methods have the drawback of tending to a local optima. Decimation approaches are the opposite of refinement meshes. The algorithms start from a very fine mesh and try to remove vertices and collapse triangles as they iterate. In [10] the initial triangulation is a full triangulation where each pixel is a vertex in the mesh. The algorithm then decimates the mesh by collapsing one of the edges of the mesh. The edge to collapse is the one that implicates less increase in the approximation error. Similar approaches were proposed in [11,12]. Finally, modification strategies start from a random arbitrary mesh and try to improve it by performing modification operations. These modification operations usually are edge swaps and the number of vertices in the initial mesh remains the same. It is the case of the algorithms proposed in [13,14]. Both propose different criteria for the selection of which are the edges that should be swapped. Given a triangulated mesh of a surface and an image registered to that surface, it is possible to map the texture from the image onto the surface using classic texture mapping approaches. Fig. 2 shows the triangulated meshes and textures produced using classical texture mapping, for each of the four projections displayed in Fig. 1. These mappings are computed independently for each location. As expected, textures that derive from projections taken closer to the surface (e.g., Fig. 2(c) and (d)) have better quality when compared to projections taken far away from the surface (e.g., Fig. 2(a) and (b)). The question is how to create an unique triangulated mesh and texture and how it should be updated with local triangulated meshes from novel projections. Let us assume that there is a way of assessing the quality of each projection and in particular of each triangle in each mesh, so that it is possible to rank the triangles with respect to their quality. At first sight, several strategies can be used to fuse the textures, namely: (1) average the textures produced by local meshes; (2) insert vertices from new triangles into the existing mesh and re triangulate, provided that these triangles yield better quality; (3) remove the triangles of the existing mesh that overlap the triangle (of better quality) to be inserted, and then insert the triangle. Option (1) consists of averaging the textures provided by each local mesh. This could be achieved by setting the alpha channel of all local meshes so that they average out. Additionally, the average could be weighted by the quality of the triangles, although this was not tested in this work. The primitive would have several layers, each with a given local triangulated mesh belonging to each projection. Fig. 3 (a) shows the results obtained using this strategy. Visually, results are not appealing. Another disadvantage concerns the need to store all local meshes, which is highly inefficient in terms of memory. As seen in Fig. 2, there are textures with much better quality than others. To average good textures with bad textures does not seem to make sense. Option (2) proposes to address the problem by considering the vertices of the meshes only (rather than the triangles). Each vertex in the new mesh is added to the current mesh. This results in a super mesh containing all the vertices of the two previous meshes in which the triangles (the configuration of the mesh) are defined arbitrarily. The idea is to fuse using an additive strategy. Fig. 3(b) shows the results obtained using this approach. Again, results are not visually appealing. In option (3), an alternative to averaging is considered: a winner takes all strategy. The idea is to select, for each region in the surface, a single triangle which will provide the texture. This selection can be done using the quality of each triangle. Note that, whenever a triangle to be inserted overlaps some triangles in the existing mesh, these triangles must first be removed so that the mesh preserves its configuration and thus the quality of the texture mapping. Results obtained from this method are shown in Fig. 3(c). Textures are visually appealing. Artifacts present in the average and the additive strategies are not visible. There is one problem however: removed triangles often overlap triangles to be inserted in just a portion of their area (partial overlap). When deleted, these triangles leave empty spaces where no texture is defined. This is visible in Fig. 3(c). None of the strategies discussed provides textures of sufficient quality. Thus the problem of incrementally updating the texture is not trivial. In the following sections, an approach is presented which is capable of generating higher quality textures. 3 Proposed approach Fig. 4 shows a diagram which describes the functioning of the system. The following sections will describe these components in detail. First, a one-shot texture mapping based in DDT is presented in Section 3.1. Here, we propose an algorithm based on the extraction of edges in the image and the construction of a constrained Delaunay triangulation, which is operated as a DDT and is very efficient. Then, the incremental texture mapping approach is presented in Section 3.2. In this case, we propose a sequence of atomic operations to conduct the insertion of a new triangle in a triangulated mesh, which minimizes the changes in the configuration of the mesh. 3.1 One-shot texture mapping using data dependent triangulation In this paper, we propose an alternative solution to view dependent texture mapping. One reason for this is that the objective of this work is to develop a mechanism for mapping texture from images onto GPP (see [2]). Those geometric primitives consist of polygons, instead of the traditional triangles. The mapping of photometric properties can be performed by mapping triangles in image space to 3D space. These procedures are executed in Graphical Processing Units (GPUs), and programmed using OpenGL [15], Direct3D [16] or other graphics libraries. These libraries also have the functionalities of mapping convex polygons, but in fact these are mere high level functions that decompose the polygons in an arbitrary way into sets of triangles and then map texture onto those triangles. We argue that, if we control the process of triangulation in such a way that the edges in the images used in the projection are aligned with the edges of the triangles in 3D space, the distortion produced by linear texture mapping is not visible, and thus, linear texture mapping may be used instead of view dependent triangulation, which is much slower. In other words, if the triangles are especially defined so that their faces represent smooth regions with constant color then, a linear texture mapping over these could in fact provide accurate projections. This procedure of creating a triangulated mesh which accommodates some input data is called DDT [17], and the mapping of images using this technique will be referred to as DDT mapping as opposed to texture mapping. Unlike in standard texture mapping approaches, where the triangulation is executed in the 3D space, DDT triangulation operates in the image space, and only after those 2D triangles are mapped onto the 3D space. Although there are many approaches in the literature to the data dependent triangulation problem, most of them are focused on the fact that such a triangulated mesh is capable of producing very good data compression ratios with respect to the real image, while still maintaining low approximation errors. Real time performance of the algorithms has seldom been debated, with authors reporting processing times of over three seconds for 512×512 images. The exception was the study conducted in [18], where DDT was parallelized, resulting in a significant speed up. We propose a simple procedure similar to [12]: edges are detected using a Hough lines detector [19] extended to obtain a description of line segments instead of lines (e.g., see [20,21]). The triangulation is a Delaunay triangulation [22]: let the image be described by M line segments with starting points s m and endpoints e m , where each detected line segment is defined as s m e m ¯ . The Delaunay triangulation (Delaunay) receives the starting and endpoints as input to define the vertices of the triangulated mesh: (4) t = Delaunay ( { s 0 , e 0 , s 1 , e 1 , … , s M − 1 , e M − 1 } ) , where t is the resulting triangulated mesh. A constrained Delaunay triangulation is a generalization of the Delaunay triangulation where line segments may be imposed as belonging to the triangulated mesh (initially proposed by [23] for 2D spaces, later generalized to N dimensional spaces by [24]). A constrained Delaunay triangulation (cDelaunay) requires two inputs, a list of points and a list of line segments (also called constraints): (5) t = cDelaunay ( { s 0 , e 0 , … , s M − 1 , e M − 1 } , { s 0 e 0 ¯ , … , s M − 1 e M − 1 ¯ } ) . In brief, what we propose is a technique in which a constrained Delaunay triangulation is executed on the image space, having as input the line segments given by a line segment detection algorithm based on hough lines. 3.2 Incremental texture mapping Section 3.1 described how a constrained Delaunay triangulation may be used to produce a data dependent triangulated mesh that conforms with edges previously detected in the image. Note that this is a one-camera, one-shot approach, since it does not consider how to map more than one image. In reality, there is always a large set of images available to use for texture mapping, either from multiple cameras or from a unique camera at different times. This section addresses this problem of merging multiple projections into a single representation. As described in Section 3.1, a DDT triangulation is executed for each image used in a projection. Thus, there will be a triangulated mesh (a list of triangles) for each image (for each projection), to which we refer as local triangulated mesh. Local triangulated meshes for the example of Fig. 1 are shown in Fig. 2. Let M be the global triangulated mesh, defined in R 2 , so that only one global mesh exists per each primitive. This global mesh should be updated when new projections are collected or, in other words, when novel local meshes are received, i.e. it should contain the result of the fusion of the several textures. A local triangulated mesh from projection index j = { k , l , t } (i.e., form a given combination of k , l , t ) is denoted as T j . Local triangulated meshes contain T j number of triangles. Individual triangles are denoted as T i j , ∀ i ∈ { 0 , 1 , … , T { k , l , t } } , when indicating the i th triangle of the local mesh j , or as T { v 1 , v 2 , v 3 } j , in the case the vertices v 1 , v 2 and v 3 are specified. Likewise, triangles in the global mesh are notated as M n , ∀ n ∈ { 0 , 1 , … , N } , where N is the number of triangles in the global mesh. When the vertices of the triangles are specified, then the notation M { V 1 , V 2 , V 3 } is used. To continuously fuse local triangulated meshes from new projections onto the global mesh, we propose a mechanism which iterates all the triangles in the local projection mesh and decides whether they should be inserted in the global mesh by computing the benefit of this operation to the overall quality of the global mesh. At iteration i , triangle T i j from the local projection mesh is referred to as candidate triangle. First, the algorithm assesses if there is overlap between ( T i j ) and any of the existing triangles in the global mesh M n , ∀ n ∈ { 0 , 1 , … , N } . Let intr ( A , B ) be a function that tests intersection between triangles A and B . The test can be written as: (6) do_intersect = intr ( T { V 1 , V 2 , V 3 } j , M n ) , ∀ n ∈ { 0 , 1 , … , N } , where N corresponds to the total number of triangles in the global mesh M . The intersection of two triangles can result in an empty set, whenever there is no intersection, in a point, a line segment, or a polygon. There are several approaches to triangle triangle intersection tests, that provide fast and efficient algorithms [25–27]. Note that there is a distinction between overlap and intersection: what must be assessed is whether or not an insertion of the candidate triangle onto the global mesh will change its configuration. Thus, an overlap test is not the same as an intersection test, since there are some cases where the triangles do intersect but the mesh configuration is not altered. The overlap test is based on a set of rules that analyse the return of the intersection function (intr, implementation from [28]), between candidate triangle T i j and global mesh triangle M { V 1 , V 2 , V 3 } . It returns yes if the triangles overlap or no otherwise. The algorithm is detailed in Eqs. (7)–(9): (7) { no ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = ∅ yes ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = list of polygons Goto (8) ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = points X Goto (9) ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = line segment L where X = { X 0 , X 1 , … , X N } and L = { S 0 E 0 ¯ , … , S N E N ¯ } ; (8) { no ⇐ ∃ V g : V g = X o , ∀ V g ∈ { V 1 , V 2 , V 3 } , ∀ o ∈ { 0 , 1 , … , N } yes ⇐ otherwise (9) { no ⇐ ∃ V g : V g = S o ∧ ∃ V h : V h = E o , ∀ V g , V h ∈ { V 1 , V 2 , V 3 } , ∀ o ∈ { 0 , 1 , … , N } yes ⇐ otherwise . Fig. 5 (a) shows a global primitive mesh M that contains a single triangle (in blue), and a candidate triangle (in red). In each case, the geometries returned by the intersection function are as follows: an empty set (d), points (a) and (e), line segments (b) and (f), and polygons (c). If we consider the cases where the insertion of the candidate triangle (in red) does not change the configuration of the already existing global mesh (in this case, the initial global mesh is composed of a single triangle, in blue), we can say that in case (a), (b) and (c) the mesh would be altered, and that, in cases (d), (e) and (f) the mesh would remain unaltered. The overlap test returns a list L of indices of triangles from the global mesh which overlap with the candidate triangle. From this, the benefit of inserting the candidate triangle in the global mesh is assessed. In this context, benefit is defined as an improvement in the quality of texture and estimated as follows: (10) { beneficial ⇐ L is empty otherwise { beneficial , ⇐ q ( T i j ) > α ⋅ max ( q ( M g ) ) , ∀ g ∈ L not beneficial , ⇐ otherwise where α ≥ 1 is a user defined cost parameter, which defines how much better the quality of candidate triangle must be to any other triangle it overlaps, in order for the insertion to be considered beneficial, and q ( ⋅ ) is an arbitrary function that returns the estimated quality of each triangle. In this work we define quality as directly proportional to the resolution of the texture. An image provides a texture of higher resolution when it is closer to the GPP. In addition to this, the focal distance of the camera should be taken into account. Thus, the quality of a triangle T j = { k , l , t } is proposed as follows: (11) q ( T { k , l , t } ) = f l D { k , l , t } , where f l is the focal distance of camera index l and D { k , l , t } is the distance between the camera l and the GPP k computed at time t . When the insertion of a candidate triangle is considered to be beneficial, the next step is to execute the insertion in the global mesh. The global primitive mesh is built as a constrained Delaunay triangulation. Hence, a description of the mesh contains a set of vertices, edges and constraints (implementation from [22] is used). After insertion, the candidate triangle should be preserved on the updated mesh (since it had a larger quality than any global mesh triangle it overlapped). The configuration of all other non overlapped global mesh triangles should also be preserved. In order to comply with those objectives, the insertion of a candidate triangle is composed of a set of atomic operations executed in sequence. Fig. 6 will be used to demonstrate why the proposed set of operations is required, by comparing it to other possibilities. Fig. 6(a) shows the initial situation: an existing mesh in blue must be altered by the insertion of a candidate triangle in red. The blue triangle has all three edges constrained (blue squares, in Fig. 6(a)). Let insert ( V , M ) be a function that inserts vertex V into mesh M . (12) M ∗ = insert ( V , M ) , ∀ V ∈ { V a , V b , V c } , where M ∗ is the updated mesh. Fig. 6(b) shows the updated mesh after the insertion of the three vertices, indices 4 , 5 and 6 (see vertices indices in the Fig. 6). The updated mesh does not preserve the configuration of the candidate triangle. In other words, there is no triangle with vertices 4 , 5 , 6 in the updated mesh. The expression that asserts if the configuration of the candidate triangle is preserved can be stated as follows: (13) { preserve T { V a , V b , V c } j , if ( ∃ M { V d , V e , V f } ∗ ∈ M ∗ ) : V d = V a ∧ V e = V b ∧ V f = V c do not preserve T { V a , V b , V c } j , otherwise . One of the reasons why the simple insertion of the vertices does not work is that the existing mesh had some constrained edges. After the mesh is updated, these constraints continue to exist (see squares on edges 1–2, 2–3, and 1–3 in Fig. 6(b)). The configuration of the candidate triangle is not kept because no constraints over the edges of that triangle are set. Hence, the second alternative is to execute an additional operation on top of the insertion of vertices V a , V b and V c . Let add_constraint ( e , M ) be a function that adds a constraint on edge e . The operation can be expressed as follows: (14) M ∗ = add_constraint ( e , M ) , ∀ e ∈ { V a – V b , V b – V c , V a – V c } where V a – V b denotes the edge defined between vertices V a and V b . Fig. 6(c) shows the updated mesh after this procedure is executed. Also in this case the configuration of the candidate triangle is not preserved. The reason is that there are conflicting constraints inserted in the mesh. For example, initially, the global mesh had a constraint over edge 1–2 (see indices in Fig. 6). At the same time the constraint V a – V b is inserted into the mesh. Since these two constraints intersect, a new vertex is created at the intersection point (vertex 7). Since a vertex is created at the intersection of the two initial constrained edges, four new edges are created (edges 4–7, 7–8, 1–7 and 7–10). All of these edges are constrained. From Fig. 6(c), one can see that the overall result of this approach is that neither the candidate triangle nor the existing mesh is preserved. The reason is that contradictory (intersecting) constraints are inserted in the mesh. The solution is to remove the constraints from edges in the global mesh that intersect edges from the candidate triangle, prior to inserting the vertices and constraints of the candidate triangle. Let E = { e 0 , e 1 , … , e N , } be the list of the global mesh constrained edges that intersect any of the candidate triangle’s edges, and remove_constraint ( e , M ) a function that removes the constraint from edge e in the mesh M. The prepared mesh M ′ is obtained as follows: (15) M ′ = remove_constraint ( e , M ) , ∀ e ∈ E , and after this, the operations described in Eqs. (12) and (14) are executed. Fig. 6(d) shows the results of this approach. The mesh preparation stage detected the following intersections (indices in Fig. 6(a) and (d)): V a – V b intersects with V 1 – V 2 , V a – V b intersects with V 1 – V 3 , V b – V c intersects with V 1 – V 2 and V b – V c intersects with V 1 – V 3 . As a result, the constraints of edges V 1 – V 2 and V 2 – V 3 are removed. Note that in Fig. 6(d), the prepared mesh (not the initial global mesh) is shown in blue, and those constraints no longer appear. More important, the candidate triangle’s configuration is preserved (triangle 4–5–6). In this particular case, the initial configuration of the mesh is lost, since there was overlap between the candidate triangle and the initial mesh triangle. We now show an example of continuous update of the global mesh: three new projections ( C j = 1 , C j = 2 , C j = 3 ) are available to update to the initial mesh M . The projections are mapped sequentially, generating updated meshes M ∗ , M ∗ ∗ , etc. Each projection contains a single triangle to map to the global mesh. Triangles T { V a , V b , V c } 1 , T { V d , V e , V f } 2 and T { V g , V h , V i } 3 , correspond to projections C j = 1 , C j = 2 , C j = 3 , respectively. The quality of the triangles is such that the following holds: (16) q ( M n ) < q ( T { V a , V b , V c } 1 ) < q ( T { V d , V e , V f } 2 ) < q ( T { V g , V h , V i } 3 ) ∀ M n ∈ M , and the mesh update cost parameter is α = 1 , which means that there is no cost associated to the updating of the mesh (see Eq. (10)). In other words, the insertion of all three candidate triangles is considered beneficial. The initial mesh is shown in Fig. 7 (a), along with the three candidate triangles. Fig. 7(b) shows the mesh after the insertion of the first candidate triangle, i.e., M ∗ . Since there is no overlap, the candidate triangle is added to the mesh M { 4 , 5 , 8 } ∗ , and edges M { 4 – 5 } ∗ , M { 5 – 8 } ∗ , and M { 4 – 8 } ∗ are constrained. Also, since there was no overlap detected, the initial configuration of the mesh is preserved. The result of the second insertion is shown in Fig. 7(c). In this case, there is overlap between candidate triangle T { V d , V e , V f } 2 (seen in Fig. 7(a)) and triangle M { 2 , 5 , 7 } ∗ (seen in Fig. 7(b)). An intersection between edges V d – V e and edge M { 5 – 7 } ∗ (seen in Fig. 7(b)), is detected. As a result, the constraint from edge M { 5 – 7 } ∗ is removed. The insertion results in a new triangle M { 9 , 10 , 11 } ∗ ∗ . Note also that the overlapping triangle M { 2 , 5 , 7 } ∗ was not preserved, i.e., it does not exist in the new mesh M ∗ ∗ . Finally, the third insertion detects that triangle T { V g , V h , V i } 3 overlaps triangles M { 3 , 4 , 5 } ∗ ∗ , M { 4 , 5 , 8 } ∗ ∗ and M { 2 , 3 , 5 } ∗ ∗ . Edges M { 3 – 4 } ∗ ∗ , M { 4 – 5 } ∗ ∗ and M { 3 – 5 } ∗ ∗ intersect the edges of T { V g , V h , V i } 3 which is why their constraints are removed (actually, in this case they disappear after the candidate triangle is inserted). The insertion of candidate triangles sometimes creates not only the candidate triangle itself, but also some additional triangles on the mesh. It is the case, for example, of triangle M { 7 , 9 , 10 } ∗ ∗ . We refer to this type of triangles as orphan triangles, meaning they have no parent projection. These are shown in grey color in Fig. 8 . Unlike triangles with parent projections, these triangles do not belong to any projection and thus they do not derive from the DDT triangulation executed over an image of some projection. Because of this, there is no guarantee that these orphan triangles are compliant with edges in the projection images. For this reason, we propose that orphan triangles are set to have the quality 0. In summary, this approach for the update of a global primitive mesh consists of a set of procedures that are capable of updating the mesh whenever new, better quality triangles are available for insertion, but at the same time the mechanism is capable of filling the gaps left empty using orphan triangles. 4 Results This section shows results both from one-shot texture mapping using DDTs, as well as results from the algorithm proposed to conduct incremental texture mapping. 4.1 One-shot texture mapping Fig. 9 (a) shows the detection of line segments in an image. Fig. 9(b) displays the result of a Delaunay triangulation with arbitrary configuration, e.g. computed by giving only the vertices as input (green dots in Fig. 9(a)). Because the triangulated mesh has an arbitrary configuration, triangle often contain areas with multiple textures. This would cause problems when using affine texture mapping. Notice the large triangle that covers part of the roof of the building, as well as a portion of the sky. This triangle contains a significant change in color and thus its affine texture mapping would result inaccurate. Fig. 9(c) shows the result of the proposed DDT approach, where a constrained Delaunay triangulation is used. This triangulation is computed using as input the vertices as in the previous case but also the detected line segments (red lines in Fig. 9(a), constrained edges also shown in (c) with red lines). In this case, the large triangle described above does not exist. In fact, there are no triangles which contain both sky and roof. Thus, we can argue that the proposed approach creates a mesh in which triangles contain smooth color transitions. The next section addresses the incremental update of these triangulation meshes. 4.2 Incremental texture mapping To show the results of incremental texture mapping, we recover the example of Section 1 (see Fig. 1): the vehicle approaches a wall panel, which has the word START written on it and collects four images in sequence (color coded black-red-orange-yellow in Fig. 10 ). The global mesh is created with the first image and then updated three times. The global triangulated mesh at each iteration is shown in Fig. 10 (top). Textures for each of these cases are show in Fig. 10 (bottom). Projection C { k = 4 , l = 1 , t = t 1 } is used to create the global mesh. Thus, the global mesh is composed only of triangles with parent projection C { k = 4 , l = front center , t = t 1 } (black triangles in Fig. 10(a)). Then, a new projection C { k = 4 , l = 1 , t = t 2 } becomes available. The global mesh is updated (Fig. 10(b)), and now contains a majority of triangles from C { k = 4 , l = 1 , t = t 2 } (red triangles). Then projection C { k = 4 , l = 1 , t = t 3 } is mapped. Since only a right side portion of the primitive is seen, orange triangles can be observed on the right side of Fig. 10(c)), while the left side retains red colored triangles from previous projections. Orphan triangles (in blue) are generated to fill the gaps that appear between the triangles with parent projections. Finally, projection C { k = 4 , l = 3 , t = t 4 } is used. This image views only the left portion of the wall panel. As such, we can see yellow triangles on the left side of (Fig. 10(d)). This example shows how the proposed mechanism is capable of creating and maintaining a global triangulated mesh which is used for enhancing the texture mapped onto the GPPs whenever new (and better) images are collected. 4.3 Projection of a single camera onto the ground plane This section shows three examples of how the global primitive mesh evolves when using a single camera to map a single primitive. We consider a similar scene to the one presented in Fig. 1. Throughout the three time instants t = t 1 , t = t 2 and t = t 3 , the vehicle is moving forward. From t 1 to t 2 the vehicle drives straight, and from t 2 to t 3 the vehicle turns slightly to the right. In this case the primitive that represents the ground plane is used for texture mapping ( k = 0 ). As a consequence, there is always a portion of the images from the projections that view the ground. In other words, at all instants any of the cameras view a portion of the ground, since they are pointed downwards. We will consider three different cases, each generating a unique scene representation: In the first case only the front center camera ( l = 1 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 1 , t = t 1 } , C { k = 0 , l = 1 , t = t 2 } and C { k = 0 , l = 1 , t = t 3 } ; In the second case only the rear center camera ( l = 4 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 4 , t = t 1 } , C { k = 0 , l = 4 , t = t 2 } and C { k = 0 , l = 4 , t = t 3 } ; In the third case only the front left camera ( l = 3 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 3 , t = t 1 } , C { k = 0 , l = 3 , t = t 2 } and C { k = 0 , l = 3 , t = t 3 } . The final global primitive meshes (those obtained after inserting projections at times t 1 , t 2 and t 3 ) for each case are displayed in Fig. 11 (left column). Fig. 11 (right column) shows the percentage of triangles of the global mesh that belong to each projection, as a function of the mission time. Note that the final position of the vehicle (which is the same for all cases) is depicted in the images, and bear in mind that, during this sequence, the vehicle moves forward from the right to the left. The triangles of the global primitive mesh are shown in colors, where each color corresponds to a particular projection. Fig. 11 (first row, left) shows the distribution of triangles according to the parent projection. In this case, the images are provided by the front center camera. As the vehicle moves forward, the ground in front of the vehicle that has been previously mapped by previous projections is now visible in images at a closer range. This leads to the effect that more recent projections tend to override older projections, i.e., red color ( t 2 ) overrides black color ( t 1 ), and yellow color ( t 3 ) overrides the other two. Fig. 11 (first row, right) shows that at time t 1 , only triangles from the first projection (black) and orphan triangles (blue) exist. Then, at time t 2 , the triangles from the second projection (red) are added to the global mesh. As a consequence, the percentage of triangles from the first projection (black) decreases. At time t 3 , the third projection again takes the major slice of percentage with respect to the previous two projections. In front facing cameras, when the vehicle is moving forward, more recent projections tend to contribute with a larger portion of the total triangles in the global mesh. The second case is shown in 11 (second row, left). Here, since the camera is facing the rear side of the vehicle, the opposite phenomena occurs: the vehicle is moving away from the ground behind it, and thus older projections were taken at closer distances to the ground. As a consequence, the red color (projection at t 2 ) overrides the yellow color (projection at t 3 ), and the black color (projection at t 1 , the oldest one) overrides all others. This is observable in Fig. 11 (second row, right), where the first projection (black) is, at all times, the one with the largest percentage of the triangles. Fig. 11 (third row, left) shows the third case. Here, since the camera is facing the left side of the vehicle, a hybrid phenomena takes place. For each projection, there is always a portion of the triangles, i.e., those that map the ground directly in front of the camera for that particular instant, that have a higher projection quality when compared to others. Fig. 11 (third row, right) also shows this tendency: the percentage of projections tends to be the same for all projections, which is why the second projection (red) when first mapped at time t 2 achieves approximately the same percentage of triangles as the first projection (black). They continue to have similar percentages also at time t 3 . At time t 3 , the third projection (yellow) obtained a higher value of percentage because the vehicle as turned slightly to the right and the left camera faced an area of the ground that was not previously mapped by any of the previous projections. 4.4 Projection of multiple cameras onto the ground plane The examples given in Section 4.3 have shown that the proposed algorithm is capable of handling multiple projections, correctly determining which are the best quality projections to map onto the global mesh. Nonetheless, those examples were simplified since only one camera was considered to provide projections in each case. In this section, the five cameras onboard the Talos are used to provide projection to be mapped onto the ground plane. The same sequence is used: the vehicle is moving forward and three time instants are used to generate projections. Each time instant t 1 , t 2 and t 3 generates five projections, one for each camera. Fig. 12 (a) shows the state of the global mesh after time t 1 . Five projections are contained in the mesh. At time t 2 , the global mesh incorporates many of the projections that are computed at this time (Fig. 12(b)). The same occurs at time t 3 (Fig. 12(c)). Note that these images are not exactly the same as those in Fig. 11, because in that case only the final global projection mesh was shown for three different examples. Here, we show the state of a single global projection mesh at times t 1 , t 2 and t 3 . Thus, in this case it is possible to see how the mesh evolved as more projections became available. The resulting mesh is an intricate mosaic of triangles coming from several projections. At time t 1 , the area of projection from the rear center camera was not connected to the areas of projection of the other cameras. Note that the red triangles in Fig. 12(a) are not connected to any triangle with a parent projection, only to orphan triangles. This unmapped region corresponds to the ground that was below the vehicle at time t 1 . Obviously, there is no coverage from the cameras for that area, so the system handles this by defining orphan triangles (blue) to cover that area. At time t 2 , the vehicle has moved forward, and the uncovered ground is now visible from the rear camera. Hence, the areas mapped by the rear cameras connect to the areas mapped by the other cameras, as seen in Fig. 12(b). At time t 3 , since the vehicle has turned to the right, the rear camera now views a different portion of the ground that had not been captured by any other camera. Note, in Fig. 12(c), how the triangles of the rear camera (the brightest yellow at the bottom right side) map a region that was not seen before and was previously covered only by orphan (blue) triangles. Fig. 12(d) shows the percentage of triangles of each projection as a function of the mission time. As each time instant, only newly acquired projections are used to update the mesh. Hence, triangles from previous iterations, if removed, will not be retested for insertion. That means each triangles is tested for insertion a single time. If a triangle is removed, it will never again be reinserted. This can be observed in the Figure, since none of the projections increases the percentage of triangles it contains. Fig. 13 shows the fifteen images used to compute these representations. As the vehicle moves and turns around, more and more of the ground that had not been viewed before is covered by new projections. This is a clear example of why integrating several projections over time is advantageous. A composite photometric description of the environment can be obtained that was impossible to compute without the capability of integrating multiple projections over time in an incremental fashion. The incremental texture mapping of an entire scenario can be observed in The scenario is composed of the entire (MIT) sequence. All five cameras onboard the Talos vehicle are used as input to the texture mapping. Geometric primitives are represented in the environment by the blue–green polygons. A blue to green colormap is used to color the primitives according to their index, the more recently detected the primitive, the closer to green it is. Photometry is represented by the texture mapped onto the primitives. Note that at each of the time instants new projections will update the global meshes of the detected polygonal primitives. Hence the scenario representation will evolve photometrically over time. Furthermore, also the geometric representation will evolve over time (see [2] for details). For a better visualization, the primitive that represents the ground plane is not textured in the video. 5 Conclusions This paper addressed the problem of how to create and update a triangulated mesh. These meshes are used for texture mapping surfaces in 3D, and the input are images collected from cameras mounted on-board a vehicle. The geometric structure onto which texture is mapped is described in detail in [2] and given as a list of polygons. Because the atomic entities of the 3D structure are defined as polygons (rather than triangles), it is possible to perform a triangulation of the convex hull of that polygon, as opposed to having an arbitrary triangulation. This triangulation is computed in the image space, and is defined as a constrained Delaunay triangulation. This makes it possible to impose line segments as constrained edges in the triangulation, which creates triangles with smooth color transitions. This, in turn, makes it possible to use affine texture mapping. Incremental texture mapping is done by creating and updating a global triangulated mesh per geometric primitive. The update of this mesh is done using meshes created from projections. In this paper, we have proposed a sequence of operations which are used for inserting triangles from the projection mesh into the global triangulation mesh. This procedure ensures that the inserted triangles maintain their configuration as well as the existing triangles which do not overlap the inserted triangles. Furthermore, the proposed algorithm fills the gaps in the mesh where there are to triangles with parent projections with orphan triangles. Using this mechanism, the holes that could exist between textures of different projections are replaced by orphan triangles where texture is interpolated, resulting in a better overall quality of the texture. To the best of our knowledge, this is the first approach in this field capable of fusing images continuously and in an incremental fashion in order to generate a single texture of good quality. Acknowledgments This work has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”, reference CEB-02502014. References [1] M. Oliveira V. Santos A.D. Sappa P. Dias Scene representations for autonomous driving: An approach based on polygonal primitives Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 1 2016 Springer International Publishing Cham 503 515 (Chapter) [2] M. Oliveira V. Santos A. Sappa P. Dias A.P. Moreira Incremental scenario representations for autonomous driving using geometric polygonal primitives Robot. Auton. Syst. 2016 10.1016/j.robot.2016.05.011 in press [3] A.S. Huang M. Antone E. Olson L. Fletcher D. Moore S. Teller J. Leonard A High-rate, Heterogeneous data set from the DARPA urban challenge Int. J. Robot. Res. 29 13 2011 1595 1601 [4] A. Huang, E. Olson, D. Moore, LCM: Lightweight communications and marshalling, in: 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, 2010, pp. 4057–4062. [5] M. Segal C. Korobkin R. van Widenfelt J. Foran P. Haeberli Fast shadows and lighting effects using texture mapping SIGGRAPH Comput. Graph. 26 2 1992 249 252 [6] P. Debevec Y. Yu G. Boshokov Efficient view-dependent image-based rendering with projective texture-mapping, Tech. Rep. 1998 [7] S. Rippa Long and thin triangles can be good for linear interpolation SIAM J. Numer. Anal. 29 1 1992 257 270 [8] M. Garland P.S. Heckbert Fast polygonal approximation of terrains and height fields, Tech. Rep. CMU-CS-95-181 1995 [9] R. Schätzl H. Hagen J.C. Barnes B. Hamann K.I. Joy Data-dependent triangulation in the plane with adaptive knot placement Geometric Modelling 2001 309 321 [10] H. Hoppe, Progressive meshes, in: Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96, 1996, pp. 99–108. [11] L. Demaret N. Dyn A. Iske Image compression by linear splines over adaptive triangulations Signal Process. 86 7 2006 1604 1616 [12] A.D. Sappa M.A. García Coarse-to-fine approximation of range images with bounded error adaptive triangular meshes J. Electron. Imaging 16 2 2007 23010 [13] N. Dyn D. Levin S. Rippa Boundary correction for piecewise linear interpolation defined over data-dependent triangulations J. Comput. Appl. Math. 39 2 1992 179 192 [14] L.L. Schumaker Computing optimal triangulations using simulated annealing Comput. Aided Geom. Design 10 3–4 1993 329 345 [15] Opengl D. Shreiner M. Woo J. Neider T. Davis OpenGL(R) Programming Guide: The Official Guide to Learning OpenGL(R), Version 2 fifth ed. 2005 Addison-Wesley Professional [16] D. Blythe The direct3d 10 system ACM Trans. Graph. 25 3 2006 724 734 [17] B. Lehner G. Umlauf B. Hamann Survey of techniques for data-dependent triangulations H. Hagen M. Hering-Bertram C. Garth GI Lecture Notes in Informatics, Visualization of Large and Unstructured Data Sets 2007 178 187 [18] M. Cervenansksy Z. Toth J. Starinsky A. Ferko M. Sramek Parallel gpu-based data-dependent triangulations Comput. Graph. 34 2 2010 125 135 [19] I. Svalbe Natural representations for straight lines and the hough transform on discrete arrays IEEE Trans. Pattern Anal. Mach. Intell. 11 9 1989 941 950 [20] V. Kamat, S. Ganesan, A robust hough transform technique for description of multiple line segments in an image, in: 1998 International Conference on Image Processing, 1998. ICIP 98. Proceedings. Vol. 1, 1998, vol. 1, pp. 216–220. [21] R. Guerreiro, P. Aguiar, Incremental local hough transform for line segment extraction, in: 2011 18th IEEE International Conference on Image Processing, ICIP, 2011, pp. 2841–2844. [22] M. Yvinec 2D triangulations CGAL User and Reference Manual, 4.0 Edition 2012 CGAL Editorial Board [23] L.P. Chew, Constrained delaunay triangulations, in: Proceedings of the Third Annual Symposium on Computational Geometry, SCG ’87, 1987, pp. 215–222. [24] J.R. Shewchuk General-dimensional constrained delaunay and constrained regular triangulations i: Combinatorial properties Discrete Comput. Geom. 39 1 2008 580 637 [25] T. Moller A fast triangle-triangle intersection test J. Graph. Tools 2 1997 25 30 [26] J.-W. Chang M.-S. Kim Efficient triangle–triangle intersection test for OBB-based collision detection Comput. Graph. 33 3 2009 235 240 [27] A.D. Sappa, M.A. García, Incremental multiview integration of range images, in: ICPR, 2000, pp. 1546–1549. [28] E. Fogel R. Wein B. Zukerman D. Halperin 2D regularized Boolean set-operations CGAL User and Reference Manual, 4.0 Edition 2012 CGAL Editorial Board Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Vítor Santos obtained a 5 year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990 1994 at the Joint Research Center, Italy. He is currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or cosupervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He is also cofounder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Angel Domingo Sappa (S’1994–M’00–SM’12) received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, where he is currently a Senior Researcher. He is a member of the Advanced Driver Assistance Systems Group. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereoimage processing and analysis, 3D modelling, and dense optical flow estimation. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. António Paulo Moreira graduated with a degree in electrical engineering at the University of Oporto, in 1986. He then pursued graduate studies at University of Porto, obtaining a M.Sc. degree in electrical engineering—systems in 1991 and a Ph.D. degree in electrical engineering in 1998. Presently, he is an Associate Professor at the Faculty of Engineering of the University of Porto and researcher and manager of the Robotics and Intelligent Systems Centre at INESC TEC. His main research interests are process control and robotics. "
    },
    {
        "doc_title": "Incremental scenario representations for autonomous driving using geometric polygonal primitives",
        "doc_scopus_id": "85006515693",
        "doc_doi": "10.1016/j.robot.2016.05.011",
        "doc_eid": "2-s2.0-85006515693",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3d representations",
            "Autonomous Vehicles",
            "Geometric structure",
            "Point cloud",
            "Polygonal primitives",
            "Range measurements",
            "Reconstruction techniques",
            "Scene reconstruction"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.When an autonomous vehicle is traveling through some scenario it receives a continuous stream of sensor data. This sensor data arrives in an asynchronous fashion and often contains overlapping or redundant information. Thus, it is not trivial how a representation of the environment observed by the vehicle can be created and updated over time. This paper presents a novel methodology to compute an incremental 3D representation of a scenario from 3D range measurements. We propose to use macro scale polygonal primitives to model the scenario. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Furthermore, we propose mechanisms designed to update the geometric polygonal primitives over time whenever fresh sensor data is collected. Results show that the approach is capable of producing accurate descriptions of the scene, and that it is computationally very efficient when compared to other reconstruction techniques.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-06-07 2016-06-07 2016-08-04 2016-08-04 2016-08-04T18:32:25 S0921-8890(16)30060-4 S0921889016300604 10.1016/j.robot.2016.05.011 S300 S300.1 FULL-TEXT 2020-11-22T06:55:28.343358Z 0 0 20160901 20160930 2016 2016-06-07T15:16:25.691999Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid orcid primabst ref vitae 0921-8890 09218890 true 83 83 C Volume 83 27 312 325 312 325 201609 September 2016 2016-09-01 2016-09-30 2016 article fla © 2016 Elsevier B.V. All rights reserved. INCREMENTALSCENARIOREPRESENTATIONSFORAUTONOMOUSDRIVINGUSINGGEOMETRICPOLYGONALPRIMITIVES OLIVEIRA M 1 Introduction 2 Related work 3 Proposed approach 3.1 One-shot scene reconstruction 3.2 Incremental scenario reconstruction 4 Results 4.1 One-shot reconstruction 4.2 Incremental reconstruction 4.3 Comparison of approaches with and without expansion 5 Conclusions Acknowledgments References BIRK 2009 53 60 A BURGARD 2009 757 758 W OLIVEIRA 2016 M HUANG 2011 1595 1601 A ONIGA 2010 1172 1182 F ZHOU 2011 669 681 K THRUN 2006 661 692 S URMSON 2006 467 508 C URMSON 2008 425 466 C MONTEMERLO 2008 M BACHA 2008 467 492 A CHEN 2011 762 775 Y DEMEDEIROSBRITO 2008 1130 1140 A BERNARDINI 1999 349 359 F BYKAT 1978 296 298 A BARBER 1996 469 483 C AICHHOLZER 1995 752 761 O OLIVEIRAX2016X312 OLIVEIRAX2016X312X325 OLIVEIRAX2016X312XM OLIVEIRAX2016X312X325XM 2018-08-04T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. 2020-11-19T10:33:14.232Z S0921889016300604 Portuguese Foundation for Science and Technology FCT Fundação para a Ciência e a Tecnologia ERDF European Regional Development Fund FEDER European Regional Development Fund Spanish Government TIN2014-56919-C3-2-R Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación CEB-02502014 SENESCYT Secretaría de Educación Superior, Ciencia, Tecnología e Innovación Operational Programme for Competitiveness and Internationalisation Fundação para a Ciência e Tecnologia POCI-01-0145-FEDER-006961 SFRH/BD/43203/2008 UID/CEC/00127/2013 SFRH/BPD/109651/2015 INCT-EN Instituto Nacional de Ciência e Tecnologia para Excitotoxicidade e Neuroproteção COMPETE POFC Programa Operacional Temático Factores de Competitividade This work has been supported by the Portuguese Foundation for Science and Technology “ Fundação para a Ciência e Tecnologia ” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador ”, reference CEB-02502014 . item S0921-8890(16)30060-4 S0921889016300604 10.1016/j.robot.2016.05.011 271599 2016-08-04T15:40:51.774542-04:00 2016-09-01 2016-09-30 true 5888810 MAIN 14 58041 849 656 IMAGE-WEB-PDF 1 fx5 12830 69 83 gr4 17555 76 219 gr2 28101 119 219 fx1 23862 150 219 gr14 15759 58 219 fx6 24677 164 125 fx3 12077 56 49 pic5 33838 164 140 pic2 25222 164 141 gr15 17370 87 219 gr8 14416 27 219 fx4 12819 69 83 gr12 27038 127 219 gr1 21146 87 219 fx2 12077 56 49 gr5 22864 164 134 gr13 16887 164 205 pic3 23197 163 140 gr7 30380 136 219 gr6 53566 152 219 gr9 16879 87 219 pic4 25191 164 141 gr11 29235 115 219 pic1 24035 163 140 gr16 24620 131 219 gr3 18915 65 219 gr10 22449 111 219 fx5 29274 15 18 gr4 68423 200 580 gr2 68835 205 376 fx1 97662 388 565 gr14 73492 200 757 fx6 187395 742 566 fx3 28364 13 11 pic5 40387 131 112 pic2 38463 131 113 gr15 65788 226 572 gr8 45793 78 631 fx4 29234 15 18 gr12 149219 356 616 gr1 83494 246 619 fx2 28364 13 11 gr5 160412 724 591 gr13 51934 475 593 pic3 36162 132 113 gr7 132506 412 662 gr6 89241 236 339 gr9 61854 228 573 pic4 34967 131 113 gr11 148501 300 572 pic1 37492 132 113 gr16 101169 373 625 gr3 81281 161 545 gr10 75175 289 570 si25 624 18 180 si21 159 15 10 si46 131 11 12 si4 139 12 10 si14 135 12 10 si10 112 11 6 si26 165 17 13 si15 182 15 22 si11 574 21 138 si12 152 11 15 si9 180 18 14 si51 661 21 142 si54 574 15 150 si2 165 13 17 si48 1945 41 389 si16 225 15 35 si34 133 11 11 si18 735 31 184 si1 309 18 50 si47 344 15 58 si41 117 10 11 si6 397 18 68 si27 143 13 12 si13 166 15 18 si36 159 15 11 si42 131 15 11 si5 318 18 49 si49 207 12 25 si3 114 10 8 si50 165 11 21 si24 174 14 22 si35 170 14 19 si22 195 14 27 si55 568 15 149 si45 139 11 13 si33 123 8 10 si32 120 8 10 si19 844 12 234 ROBOT 2641 S0921-8890(16)30060-4 10.1016/j.robot.2016.05.011 Elsevier B.V. Fig. 1 Plane detection examples using RANSAC: (a) five best RANSAC candidates for the input point cloud in grey; (b) a detail of (a); (c) without using clustering; (d) using clustering. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Scenario reconstruction approach (1): BPA surface reconstruction over the accumulated point cloud of three scenes. Fig. 3 Scenario reconstruction approach (2): Accumulation of scene reconstructions over multiple scenes, each marked with a different color; (a) BPA; (b) GPP 2, shown without the ground plane for an easier visualization. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Orthogonal component of the expansion operation: (a) points are tested for their orthogonal distance to the support plane; (b) included points are projected to the support plane. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Successive iterations of the longitudinal expansion of GPPs. Solid lines in color represent the convex hull at the start of a given iteration, dashed lines the expanded polygon. Crosses over points mean they were added to the polygon in a given expansion: (a) initial situation; (b), (c), (d), (e) and (f) iterations 0–4, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 MIT sequence. Fig. 7 Location C of the MIT sequence: (a) 3d view; (b) top view; (c) satellite view of the location; (d) front 6 mm camera; (e) front; (f) rear; (g) left (h) right. Fig. 8 Detection of geometric polygonal primitives in the data-sets of the MIT sequence: (a) location C; (b) location D. Fig. 9 Cascade processing analysis for MIT sequence locations A through E: (a) the number of points left to process for a given input point cloud, as a function of the index of the detected primitive; (b) the time it takes to perform the detection of each of the geometric polygonal primitives as a function of the primitives index. Fig. 10 Reconstruction of location E of MIT sequence: (a) BPA; (b) GT; (c) POIS; (d) GPP2. The observation of Fig. 16(e) may help the reader better understand the viewpoints in these images. Fig. 11 Qualitative analysis of the one sided Hausdorff distance in location C sequence 1: (a) GT; (b) POIS; (c) GPP 1; (d) GPP 2; A Red–Green–Blue color map is used to code the distance. Red represents zero distance and blue maximum distance. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Results from the Hausdorff distance obtained when using alternatives for the GPP 2 method for location E, sequence 1: (a) the standard GPP 2, with ground plane and convex hull; (b) discarded ground plane, convex hull; (c) with ground plane, concave hull; (d) discarded ground plane, concave hull. A Red–Green–Blue color map is used to code the distance. Red represents zero distance and blue maximum distance. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 GPP reconstruction for sequence 1: (a) The reconstructed scenario after the input point cloud of location A is received; (b) after location B; (c) after location C; (d) after location D; (e) after location E. Fig. 14 (a) Comparison between the number of polygons generated by the algorithm using expansion and not using expansion through sequence 1; (b) number of points to use as input to the detection in both cases; (c) Total area of the detected polygons. Fig. 15 Analysis of the evolution of each of the polygons through sequence 1. Only pair index polygons are shown to simplify the graphs: (a) number of support points per polygon; (b) total area of the primitives. Fig. 16 Evolution of polygonal primitive 0 (the ground plane) through sequence 1: (a) location A; (b) location B; (c) location C; (d) location D; (e) location E. Table 1 LIDAR sensor systems mounted on some autonomous vehicle systems of recent years. Institution Vehicle Ref. 3D sensor type Total a 2D laser 3D laser Stanford U. Stanley b [11] 5 × Sick LMS 291 – 67.5 CMU Sandstorm b [12] 3 × Sick LMS 291 – 50.5 Highlander b Riegl Q140i – CMU Boss b [13] 6 × Sick LMS 291 Vel. HDL-64 2305.0 2 × Continental ISF 172 – 2 × IBEO Alasca XT – Stanford U. Junior c [14] 4 × SICK LMS 291 Vel. HDL-64 2278.0 2 × IBEO Alasca XT – Virginia Tech Odin c [15] 4 × Sick LMS 291 – 90.0 2 × IBEO Alasca XT – IBEO Alasca AO – MIT Talos c [6] 12 × Sick LMS 291 Vel. HDL-64 2361.2 U. Munich MuCar-3 d [16] – Vel. HDL-64 2200.0 Google Driverless Car [17] – Vel. HDL-64 2200.0 a Estimation of total 3D data throughput of all LIDAR sensors mounted on the vehicle, given as points × 10 3 / s . b These vehicles participated in the Defense Advanced Research Projects Agency (DARPA) Grand Challenge 2006. c These vehicles participated in the DARPA Urban Challenge 2007. d This vehicle participated in the Civilian European Land Robot Trial (ELROB) Trial 2009. Table 2 Information on each of the locations defined in the MIT sequence. Columns description: pt , number of points; size, memory size in mega bytes; t , mission time in seconds; d , traveled distance in meters. Location name Location snapshot Sequence accumulated pt ( × 10 6 ) Size (MB) a pt ( × 10 6 ) Size (MB) a t (s) d (m) A 1.3 15.6 1.3 15.6 1 0 B 1.3 15.6 13.0 156.0 11 75 C 1.3 15.6 26.0 312.0 21 125 D 1.3 15.6 39.0 468.0 31 140 E 1.3 15.6 52.0 624.0 41 190 a Computed from the number of points times the three xyz dimensions times the four bytes for each dimension (type float32). It is an approximate value since additional data is present in the message, such as the time stamp, the coordinate frame identification, etc. Table 3 Comparison of the computation time of several approaches for surface reconstruction on the MIT sequence. Results were obtained with an i7-860 2.8 GHz quad core processor, Ubuntu operating system. Sequence/Location Processing time (s) BPA GT POIS GPP 1 GPP 2 A 659.0 154.0 63.2 16.3 27.3 B 752.9 157.5 61.6 25.3 17.4 C 488.2 156.3 56.3 13.5 49.4 D 480.4 142.4 52.6 25.2 25.2 E 558.8 149.0 57.9 47.4 58.1 Average 585.9 151.8 58.3 25.5 35.5 Table 4 Comparison of the accuracy of the several approaches using BPA results as ground truth and Hausdorff distance as metric. Location Hausdorff distance (m) GT POIS GPP 1 GPP 2 max mean max mean max mean max mean A 11.7 0.15 14.0 1.39 7.6 1.02 7.6 0.87 B 11.8 0.12 14.1 1.39 12.7 0.94 12.6 0.81 C 12.7 0.18 13.9 1.06 8.9 0.87 8.9 0.69 D 13.8 0.10 13.9 1.90 7.6 0.86 7.6 0.69 E 12.5 0.14 14.0 1.42 14.0 1.25 14.0 1.11 Average 12.5 0.14 13.9 1.43 10.2 0.99 10.1 0.83 Table 5 Comparison of the Hausdorff distance accuracy of the GPP 2 approach using: the standard approach, convex hull and ground plane included (also in Table 4); the convex hull with no ground plane included; the concave hull with ground plane; and the concave hull without ground plane. GPP 2 Hausdorff distance (m) Hull Convex Convex Concave Concave Ground plane Included Not included Included Not included Location max mean max mean max mean max mean A 7.6 0.87 1.8 0.15 6.8 0.71 1.2 0.13 B 12.6 0.81 1.5 0.11 12.6 0.53 1.1 0.08 C 8.9 0.69 1.9 0.16 6.6 0.52 1.9 0.12 D 7.6 0.69 2.2 0.14 7.3 0.59 2.1 0.11 E 14.0 1.11 1.7 0.10 8.8 0.32 1.4 0.08 Average 10.1 0.83 1.8 0.13 8.4 0.53 1.5 0.10 Table 6 Online videos showing the system running for the MIT sequence. The non incremental approach corresponds to the continuous processing without using the expansion mechanism. It is possible to observe that because there is no expansion, the primitives are duplicated. URL Description Input data from MIT sequence Incremental approach Non incremental approach Incremental scenario representations for autonomous driving using geometric polygonal primitives Miguel Oliveira a b ⁎ Vitor Santos b Angel D. Sappa c d Paulo Dias b A. Paulo Moreira a e a INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal b IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal c Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica del Litoral, ESPOL, Campus Gustavo Galindo, Km 30.5 vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica del Litoral, ESPOL, Campus Gustavo Galindo Km 30.5 vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador d Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center, Campus UAB Bellaterra Barcelona 08193 Spain e FEUP - Faculty of Engineering, University of Porto, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal FEUP - Faculty of Engineering, University of Porto R. Dr. Roberto Frias s/n Porto 4200-465 Portugal ⁎ Corresponding author at: INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal. INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal When an autonomous vehicle is traveling through some scenario it receives a continuous stream of sensor data. This sensor data arrives in an asynchronous fashion and often contains overlapping or redundant information. Thus, it is not trivial how a representation of the environment observed by the vehicle can be created and updated over time. This paper presents a novel methodology to compute an incremental 3D representation of a scenario from 3D range measurements. We propose to use macro scale polygonal primitives to model the scenario. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Furthermore, we propose mechanisms designed to update the geometric polygonal primitives over time whenever fresh sensor data is collected. Results show that the approach is capable of producing accurate descriptions of the scene, and that it is computationally very efficient when compared to other reconstruction techniques. Keywords Incremental scene reconstruction Point clouds Autonomous vehicles Polygonal primitives 1 Introduction Recent research in the fields of pattern recognition suggest that the usage of 3D sensors improves the effectiveness of perception, “since it supports good situation awareness for motion level tele-operation as well as higher level intelligent autonomous functions” [1]. Nowadays, autonomous robotic systems have at their disposal a new generation of 3D sensors, which provide 3D data of unprecedented quality [2]. In robotic systems, 3D data is used to compute some form of internal representation of the environment. In this paper, we refer to this as 3D scene representation or simply 3D representation. The improvement of 3D data available to robotic systems should pave the road for more comprehensive 3D representations. In turn, advanced 3D representations of the scenes are expected to play a major role in future robotic applications since they support a wide variety of tasks, including navigation, localization, and perception [3]. In summary, the improvement in the quality of 3D data clearly opens the possibility of building more complex scene representations. In turn, more advanced scene representations will surely have a positive impact on the overall performance of robotic systems. Despite this, complex scene representations have not yet been substantiated into robotic applications. The problem is how to process the large amounts of 3D data. In this context, classical computer graphics algorithms are not optimized to operate in real time, which is a non-negotiable requirement of the majority of robotic applications. Unless novel and efficient methodologies that produce compact, yet elaborate scene representations, are introduced by the research community, robotic systems are limited to mapping the scenes in classical 2D or 2.5D representations or are restricted to off-line applications. Very frequently, the scenarios where autonomous systems operate are urban locations or buildings. Such scenes are often characterized for having a large number of well defined geometric structures. In outdoor scenarios, these geometric structures could be road surfaces or buildings, while in indoor scenarios they may be furniture, walls, stairs, etc. We refer to the scale of these structures as a macro scale, meaning that 3D sensor may collect thousands of measurements of those structures in a single scan. A scene representation is defined by the surface primitive that is employed. For example, triangulation approaches make use of triangle primitives, while other approaches such as Poisson surface reconstruction resort to implicit surfaces. Triangulation approaches generate surface primitives that are considered to have a micro scale, since a geometric structure of the scene could contain hundreds or thousands of triangles. Micro scale primitives are inadequate to model large scale environments because they are not compact enough. In this paper, we present a novel methodology to compute a 3D scene representation. The algorithm uses macro scale polygonal primitives to model the scene. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. The proposed representation addresses the problems that were raised in the previous lines: the representation is compact and can be computed much faster than most others, while at the same time providing a sufficiently accurate geometric representation of the scene from the point of view of the tasks required by an autonomous system. The second problem addressed in this paper is the reconstruction of large scale scenarios from a continuous throughput of massive amounts of 3D data. We will use the distinction between the terms scene and scenario. Let scenario refer to a particular location that should be reconstructed, e.g., a city, a road or a building. By scene, we refer to the portion of the scenario that is viewed by the vehicle at a particular time. Thus, the scenario is an integration of scenes over time. In the case of large scale scenarios, the compactness of a given scene representation is even more important. In this paper, we focus also on how the representation may evolve by integrating 3D data from multiple measurements over time. This is an extended version of [4]. The new material covers mostly the incremental part of the geometric reconstruction. There is also the possibility of adding texture to the geometric scene description. For further details on this see [5]. For testing and evaluation purposes, we use a data-set from the Massachusetts Institute of Technology (MIT) Team, taken from their participation in the DARPA Urban Challenge [6]. From this data-set we have extracted a 40 s sequence which will be used to assess the proposed algorithms. For the remainder of the paper, this sequence is referred to as MIT sequence. Using this data-set, we aim at reconstructing large portions of the urban environment in which the competition took place. The remainder of this paper is organized as follows: Section 2 reviews the state of the art, Section 3 presents the proposed approach. Results are given in Section 4 and conclusions in Section 5. 2 Related work At first glance, it would seem plain to translate the improvement on the quality of the 3D data into an enhancement of the 3D representations. However, the fact is that the majority of the robotic systems, namely autonomous vehicles, continue to rely on classic 2D or 2.5D scene representations [7], such as occupancy grids [8] or elevation maps [9], or use discretized grid-like approaches as in octrees [10]. The reason for that is that autonomous vehicles commonly require a large array of sensors installed on-board and, as a consequence, collect large amounts of range measurements every second. Table 1 shows an estimate of the amount of 3D data (measured by LIDAR systems alone) generated by several autonomous vehicles. Simplified 2D or 2.5D representations are used so that they can be computed in real time using large amounts of data. More advanced 3D representations have not been introduced in robotics because they fail to abide to the requirements of real time processing using the 3D data produced by new generation LIDAR sensors. One example of this is the methodologies used in the computer graphics research field: traditional algorithms such as building of triangular meshes are unable to operate in real time with the throughput of data provided by new generation 3D sensors. Some authors have tried to optimize triangulation algorithms (e.g., [2,7]), and they report near real time performances. Note that these results were obtained using point clouds from a Microsoft Kinect 3D camera. 1 1 On the other hand, the results provided towards the end of this paper are obtained using point clouds from a Velodyne HDL-64E Lidar, 2 2 and therefore results are not directly comparable. Scene reconstruction is defined as the computation of a geometric 3D model from multiple measurements. These measurements could be obtained from stereo systems, range sensors, etc. Scene reconstruction may also include the texturing of the generated 3D model. Scene reconstruction methodologies are grouped into two different approaches: surface based representations or volumetric occupancy representations. In the first, the underlying surfaces of the scene that generated the range measurements are estimated, while in the second, the range measurements are grouped into cells of a grid, and are then labeled free or occupied. Traditional surface based representations include several 3D triangulations methodologies, such as 3D Delaunay triangulation [18], or Ball Pivoting Algorithm (BPA) [19]. There are also some alternative higher order surface representation methods such as Poisson surface reconstruction [20], Orientation Inference Framework [21] or learning approaches [22]. However, most of these methods do not tackle well noisy range measurements and, above all, since these methods involve a large number of nearest neighbor queries, they are very slow to compute. One attempt to accelerate the triangulation of point clouds was done in [7], but authors report they have only achieved near real time. Volumetric occupancy representations include occupancy grids [8], elevation maps [9], multi-level surface maps [23] or octrees [10]. While these representations are easier to compute, they do not provide accurate information about the geometry of the scene. The BPA triangulation was proposed in [24]. The BPA computes a triangle mesh interpolating a given point cloud. The principle of the BPA is very simple: Three points form a triangle if a ball of a user-specified radius touches them without containing any other point. Starting with a seed triangle, the ball pivots around an edge (i.e., it revolves around the edge while keeping in contact with the edge’s endpoints) until it touches another point, forming another triangle. The process continues until all reachable edges have been tried, and then starts from another seed triangle, until all points have been considered. Although all range points are considered in the computation of the mesh, which accounts for the accuracy of the methodology, this fact also hampers the computational performance of the algorithm. The Greedy Triangulation (GT) is an approach designed for fast surface reconstruction from large noisy data sets [7]. Given an unorganized 3D point cloud, the algorithm recreates the underlying surface’s geometrical properties using data resampling and a robust triangulation algorithm, the authors claim to achieve near real time. For resulting smooth surfaces, the data is resampled with variable densities according to previously estimated surface curvatures. One of the advantages of this method is that, since a greedy search is executed, it is expected to be faster than other standard triangulation approaches. Poisson surface reconstruction was initially proposed in [25]. In this approach, surface reconstruction of a point cloud with estimated normals is viewed as a spatial Poisson problem. The Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, a Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. The work from [26] proposes an expectation maximization based method for producing a scene representation based on planes, rather than polygons. Also, this method is not intended for real time applications, since creating a scene representation may take up to twenty minutes. Finally, this work does not discuss how new sensor measurements could be integrated in the existing representation, therefore not focusing on the incremental part of scenario reconstruction. 3 Proposed approach In this section we will explain in detail the methodologies of our approach. First, we describe the scene reconstruction algorithm (see Section 3.1) and then, in Section 3.2, we describe how a scenario is created over time from a continuous throughput of 3D data. 3.1 One-shot scene reconstruction This work proposes to explore the usage of Geometric Polygonal Primitives (GPP) to perform scene reconstruction. In other words, the idea is to describe a scene by a list of macro scale polygons. The detection of geometric polygonal primitives is simple when compared to the detection of other more complex primitives. Furthermore, given that road environments are often geometrically structured, it seems feasible to represent the 3D structure with a set of planar polygons. In addition to that, polygons are compact representations: geometric polygonal primitives are described by a support plane and a bounding polygon. Let G i represent the i th polygonal geometric primitive of a given scene, with the support plane Hessian coefficients denoted by G p i = [ a i b i c i d i ] . The search for the support plane is done on a given input point cloud P using a Random Sample Consensus (RANSAC) procedure [27]. Note that there are other algorithms which are more efficient for detecting planes in 3D data [28]. However, an analysis of the impact of these alternative approaches is out of the scope of the current paper. RANSAC is an iterative method to estimate parameters of a mathematical model from a set of observed data points. The assumption is that data consists of inliers, i.e., data whose distribution can be explained by some set of model parameters, and outliers, data that does not fit the model. The input to the RANSAC algorithm is a set of observed data values, a parameterized model which is fitted to the observations, and the output are the model parameters, i.e., in the case of detecting the support plane of polygonal primitives, the Hessian coefficients. Fig. 1 (a) shows in different colors the inlier points of the five best candidates of a RANSAC search. Wall like structures are correctly detected. Fig. 1(c) shows the inliers (signaled in green) of a RANSAC plane detection. In this case, range measurements from two separate walls have been signaled as inliers of a single support plane. To address this issue, the set of inliers of each support plane hypothesis is used as input to a clustering procedure. By using the proposed clustering algorithm, it is possible to separate the two walls into separate polygons, as shown in Fig. 1(d). Polygons are computed using a 2D convex hull operation on the (clustered) RANSAC inliers. In this work, the implementation provided in [29] is used to compute the 2D convex hull, based on a non recursive version of [30,31]. To increase the efficiency of the algorithm, we propose to conduct the detection of polygonal primitives in a cascade like processing configuration. This should be more efficient and fast to process. The input point cloud contains a large amount of 3D points: we assume each 3D point can only belong to a single polygonal primitive. Let S k be the point cloud containing the support points of primitive k , and P k be the input point cloud in which the primitive was searched. The input point cloud for the search of the next primitive, P k + 1 , is obtained by removing the support points of primitive k : (1) P k + 1 = { ∈ P k ∣ ∉ S k } . Since every iteration of primitive detection will conduct a search on a smaller point cloud, it is expected that the cascade configuration is capable of reducing the processing time. Algorithm 1 details the complete procedure for the detection of a set of polygonal primitives given a point cloud. 3.2 Incremental scenario reconstruction We use the term scenario reconstruction to designate an incremental process of reconstruction of scenes from a continuous stream of sensor data. At first sight, there are three alternatives for performing scenario refinement: (1) store raw measurements and, in the end, reconstruct using the entire data; (2) reconstruct periodically and fuse partial scene reconstructions; (3) reconstruct with the first input data then make the representation evolve as new data arrives. Approach (1) is the most immediate, since there are well known surface reconstruction algorithms which reconstruct a surface from a single point cloud. This approach merely merges all input point clouds into a single point cloud, the accumulated point cloud, and then, standard surface reconstruction algorithms may be applied using the accumulated point cloud as input. One downside of this method is that the process of reconstruction can only begin after all point clouds have been collected. This is not suited for usage in real time applications. Furthermore, the amount of data that results from the accumulation of point clouds should be very large, which in turn might cause problems for reconstruction algorithms. Fig. 2 shows an example of a scenario reconstruction (with BPA) using as input the accumulated point clouds of three scenes. This reconstruction took over 2 h to complete. Alternative (2) proposes a fusion of (partially) reconstructed scenes for each of the locations. Taking the example of Fig. 2: if the three scenes are reconstructed independently using BPA, the overall process takes 488.2 + 480.4 + 558.8 = 1487.4 s (this is without considering the overhead attached to the process of fusion). Fig. 3 shows the reconstructed scenarios obtained using this alternative. The downsides of this alternative are the need to define a strategy to fuse the reconstructed scenes in order to obtain a global scenario representation. In addition to this, note that the reconstructed scenes overlap. In overlapping regions, reconstruction is carried out multiple times without originating an improved description, therefore waisting computational resources. Alternative (3) proposes an incremental scenario reconstruction. Unlike in the other two approaches, in this case, the reconstruction of a given scenario receives as input not only the current sensor data (a point cloud), but also a description of the scenario as seen by preceding reconstructions. This method appears to be more interesting than the others, since a scenario representation is updated or refined when new sensor data arrives. An illustrative example: a vehicle is moving on a road and there is a long wall on the side of the road. At the beginning of the road, sensors see only a portion of the wall and the reconstructed surface corresponds only to the visible part of the wall. As the vehicle moves forward, additional areas of the wall become visible to the sensors. These additional range measurements of the wall should be used to update the already existing shape primitive that represents the wall, rather than to create a new shape primitive which represents the new visible portion of the wall, and that overlaps, to some extent, the first wall primitive. In the following lines, we present mechanisms that enable a scene representation based on GPPs to be incrementally refined from novel point cloud data. To update the representation, an operation called expansion is executed for each of the existing GPPs. The expansion receives as input a list of points (the 3D point cloud) as well as the definition of the polygon that is to be expanded. It is composed of two parts, a perpendicular and a longitudinal expansion, and is defined as follows (see Fig. 4 ): Let P represent an input point cloud that is received at a given time and that contains several points (triangles and diamonds in Fig. 4(a)), and the scenario representation that was previously computed, being composed of a single primitive G (black solid line polygon in Fig. 4(a)). The primitive has a support plane, as well as a local coordinate system represented by red–green–blue lines. The first step is to compute a new point cloud P ort that is given by the points of P whose distance to the plane is smaller than the perpendicular expansion threshold T ort : (2) P ort = { ∈ P ∣ d j < T ort } , where d j is the distance of point to the support plane of G . Only points that lie close to the primitives support plane are stored in P ort and used in the next steps. In Fig. 4(a), some points are included in P ort (triangles) and others discarded (diamonds). Then, the points in P ort are projected into the primitives support plane and their coordinates transformed into the primitives local coordinate frame. In this local reference frame, the projected points always have z value equal to zero, which is why only the x and y coordinates are stored, i.e., points are defined in R 2 . Let J be the point cloud that contains the x and y coordinates of the projected points viewed from the primitive local coordinate system. Fig. 4(b) shows the projections of the triangles of Fig. 4(a) to the support plane (circles). This process is called the orthogonal part of the expansion. From here onward, all computations are performed in R 2 , which significantly speeds up the computation. The second part of the expansion is referred to as longitudinal expansion. Fig. 5 (a) shows an example point cloud J that contains several points. Let us consider that some of these points actually belong to the same object that the primitive represents (circles in Fig. 5(a)), and others do not (squares in Fig. 5(a)). Since these points are obtained from new data, not all of them are contained inside the bounding polygon of the corresponding primitives. Therefore, the primitive should expand to accommodate these new points. To do this, an iterative process is proposed. The first step is to offset the existing bounding polygon of the primitive. The algorithm used was introduced in [32] and the implementation is from [33]. The offsetting operation generates a new larger polygon, and in every iteration, the polygon grows as detailed next. The bounding polygon of the primitive is referred to as P , and the grown or extended polygon is referred to P ˆ . Then, all points in J are tested to see if they are inside P ˆ . The final stage is to compute a new convex hull. This new convex hull is computed from the point set that contains both the points of the previous convex hull and the points to which the polygon expanded to. The process is repeated using the newly computed convex hull as the starting hull. The iterative expansion stops when the extended polygon does not contain points inside it. Fig. 5 shows an example of an iterative longitudinal expansion. Fig. 5(a) shows the initial situation; Fig. 5(b) shows the start of the iterative process. The expanded polygon (dashed line), is offset from the initial bounding polygon (solid blue line). Points are tested to see whether they are inside the offset polygon (annotated with crosses). The process repeats until, in the fifth iteration (Fig. 5(f)), no new points are found inside the offset polygon. This causes the iterative search to finish. The expansion operation changes the polygon from its initial state Fig. 5(a) to a new shape, solid magenta line in Fig. 5(f). We propose to conduct the expansion of the current primitives using also a cascade configuration, based on the following reasoning. Each range measurement is obtained from a laser beam reflection of a single physical object. Thus, we can assume that each 3D point is explained by a GPP. Under this assumption, the points that have been assigned to a given primitive by an expansion operation, can only belong to that primitive and no other. Because of this, expanded points are removed from the input point cloud and are not a part of subsequent expansions (of other primitives) nor part of detections of new primitives. Since all the points that are taken by the expansion of a primitive are removed from the input point cloud, the subsequent expansions or searches are accelerated since that less points need to be analyzed. In terms of configuration, a cascade processing recommends that the faster stages are computed first. The expansion of primitives is faster than the detection. Because of this, when a new input point cloud is received, all existing primitives are first expanded and only then the remainder non expanded points are used for searching new primitives. Algorithm 2 describes the architecture of the complete algorithm for the geometric polygonal primitives representation computation, including both the detection and expansion operations. 4 Results In order to evaluate the proposed 3D processing techniques, a complete data-set with 3D laser data, cameras and accurate egomotion is required. The MIT autonomous vehicle Talos competed in the DARPA Urban Challenge and achieved fourth overall place. The data logged by the robot is publicly available [6]. In total, the MIT logs sum up to 315 GB of data. We have cropped a small sequence of 40 s (200 m of vehicle movement) at the start of the race (see Fig. 6 ). The sequence contains a continuous stream of sensor data, but in addition we have marked five locations (A through E) which are used to facilitate the analysis of the results. Additional information on each location is given in Table 2 . Fig. 7 shows images from all cameras, isometric and top views of the 3D data, and a satellite photograph of location C. The proposed approach is evaluated by analyzing how the scenario contained in this MIT sequence is reconstructed. Fig. 8 shows the polygonal primitives detected at locations C and D. It is possible to observe that most of the relevant planes are picked up by the algorithm. 4.1 One-shot reconstruction The detection of polygonal primitives is operated in a cascade-like configuration. In other words, the algorithm will search for polygonal primitives on a given input point cloud. After the first primitive is found, all the range measurements that are explained by that primitive are removed from the input point cloud. The second primitive is then searched in a smaller point cloud and so on. Since the search for a primitive is done over a decreasing size point cloud, it is expected that the search becomes faster as the primitives are extracted. In Fig. 9 an analysis of the computation time of each primitive is displayed. Primitives with higher numbers are detected in posterior phases. Fig. 9(a) shows the number of points remaining in the input point cloud as a function of the polygon number. Results are shown for all locations in the MIT sequence. The number of detected primitives varies from location to location. It is also possible to observe that, as expected, the number of remaining points decreases with the increase in the number of detected primitives. Also, the reduction in the number of points is larger for primitives detected earlier in the process. Hence, since the algorithm tends to remove the largest portion of points at the early stages of the cascade processing, this means that the latter stages will also be more efficient to compute. The reason for this behavior is the RANSAC algorithm. Because RANSAC will search for the larger consensus, it will most likely select planes that are supported by a greater number of points. In this way, RANSAC tends to select first polygons with the largest amount of primitive support points. As a consequence, the largest decreases in the input point cloud occur early in the cascade, which in turn accelerates the subsequent detection stages of the cascade. The detection time per primitive is shown in Fig. 9(b). The detection time tends to decrease with the increase in polygon number, for the reasons that were previously reported. We compare the proposed approach with three surface reconstruction methodologies: Ball Pivoting Algorithm (BPA) [24], Greedy Triangulation (GT) [7] and Poisson Surface Reconstruction (POIS) [25]. Two different parameter configurations for the proposed approach are used. In the first GPP 1, parameters are set so that only very large polygons are detected. Processing time is faster, since a lot of polygons are discarded but, on the other hand, the accuracy or completeness of the scene representation is not very good. The second alternative, GPP 2, is configured so that even small polygons are detected, which should provide a more accurate scene description at the cost of a higher computation time. Fig. 10 (a) shows that the BPA method Fig. 10(d) shows results from the GPP. Since our approach uses primitives to define macro size structures, the number of polygons used to represent the scene is small. Even though, it can be said that the most relevant polygons are part of the representation. Table 3 shows the computation times that each algorithm took to reconstruct each of the locations in the sequence. The GPP methodology is the fastest one. This efficiency is related to the simplicity of the computed representation, and to the fact that RANSAC analyzes only a small sample of points in the input point cloud, which means that not all input points are visited in order to reconstruct the scene, as is the case with the slower triangulation approaches. To measure the accuracy of each reconstruction approach, the results obtained by BPA (the most accurate algorithm) are used as reference. Let X and Y be two meshes. The Hausdorff distance between those meshes d H ( X , Y ) is computed as: (3) d H ( X , Y ) = max ( sup x ∈ X ( inf y ∈ Y d ( x , y ) ) , sup y ∈ Y ( inf x ∈ X d ( x , y ) ) ) , where sup and inf are the supremum and infimum, respectively. In this particular case, a variation of the Hausdorff distance, called the one sided Hausdorff distance is used where only the sup x ∈ X ( inf y ∈ Y d ( x , y ) ) part is computed, because we only wish to measure how distant is each approach to the ground truth and not the other way around. In this case, the X meshes are given by each of the algorithms and the Y mesh is supplied by the ground truth mesh BPA. Table 4 shows the Hausdorff distance values obtained by GT, POIS, GPP 1 and GPP 2 using BPA meshes as ground truth. The algorithm that obtains the best mean results is GT with an average error of 0.14 m. The accuracy exhibited by the GPP 1 and GPP 2 approaches is about 0.99 and 0.83 m, respectively. Fig. 11 shows a graphical representation of the error for all of the approaches. For each approach, the output mesh has been sampled and the points are shown with color associated to the computed one sided Hausdorff distance of each point. A Red–Green–Blue colormap is used to code the distance. Red represents zero distance and blue maximum distance. In Fig. 11(a), corresponding to the GT approach, almost all points have red color, resulting in low mean error. The POIS approach, represented in Fig. 11(b), shows many points in blue and green color, e.g., points whose minimum distance to the ground truth sampled points was very large. This is why POIS shows low accuracy values. In the case of the GPP approaches, 11(c) and (d), some regions of the sampled points are more prone to have large error distances, while those in red seem to perfectly fit the ground truth mesh. The reason for this is that the BPA methodology, that was selected to serve as ground truth, does not perform interpolation over occluded areas, as the GPP approaches do. Most of the errors appear in the polygonal primitive that represents the ground plane; that occurs because this is the one that suffers more from occlusion from other planes. Errors may also result from the usage of convex hulls to compute the boundary polygons. We have investigated this by using alternatives to the proposed approach where the ground plane polygon is suppressed, and where concave hulls are used. Results are shown in Table 5 . We can observe that, with the option of ground plane suppression and concave hull, the mean accuracy of GPP 2 improves to 0.1 m. Fig. 12 shows a visual analysis of the Hausdorff distance errors for these variations of GPP 2. It is possible to observe that regions with error, e.g., in blue and green, decrease considerably when the concave hull is used, but in particular when the ground plane polygon is discarded. 4.2 Incremental reconstruction This section presents several results and analysis of the expansion mechanism of the geometric polygonal primitives. The polygonal primitives algorithm without the expansion mechanism is compared against the same algorithm using the expansion mechanism. These two algorithms will be referred to as “with expansion” and “without expansion”. By using this evaluation, it is possible to assess what are the benefits or disadvantages of the expansion mechanism. All five locations of sequence 1 are used to obtain results. Parameters used in the detection are similar to the GPP 1 set. Fig. 13 shows a reconstruction of the scene using the expansion mechanism. The state of the reconstructed scenario at each location is shown. One clear advantage of this representation is that there are no overlapping primitives. A qualitative analysis of the results present in Fig. 13 shows that the most important features of the scenario are contained in the representation, especially if the task in mind is navigation. To evaluate the computational complexity of the proposed algorithms, we measure the amount of detail of the representation that they produce when given a fixed amount of time to reconstruct a scene. Fig. 14 (a) shows the number of polygons created in both algorithms. Although at the beginning of the sequence both algorithms generate a similar number of primitives, after some locations the algorithm without expansion shows a greater number of primitives. The explanation is that since the algorithm without expansion does not compare the stored primitives with the new data, it ends up duplicating primitives. On the contrary, when using the expansion mechanism, the duplication of primitives is avoided which leads to a smaller number of primitives. Note that just because a representation has more primitives it is not necessarily better. In fact, if there are duplicated primitives, the representation may be worse than another with a smaller number of non duplicated primitives. Fig. 14(b) shows the number of points that are given as input for the detection mechanism. In the case of the algorithm without expansion, all of the input points are passed on to the detection component, i.e., approximately 400000 points per scan. When the expansion is active, a large number of points are explained by the expansion component and are not fed into the detection. In conclusion, the expansion mechanism takes a small amount of time when compared to the detection and other processes, and is able to quickly explain a large portion of the input points, thus filtering out many points which are not sent to other slower components. Finally, in Fig. 14(c), the total accumulated area of the primitives is shown. There is a clear difference between the with and without expansion approaches. This difference is due to the duplication of primitives in the case of the without expansion approach. 4.3 Comparison of approaches with and without expansion Next, we focus on characterizing how the polygonal primitives evolve. Only the algorithm with expansion is portrayed, since in the without expansion approach primitives are static. Fig. 15 (a) shows the number of support points assigned to each primitive. Only primitives of even index are shown. We can see that primitives are initialized in different locations in the sequence: at location A primitive 0 is created, while primitives 2, 4 and 6 are created at location B. These results show that most primitives significantly increase their number of support points throughout the sequence: Primitive 0 was detected at location A with 0.4 × 50 × 10 4 = 200 kpoints, and at location E it already supported 1.4 × 50 × 10 4 = 700 kpoints. In other words, it increased the number of support points by 350%. Another example, primitive 10, detected at location D with 3 kpoints, has at location E around 7 kpoints. A 230% increase between consecutive locations. The same analysis holds when considering the area of the primitives (see Fig. 15(b)): significant increases in the area of the primitives bounding polygons are also observed. All these observations, both in number of support points as well as in terms of area, show that polygons grow considerably after being detected. If these primitives were not expanded, the additional support points and area would have to be handled by a detection mechanism. Fig. 16 shows how primitive 0, i.e., the ground plane primitive, evolves over sequence 1. The primitive expands at every iteration to accommodate newly observed data points that belong to the ground plane. Table 6 provides the links for some videos that show how the system processes the data stream from the MIT sequence. It is possible to see the difference between an incremental versus a non incremental (without expansion) approach. 5 Conclusions This paper proposes a novel approach to produce scene representations using the array of sensors on-board autonomous vehicles. Since roads are semi structured environments with a great deal of macro size geometric structures, we argue that the use of polygonal primitives is well suited to describe these scenes. Furthermore, we propose mechanisms designed to update the polygonal primitives as new sensor data is collected. Results have shown that the proposed approach is capable of producing accurate descriptions of the scene, and that it is considerably faster than all the approaches used in this evaluation. The proposed expansion mechanism updates previous descriptions of the scene, therefore not creating duplicate representations of the same objects. In addition to this, the expansion mechanism is capable of efficiently filtering out data points that otherwise would be handled by (slower) detection mechanisms. Future work will include the addition of texture on the polygons generated by the proposed algorithm. In this way, we expect to have the means to produce scene representations that can be used not only for standard task such as obstacle detection and motion planning, but also for more complex endeavors such as recognizing patterns in the scene. Acknowledgments This work has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”, reference CEB-02502014. References [1] A. Birk N. Vaskevicius K. Pathak S. Schwertfeger J. Poppinga H. Buelow 3-d perception and modeling IEEE Robot. Autom. Mag. 16 4 2009 53 60 [2] R.B. Rusu, S. Cousins, 3D is here: Point Cloud Library (PCL), in: IEEE International Conference on Robotics and Automation, ICRA, Shanghai, China, 2011. [3] W. Burgard P. Pfaff Editorial: Three-dimensional mapping, part 1 J. Field Robot. 26 10 2009 757 758 [4] M. Oliveira, V. Santos, A.D. Sappa, P. Dias, Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 1, Springer International Publishing, Cham, 2016, Ch. Scene Representations for Autonomous Driving: An Approach Based on Polygonal Primitives, pp. 503–515. [5] M. Oliveira V. Santos A. Sappa P. Dias A.P. Moreira Incremental texture mapping for autonomous driving Robot. Auton. Syst. 2016 (submitted January 2016) [6] A.S. Huang M. Antone E. Olson L. Fletcher D. Moore S. Teller J. Leonard A High-rate, Heterogeneous Data Set from the DARPA Urban Challenge Int. J. Robot. Res. 29 13 2011 1595 1601 [7] Z.C. Marton, R.B. Rusu, M. Beetz, On fast surface reconstruction methods for large and noisy datasets, in: Proceedings of the IEEE International Conference on Robotics and Automation, ICRA, Kobe, Japan, 2009. [8] T. Weiss, B. Schiele, K. Dietmayer, Robust driving path detection in urban and highway scenarios using a laser scanner and online occupancy grids, in: Intelligent Vehicles Symposium, 2007 IEEE, 2007, pp. 184–189. [9] F. Oniga S. Nedevschi Processing dense stereo data using elevation maps: Road surface, traffic isle, and obstacle detection IEEE Trans. Veh. Technol. 59 3 2010 1172 1182 [10] K. Zhou M. Gong X. Huang B. Guo Data-parallel octrees for surface reconstruction IEEE Trans. Vis. Comput. Graphics 17 5 2011 669 681 [11] S. Thrun M. Montemerlo H. Dahlkamp D. Stavens A. Aron J. Diebel P. Fong J. Gale M. Halpenny G. Hoffmann K. Lau C.M. Oakley M. Palatucci V. Pratt P. Stang S. Strohband C. Dupont L.-E. Jendrossek C. Koelen C. Markey C. Rummel J. van Niekerk E. Jensen P. Alessandrini G.R. Bradski B. Davies S. Ettinger A. Kaehler A.V. Nefian P. Mahoney Stanley: The robot that won the darpa grand challenge J. Field Robot. 23 9 2006 661 692 [12] C. Urmson J. Anhalt D. Bartz M. Clark T. Galatali A. Gutierrez S. Harbaugh J. Johnston H. Kato P.L. Koon W. Messner N. Miller A. Mosher K. Peterson C. Ragusa D. Ray B.K. Smith J.M. Snider S. Spiker J.C. Struble J. Ziglar W.R.L. Whittaker A robust approach to high-speed navigation for unrehearsed desert terrain J. Field Robot. 23 8 2006 467 508 [13] C. Urmson J. Anhalt H. Bae J.A.D. Bagnell C.R. Baker R.E. Bittner T. Brown M.N. Clark M. Darms D. Demitrish J.M. Dolan D. Duggins D. Ferguson T. Galatali C.M. Geyer M. Gittleman S. Harbaugh M. Hebert T. Howard S. Kolski M. Likhachev B. Litkouhi A. Kelly M. McNaughton N. Miller J. Nickolaou K. Peterson B. Pilnick R. Rajkumar P. Rybski V. Sadekar B. Salesky Y.-W. Seo S. Singh J.M. Snider J.C. Struble A.T. Stentz M. Taylor W.R.L. Whittaker Z. Wolkowicki W. Zhang J. Ziglar Autonomous driving in urban environments: Boss and the urban challenge J. Field Robot. 25 8 2008 425 466 Special Issue on the 2007 DARPA Urban Challenge, Part I [14] M. Montemerlo J. Becker S. Bhat H. Dahlkamp D. Dolgov S. Ettinger D. Haehnel T. Hilden G. Hoffmann B. Huhnke D. Johnston S. Klumpp D. Langer A. Levandowski J. Levinson J. Marcil D. Orenstein J. Paefgen I. Penny A. Petrovskaya M. Pflueger G. Stanek D. Stavens A. Vogt S. Thrun Junior: The stanford entry in the urban challenge J. Field Robotics 2008 [15] A. Bacha C. Bauman R. Faruque M. Fleming C. Terwelp C. Reinholtz D. Hong A. Wicks T. Alberi D. Anderson S. Cacciola P. Currier A. Dalton J. Farmer J. Hurdus S. Kimmel P. King A. Taylor D.V. Covern M. Webster Odin: Team victortango’s entry in the darpa urban challenge J. Field Robot. 25 8 2008 467 492 [16] T. Luettel, M. Himmelsbach, F. von Hundelshausen, M. Manz, A. Mueller, H.-J. Wuensche, Autonomous Offroad Navigation Under Poor GPS Conditions, in: Proceedings of 3rd Workshop On Planning, Perception and Navigation for Intelligent Vehicles, PPNIV, IEEE/RSJ International Conference on Intelligent Robots and Systems, St. Louis, MO, USA, 2009. [17] Wikipedia, Google driverless car—Wikipedia, the free encyclopedia, [Online; accessed November 2015], 2015. [18] R. Jovanovic, R. Lorentz, Compression of volumetric data using 3d delaunay triangulation, in: 2011 4th International Conference on Modeling, Simulation and Applied Optimization, ICMSAO, 2011, pp. 1–5. [19] A. Specht, M. Devy, Surface segmentation using a modified ball-pivoting algorithm, in: 2004 International Conference on Image Processing, 2004. ICIP’04. Vol. 3, 2004, pp. 1931–1934. [20] C. Yin, D. Gang, C. Zhi-quan, L. Hong-hua, L. Jun, J. Shi-yao, An algorithm of cuda-based poisson surface reconstruction, in: 2010 International Conference on Audio Language and Image Processing, ICALIP, 2010, pp. 203–207. [21] Y.-L. Chen S.-H. Lai An orientation inference framework for surface reconstruction from unorganized point clouds IEEE Trans. Image Process. 20 3 2011 762 775 [22] A. de Medeiros Brito A. Doria Neto J. Dantas de Melo L. Garcia Goncalves An adaptive learning approach for 3-d surface reconstruction from point clouds IEEE Trans. Neural Netw. 19 6 2008 1130 1140 [23] C. Rivadeneyra, I. Miller, J. Schoenberg, M. Campbell, Probabilistic estimation of multi-level terrain maps, in: IEEE International Conference on Robotics and Automation, 2009. ICRA’09. 2009, pp. 1643–1648. [24] F. Bernardini J. Mittleman H. Rushmeier C. Silva G. Taubin The ball-pivoting algorithm for surface reconstruction IEEE Trans. Vis. Comput. Graphics 5 4 1999 349 359 [25] M. Kazhdan, M. Bolitho, H. Hoppe, Poisson surface reconstruction, in: SGP’06: Proceedings of the Fourth Eurographics Symposium on Geometry Processing, Eurographics Association, 2006, pp. 61–70. [26] R. Triebel, W. Burgard, F. Dellaert, Using hierarchical em to extract planes from 3d range scans, in: Proceedings of the 2005 IEEE International Conference on Robotics and Automation, 2005, pp. 4437–4442. [27] M.A. Fischler, R.C. Bolles, Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography, in: ACM, Los Angeles, California, 1981. [28] A. Nurunnabi, D. Belton, G. West, Robust segmentation for multiple planar surface extraction in laser scanning 3d point cloud data, in: 2012 21st International Conference on Pattern Recognition, ICPR, 2012, pp. 1367–1370. [29] S. Hert, S. Schirra, 2D convex hulls and extreme points, in: CGAL User and Reference Manual, 4.0 Edition, CGAL Editorial Board, 2012. [30] A. Bykat Convex hull of a finite set of points in two dimensions Inform. Process. Lett. 7 1978 296 298 [31] C.B. Barber D.P. Dobkin H. Huhdanpaa The quickhull algorithm for convex hulls ACM Trans. Math. Software 22 4 1996 469 483 [32] O. Aichholzer F. Aurenhammer D. Alberts B. Gartner A novel type of skeleton for polygons J.UCS 1 12 1995 752 761 [33] F. Cacciola, 2D straight skeleton and polygon offsetting, in: CGAL User and Reference Manual, 4.0 Edition, CGAL Editorial Board, 2012. Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Vítor Santos obtained a 5 year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990–1994 at the Joint Research Center, Italy. He his currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or cosupervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He his also cofounder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Angel Domingo Sappa (S’94-M’00-SM’12) received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, where he is currently a Senior Researcher. He is a member of the Advanced Driver Assistance Systems Group. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereoimage processing and analysis, 3D modeling, and dense optical flow estimation. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. António Paulo Moreira graduated with a degree in electrical engineering at the University of Oporto, in 1986. He then pursued graduate studies at University of Porto, obtaining an M.Sc. degree in electrical engineering-systems in 1991 and a Ph.D. degree in electrical engineering in 1998. Presently, he is an Associate Professor at the Faculty of Engineering of the University of Porto and researcher and manager of the Robotics and Intelligent Systems Centre at INESC TEC. His main research interests are process control and robotics. "
    },
    {
        "doc_title": "Wavelet-based visible and infrared image fusion: A comparative study",
        "doc_scopus_id": "84973659355",
        "doc_doi": "10.3390/s16060861",
        "doc_eid": "2-s2.0-84973659355",
        "doc_date": "2016-06-10",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Comparative studies",
            "Different wavelets",
            "Evaluation metrics",
            "Fusion evaluation",
            "Longwave infrared",
            "Quantitative values",
            "Spectral image fusions",
            "State-of-the-art approach"
        ],
        "doc_abstract": "© 2016 by the authors; licensee MDPI, Basel, Switzerland.This paper evaluates different wavelet-based cross-spectral image fusion strategies adopted to merge visible and infrared images. The objective is to find the best setup independently of the evaluation metric used to measure the performance. Quantitative performance results are obtained with state of the art approaches together with adaptations proposed in the current work. The options evaluated in the current work result from the combination of different setups in the wavelet image decomposition stage together with different fusion strategies for the final merging stage that generates the resulting representation. Most of the approaches evaluate results according to the application for which they are intended for. Sometimes a human observer is selected to judge the quality of the obtained results. In the current work, quantitative values are considered in order to find correlations between setups and performance of obtained results; these correlations can be used to define a criteria for selecting the best fusion strategy for a given pair of cross-spectral images. The whole procedure is evaluated with a large set of correctly registered visible and infrared image pairs, including both Near InfraRed (NIR) and Long Wave InfraRed (LWIR).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A hybrid top-down bottom-up approach for the detection of cuboid shaped objects",
        "doc_scopus_id": "84978786400",
        "doc_doi": "10.1007/978-3-319-41501-7_57",
        "doc_eid": "2-s2.0-84978786400",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bench-mark problems",
            "Bottom up",
            "Bottom up approach",
            "Design and implements",
            "Point cloud",
            "Top down approaches",
            "Topdown",
            "Visual perception"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.While bottom-up approaches to object recognition are simple to design and implement, they do not yield the same performance as top-down approaches. On the other hand, it is not trivial to obtain a moderate number of plausible hypotheses to be efficiently verified by topdown approaches. To address these shortcomings, we propose a hybrid top-down bottom-up approach to object recognition where a bottom-up procedure that generates a set of hypothesis based on data is combined with a top-down process for evaluating those hypotheses. We use the recognition of rectangular cuboid shaped objects from 3D point cloud data as a benchmark problem for our research. Results obtained using this approach demonstrate promising recognition performances.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representations for autonomous driving: An approach based on polygonal primitives",
        "doc_scopus_id": "84951868739",
        "doc_doi": "10.1007/978-3-319-27146-0_39",
        "doc_eid": "2-s2.0-84951868739",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3-D scene representation",
            "Autonomous driving",
            "Autonomous Vehicles",
            "Geometric structure",
            "Macro scale",
            "Novel methodology",
            "Point cloud",
            "Scene reconstruction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.In this paper, we present a novel methodology to compute a 3D scene representation. The algorithm uses macro scale polygonal primitives to model the scene. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Results show that the approach is capable of producing accurate descriptions of the scene. In addition, the algorithm is very efficient when compared to other techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Robotics: Using a competition mindset as a tool for learning ROS",
        "doc_scopus_id": "84951780896",
        "doc_doi": "10.1007/978-3-319-27146-0_58",
        "doc_eid": "2-s2.0-84951780896",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Collaborative games",
            "Course",
            "Design features",
            "Game simulation",
            "ROS"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.In this article, a course that explores the potential of learning ROS using a collaborative game world is presented. The competitive mindset and its origins are explored, and an analysis of a collaborative game is presented in detail, showing how some key design features lead participants to overcome the challenges proposed through cooperation and collaboration. The data analysis is supported through observation of two different game simulations: the first, where all competitors were playing solo, and the second, where the players were divided in groups of three. Lastly, the authors reflect on the potentials that this course provides as a tool for learning ROS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D object perception and perceptual learning in the RACE project",
        "doc_scopus_id": "84949908319",
        "doc_doi": "10.1016/j.robot.2015.09.019",
        "doc_eid": "2-s2.0-84949908319",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Interactive learning",
            "Memory systems",
            "Open-ended learning",
            "Point cloud"
        ],
        "doc_abstract": "© 2015 Elsevier B.V.This paper describes a 3D object perception and perceptual learning system developed for a complex artificial cognitive agent working in a restaurant scenario. This system, developed within the scope of the European project RACE, integrates detection, tracking, learning and recognition of tabletop objects. Interaction capabilities were also developed to enable a human user to take the role of instructor and teach new object categories. Thus, the system learns in an incremental and open-ended way from user-mediated experiences. Based on the analysis of memory requirements for storing both semantic and perceptual data, a dual memory approach, comprising a semantic memory and a perceptual memory, was adopted. The perceptual memory is the central data structure of the described perception and learning system. The goal of this paper is twofold: on one hand, we provide a thorough description of the developed system, starting with motivations, cognitive considerations and architecture design, then providing details on the developed modules, and finally presenting a detailed evaluation of the system; on the other hand, we emphasize the crucial importance of the Point Cloud Library (PCL) for developing such system.1",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2015-10-09 2015-10-09 2015-12-02 2015-12-02 2016-12-26T09:26:02 S0921-8890(15)00214-6 S0921889015002146 10.1016/j.robot.2015.09.019 S300 S300.2 FULL-TEXT 2016-12-26T04:38:02.271089-05:00 0 0 20160101 20160131 2016 2015-10-09T15:05:55.611708Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 75 75 PB Volume 75, Part B 38 614 626 614 626 201601 January 2016 2016-01-01 2016-01-31 2016 Special Section on 3D Perception with PCL; Edited by Matteo Munaro, Radu Bogdan Rusu, and Emanuele Menegatti article fla Copyright © 2015 Elsevier B.V. All rights reserved. 3DOBJECTPERCEPTIONPERCEPTUALLEARNINGINRACEPROJECT OLIVEIRA M 1 Introduction 2 Related work 3 A dual memory approach 4 The RACE object perception system 4.1 Architecture 4.2 The PCL foundation of this work 4.3 Object perception 4.4 User perception 4.5 Addressing computational issues 5 Perceptual learning 6 Profiling and evaluation 6.1 Profiling 6.2 Off-line evaluation of the perceptual learning approach 6.3 Open-ended learning evaluation 7 Conclusions Acknowledgments References HERTZBERG 2014 297 304 J KASAEI 2015 1 17 H MOKHTARIHASSANABAD 2015 V INTELLIGENTAUTONOMOUSSYSTEMS13PROCEEDINGS13THINTERNATIONALCONFERENCEIAS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES COUSINS 2010 12 14 S COUSINS 2010 12 14 S NAU 2003 379 404 D BARSALOU 1999 577 609 L OLIVEIRA 2014 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS LIM 2011 492 509 G CORADESCHI 2003 85 96 S ZAMAN 2013 S IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATION INTEGRATEDMODELBASEDDIAGNOSISREPAIRARCHITECTUREFORROSBASEDROBOTSYSTEMS TULVING 1991 3 32 E MEMORYORGANIZATIONLOCUSCHANGE CONCEPTSHUMANMEMORY TULVING 2005 4 56 E MISSINGLINKINCOGNITION EPISODICMEMORYAUTONOESISUNIQUELYHUMAN WOOD 2011 81 103 R SEABRALOPES 2007 53 81 L SEABRALOPES 2008 277 297 L KIRSTEIN 2012 90 105 S KRUGER 2011 740 757 N ALDOMA 2012 A CLAPES 2013 799 808 A RUSU 2009 3212 3217 R ROBOTICSAUTOMATION2009ICRA09IEEEINTERNATIONALCONFERENCE FASTPOINTFEATUREHISTOGRAMSFPFHFOR3DREGISTRATION SEABRALOPES 2001 10 14 L SAHIB 2013 S LIM 2014 153 160 G ROBOTHUMANINTERACTIVECOMMUNICATION2014ROMAN23RDIEEEINTERNATIONALSYMPOSIUM INTERACTIVETEACHINGEXPERIENCEEXTRACTIONFORLEARNINGABOUTOBJECTSROBOTACTIVITIES EVANS 2008 255 278 J COHENOR 1995 453 461 D BARBER 1996 469 483 C FISCHLER 1981 381 395 M RUSU 2009 R SEMANTIC3DOBJECTMAPSFOREVERYDAYMANIPULATIONINHUMANLIVINGENVIRONMENTS YI 2000 76 78 W JOHNSON 1999 433 449 A CONNOR 2010 599 608 M MUNARO 2013 53 68 M FRONTIERSINTELLIGENTAUTONOMOUSSYSTEMS ASOFTWAREARCHITECTUREFORRGBDPEOPLETRACKINGBASEDROSFRAMEWORKFORAMOBILEROBOT BENTLEY 1975 509 517 J CHAUHAN 2011 341 354 A OLIVEIRAX2016X614 OLIVEIRAX2016X614X626 OLIVEIRAX2016X614XM OLIVEIRAX2016X614X626XM 2017-12-02T00:00:00Z UnderEmbargo item S0921-8890(15)00214-6 S0921889015002146 10.1016/j.robot.2015.09.019 271599 2016-12-26T04:38:02.271089-05:00 2016-01-01 2016-01-31 true 1868217 MAIN 13 53302 849 656 IMAGE-WEB-PDF 1 fx1 12594 28 219 gr1 23670 164 191 gr2 22730 164 219 gr3 20164 142 219 gr4 35390 128 219 gr5 22858 164 135 gr6 25048 164 212 gr7 25354 164 214 gr8 20489 130 219 gr9 19009 87 219 pic1 32766 164 140 pic2 33052 164 140 pic3 29650 164 140 pic4 26625 164 140 pic5 33620 164 140 pic6 30488 164 140 fx1 37984 59 461 gr1 80010 404 472 gr2 59338 275 367 gr3 101946 453 697 gr4 71163 198 339 gr5 217247 903 744 gr6 79263 291 376 gr7 75798 288 376 gr8 110529 378 636 gr9 94838 251 629 pic1 40427 132 113 pic2 44745 132 113 pic3 38146 132 113 pic4 40704 132 113 pic5 42891 132 113 pic6 38759 132 113 fx1 99682 263 2041 gr1 377574 1791 2091 gr2 203931 1218 1625 gr3 571474 2005 3085 gr4 362193 876 1500 gr5 1221470 3998 3295 gr6 269699 1291 1667 gr7 275599 1277 1667 gr8 756745 1677 2818 gr9 562149 1110 2784 pic1 97072 584 500 pic2 111383 584 500 pic3 86166 584 500 pic4 98879 584 500 pic5 122229 584 500 pic6 90792 584 500 si1 60 4 15 si13 914 48 174 si14 227 15 50 si15 1101 53 201 si17 354 15 57 si18 1092 40 178 si19 211 14 25 si2 195 11 42 si20 724 19 149 si21 145 8 14 si22 148 11 19 si23 1862 32 394 si24 171 11 18 si25 124 8 10 si3 113 10 8 si4 124 8 11 si5 1056 62 192 si6 136 13 11 si7 169 12 35 si8 136 12 10 si9 159 11 17 ROBOT 2547 S0921-8890(15)00214-6 10.1016/j.robot.2015.09.019 Elsevier B.V. Fig. 1 A high-level overviews of the RACE architecture. Fig. 2 Abstract cognitive architectures for hybrid reactive–deliberative robots: (a) with a single memory system; (b) with a dual memory system. Fig. 3 Architecture of the developed object perception and perceptual learning system. Fig. 4 Visualization of tracking, pointing and labeling. Fig. 5 Processing time in the object perception pipelines in the video sequence, comparing nodes (left column) with nodelets (right column). Six objects appear in the video, corresponding to pipelines 1 through 6. (a) tracker nodes; (b) feature extraction nodes; (c) object recognition nodes; (d) tracker nodelets; (e) feature extraction nodelets; (f) object recognition nodelets. Fig. 6 Perceptual memory usage during the experiment, in logarithmic scale. The blue (upper) curve represents the total size of all point clouds of object views extracted by the trackers. The green (middle) curve represents the total accumulated size of all point clouds of key views. The red (bottom) curve represents the actual perceptual memory content (shape-based representations of key views). Fig. 7 Object recognition performance (precision, recall and F-measure) during the experiment. Each point in these curves is computed based on the object recognition results in the previous 20 s. Fig. 8 (a) Simulated teacher experiment no. 3; (b) protocol success versus the number of learned categories, for the same experiment. Fig. 9 System performance during simulated user experiments: (a) global success vs. number of learned categories, a measure of how well the system learns; (b) number of learned categories vs. number of question/correction iterations, represents how fast the system learned object categories. Table 1 PCL functionalities used in the RACE object perception and perceptual learning system. The modules listed in the third column are those represented in Fig. 3. PCL class [refs.]/parameters Usage in RACE RACE modules ConditionalRemoval Removing points outside a 3D box a Object DetectorObject Tracker VoxelGrid [30] voxel size=0.015 m Downsampling a point cloud b Object Detector Object Tracker ConvexHull [31,32] Convex hulls of tables c Tabletop Segmenter RandomSampleConsensus [33] RANSAC iterations=200 Table plane detection d Tabletop Segmenter ExtractPolygonalPrismData minimum z=0.01 mmaximum z=0.6 m Tabletop object detection e Object Detector EuclideanClusterExtraction [34] minimum cluster size=10maximum cluster size = 10,000 clustering step=0.08 m Object segmentation f Object Detector PCA [35] Estimating the orientation of objects g Object Tracker ParticleFilterTracker [36] max number of particles=200 Tracking tabletop objects using RGBD h Object Tracking SpinImageEstimation [37] support length=0.1 mimage width=8, support angle=90° Features for representing object views i Feature Extractor KdTree [38] K = 1 Computation of distance between views j Object Recognizer Object Conceptualizer a b c d e f g h i j Table 2 Sequence of events in the experiment (see video). Time (s) Event Description 25 T1 in A Mug (T1) is placed on the table 40 T1 is a Mug T1 is labeled as Mug 60 T2 in A Vase (T2) is placed on the table 75 T2 is a Vase T2 is labeled as a Vase 90 T3 in Another Mug (T3) is placed on the table 135 T3 out T3 is removed from the table 140 T1 out T1 is removed from the table 145 T2 out T2 is removed from the table 165 T4 in A Plate (T4) is placed on the table 170 T4 is a Plate T4 is labeled as a Plate 175 T5 in A Bottle (T5) is placed on the table 190 T5 is a Bottle T5 is labeled as a Bottle 210 T6 in A Spoon (T6) is placed on the table Table 3 Average object recognition performance (F1 measure) for different parameters: voxel size (VS), image with (IW), support length (SL) and classification threshold (CT). Table 4 Summary of simulated teacher experiments a . Exp# #Iterations #Categories #Instances GS APS 1 706 22 14.55 0.65 0.76 2 217 13 8.69 0.68 0.86 3 533 24 10.46 0.68 0.74 4 699 21 15.57 0.63 0.74 5 747 29 11.31 0.69 0.79 6 711 31 9.83 0.72 0.75 7 1041 36 12.06 0.70 0.77 8 252 13 10.08 0.64 0.87 9 412 19 10.37 0.68 0.81 10 393 20 8.9 0.70 0.84 a Exp#, experiment number; #Categories, number of categories learned; #Iterations, number of iterations in the experiment; #Instances, average number of instances per category at the end of the experiment; GS, global success; APS, average protocol success. 3D object perception and perceptual learning in the RACE project Miguel Oliveira a Luís Seabra Lopes a b ⁎ Gi Hyun Lim a S. Hamidreza Kasaei a Ana Maria Tomé a b Aneesh Chauhan c a IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro Portugal b Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro Portugal c Center of Automation and Robotics, Universidad Politécnica de Madrid, Spain Center of Automation and Robotics, Universidad Politécnica de Madrid Spain ⁎ Corresponding author at: IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal. IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro Portugal This paper describes a 3D object perception and perceptual learning system developed for a complex artificial cognitive agent working in a restaurant scenario. This system, developed within the scope of the European project RACE, integrates detection, tracking, learning and recognition of tabletop objects. Interaction capabilities were also developed to enable a human user to take the role of instructor and teach new object categories. Thus, the system learns in an incremental and open-ended way from user-mediated experiences. Based on the analysis of memory requirements for storing both semantic and perceptual data, a dual memory approach, comprising a semantic memory and a perceptual memory, was adopted. The perceptual memory is the central data structure of the described perception and learning system. The goal of this paper is twofold: on one hand, we provide a thorough description of the developed system, starting with motivations, cognitive considerations and architecture design, then providing details on the developed modules, and finally presenting a detailed evaluation of the system; on the other hand, we emphasize the crucial importance of the Point Cloud Library (PCL) for developing such system. 1 1 This paper is a revised and extended version of Oliveira et al. (2014). Keywords 3D object perception Point-Cloud Library Dual memory systems Open-ended learning Interactive learning 1 Introduction One of the primary challenges of service robotics is the adaptation of robots to new tasks in changing environments, where they interact with non-expert users. The European project RACE (Robustness by Autonomous Competence Enhancement [1,2]), recently closed, assumed that versatility and competence enhancement can be obtained by learning from experiences. The project focused on acquiring and conceptualizing experiences about objects [3], scene layouts [4] and activities [5] as a means to enhance robot competence over time thus achieving robustness. Stimuli for learning can be collected, either autonomously by robots, or when they receive appropriate feedback from users. The functional components of the RACE architecture are represented by boxes in Fig. 1 . Each component may contain one or more modules, which are implemented as nodes (or nodelets) over the Robot Operating System (ROS) [6,7]. The Reasoning and Interpretation component includes a temporal reasoner, a spatial reasoner and a description logics reasoner. Perception contains several modules for symbolic proprioception and exteroception, which generate occurrences. The Experience Management and Conceptualization component pre-processes occurrences, extracts relevant experiences, uses them to create new concepts and stores these in the Memory component. The User Interface component receives instructions from the user and relays them to the Planning component. Planning is carried out using SHOP2, a Hierarchical Task Network planner [8]. The produced plans are executed by the Plan Execution Management component. One of the challenges in this type of projects is to ground [9] the semantic representations maintained by the robot, namely the model of the world state and the learned concepts, into the perception and action capabilities of the robot itself [10,11]. At least two types of grounding are involved here. For internal symbols that refer to real-world objects (e.g. “mug23”), the robot must maintain a perception-mediated mapping of symbols to objects. This is often called anchoring [12] and relies to some extent on (visual) tracking capabilities at the perception level and on semantic interpretation capabilities at the reasoning level. For category symbols (e.g. “Mug”), the robot must ground their meanings on concrete observations of instances of the categories. A related challenge is how to combine semantic (i.e. symbolic, relational, logic-based) with perceptual (numeric, pattern-based) representations, and how to store different types of representations. After analyzing the requirements of the different components of the RACE architecture, a key decision was made by the project: instead of a single memory system, two independent memory systems would be used, one for semantic information (the Semantic Memory) and the other for perceptual information (the Perceptual Memory) [2]. Through perceptual learning capabilities, the developed object perception system can be applied to open-ended environments. In this case, “open-ended” means that the robot does not know in advance which object categories it will have to learn, which observations will be available, and when they will be available to support this learning. This kind of perception system must comprise a significant number of software modules, which must be closely coupled in their structure and functionality [13]. Three main design options address the key computational issues involved in processing and storing perception data. First, a lightweight, NoSQL database, is used to implement the perceptual memory. Second, a thread-based approach with zero copy transport of messages is used in implementing the modules. Finally, a multiplexing scheme, for the processing of the different objects in the scene, enables parallelization. This way, the system is capable of real time object detection, tracking and recognition. The developed perception and perceptual learning capabilities target objects in table-top scenes, e.g. in a restaurant environment. These capabilities are fully integrated in the RACE architecture and are running on the PR2 robot used by the project. This work heavily relies on Point Cloud Library (PCL) functionalities, as will be detailed in Section 4. Because the developed perception system is a complex network of processing nodes, the whole paper is organized in such a way that the organization of the system and the module functionalities are well justified and presented in detail. However, since PCL is used in nearly every module of the system, the system could not have been developed easily without PCL. Therefore, this work shows the current importance of PCL in building sophisticated 3D perception systems in robotics and other domains. The remaining part of this paper is organized as follows. In the next section, related works are discussed. Memory and cognitive architecture issues are discussed in Section 3, leading to the choice of a dual memory approach and to the development of a perceptual memory system. The RACE object perception system and the perceptual learning approach are described in Sections 4 and 5. Profiling and evaluation of the developed system is the topic of Section 6. Finally, in Section 7, the conclusion is presented and future research is discussed. 2 Related work As robots are expected to increasingly interact and collaborate closely with humans, robotics researchers need to look at human cognition as a source of inspiration. Learning is closely related to memory in human cognition. In the cognitive science literature, the existence of multiple memory systems is widely accepted [14,15]. Biological findings about memory and learning have served as inspiration for the development of computational models and applications. Wood et al. [16] present a thorough review and discussion on memory systems in animals as well as artificial agents, having in mind further developments in artificial intelligence and cognitive science. In [17], an open-ended object category learning system, based on one-class learning and human–robot interaction, is described. The authors also proposed a teaching protocol for performance evaluation in open-ended learning. In [18], a multi-classifier system with similar goals is described in which a meta-learning component monitors classifier performance, reconfigures classifier combinations and chooses the classifier to be used for prediction. These works are based on 2D images collected in static scenes. Since there is no continuous stream of data being stored, memory requirements are easily satisfied. Kirstein et al. [19] proposed a lifelong approach for interactive learning of multiple categories from 2D perception, in this case based on vector quantization. This involves selecting the most crucial features from a series of high dimensional feature vectors that almost exclusively belong to each specific category. However, they still follow a standard train-and-test procedure, which is not plausible in open-ended scenarios. Moreover, the authors did not provide details on their memory system or computational architecture. The work is also not integrated in a hybrid perceptual/semantic processing system. Kruger et al. [20] use the so-called “object–action complexes” (OAC) to bind objects, actions and attributes associated with an agent in a causal way. These OACs are learnable/refinable semantic representations. To ground OACs, an agent requires an object perception and learning system, such as the one we propose below. Heintz et al. [21] propose a hierarchical framework designed to anchor symbols to continuous streams of sensor data. The approach dynamically constructs and maintains data association hypotheses at multiple levels. A traffic monitoring application is used to illustrate the system. This work proposes a general approach to the anchoring problem, but does not address object perception and learning. Willow Garage developed the Object Recognition Kitchen (ORK), 2 2 a 3D object recognition system built on top of the Ecto framework. 3 3 Ecto organizes computation as a directed acyclic graph, which implies important limitations in the architecture of the perception system. Moreover, in ORK, training/learning and detection/recognition are two separate stages. Such approach is not suitable for developing open-ended learning agents. In contrast, our system allows for concurrent or interleaved learning and recognition, and real-time performance is achieved through nodelets and multiplexing. Although the Point-Cloud Library (PCL) is increasingly popular, we do not know of other systems building upon PCL to integrate 3D object perception, memory, learning, recognition and interaction. Aldoma et al. [22] reviewed several state-of-the-art 3D shape descriptors from PCL to develop 3D object recognition and pose estimation capabilities. Throughout the paper, the properties, advantages and disadvantages of different local and global shape descriptors are considered. They also proposed two pipelines for object recognition based on PCL. In the first pipeline, an object view is described by a set of local shape features, which are computed around keypoints. Afterward, each feature is compared against all the features of all models in a database using Euclidean distance. The second pipeline is based on global descriptors, i.e. high-dimensional representations usually calculated for object candidates (subsets of the scene’s point cloud obtained through segmentation). Clapés et al. [23] proposed an automatic surveillance system for user identification and object recognition. In this work, the position of the RGB-D camera is fixed and the authors employed a background subtraction strategy to segment users and objects in the scene. In the case of object detection, the remaining connected components (those not previously selected as being part of the user) are considered as object candidates. During the recognition stage, Fast Point Feature Histogram (FPFH) [24] features are computed for each detected object view and matched against the training models. There is no learning process involved. 3 A dual memory approach Arguably, robots that interact closely with non-expert users should be [25]: animate, meaning that they react appropriately to different events, based on a tight coupling of perception and action; adaptive, to cope with changing users, tasks and environments, which requires reasoning and learning capabilities; and accessible, that is, they should be easy to command and instruct, and they should also be able to explain their beliefs, motivations and intentions. In an abstract architecture for intelligent robots, as shown in Fig. 2 (a), a Perception component processes all momentary information coming from sensors, including sensors that capture the actions and utterances of the user. A Reasoning component updates the world model and determines plans to achieve goals. An Action component reactively dispatches and monitors the execution of actions, taking into account the current plans and goals. Action processing ranges from low-level control to high-level execution management. Finally, a Learning component, which typically runs in the background, analyzes the trace of foreground activities recorded in a Memory component and extracts and conceptualizes possibly interesting experiences. The resulting conceptualizations are stored back in memory. Each component in such abstract architecture decomposes into a set of software modules, possibly distributed across multiple computers. The reasoning component manipulates primarily semantic representations of the current world state, goals and plans, that is, representations that are symbolic and relational in nature. In RACE, where case studies were carried out in a restaurant environment, semantic representations describe tables, chairs, table-top objects, guests, the robot, etc., the categories of these objects, the relations between them, and the actions and events that change these relations. The semantic information flowing between reasoning, execution management and memory is typically of small size, and its processing tends to be slow. One of the challenges in a project like RACE was to combine and store semantic and perceptual representations. Standard SQL databases do not cope well neither with semantic data nor with perception data, as both tend to be partially unstructured and/or of variable size. This suggests that modern NoSQL databases [26] should be used. Semantic data represents the world in terms of instances, categories and relations between them. A semantic representation of the state of the world can be simply a set of subject–predicate–object triples. A special kind of database, the triplestore, which shares some features with both SQL and graph databases, is especially optimized to store information in the form of a set of triples. Triplestores are clearly one of the database types to take into account when developing memory systems for robots. An RDF triplestore was in fact the choice for the initial memory component in the RACE architecture [1]. The contents of this memory system, which is used as blackboard for all processes, is semantic in nature. It keeps track of the evolution of both the internal state of the robot and the events observed in the environment. Access to the triplestore is granted via a ROS node that provides database query and write services for all other nodes (an interface node). Information exchange is performed using either publisher/subscriber or client/server mechanisms. ROS communications are a robust framework [13]. However, when the size of the messages is large (e.g., when passing 3D point clouds), the communication between processes is slow. In the case of perception related data, its large size implies large ROS messages to be passed between the database interface node and the other nodes. This is a major constraint, especially considering that, unlike semantic data, perceptual data flows continuously at the sensor output frequency. Using a database interface node creates a bottleneck for accessing the database, since it handles access requests in a first in, first out basis. Moreover, although triplestores are well suited for storing semantic information, they can hardly be considered suited for storing perception data. In fact, the perception modules will primarily process numeric information organized in structures like vectors and matrices, possibly grouped in sets. For instance the raw perception data about an object, after detection, can be a 3D point cloud, which is a set of points described by their 3D coordinates and possibly RGB information. Based on the point cloud, shape features can be extracted, and the object can be represented by a set of local shape features, where each of them can be a 2D shape histogram. To ensure timely reaction to events in the environment, perception modules run continuously at the frame rate of the used sensors. Although raw data tends to be massive (high-dimensional), the perception modules must run fast, and whatever memory support they use, must also be lightweight. In the context of RACE, to accommodate semantic and perceptual information in the same database, the only option would be to replace the triplestore with a more generic kind of database. However, we would loose the special features of triplestores, which are optimized for storing triples. In alternative, two different databases can be used, one for semantic information, and the other for perceptual information. The second alternative, which seems more promising, allows to use databases that are well suited for the kinds of data that each will store. In RACE, we converged to the second option. Fig. 2(b) shows an abstract architecture diagram in which we make explicit the dual memory approach. In what concerns reasoning, we make explicit both interpretation and planning capabilities. One of the most basic interpretation capabilities is anchoring, i.e. connecting object symbols used in the semantic memory to the perception of those objects that is recorded in the perceptual memory. Interpretation also includes computing spatial relations between objects to keep an updated relational model of the scene around the robot. In turn, this scene model can be taken into account for anchoring. The perceptual memory contains, not only object perception data, but also object category knowledge, in the form of perceptual categories that enable to recognize instances of those categories. These perceptual categories are learned in an open-ended fashion with user mediation [27]. The perceptual learning component primarily uses data from perceptual memory (e.g. shape features of objects) as well as from the semantic memory (e.g. teaching instructions from the user). In RACE, the implementation of the perceptual memory was carried out using a flexible and scalable NoSQL database which operates in memory (see the next section for details). It is worth emphasizing that, although our design choices were guided primarily by engineering criteria, we converged to a solution that is biologically and cognitively plausible. In fact, as previously pointed out, human memory is not a single monolithic system, but rather a combination of several memory subsystems specialized for storing different types of information and supporting different functionalities [14,15]. In particular, our perceptual memory resembles the so-called Perceptual Representation Memory System, used in human cognition for enhancing the identification of objects as structured physical–perceptual entities, a process referred to as perceptual priming [14]. Another key distinction in cognitive science is between processes that are fast, automatic and unconscious, and processes that are slow, deliberative and conscious [28]. Our dual semantic/perceptual memory approach is also in line with these findings. 4 The RACE object perception system The work presented in this paper was developed as an extension to the initial RACE architecture [1] (see also Fig. 1). In particular, the work focused on extending the perceptual capabilities of the system. The perception system developed around the perceptual memory supports the anchoring of object symbols into perceived object data as well as the grounding of category symbols into perceptual categories. Since the initial integration of the object perception system, the RACE system included basic capabilities for object symbol anchoring, allowing perceived objects to be represented not only in the perceptual memory, but also in the semantic memory. The object perception system targets table-top scenes in a restaurant scenario. This system became a salient portion of the full RACE system [2]. 4.1 Architecture The developed perception system is composed of six functional components: Object Detection, Multiplexed Object Perception, User Interface, Reasoning and Interpretation, Memory and Conceptualization. These are represented by the dashed rectangles in Fig. 3 . Functional components in Fig. 3 correspond to those highlighted in bold in the high-level RACE architecture (Fig. 1) and to the perception, interpretation, memory and perceptual learning components in Fig. 2(b). In turn, each functional component contains one or more software modules (solid line rectangles in Fig. 3). Arrows signal the exchange of information between software modules. Each software module is organized into a ROS package and will typically correspond to a node or a nodelet 4 4 at runtime. The implementation of the perceptual memory was carried out using LevelDB, a lightweight, flexible and scalable NoSQL database developed by Google. 5 5 LevelDB is a key–value storage database that provides an ordered mapping from string keys to string values. In addition, LevelDB operates in memory and is copied to the file system asynchronously. This significantly improves its access speed. 4.2 The PCL foundation of this work This work heavily relies on PCL [29] 6 6 functionalities. Table 1 lists the PCL classes used by the object perception and perceptual learning system along with values we typically use for the main configuration parameters. The reason why they are used as well as the modules in which they are used are also given. It can be seen from Table 1 that several PCL functionalities are used by our system. For point cloud size reduction, we use both conditional removal filter as well as voxel grid filters. Table planes are detected using RANSAC, and their boundaries extracted by estimating the 2D convex hull. Points belonging to tabletop objects are extracted using the polygonal prism extraction method. Tabletop objects are segmented using Euclidean cluster extraction. The pose of newly detected tabletop objects is estimated using PCA, which is useful to define well oriented bounding boxes. Objects are tracked over time using the particle filter tracker. Object views are represented by spin image feature descriptors, and object recognition uses an optimized view-to-view distance calculation based on K-d trees. Since PCL is used in nearly every module of the system, the system could not have been developed easily without PCL. This work shows the current importance of PCL in building sophisticated 3D perception systems in robotics and other domains. 4.3 Object perception An RGB-D sensor is used for the perception of both the user and the table-top scene. The starting point for the perception of the table-top scene is the Table-Top Segmenter (TTS) module, which uses ROS 7 7 and PCL functionalities to isolate (partial) point clouds of the objects placed on the table (see Table 1) [29,39]. The Object Detector (OD) module periodically requests the current list of objects from TTS. Then, OD will check if any of those objects is already being tracked. To do this, OD matches the point clouds of all objects on the table with the estimated bounding boxes of all objects currently being tracked. The percentages of points of the tabletop objects that lie inside the bounding boxes of the tracked objects are computed. A large percentage indicates that the tracked object and the segmented object are the same. Point clouds that cannot be matched with any of the tracked bounding boxes are assumed to represent new objects just added to the scene. OD will assign a new identifier (track_id) to each newly detected object. Also for each new object, OD will launch an object perception pipeline which contains three modules: Object Tracking, Feature Extraction and Object Recognition. Fig. 4 shows a situation where two objects are segmented and tracked, i.e., they have bounding boxes around them. Object Tracking (OT) is responsible for keeping track of the target object over time while it remains visible. Tracking is an essential base for anchoring. On initialization, OT receives the point cloud of the detected object and computes a bounding box for that point cloud, the center of which defines the pose of the object. A particle filter tracking approach from PCL (see Table 1) is then used to predict the next probable pose of the object. In each cycle, OT sends out the tracked pose of the object both to OD (as mentioned above) and to the Interpretation component. At a lower rate, OT sends the point cloud of the object (i.e. containing the points inside the predicted bounding box) to Feature Extraction. As expected, the system is sensitive to the speed with which objects move. If an object moves very fast, tracking is lost, then a new object is detected, and a new object perception pipeline is initiated. Nonetheless, our experiments have shown that the system is able to cope with users picking up the objects and moving them around in natural movements with typical speeds. The Feature Extraction (FE) module computes and stores object representations in the perceptual memory. Objects are represented by sets of local shape features computed in certain keypoints. For efficiency reasons, the number of keypoints should be much smaller than the total number of points. To select keypoints, a voxelized grid approach from PCL is used (see Table 1). We select, for each voxel, the point that is closest to the voxel center [3]. Thus, there will be one keypoint per voxel. The surrounding shape in each keypoint is described by a spin-image [37]. Spin-images are pose invariant, and therefore a suitable local shape descriptor for 3D perception in service robots. They are computed by projecting the 3D surface points of the object to the keypoint’s tangent plane We use an implementation of spin-image estimation available from PCL (see Table 1). In addition to storing object representations in the perceptual memory, FE also sends them to Object Recognition (OR). The perceptual categories learned so far and stored in the perceptual memory are used by OR to predict the category of the target object. OR is a low frequency module, which runs at 1 Hz. Accordingly, FE receives object point clouds from OT and sends the extracted representations for recognition at the same frequency. Thus, only OT itself uses object point clouds at the frame rate of the sensor (30 Hz). For better representing an object, it is important to store different views, which is possible when the object is moved (and thus its pose relative to the sensor changes). In contrast, storing all object representations computed by FE while the object is static would lead to unnecessary accumulation of highly redundant data. On a different line, it is important to minimize noise effects possibly affecting object views. Thus, to optimize memory usage while keeping potentially relevant and distinctive information, a heuristic is used to select key views, that is, object views that should be stored. Whenever the tracking of an object is initialized, or when it becomes static again after being moved, three consecutive object views are stored, provided that the hands of the user are not detected near the object. In case the hands are detected near the object, storing key views is postponed until the hands are withdrawn. OT is responsible for marking object views as key views. Then, when FE receives a point cloud marked as key view, it will store the respective representation in the perceptual memory. Object recognition results are also written to the perceptual memory, where the Interpretation component can fetch them to support symbol anchoring. The current implementation is capable of anchoring symbols that refer to objects only while these remain visible. Further work is ongoing to enable anchoring object symbols when the visual tracking is lost. 4.4 User perception The perceptual memory supports, not only the anchoring of object symbols into perceived object data, but also the grounding of category symbols into perceptual categories. Perceptual categories are acquired with user mediation, that is, the user points to objects and provides their category names. Verbal input is provided through interactive markers in RVIZ, a 3D visualization tool for ROS. Point gesture recognition is based on tracking the skeleton of the user. The Skeleton Tracker (ST) module is the one available in OpenNI. 8 8 It tracks the user skeleton pose over time based on RGB-D data. The skeleton pose information is passed to the Gesture Recognizer (GR) module, which computes a pointing direction. Currently, the pointing direction is assumed to be the direction of the right forearm (see an example in Fig. 4). The pointing direction is then passed to the Interpretation component. Upon receiving verbal input, the Interpretation component checks if the received pointing direction intersects the bounding box of any of the objects currently on the table according to the world state recorded in the semantic memory. If that is the case, then a teaching instruction is recorded in the semantic memory, stating that the target object was taught to belong to the given category. Teaching instructions trigger perceptual learning to create and/or update object categories. 4.5 Addressing computational issues In contrast with the reasoning processes supported by the semantic memory, the processes developed around the perceptual memory must run fast to cope with the continuous stream of massive sensor data. As pointed out, one of the reasons for using LevelDB to implement the perceptual memory is the fact that it operates in RAM. There is, however, the limitation that simultaneous access to LevelDB is only possible by threads within the same process. To comply with this constraint while keeping ROS as the framework for the newly developed modules, we use ROS nodelets. 9 9 Nodelets, which run as threads of a single process, were designed to provide a way of concurrently running different modules with zero copy transport between publisher and subscriber calls (as an example, see [40]). The motivation for ROS nodelets comes from systems with high throughput data flows as is common in perception systems. It is not surprising, therefore, that the developers of PCL and ROS nodelets are the same. In our system, in addition to handling high throughput data flows, nodelets come handy to implement modules that need to simultaneously access the perceptual memory (LevelDB). Another way of optimizing perception is to parallelize computations. In our system, instead of tracking all objects in a single tracking module, there is a tracker for each object. Similar strategy is used for feature extraction and object recognition. In other words, object perception is designed to be multiplexed. Every time a new object is detected, a corresponding instance of the object perception pipeline (see Fig. 3) is launched. Thus there are as many object perception pipelines as the number of currently tracked objects, and each pipeline targets a specific object. Since the modules in an object perception pipeline run as independent nodes/nodelets, they can be distributed to different CPU cores, thus improving the overall computational efficiency of perception. Note that the three modules in the object perception pipeline are traditionally amongst the heaviest in terms of computational requirements. The parallelization is aimed at the hotspot or bottleneck of the computation flow and takes full advantage of modern multi-core machines. In fact, experiments with a non-multiplexed version of this architecture show that it cannot run in real-time. We can easily configure the perception and perceptual learning modules to be launched with different runtime configurations, that is, using ROS nodelets only, ROS nodes only, or a combination of both. By default, the object perception pipelines, the perceptual learning module and the perceptual memory run as a set of nodelets of a single process. When debugging is necessary, we use a configuration where all modules run as nodes. In this configuration, the modules access the perceptual memory (LevelDB) using ROS services provided by a database interface node. 5 Perceptual learning Although other kinds of perceptual categories could be considered, perceptual learning currently focuses on object categories. We approach object category learning from a long-term perspective and with emphasis on open-endedness, i.e. not assuming a pre-defined set of categories [17,18]. For example, when learning how to serve a coffee [1], if the robot does not know how a mug looks like, it may ask the user to point to one. Such situation provides an opportunity to collect an experience for learning. Concerning category formation, a purely memory-based learning approach is adopted, in which a category is represented by a set of views of instances of the category. The recording of a teaching instruction in the semantic memory triggers the perceptual learning component. If a new category was taught, the key views of the target object stored by the feature extraction module (see above Section 4.3) are used to initialize the category. If the category is previously known, the key views are added to the existing category representation only if the agent cannot correctly recognize the category of the target object. The current approach differs from our recent previous work [3,10], in the distance measures used for classification, as will be pointed out. In order to estimate the dissimilarity between a target object view, t , and an object view, o , contained in a category model in the perceptual memory, the following distance function is used: (1) D ( t , o ) = ∑ l = 1 q min k d ( t l , o k ) q , where t l , l = 1 , …, q , are the spin-images of the target object, o k represents the spin-images of the model object, and q is the number of target object’s spin-images (see Section 4.3 and [37] on spin-images). Since a linear search in high-dimensional spaces has a high computational cost, which is not suitable in the case of an autonomous service robot, a fast approximate nearest neighbor search based on k-d trees [41] from PCL (see Table 1) is used instead of a traditional nearest neighbor search. The next step is to compute the object–category distance between the target object, t , and a certain category, C, as the average distance of the instances of C to t : (2) O C D ( t , C ) = ∑ u ∈ C D ( t , u ) n , where n = | C | is the total number of category instances. In object recognition, instances are more spread in some categories than in others. Normalizing distances will help to prevent misclassification. Distance normalization is based on the following intra-category distance: (3) I C D ( C ) = ∑ u ∈ C ∑ v ∈ C , v ≠ u D ( u , v ) n . ( n − 1 ) . The normalized distance of target object, t , to the category, C, N D ( t , C ) , is computed as follows: (4) N D ( t , C ) = 2 × O C D ( t , C ) I C D ( C ) + I C D ¯ where I C D ¯ is the average of the intra-category distances of all categories, i.e. I C D ¯ = ∑ i = 1 m I C D ( C i ) / m , and m is the number of categories. Finally, the target object is classified based on the minimum normalized distance of the known categories to the object. If, for all categories, the normalized distance is larger than a given Classification Threshold, C T , then the object is predicted to belong to an unknown category 10 10 By default, CT=2.0 is used. See Section 6.2 for an evaluation of alternatives. : (5) C a t e g o r y ( t ) = { unknown , if m i n C N D ( t , C ) > C T a r g m i n C N D ( t , C ) , otherwise . In open-ended environments where some objects may belong to not yet known categories, recognizing that an object belongs to an unknown category is important. It may prevent the agent from making further decisions based on wrong assumptions. On the other hand, detecting that an object belongs to an unknown category may be used by the agent to trigger some exploration or interaction process leading to the acquisition of a new category. In our recent previous work [3,10], the object–category distance was measured as the minimum distance between the target object and known instances of a given category. This measure was then normalized by an intra-category distance. If the normalized measure was larger than a given classification threshold, the target object could not belong to that category. The combination of minimum distance with normalization and threshold led to bad decisions in some limit situations. The new formulation here presented solves these problems. In addition, by taking into account the average intra-category distance, the current normalization is also more stable than the previously used normalization method. 6 Profiling and evaluation This section presents three sets of results. First, based on a session where users manipulate objects on a table and interact with the developed perception and perceptual learning system, we carry out a profiling analysis of the main modules of the system. Second, we present an off-line evaluation of the perceptual learning approach under different configurations of the system. Finally, we report on open-ended learning experiments carried out on a public domain dataset. 6.1 Profiling For profiling the modules, two human users interacted with the system in a short session of nearly 4 min (Table 2 ). All raw data from the RGB-D sensor as well as verbal input from the users during the session was recorded in a rosbag, which was then used to test different configurations of our system. Note that, when the system starts, the set of categories known to the system is empty. There is a video 11 11 The video can be found at that illustrates the behavior of the main modules of the system, from user/object tracking to learning and recognition. As discussed in Section 4.5, nodelets can significantly improve efficiency since they support zero copy transport and they enable simultaneous access to LevelDB. Fig. 5 compares the processing time of the object perception modules. The tracker modules (Fig. 5(a) nodes and (d) nodelets) tend to display a stable processing time shortly after their initialization. This is explained by the fact that the size of the input data is more or less stable over time. In this case, nodelets are more efficient when compared to nodes: for example for pipelines 1–3 in the 100–150 time interval, nodes display an average processing time of 45 ms, compared to 25 ms in the case of nodelets. Since the trackers do not access the database, the main factor contributing to the increase in efficiency is the zero copy transport. The messages that are received (sensor point cloud) and sent (partial object point cloud) by the trackers are of large size, which explains why zero copy transport enables such a significant improvement. The feature extraction modules (Fig. 5(b) nodes and (e) nodelets) show a different behavior. These modules periodically compute the spin-image representation from the partial object point cloud. At some points, the point cloud is signaled to belong to a key view, which will trigger the writing of that representation to the perceptual memory. The curves show these points in time with a rapid increase in processing time. Nodelets also display these peaks, but because access to the database is much faster, the peaks are smaller, as is the average processing time. The object recognition modules (Fig. 5 (c) nodes and (f) nodelets) receive a representation of the current object view from the feature extraction, and compare it against the representations of all known category views. Thus, they are continuously reading the database in the search for an update to the known categories. As a result, the larger the size of the database, the slower the reading of the complete set of categories. However, in the case of nodelets, this deterioration is minor when compared with nodes, since accessing the database is much more efficient. Fig. 6 shows the memory usage of the system. Notice that at the end of the experiment the memory size would be above 1 MB if all object point clouds extracted by the trackers would be stored (roughly 5 Kb/s). In a continuously running system, this rate of data accumulation would be hard to handle, and would not bring any real benefit. The total size of the point clouds of all the selected key views is much smaller (one order of magnitude in this experiment). The data actually accumulated in memory (shape representations based on spin-images) is even smaller. Fig. 7 shows the evolution of object recognition performance throughout the experiment. When the first Mug (T1) is placed on the table the system recognizes it as Unknown. After some time, the instructor labels T1 as a Mug and the system starts displaying a precision of 1.0. However, the recall score is under 0.2, because the system classified T1 as Unknown several times before the user labeled the object. After the labeling, the recall starts improving continuously. The instructor then places a Vase (T2) on the table. Because the category Vase has not been taught yet, the performance goes down. After labeling T2 as Vase, performance starts going up again. When a second Mug (T3) enters the scene, the system can correctly recognize it and the scores continue to increase. Then, a Plate (T4) enters the scene, causing recall to drop. Successively, the Plate is taught, a Bottle is placed on the table and then taught, and eventually performance starts going up again. This illustrates the process of acquiring categories in an open-ended fashion with user mediation. 6.2 Off-line evaluation of the perceptual learning approach More systematic experiments have been performed to evaluate the object category learning and recognition approach. An object dataset has been acquired, which contains 339 views of 10 categories of objects: bottle, bowl, flask, fork, knife, mug, plate, spoon, teapot and vase. In addition, there are 31 views of unknown and false objects. The point clouds of the objects were segmented using an offline version of the Object Detection (OD) module. Detected objects were manually labeled. The performance of the system was measured using a leave-one-out cross validation scheme. A total of 24 experiments were performed for different values of four parameters of the system, namely the voxel size (VS) which is related to number of key points extracted from each object view, the image width (IW) and support length (SL) of spin images, and the classification threshold (CT). Results are presented in Table 3 . The parameters that obtained the best average F1 score were selected as the default system parameters. They are the following: VS=0.03, IW=8, SL=0.1 and CT=2. The F-measure of the proposed system with the default parameters was 0.94. Results show that the overall performance of the recognition system is promising. Spin images are capable of collecting distinctive traits of the local surface patches of each object. 6.3 Open-ended learning evaluation Although there are well established methodologies to evaluate learning systems, e.g., k-fold cross validation, leave-one-out, etc., these approaches follow the classical train-and-test procedure, i.e., two separate stages, training followed by testing. Training is accomplished offline, and once it is complete the testing is performed. These methodologies are not well suited to evaluate open-ended learning systems, because they do not abide to the simultaneous nature of learning and recognition and because the number of categories must be predefined. A teaching protocol for evaluating open-ended learning systems was proposed in [17,42]. The protocol relies on three basic actions to interact with an object recognition system: teach, used for teaching a new object category; ask, used to ask the system what is the category of an object view; and correct, used for providing the system with an additional (labeled) view of an existing category. The idea is to continuously ask the system to recognize previously unseen views of known categories and provide corrections when needed. This way, the system is trained, and at the same time the recognition performance of the system is continuously estimated. A simulated teacher was developed to automate experiments following the teaching protocol [3]. To operate, the simulated teacher must be connected to a data-set of object views. In this work, we use the Washington RGB-D Object Dataset [43]. This dataset contains images of 300 common household objects from 51 categories. In the experiments presented, the system begins without category knowledge, i.e., it knows zero categories at start, and the training instances are gradually given to the system. Thus, object category models are incrementally built. Although the evaluation protocol is designed to test the system only using views of known categories, the system is designed to identify unknown categories when the views to be recognized are too far from all models in memory. Therefore, we use F-measure, which combines precision and recall, as the indicator of recognition performance. Protocol success is a local F-measure, computed in a sliding window of size 3 n (as defined in [42]), where n is the number of categories that have already been introduced. According to the teaching protocol, the system is ready to learn a new object category when the protocol success is above a threshold (0.67 in all experiments presented), and at least one instance of every known category has been tested. Simulated teacher experiments can be used to evaluate the performance of open-ended learning systems using several measures, namely: • The number of learned categories at the end of an experiment, an indicator of how much the system is capable of learning; • The number of question/correction iterations required to learn those categories and the average number of stored instances per category, indicators of time and memory resources required for learning; • Global success, an F-measure computed using all predictions in a complete experiment, and the (local) protocol success defined above, indicators of how well the system learns. Since the order of introducing new categories may affect the performance of the system, ten experiments were performed using random sequences of introduction of categories. Table 4 summarizes the 10 experiments. Fig. 8 (a) shows the evolution of the teaching protocol success in experiment 3. The introduced categories are signaled in the plot. Fig. 8(b) shows the protocol success as a function of the number of learned categories. Fig. 9 (a) shows the global success (i.e. since the beginning of the experiment) as a function of the number of learned categories. In this figure we can see that the global success decreases as more categories are learned. This is expected since the number of categories known by the system makes the classification task more difficult. Finally, Fig. 9(b) shows the number of learned categories as a function of the number of protocol iterations. This gives a measure of how fast the learning occurred in each of the experiments. 7 Conclusions This paper describes the 3D object perception and learning system developed for the RACE project. The system is designed to detect and track tabletop objects. It is also capable of classifying the tracked objects according to the object categories that are known by the system. Furthermore, this object category knowledge may be enhanced in real time through an RVIZ interface that enables humans to teach additional object categories. The paper describes a dual memory approach in which two databases with different characteristics are used to store semantic and perceptual data. The proposed nodelet software architecture is designed for efficiency purposes, since it enables actual simultaneous access to the perceptual database. Results show that the nodelet based approach is significantly faster when compared to the standard node approach. In addition, we also propose a multiplexed object perception pipeline, in which a set of nodes is launched during execution to handle each newly detected object. This mechanism allocates separate computational resources for each tracked object, which creates a dynamic computation graph in run time, and facilitates the parallelization of processing. As a consequence, the system is capable of simultaneously tracking several objects moving in the scene. The RACE object perception and learning system contains a large number of nodes that process 3D data for different purposes, e.g., segmentation, feature extraction, tracking, etc. Thus, it also serves as a good example of how PCL functionalities can be used to create an efficient and complex 3D perception system. This paper presented a perceptual memory system designed to enable grounding of object symbols and object category symbols in an open ended fashion. This system is integrated in a dual memory architecture which also includes a semantic memory component. The dual memory approach contributes to the optimization of both the semantic and perceptual components, and is in line with findings in cognitive science regarding the human memory system. The perceptual memory implementation was carried out using a lightweight, NoSQL database which, when combined with a nodelet based infrastructure, allows the simultaneous access of several modules to the storage. The system also supports runtime multiplexing of the object perception pipelines, which leads to the parallelization of the bottlenecks in the data processing. Results show that the perceptual memory combined with a nodelet infrastructure significantly outperforms a node based approach. The presented architecture can seamlessly integrate user-mediated experience acquisition, conceptualization and recognition. The system is open-ended since it can continuously acquire new object categories. Because the system is open-ended and receives a continuous stream of data, ongoing work is using data stream clustering methods to update a dictionary of local features [44]. Acknowledgments This work was funded by the EC 7th FP theme FP7-ICT-2011-7, Grant agreement No. 287752 (project RACE — Robustness by Autonomous Competence Enhancement). We would like to thank the other RACE project partners for their efforts in the integration and the demonstrations, and especially to the Technical Aspects of Multimodal Systems (TAMS) group, University of Hamburg, for making the PR2 robot available to the project. References [1] S. Rockel, B. Neumann, J. Zhang, K.S.R. Dubba, A.G. Cohn, S˘. Konec˘ný, M. Mansouri, F. Pecora, A. Saffiotti, M. Günther, S. Stock, J. Hertzberg, A.M. Tomé, A.J. Pinho, L. Seabra Lopes, S. von Riegen, L. Hotz, An ontology-based multi-level robot architecture for learning from experiences, in: Designing Intelligent Robots: Reintegrating AI II, AAAI Spring Symposium on, Stanford, USA, 2013. [2] J. Hertzberg J. Zhang L. Zhang S. Rockel B. Neumann J. Lehmann K.S.R. Dubba A.G. Cohn A. Saffiotti F. Pecora M. Mansouri Š Konec˘ný M. Günther S. Stock L. Seabra Lopes M. Oliveira G.H. Lim H. Kasaei V. Mokhtari L. Hotz W. Bohlken The RACE project KI—Künstliche Intelligenz 28 4 2014 297 304 [3] H. Kasaei M. Oliveira G.H. Lim L. Seabra Lopes A.M. Tomé Interactive open-ended learning for 3D object recognition: An approach and experiments J. Intell. Robot. Syst. 2015 1 17 [4] K. Dubba, M. de Oliveira, G. Lim, H. Kasaei, L. Seabra Lopes, A. Tomé, A. Cohn, Grounding language in perception for scene conceptualization in autonomous robots, in: AAAI 2014 Spring Symposium on Qualitative Representations for Robots, 2014. [5] V. Mokhtari~Hassanabad G.H. Lim L. Seabra~Lopes A.J. Pinho Gathering and conceptualizing plan-based robot activity experiences E. Menegatti N. Michael K. Berns H. Yamaguchi Intelligent Autonomous Systems 13: Proceedings of the 13th International Conference IAS-13 Advances in Intelligent Systems and Computing vol. 302 2015 Springer [6] S. Cousins B. Gerkey K. Conley W. Garage Welcome to ROS topics IEEE Robot. Autom. Mag. 17 1 2010 12 14 [7] S. Cousins B. Gerkey K. Conley W. Garage Sharing software with ROS IEEE Robot. Autom. Mag. 17 2 2010 12 14 [8] D.S. Nau T.-C. Au O. Ilghami U. Kuter J.W. Murdock D. Wu F. Yaman Shop2: An htn planning system J. Artificial Intelligence Res. 20 2003 379 404 [9] L. Barsalou Perceptual symbol systems Behav. Brain Sci. 22 4 1999 577 609 [10] M. Oliveira G.H. Lim L. Seabra Lopes H. Kasaei A. Tome A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2014 IEEE Chicago, Illinois [11] G.H. Lim I.H. Suh H. Suh Ontology-based unified robot knowledge for service robots in indoor environments Systems, IEEE Trans. Syst. Man Cybern. 41 3 2011 492 509 [12] S. Coradeschi A. Saffiotti An introduction to the anchoring problem Robot. Auton. Syst. 43 2–3 2003 85 96 special issue on perceptual anchoring [13] S. Zaman G. Steinbauer J. Maurer P. Lepej S. Uran An integrated model-based diagnosis and repair architecture for ROS-based robot systems IEEE International Conference on Robotics and Automation 2013 Karlsruhe Germany [14] E. Tulving Concepts of human memory L. Squire G. Lynch N. Weinberger J. McGaugh Memory: Organization and Locus of Change 1991 Oxford Univ. Press 3 32 [15] E. Tulving Episodic memory and autonoesis: Uniquely human? J.M.H.S. Terrace The Missing Link in Cognition 2005 Oxford Univ. Press NewYork, NY 4 56 [16] R. Wood P. Baxter T. Belpaeme A review of long-term memory in natural and synthetic systems Adapt. Behav. 20 2011 81 103 [17] L. Seabra~Lopes A. Chauhan How many words can my robot learn?: An approach and experiments with one-class learning Interact. Stud. 8 1 2007 53 81 [18] L. Seabra~Lopes A. Chauhan Open-ended category learning for language acquisition Connect. Sci. 20 4 2008 277 297 [19] S. Kirstein H. Wersing H.-M. Gross E. Körner A life-long learning vector quantization approach for interactive learning of multiple categories Neural Netw. 28 2012 90 105 [20] N. Krüger C. Geib J. Piater R. Petrick M. Steedman F. Wörgötter A. Ude T. Asfour D. Kraft D. Omrčen A. Agostini R. Dillmann Object-action complexes: Grounded abstractions of sensory-motor processes Robot. Auton. Syst. 59 10 2011 740 757 [21] F. Heintz, J. Kvarnstrom, P. Doherty, A stream-based hierarchical anchoring framework, in: Intelligent Robots and Systems, 2009, IROS 2009, IEEE/RSJ International Conference on, 2009, pp. 5254–5260. [22] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library IEEE Robot. Autom. Mag. 1070 9932 2012 [23] A. Clapés M. Reyes S. Escalera Multi-modal user identification and object recognition surveillance system Pattern Recognit. Lett. 34 7 2013 799 808 [24] R.B. Rusu N. Blodow M. Beetz Fast point feature histograms (FPFH) for 3D registration Robotics and Automation, 2009, ICRA’09, IEEE International Conference on 2009 IEEE 3212 3217 [25] L. Seabra~Lopes J. Connell Semisentient robots: routes to integrated intelligence IEEE Intell. syst. 16 5 2001 10 14 [26] S. Sahib A review of non relational databases, their types, advantages and disadvantages Int. J. Eng. Technol. 2 2 2013 [27] G.H. Lim M. Oliveira V. Mokhtari S. Hamidreza~Kasaei A. Chauhan L. Seabra Lopes A.M. Tomé Interactive teaching and experience extraction for learning about objects and robot activities Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on 2014 IEEE 153 160 [28] J.S. Evans Dual-processing accounts of reasoning, judgment, and social cognition Ann. Rev. Psychol. 59 1 2008 255 278 [29] R. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: Robotics and Automation, ICRA, 2011 IEEE International Conference on, 2011, pp. 1–4. [30] D. Cohen-Or A. Kaufman Fundamentals of surface voxelization Graph. Models Image Process. 57 6 1995 453 461 [31] C.B. Barber D.P. Dobkin H. Huhdanpaa The Quickhull algorithm for convex hulls ACM Trans. Math. Softw. 22 4 1996 469 483 [32] N.M. Amato, F.P. Preparata, An NC parallel 3D convex hull algorithm, in: Proceedings of the ninth annual symposium on Computational geometry, SCG’93, 1993, pp. 289–297. [33] M.A. Fischler R.C. Bolles Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [34] R.B. Rusu Semantic 3D object maps for everyday manipulation in human living environments (Ph.D. thesis) 2009 Computer Science department, Technische Universitaet Muenchen Germany October [35] W. Yi S. Marshall Principal component analysis in application to object orientation Geo-spatial Inf. Sci. 3 3 2000 76 78 [36] Y. Salih, A. Malik, 3D tracking using particle filters, in: Instrumentation and Measurement Technology Conference, I2MTC, 2011 IEEE, 2011, pp. 1–4. [37] A. Johnson M. Hebert Using spin images for efficient object recognition in cluttered 3D scenes IEEE Trans. Pattern Anal. Mach. Intell. 21 5 1999 433 449 [38] M. Connor P. Kumar Fast construction of k-nearest neighbor graphs for point clouds IEEE Trans. Vis. Comput. Graphics 16 4 2010 599 608 [39] R. Rusu, N. Blodow, Z. Marton, M. Beetz, Close-range scene segmentation and reconstruction of 3D point cloud maps for mobile manipulation in domestic environments, in: Intelligent Robots and Systems, 2009, IROS 2009, IEEE/RSJ International Conference on, 2009, pp. 1–6. [40] M. Munaro F. Basso S. Michieletto E. Pagello E. Menegatti A software architecture for RGB-D people tracking based on ROS framework for a mobile robot Frontiers of Intelligent Autonomous Systems Studies in Computational Intelligence vol. 466 2013 53 68 [41] J.L. Bentley Multidimensional binary search trees used for associative searching Commun. ACM 18 9 1975 509 517 [42] A. Chauhan L. Seabra~Lopes Using spoken words to guide open-ended category formation Cogn. Process. 12 2011 341 354 [43] K. Lai, L. Bo, X. Ren, D. Fox, A large-scale hierarchical multi-view RGB-D object dataset, in: Robotics and Automation, ICRA, 2011 IEEE International Conference on, 2011, pp. 1817–1824. [44] M. Oliveira, L. Seabra Lopes, G.H. Lim, H. Kasaei, A.D. Sappa, A. Tome, A. Chauhan, Concurrent learning of visual codebooks and object categories in open-ended domains, in: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, IEEE (in press). Miguel Oliveira received the B.Sc., and M.Sc., degrees in Mechanical Engineering from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering with specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Luís Seabra Lopes is an Associate Professor of Informatics in the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, Portugal. He received a Ph.D. in Robotics and Integrated Manufacturing from the New University of Lisbon, Portugal, in 1998. He has longstanding interests in robot learning, cognitive robotic architectures, and human–robot interaction. Gi Hyun Lim received the B.S. degree in Metallurgical Engineering and the M.S. and Ph.D degrees in Electronics and Computer Engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Post-doctoral Researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal. His research interests lie in the area of intelligence and learning for robots, including perception and semantics. From 1997 to 1998, he was an Engineer with the Dongbu Electronics, Ltd., Chungcheongbuk-do, Korea, where he was involved in research on semi-conductor factory automation. From 1999 to 2005, he was a Senior Research Engineer at a venture business, where he was involved in research on real-time operating systems and embedded systems. S. Hamidreza Kasaei obtained B.Sc. (2010) and M.Sc. (2012) in Computer Engineering field of Artificial Intelligence from the University of Isfahan (Iran). Currently he is a Ph.D. student at the University of Minho, Aveiro and Porto (Portugal), where he works on 3D object category learning and recognition in open-ended domains as a research student at the Institute of Electronics and Telematics Engineering of Aveiro. He worked on Middle size soccer robot and Humanoid robot and obtained different ranks in Robocup competition. His main research interest are in computer vision, robotics and multi agent systems. Ana Maria Tomé is an Associate Professor of Electrical Engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as machine learning applications. Aneesh Chauhan is a Post-doctoral Researcher in the Computer Vision Group at the Centre of Automatics and Robotics at Universidad Politecnica de Madrid. He holds a Bachelor of Engineering degree in Computer Science and Engineering from Baba Ambedkar Marathwada University, Maharashtra, India, a Master of Science degree in Autonomous Systems from the University of Exeter, UK and Ph.D. in Informatics Engineering from Universidade de Aveiro, Aveiro, Portugal. His research interests include intelligent robotics, language grounding, human–robot interaction as well as the application of computer vision and machine learning approaches for autonomous perception tasks. "
    },
    {
        "doc_title": "Concurrent learning of visual codebooks and object categories in open-ended domains",
        "doc_scopus_id": "84958162360",
        "doc_doi": "10.1109/IROS.2015.7353715",
        "doc_eid": "2-s2.0-84958162360",
        "doc_date": "2015-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Bag-of-words models",
            "Gaussian Mixture Model",
            "Multiple levels",
            "Object categories",
            "On-line fashion",
            "Updated using",
            "Visual object recognition",
            "Visual objects"
        ],
        "doc_abstract": "© 2015 IEEE.In open-ended domains, robots must continuously learn new object categories. When the training sets are created offline, it is not possible to ensure their representativeness with respect to the object categories and features the system will find when operating online. In the Bag of Words model, visual codebooks are usually constructed from training sets created offline. This might lead to non-discriminative visual words and, as a consequence, to poor recognition performance. This paper proposes a visual object recognition system which concurrently learns in an incremental and online fashion both the visual object category representations as well as the codebook words used to encode them. The codebook is defined using Gaussian Mixture Models which are updated using new object views. The approach contains similarities with the human visual object recognition system: evidence suggests that the development of recognition capabilities occurs on multiple levels and is sustained over large periods of time. Results show that the proposed system with concurrent learning of object categories and codebooks is capable of learning more categories, requiring less examples, and with similar accuracies, when compared to the classical Bag of Words approach using codebooks constructed offline.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An adaptive object perception system based on environment exploration and Bayesian learning",
        "doc_scopus_id": "84933040723",
        "doc_doi": "10.1109/ICARSC.2015.37",
        "doc_eid": "2-s2.0-84933040723",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Bayesian learning",
            "Cognitive robotics",
            "Environment exploration",
            "Object perception",
            "Perception capability",
            "Probabilistic models",
            "Qualitative analysis",
            "Quantitative evaluation"
        ],
        "doc_abstract": "© 2015 IEEE.Cognitive robotics looks at human cognition as a source of inspiration for automatic perception capabilities that will allow robots to learn and reason out how to behave in response to complex goals. For instance, humans learn to recognize object categories ceaselessly over time. This ability to refine knowledge from the set of accumulated experiences facilitates the adaptation to new environments. Inspired by such abilities, this paper proposes an efficient approach towards 3D object category learning and recognition in an interactive and open-ended manner. To achieve this goal, this paper focuses on two state-of-the-art questions: (i) How to use unsupervised object exploration to construct a dictionary of visual words for representing objects in a highly compact and distinctive way. (II) How to learn incrementally probabilistic models of object categories to achieve adaptability. To examine the performance of the proposed approach, a quantitative evaluation and a qualitative analysis are used. The experimental results showed the fulfilling performance of this approach on different types of objects. The proposed system is able to interact with human users and learn new object categories over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A probabilistic approach for color correction in image mosaicking applications",
        "doc_scopus_id": "84920982597",
        "doc_doi": "10.1109/TIP.2014.2375642",
        "doc_eid": "2-s2.0-84920982597",
        "doc_date": "2015-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Color correction",
            "Color transfers",
            "Connected region",
            "Evaluation metrics",
            "Image histograms",
            "Image mosaicking",
            "Mapping functions",
            "Probabilistic approaches"
        ],
        "doc_abstract": "© 2014 IEEE.Image mosaicking applications require both geometrical and photometrical registrations between the images that compose the mosaic. This paper proposes a probabilistic color correction algorithm for correcting the photometrical disparities. First, the image to be color corrected is segmented into several regions using mean shift. Then, connected regions are extracted using a region fusion algorithm. Local joint image histograms of each region are modeled as collections of truncated Gaussians using a maximum likelihood estimation procedure. Then, local color palette mapping functions are computed using these sets of Gaussians. The color correction is performed by applying those functions to all the regions of the image. An extensive comparison with ten other state of the art color correction algorithms is presented, using two different image pair data sets. Results show that the proposed approach obtains the best average scores in both data sets and evaluation metrics and is also the most robust to failures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive Open-Ended Learning for 3D Object Recognition: An Approach and Experiments",
        "doc_scopus_id": "84945464134",
        "doc_doi": "10.1007/s10846-015-0189-z",
        "doc_eid": "2-s2.0-84945464134",
        "doc_date": "2015-01-31",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Feature descriptors",
            "High level knowledge",
            "Leave-one-out cross validations",
            "Nearest neighbor classification",
            "Open-ended learning",
            "Precision and recall",
            "Spin images"
        ],
        "doc_abstract": "© 2015, Springer Science+Business Media Dordrecht.3D object detection and recognition is increasingly used for manipulation and navigation tasks in service robots. It involves segmenting the objects present in a scene, estimating a feature descriptor for the object view and, finally, recognizing the object view by comparing it to the known object categories. This paper presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In this paper, “open-ended” implies that the set of object categories to be learned is not known in advance. The training instances are extracted from on-line experiences of a robot, and thus become gradually available over time, rather than at the beginning of the learning process. This paper focuses on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D scenes in an open-ended manner? (2) How to acquire and use high-level knowledge obtained from the interaction with human users, namely when they provide category labels, in order to improve the system performance? This approach starts with a pre-processing step to remove irrelevant data and prepare a suitable point cloud for the subsequent processing. Clustering is then applied to detect object candidates, and object views are described based on a 3D shape descriptor called spin-image. Finally, a nearest-neighbor classification rule is used to predict the categories of the detected objects. A leave-one-out cross validation algorithm is used to compute precision and recall, in a classical off-line evaluation setting, for different system parameters. Also, an on-line evaluation protocol is used to assess the performance of the system in an open-ended setting. Results show that the proposed system is able to interact with human users, learning new object categories continuously over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multimodal inverse perspective mapping",
        "doc_scopus_id": "85027949320",
        "doc_doi": "10.1016/j.inffus.2014.09.003",
        "doc_eid": "2-s2.0-85027949320",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Co-ordinate system",
            "Computation time",
            "Intelligent transportation systems",
            "Inverse perspective mappings",
            "Laser range finders",
            "Multimodal sensor fusion",
            "Obstacle detection",
            "Robust solutions"
        ],
        "doc_abstract": "© 2014 Elsevier B.V.Over the past years, inverse perspective mapping has been successfully applied to several problems in the field of Intelligent Transportation Systems. In brief, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. There is, however, a significant limitation in the inverse perspective mapping: the presence of obstacles on the road disrupts the effectiveness of the mapping. The current paper proposes a robust solution based on the use of multimodal sensor fusion. Data from a laser range finder is fused with images from the cameras, so that the mapping is not computed in the regions where obstacles are present. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping. Furthermore, the proposed approach is also able to cope with several cameras with different lenses or image resolutions, as well as dynamic viewpoints.",
        "available": true,
        "clean_text": "serial JL 272144 291210 291773 291872 291874 291884 31 Information Fusion INFORMATIONFUSION 2014-09-22 2014-09-22 2015-02-05T15:31:51 S1566-2535(14)00103-1 S1566253514001031 10.1016/j.inffus.2014.09.003 S300 S300.1 FULL-TEXT 2015-05-15T06:40:15.932961-04:00 0 0 20150701 20150731 2015 2014-09-22T01:31:58.066333Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor primabst ref 1566-2535 15662535 true 24 24 C Volume 24 10 108 121 108 121 201507 July 2015 2015-07-01 2015-07-31 2015 Regular Papers article fla Copyright © 2014 Elsevier B.V. All rights reserved. MULTIMODALINVERSEPERSPECTIVEMAPPING OLIVEIRA M 1 Introduction 2 Related work 3 Problem formulation 4 Solutions for direct and inverse projections 4.1 Direct projection 4.2 Inverse projection 5 Road and vehicle reference systems 6 Mappable versus unmappable pixels 6.1 Half space of projection 6.2 Desired area of perception 6.3 Image boundaries 6.4 Laser generated polygon 6.5 Image projection polygon 7 Results 7.1 Test platforms 7.2 Computational performance 7.3 IPM accuracy 7.4 IPM accuracy using LRF 7.5 Tests in real environments 8 Conclusions Acknowledgements References ELFAOUZI 2011 4 10 N STILLER 2011 244 252 C SATHYANARAYANA 2011 293 303 A KASTRINAKI 2003 359 381 V SIMOND 2007 4283 4288 N IROS OBSTACLEDETECTIONIPMSUPERHOMOGRAPHY LUNDQUIST 2011 253 263 C MALLOT 1991 177 185 H EHLGEN 2008 657 665 T FANG 2009 463 468 H LI 2011 232 242 S DORNAIKA 2011 954 966 F BERTOZZI 1998 62 81 M TAN 2006 153 165 S THRUN 2006 21 28 S PROCEEDINGSROBOTICSSCIENCESYSTEMSCONFERENCEUNIVERSITYPENNSYLVANIA PROBABILISTICTERRAINANALYSISFORHIGHSPEEDDESERTDRIVING SAPPA 2008 476 490 A EVANS 1998 417 428 O OLIVEIRAX2015X108 OLIVEIRAX2015X108X121 OLIVEIRAX2015X108XM OLIVEIRAX2015X108X121XM item S1566-2535(14)00103-1 S1566253514001031 10.1016/j.inffus.2014.09.003 272144 2015-02-06T01:52:57.118652-05:00 2015-07-01 2015-07-31 true 4678031 MAIN 14 56693 849 656 IMAGE-WEB-PDF 1 si96 2302 42 287 si89 1119 25 221 si82 1911 88 269 si77 1032 23 210 si75 2515 72 419 si68 2998 52 505 si64 900 19 199 si61 1977 73 349 si49 778 19 160 si48 534 18 95 si43 3092 99 304 si42 3233 117 291 si41 3484 94 344 si40 3460 94 336 si39 3452 94 336 si38 795 18 194 si35 2902 88 230 si34 2560 73 295 si31 941 73 136 si30 1750 73 230 si29 456 34 59 si27 768 20 161 si19 1254 73 150 si17 625 18 133 si99 514 18 85 si98 480 14 78 si97 483 14 78 si95 298 13 31 si94 269 14 38 si93 480 14 78 si92 480 14 78 si91 483 14 78 si90 209 17 14 si9 298 13 31 si88 417 19 60 si87 313 16 36 si86 238 16 21 si85 303 19 30 si84 314 17 30 si83 313 16 36 si81 314 17 30 si80 850 20 215 si8 313 16 36 si79 238 16 21 si78 185 12 13 si76 185 12 13 si74 297 14 39 si73 202 14 12 si72 190 13 10 si71 303 19 30 si70 560 22 95 si7 238 16 21 si69 303 19 30 si67 366 23 44 si66 542 20 122 si65 597 20 141 si63 227 15 14 si62 204 12 13 si60 548 20 128 si6 303 19 30 si59 897 20 266 si58 291 15 35 si57 267 15 23 si56 380 16 57 si55 971 18 229 si54 360 17 44 si53 252 14 43 si52 632 15 168 si51 312 15 39 si50 360 17 44 si5 417 19 60 si47 239 14 25 si46 469 17 70 si45 442 17 63 si44 435 17 64 si4 307 17 31 si37 436 20 87 si36 542 20 122 si33 692 19 153 si32 726 19 157 si3 303 19 30 si28 436 20 87 si26 543 20 127 si25 542 20 122 si24 211 16 11 si23 233 13 17 si22 223 11 17 si21 233 14 17 si20 221 11 16 si2 350 15 49 si18 206 12 14 si16 596 20 145 si15 598 20 143 si14 269 14 38 si13 223 15 22 si12 281 14 39 si11 250 15 24 si100 298 13 31 si10 298 13 31 si1 350 15 50 gr1 44906 389 524 gr9 41885 428 364 gr8 44865 235 467 gr7 32038 236 466 gr6 25070 217 468 gr5 25961 284 378 gr4 34971 293 478 gr3 23671 198 466 gr20 54834 327 585 gr2 45568 455 520 gr19 58451 484 489 gr18 45786 188 556 gr17 43286 304 374 gr16 37574 285 378 gr15 95152 239 574 gr14 89711 336 757 gr13 54065 303 588 gr12 46822 272 378 gr11 60911 228 364 gr10 48950 228 638 gr1 11248 163 219 gr9 5045 163 139 gr8 6926 110 219 gr7 4310 111 219 gr6 3621 101 219 gr5 4347 164 218 gr4 4393 134 219 gr3 3696 93 219 gr20 9908 123 219 gr2 5027 164 187 gr19 13296 164 166 gr18 8433 74 219 gr17 6987 164 201 gr16 6571 164 217 gr15 12873 91 219 gr14 6111 97 219 gr13 10824 113 219 gr12 12837 158 219 gr11 19077 137 219 gr10 8740 78 219 INFFUS 667 S1566-2535(14)00103-1 10.1016/j.inffus.2014.09.003 Elsevier B.V. Fig. 1 Two input images (a) and (b) and their corresponding IPM projected images, respectively (c) and (d). In (b), the presence of a blue color vehicle on the road causes the IPM image to present blue artifacts (d). The yellow lines show the areas of the images that are used for IPM projection. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 (a) A typical road scene with a camera mounted on the host vehicle facing the road. The camera reference system is labelled X c Y c Z c and the road reference system is labelled X r Y r Z r . (b) An example of an image captured by the camera. This image is used as input to IPM. (c) The output image of IPM. Since the road is viewed from above no perspective distortion is present. Fig. 3 A typical road scene. The host vehicle has a camera mounted on the roof. Note that the figure shows the reference systems of both the vehicle and the road, since they may not coincide. Fig. 4 An example of a pixel that cannot be projected (green) since its optical ray intersects the road plane on the back of the image plane. Inversely, the pixel in red is projectable. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 The half space of projection computed after the image plane. Fig. 6 The desired area of perception, polygon ( ψ dap ) in green. All vertices of this polygon should be contained by the half space of projection, according to (19). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 The projection of image boundary pixels onto the road plane results in the image boundaries polygon ( ψ Γ ) . Fig. 8 A typical urban road scenario with several obstacles near the host vehicle. Fig. 9 A road scenario with several obstacles: isometric view (a) and top view (b). The projection polygon ( ψ projection ) is shown in red. It is obtained by the intersection of the desired area of perception ( ψ dap ) in green, the image boundaries polygon ( ψ Γ ) in blue, and the laser generated polygon ( ψ laser ) in yellow. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 Two of the test platforms used for testing the proposed approach: dual camera PTU unit (a); the ATLASMV small scale robot (c), equipped with a LRF and a multi-camera perception unit (b). Fig. 11 The entire projection obtained using Camera 0 of the ATLASMV test platform (a). The correctly projected pixels (b). Pixels that where incorrectly projected (c). Bellow each image, an enlarged region of the pixels in shown. Fig. 12 The ATLASCAR full scale robotic platform. It is equipped with an active perception unit (A), a stereo rig (B), three LRF (C, D, H), a thermal vision camera (F), GPS (G) and an inertial measurement unit (E). Fig. 13 Some key frames of the test sequence. First row: images taken from Camera 0, the blue area is the area of projected pixels, the red is the area outside the desired area of perception and the green area is the area outside the half plane of projection; Second row: a map of the projection. Projected/unprojected pixels from Camera 0 in green/red. Projected/unprojected pixels from Camera 1 in magenta/blue; Third row: the IPM resulting image after mapping both cameras. In columns, different snapshots of the test sequence: 0 (a), 2 (b), 5.5 (c) and 12 (d) seconds. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 14 (a) Time taken to perform the IPM projection for both cameras. The classic IPM time and the time of the proposed approach are shown. (b) Percentage of time saved and number of projected pixels. Proposed approach compared to the classic approach. Key frames of Fig. 13 signaled as the vertical black lines. Fig. 15 The resulting IPM projection when errors in yaw (a), (b) and in pitch (d), (e) are introduced in the calculation. The reference projection, where no errors were introduced, is shown in (c). Fig. 16 IPM accuracy ( η IPM ) scores for errors in camera pose. Results are presented for errors in yaw and pitch angles. Fig. 17 IPM accuracy ( η IPM ) for the classic IPM (dotted lines) and the proposed approach (dashed lines). Fig. 18 Some tested scenarios: (a) obstacle at 0.3m in front; (b) obstacle at 0.5m to the left; (c) obstacle at 0.75m to the right; and (d) obstacle at 1.5m in front. Fig. 19 Comparison of the classical IPM (middle column) with the proposed Multimodal IPM (right column). The input image (left column) shows the image projection polygon highlighted in yellow. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Using the proposed IPM approach in real scenarios. (a) Images of the three cameras on-board the ATLASCAR; (b) the distribution of mapping for each camera; (c) IPM using just green camera; and (d) IPM using all cameras Multimodal inverse perspective mapping Miguel Oliveira a ⁎ Vitor Santos b a Angel D. Sappa c a Institute of Electronics and Telematics Engineering of Aveiro, Campus Universitario de Santiago, 3800 Aveiro, Portugal Institute of Electronics and Telematics Engineering of Aveiro Campus Universitario de Santiago 3800 Aveiro Portugal b Department of Mechanical Engineering, University of Aveiro, Campus Universitario de Santiago, 3800 Aveiro, Portugal Department of Mechanical Engineering University of Aveiro Campus Universitario de Santiago 3800 Aveiro Portugal c Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center Campus UAB 08193 Bellaterra Barcelona Spain ⁎ Corresponding author. Over the past years, inverse perspective mapping has been successfully applied to several problems in the field of Intelligent Transportation Systems. In brief, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. There is, however, a significant limitation in the inverse perspective mapping: the presence of obstacles on the road disrupts the effectiveness of the mapping. The current paper proposes a robust solution based on the use of multimodal sensor fusion. Data from a laser range finder is fused with images from the cameras, so that the mapping is not computed in the regions where obstacles are present. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping. Furthermore, the proposed approach is also able to cope with several cameras with different lenses or image resolutions, as well as dynamic viewpoints. Keywords Inverse perspective mapping Multimodal sensor fusion Intelligent vehicles 1 Introduction Intelligent Transportation Systems offers one of the most relevant frameworks for data fusion [1]. Within this scope, Advanced Driver Assistance Systems (ADAS) are considered to have paramount importance, since they have a significant impact in the safety of both passengers and pedestrians alike [2]. One very important aspect of ADAS is road detection or lane marker detection techniques [3], i.e., the automatic estimation by the vehicle of its position with respect to the road. Robust road position awareness is one of the primary features that an intelligent vehicle should present [4]. This paper focuses on a particular technique called Inverse Perspective Mapping (IPM) which is very often used in vision-based road estimation algorithms as a pre-processing component. IPM uses information from the camera’s position and orientation towards the road to produce a bird’s eye view image where perspective effects are removed. The correction of perspective allows much more efficient and robust road detection, lane marker tracking, or pattern recognition algorithms to be implemented. In reality, IPM has been employed not only with the purpose of detecting the vehicle’s position with respect to the road, but also in many other ADAS related applications, e.g., obstacle detection [5,6], free space estimation [7], pedestrian detection [8] or ego motion estimation [9]. Therefore, IPM is of paramount importance to a large number of automated tasks that should be handled by an intelligent vehicle. If IPM provides input to many other algorithms, then special care should be given to it. In this paper, we focus on the robustness of the IPM algorithm, since it still presents some limitations when applied in the context of on-board road mapping. IPM works under three core assumptions: the road must be a flat surface, there should be a rigid body transformation from the camera to the road, and the road should be free of obstacles. We focus on the last assumption: obstacle free road. This is the least realistic assumption of all, since very often the roads are populated by other vehicles, protection walls, pedestrians, etc. In fact, the classical IPM algorithm fails to produce an accurate projected image when the input images from the cameras contain other things other than the road itself. An example of this is shown in Fig. 1 : in (a) the road is free of obstacles, which leads to an IPM image (c) which is very accurate. In Fig. 1(b), there is another vehicle on the road. As a consequence, the resulting IPM image (d) contains artifacts which might be misleading for example for pattern recognition algorithms. In this paper, we propose an extension to the classical IPM that can still compute accurate IPM images when obstacles are in front of the vehicle. To accomplish this, we make use of an additional laser range finder (LRF). These sensors are especially designed for obstacle detection purposes. In fact, nowadays, some of-the-shelf vehicles already have these sensors installed to assist collision avoidance or collision mitigation systems. 1 last accessed February 2014. 1 In comparison with alternative on-board distance measuring devices, such as stereo vision or radar, LRFs produce more accurate data at higher output frequencies. They are also robust, since they work in challenging conditions such as fog, shadows or even at night. The core idea behind the proposed approach is to fuse the laser data with the pixels in the image in an attempt to identify which pixels should be used as input for IPM. As will be shown, this multimodal sensor fusion framework is capable of producing much more accurate IPM images when compared to the classical IPM approach. In addition to this, we will show that the proposed approach is faster to compute than the classical IPM. The reason for this is that our approach employs a faster direct projection mechanism (from 3D world to image, i.e., 3D points to pixels) to discover mappable pixels, before operating the slower IPM based inverse projection (from pixels to 3D points). Classical IPM approaches make no considerations on this topic (e.g., [10–12]). The paper is organized as follows. First, Section 2 presents the state of the art on IPM techniques, then Section 3 presents the mathematical formulation of the problem to be used in Section 4; the new reference system, proposed in the current work, is introduced in Section 5. Finally, Section 6 describes the proposed multimodal inverse perspective mapping through the computation of the mappable pixels. Experimental results are given in Section 7, and conclusions are presented in Section 8. 2 Related work Over the last decades, IPM has been successfully applied to several problems, especially in the field of Intelligent Transportation Systems. Although it was some years ago that authors began to mention the advantages of IPM (e.g., [13,14]), several recent publications (e.g., [15–17]) show that this is still a topic of interest to the robotics, computer vision and Intelligent Transportation Systems communities. The core application of IPM is the determination of the vehicle’s position with respect to the road, commonly referred to as “road detection” or “lane marker detection”. There are several examples of using IPM for assisting road detection in the literature (e.g., [18,12,19,20]). The usage of IPM on-board a vehicle may also aid other automatic detection systems such as generic obstacle detection [21,22], free space estimation [23,7], pedestrian detection [8,24], or optical flow computation [25]. The IPM method receives as input the image from the camera, the 6D position of the camera with respect to the road reference system (i.e., extrinsic parameters), and a description of the properties of the lens (i.e., intrinsic parameters). Under the assumption that the road ahead of the vehicle is flat, that there is a fixed rigid body transformation from the camera to the road’s reference frame, and that there are no obstacles present, the input image pixels are mapped to the road reference system, and a new image is produced where perspective effects are removed. The image that is produced by the IPM will, henceforward, be named simply IPM image. Considering on-board road detection setups, cameras are usually mounted somewhere close to the rear view mirror inside the vehicle, facing the road in front of it. The camera’s position and orientation induces perspective associated effects to the captured road images. The IPM technique consists of transforming the images by mapping the pixels to a new reference frame where the perspective effect is corrected. This reference frame is usually defined on the road plane, so that the resulting image becomes a top view of the road. Fig. 2 (a) shows an example of a road scene; Fig. 2(b) depicts the input image captured by the camera; and Fig. 2(c) represents the image produced using IPM. One of the advantages of IPM is that the subsequent perception algorithms can be computed in the IPM resulting image, which is defined in a new reference system where the geometric properties of road painted patterns are independent from the perspective of the camera, i.e., from the position of the camera. In [14], the authors claim that the parallelization of road features is crucial for curvature determination. Another advantage is that, since the perspective effect associates different meanings to different image pixels, depending on their position in the image, after the removal of the perspective effect, each pixel represents the same portion of the road, allowing a homogeneous distribution of the information among the pixels of the resulting IPM image [22]. Other authors have also employed steerable filters for lane markings detection and sustain that filtering on the IPM image allowing a single kernel size to be used over the entire area of interest [18]. Furthermore, since images are mapped to a new reference system, several cameras may be used to produce a single IPM image mosaicking, which is a subject also present in the literature [22,20]. It should also be noted that IPM requires no explicit feature detection, which contributes to the overall robustness of the algorithm. In addition, there are also dedicated hardware systems being developed to compute the IPM images [26]. Given this, it is fair to say that IPM is a cornerstone in the development of on-board video processing systems. It assists, or is very frequently a primary step, in road modelling, obstacle and pedestrian detection, free space estimation and many other advanced drivers assistance systems. Despite the advantages of IPM, the current state of the art on this method has some limitations. These derive mostly from the fact that the classical IPM algorithm makes three assumptions: static position of the camera with respect to the road, flat road plane, and obstacle free road. Each of these assumptions and proposed solutions are described in the following lines. Since the position of the camera with respect to the road plane is considered static, pitch and roll variations from the host vehicle (and thus of the camera which is rigidly attached to it) are neglected. Pitch variations occur during demanding brake or acceleration maneuvers, while roll changes are expected to appear during hard turns. When the vehicle rolls or pitches, the position of the camera with respect to the road changes. As a consequence, the accuracy of IPM decreases during these maneuvers. This problem has been identified in [27,28,19,29]. In fact, some authors claim that even a small error in the vehicle’s roll/pitch estimation leads to a massive terrain classification error [30]. In [31] an algorithm is proposed that also fuses vision and laser data. However, in this case, the objective is to correct the laser range measurements, rather than to correct the projection of the image pixels, as is proposed in the current paper. In that paper, a stereo vision system is used to detect the road plane and thus estimate the position of the lasers with respect to the road. With this information, it is possible to compensate for roll or pitch variations continuously, which in turn is used for correcting raw laser scan data. Another assumption that is generally made, is to consider the road as a flat surface. The approximation of the road surface to a plane is acceptable. Nonetheless, in some specific cases such as a road climbs, this could also be a factor for low IPM accuracy. In [32] a solution to this problem is proposed, where the “height” of the lane markings is estimated with respect to a reference plane. Using this technique, it is possible to compute IPM images in sloped roads. The final assumption is that there are no obstacles on the road. This is often the case when other vehicles, buildings or pedestrians appear in the image. When these obstacles are present in the image, the mapping of IPM is disrupted because, in the classical IPM approach, all pixels from the input image are assumed to be on the road plane and are thus used in the projection. In real automotive applications it is unfeasible to assume an obstacle-free scenario. Nonetheless, no previous solution has been proposed. In this paper we propose a multimodal laser vision sensor fusion strategy that addresses the obstacle-free road assumption. 3 Problem formulation Let c R r be the classical 3 × 3 rotation matrix in 3D and c T r be the 3 × 1 translation vector in 3D that relates two coordinate systems. Their combination maps a point in the 3D road coordinate system Q r = [ X r Y r Z r ] T to a point in the camera’s coordinate system Q c = [ X c Y c Z c ] T : (1) Q c = c R r · Q r + c T r . Let K be the intrinsic parameters matrix of a given camera, represented as: (2) K = α x β x 0 0 α y y 0 0 0 1 , where α x and α y are the lens scaling factors in both directions, x 0 and y 0 the principal point coordinates in pixels and β the skewness factor. These parameters can be obtained by an offline calibration since they are constant for each camera-lens setup. The projection of an arbitrary 3D point Q = [ X Y Z ] T to a point q h = [ u v w ] T in the camera’s homogeneous image coordinate system, is described as: (3) q h = K ( c R r · Q + c T r ) . Finally, the coordinates of a pixel q = [ x y ] T are obtained by adjusting the homogeneous coordinates with the scaling factor w: (4) q = q h w . For simplification purposes, the current paper will use the following notation: (5) K · c R r = P = p 11 p 12 p 13 p 21 p 22 p 23 p 31 p 32 p 33 , and also: (6) K · c T r = t = t 1 t 2 t 3 . The above formulation may describe the projection of a point to a pixel in the image (direct projection), or it may be used to obtain the 3D point from the pixel coordinates (inverse projection). The direct projection (dp) may be formulated as dp : R 3 → Z 2 , Q → q . In the case of inverse perspective mapping, what is sought is the 3D coordinates of a given pixel. This is the inverse projection (ip), defined as ip : Z 2 → R 3 , q → Q . 4 Solutions for direct and inverse projections The following subsections present the general form solutions for the direct and inverse projections. 4.1 Direct projection As discussed in Section 3, the direct projection aims at obtaining the pixel coordinates of a 3D world point projected to the image. Eq. (3) may then be rewritten as: (7) u v w = p 11 p 12 p 13 p 21 p 22 p 23 p 31 p 32 p 33 X Y Z + t 1 t 2 t 3 . Using (4), we get the definition of the direct projection dp (8) x = p 11 X + p 12 Y + p 13 Z + t 1 p 31 X + p 32 Y + p 33 Z + t 3 , y = p 21 X + p 22 Y + p 23 Z + t 2 p 31 X + p 32 Y + p 33 Z + t 3 , this system of equations defines the direct projection of a point in the world reference system Q = [ X Y Z ] T to a pixel in image coordinates q = [ x y ] T . 4.2 Inverse projection The inverse projection is the problem of obtaining the real world coordinates of a point from a pixel in the image. The problem is under-defined, since the three real world coordinates are sought from only two pixel coordinates. In IPM, the system is completed by defining the plane onto which the pixel is projected. Let an arbitrary plane, defined as: (9) Π : aX + bY + cZ + d = 0 , be the plane that contains the projection of the pixel. The system of equations in (3) may be extended to include the constraint of the projection plane, defined in (9): (10) w x y 1 0 = p 11 p 12 p 13 0 p 21 p 22 p 23 0 p 31 p 32 p 33 0 a b c d X Y Z 1 + t 1 t 2 t 3 0 , rearranging this formulation, the equations for inverse perspective mapping can be obtained. First, variable d may be moved inside the translation vector: (11) w x y 1 0 = p 11 p 12 p 13 0 p 21 p 22 p 23 0 p 31 p 32 p 33 0 a b c 0 X Y Z 1 + t 1 t 2 t 3 d , then, (11) may be rearranged: (12) - t 1 - t 2 - t 3 - d = p 11 p 12 p 13 0 p 21 p 22 p 23 0 p 31 p 32 p 33 0 a b c 0 X Y Z 1 - w x y 1 0 , and finally, the vector of pixel coordinates can be embedded inside the projection matrix: (13) - t 1 - t 2 - t 3 - d = p 11 p 12 p 13 - x p 21 p 22 p 23 - y p 31 p 32 p 33 - 1 a b c 0 ︸ A X Y Z w , rearranging the system of equations results in the inverse projection (ip) of a pixel to a known plane: (14) X Y Z w = p 11 p 12 p 13 - x p 21 p 22 p 23 - y p 31 p 32 p 33 - 1 a b c 0 - 1 - t 1 - t 2 - t 3 - d , this is a valid solution whenever matrix A is invertible and not singular. In other words, the projection formulation is invalid when the projection plane and the image plane are parallel and the projection plane is behind the image plane. The term behind will be clarified in Section 6.1 with the introduction of the half space of projection. 5 Road and vehicle reference systems In the classic IPM formulation the camera and road reference systems have a known static transformation between them. The IPM projection will transform the pixels from the camera to the road reference system. In the current paper we use an additional reference system, the vehicle reference system. The vehicle reference system is fixed to the host vehicle. It is the reference system to which all sensors on the vehicle are related. Therefore, a fixed, rigid body transform is used to represent the pose of the camera with respect to the vehicle reference system. Hence, three reference systems are used: the camera system { X c Y c Z c } , the road reference system { X r Y r Z r } and the vehicle reference system { X v Y v Z v } . Fig. 3 shows the reference systems for the vehicle, road and camera. The general camera to road reference systems transformation was introduced in (1). Let the rotation and translation matrices of (1) be assembled into a global transformation matrix c H r in homogeneous format, so that: (15) Q c = c H r · Q r , the global transformation from the camera to the road is obtained as the product of a fixed camera to vehicle transformation and a dynamic (pitch, roll, therefore time dependent) vehicle to road transformation. (16) Q c = c H v · v H r ( t ) · Q r . In the general mathematical model proposed here, the classic IPM approach may still be used: v H r ( t ) is constant for all values of t, i.e., the coefficients of (9) are defined to represent the X v Y v plane Π road : a r = b r = d r = 0 and c r = 1 ; or the road plane may be actually detected, if v H r ( t ) is estimated over time using stereo or laser sensors pointed towards the road, i.e., some estimation function of the parameters in (9) is running continuously. An example of real time estimation of road to vehicle transformation is presented in [33]. 6 Mappable versus unmappable pixels IPM is the application of (14) to the pixels in the image. However, in a given image, not all pixels may be interesting or even possible to project. The current work addresses this problem by using a laser sensor to detect mappable regions, together with a set of criteria to select which pixels should be mapped. In summary, the fusion mechanism we propose is the following: using several criteria, we compute a set of polygons in 3D (defined in the road’s reference frame). Each polygon delimits the area of the road which, in accordance with the corresponding criteria, should be mapped using IPM. Then, we fuse all these criteria by computing a polygon (the projection polygon) which results from the intersection of the several criteria driven polygons. The projection polygon now encodes the region of the road that should be mapped using IPM. However, the projection polygon is defined in the road’s reference frame (in 3D). Hence, to use this information as input to an IPM projection, we first need to project the projection polygon onto the image plane (this is done using a direct projection mechanism), which we call the image projection polygon. The image projection polygon is the tool that allows the pixels to be labelled as mappable or not: pixels inside this polygon should be mapped and pixels outside the polygon are skipped. Note that in this approach we are fusing multimodal data, since that some of the criteria we use are related to the vision sensor and others to the LRF sensor data. The following subsections present the different criteria used to find which pixels in an image are possible to be projected. 6.1 Half space of projection Eq. (14) is the mathematical solution of the intersection of the optical ray of a given pixel with the road plane. Because of this, a pixel above the horizon line in the image will be projected to the back of the camera’s plane. Fig. 4 shows the projection rays of two pixels, one is projectable and the other should be discarded. Although the presented solution is a valid mathematical solution, for the proposed model, however, the unprojectable pixels must be handled in accordance. This is done by first computing the image plane. The image plane divides the three-dimensional Euclidean space into two parts. One of them is called half space of projection. It is defined as the region of the Euclidean space where all points contained by it may be virtually projected into the image plane. The image plane is defined as Π image : a i X + b i Y + c i Z + d i = 0 ; it is obtained as follows: Let M 0 , M 1 and M 2 be three non collinear points in the X r Y r plane of the road reference system. As an example M 0 = [ 0 0 0 ] T , M 1 = [ 1 0 0 ] T and M 2 = [ 0 1 0 ] T . The points are projected from the cameras reference frame by means of the transformation matrix defined in (1). In the camera’s reference system, those points are contained by the image plane and may be used to define two vectors whose cross product defines the vector normal to the image plane: (17) a i b i c i = C R V · M 0 - M 1 ⊗ C R V · M 0 - M 2 , where ⊗ denotes the cross product. The remaining image plane parameter d i is obtained by substituting in the plane equation one of the projected points: (18) d i = - ( a i X 0 + b i Y 0 + c i Z 0 ) . Having the parameters of the image plane, and a test point Q t = [ X t Y t Z t ] T that is sure to be inside the half space of projection (for example a point a couple of meters in front of the host vehicle), a test is devised to assess if a point Q = [ X Y Z ] T belongs to the half space of projection (denoted as Π image + ): (19) Q ∈ Π image + , if ( a i X t + b i Y t + c i Z t + d i ) ( a i X + b i Y + c i Z + d i ) > 0 , Q ∉ Π image + , otherwise. The half space of projection in (19) is shown in Fig. 5 . It is used to define projectable polygons in 3D, as detailed in the following sections. 6.2 Desired area of perception For an autonomous system, it is important to define the area of perception that it requires to effectively navigate. A very large perception area increases the computational cost, while a small perception area might make the system unfit to handle quick variations in the road scenario. This section addresses the desired perception limits, i.e., how the programmer can effectively set an area of interest for the host vehicle to perform the IPM operation. In the case of a vehicle travelling in urban scenarios for example, perhaps 30m of view range are sufficient. The desired area of perception is formally defined as a polygon ψ dap in the road’s projection plane. This polygon must be contained in the half space of projection ( ψ dap ⊂ Π image + ). Fig. 6 shows an example of an area of perception. Currently, ψ dap is set as a four vertices polygon, defining, in the road plane, a rectangle in front of the host vehicle. The rectangle’s side in the direction of the vehicle’s movement may dynamically increase size depending on the vehicle speed. 6.3 Image boundaries Besides the desired area of perception, other regions of the road plane must be defined in order to perform an effective IPM operation. The camera lens properties and orientation towards the road plane define a possible area of projection. Let γ be the list of pixels in the image boundaries, obtained from all image pixels q ( γ ⊆ q ) that are in accordance with: (20) q i = x i y i ∈ γ , if ( x i = 1 ∨ x i = W ∨ y i = 1 ∨ y i = H ) , q i ∉ γ , otherwise, where W and H are the image width and height respectively. The boundaries of the image are then projected onto the road plane using the inverse projection ip from (14), and the real world coordinates of the image boundary pixels Γ are obtained. The half space of projection is again used to assert the validity of 3D points: (21) Γ = ip ( γ ) , ∀ ip ( γ ) ∈ Π image + . The list of world points Γ are used to form the vertices of the polygon ψ Γ (an illustration is shown in Fig. 7 ). 6.4 Laser generated polygon The IPM technique requires that the road surface seen from the cameras is flat. This might not always be the case, particularly when other vehicles or obstacles lie on the road, as shown in the IPM resulting images published by some authors [18,22,10]. In these examples, artifacts are generated in the regions of the image where the flat road assumption fails. Vehicles are mapped as if they had been painted on the road (see Fig. 1(b) and (d)). Some authors have taken advantage of this phenomenon to detect obstacles in the road, by using the differences in two IPM images, from a pair of stereo cameras [22]. This method is called stereo IPM. Although the latest is a valid approach, the fact is that calibration issues tend to disrupt the perfect mapping of stereo images. Because of this, it may sometimes be difficult to distinguish if disparities in the IPM stereo are due to a sub-optimum calibration or to an obstacle that lies on the road surface. There is also work related to sensor integration using both vision and laser in autonomous vehicles [31], but in this case the objective was to enhance obstacle detection. Fig. 8 shows a typical urban road scenario with several obstacles near the host vehicle. Let Q laser = [ X laser Y laser Z laser ] T be the 3D points obtained by the laser range finder, referenced in the world coordinate system. Assuming that objects picked up by the laser have a vertical expression, the coordinates where obstacles touch the floor, i.e., the object baseline Q bln , is obtained by the vertical projection of laser points onto the road plane: (22) Q bln = X laser Y laser - ( a r X laser + b r Y laser + d r ) c r . The laser generated polygon ψ laser is defined by the list of vertices at generic coordinates given by Q bln . 6.5 Image projection polygon As stated before, the core of IPM is applying (14) to the pixels in the image that are known to be on the projection plane. The objective is to be able to define for the input image which pixels are possible (and desirable) to map. The proposed approach defines three polygons in the road plane: a polygon defining the desired area of perception ( ψ dap ), a polygon corresponding to the boundaries of the image ( ψ Γ ) and a polygon defining the laser scanned objects ( ψ laser ). The resultant projection polygon ( ψ projection ) is obtained by the intersection of the three other polygons: (23) ψ projection = ψ dap ⋂ ψ Γ ⋂ ψ laser , where ⋂ represents polygon intersection. The projection polygon is composed of a list of vertices, i.e., 3D points defined the road reference system (see fig. 9 ). The vertices defined in the road reference system are direct projected into the image plane using (8). The result is a list of 2D vertices that define a polygon in the image plane. This is called the image projection polygon. Inside the polygon are all pixels that should be mapped using IPM. Since perspective transformation is an affine transformation, the image projection polygon is calculated as the direct projection of the vertices of the projection polygon in the road plane. 7 Results Several experiments have been devised to obtain quantitative results of the proposed IPM methodology. First, the platforms used to obtain the results are presented: a dual camera pan and tilt unit (PTU), a small scale robot and finally, a full scale autonomous vehicle. The computational performance of the proposed approach is compared to the classic IPM using a measure of the accuracy of IPM. Results are presented for the accuracy of the proposed approach. Also, a comparative study of the classic IPM versus the laser assisted IPM shows that the accuracy of the latest is much better when obstacles appear in the area of projection. Finally, this section ends with some qualitative results, providing images of IPM from on-board cameras of a full scale vehicle. 7.1 Test platforms In order to assess the performance of the proposed methodology, the test platforms depicted in Fig. 10 where used. Fig. 10(a) shows the dual camera PTU. The servo actuated PTU controlled through RS232 serial protocol was used so that the IPM is tested when the cameras move into different positions. The cameras have different lenses and also different image resolution. Camera 0 has a wide angle lenses and a resolution of 800 × 600 pixels, while Camera 1 has a tele-lens and a resolution of 320 × 240 pixels. This platform is used to assess the computational performance of the proposed approach. The time taken to perform IPM on both cameras is measured during a test where the PTU moves the cameras to different positions. Fig. 10(c) shows the ATLASMV robotic platform. It is a small scale autonomous robot built for participating in an autonomous driving competition. It is equipped with four cameras and a LRF. The side cameras (Fig. 10(b)), used to map the road in front of the robot, have wide angle lenses and produce images with a resolution of 320 × 240 pixels. The ATLASMV is used in two tests: one for measuring the accuracy of IPM, another to assess the effects of using the LRF to assist IPM. The quantitative results obtained from the accuracy of IPM are calculated using a color 2 For interpretation of color in Figs. 10, 11 and 20, the reader is referred to the web version of this article. 2 calibrated grid (shown in Fig. 10(c), below the robot and, in Fig. 11 , viewed from the cameras). The grid is a 3 × 1 m sheet of paper marked with a special colored pattern. The grid is placed in a known position in front of the cameras. Using the position and rotation of the cameras with respect to the calibration grid, a virtual image of the grid is produced to overlap the resultant IPM image. This virtual image of the grid serves as a test mask for measuring the IPM accuracy ( η IPM ): after projection using IPM, pixels are labelled with a color that should match the color of the virtual image. The accuracy of the projection is obtained as the ratio between correctly projected pixels and the total projected pixels: (24) η IPM = number _ of _ correct _ projections total _ number _ of _ projections . Fig. 11(a) shows all the pixels of a given projection. Pixels classified as correctly and incorrectly projected are displayed in Fig. 11(b) and (c) respectively. Also, the virtual grid is overlaid onto the images. The final test platform is the ATLASCAR (Fig. 12) , a real scale robotic platform [34] used for research on autonomous driving and advanced driver assistance systems. It is equipped with cameras and lasers. Results will show IPM images using the three on-board cameras. 7.2 Computational performance The computational performance of the IPM transformation has been a concern of some authors [21,35]. Its implementation on on-board systems requires real time performance from the systems. In order to test the performance of the proposed approach, the dual camera PTU setup was used (Fig. 10(a)). In a classic IPM, all pixels in a given image are inverse projected, i.e., (14) is applied to all pixels. On the other hand, the proposed approach first computes the image projection polygon, and then applies (14) only to the pixels that should be projected. The computational demand of an IPM operation depends on the amount of projected pixels, which in turn depends on the camera’s pose towards the projection plane. For example, a camera pointing to the sky will have only a small amount of pixels viewing the road plane. To compare the performance of classic IPM with the proposed approach a 14s test sequence was devised. Since the orientation of the camera’s towards the road plane changes the amount of projectable pixels, during the 14s of the test, the PTU is ordered to go to specific positions: • State 1 (0–5.5s) the PTU is moving upwards. This causes an increasingly smaller amount of mappable pixels for both cameras. • Stage 2 (5.5–8.5s) moves the PTU down and the inverse phenomena occurs. • Stage 3 (8.5–14s) maintains a fixed tilt and the PTU pans increasingly to the left, which will make Camera 1 to have increasingly less mappable pixels. Fig. 13 shows some IPM resulting images of key points in the test sequence. Fig. 14 (a) compares the projection time of both cameras using the classic IPM and the proposed approach. Fig. 14(b) indicates the amount of projected pixels and the time saved using the proposed approach in relation to the classic IPM. From 0 to 5.5s, the PTU is moving upwards and so the pitch angle of the cameras is changing. This is observable in the difference of mapping in Fig. 13(a)–(c). Fig. 14(b) shows a reduction in the number of projected pixels for each camera. In Fig. 14(a), a reduction of IPM projection time using the proposed approach is clearly noted. Camera 0 takes more time to project than Camera 1 because the resolution of the images is different ( 800 × 600 pixels and 320 × 240 pixels, respectively). From 5.5 to 8.5s the PTU is moving downwards and the effects are the inverse. From 8.5 to 14s the change in pan angle causes Camera 1 to view increasingly less of the desired area of perception. Fig. 14(b) shows a decrease in the number of projected pixels during this period. 7.3 IPM accuracy Although many researchers have employed the IPM operation in order to ease the road recognition process [2–5], the fact is that no reporting of the accuracy of each implementation was found in the literature. Despite some insights on the topic of accuracy measurement for general projective geometry [10,11], a method had to be devised for this particular application to provide a quantitative analysis of the proposed method. For this experience the dual camera PTU setup was used (Fig. 10(a)). The calibration grid presented in Section 7.1 was employed and an accuracy of η IPM = 0.85 was achieved for the system. Because the current paper is the first to present such quantitative results, a measure of the quality of this value is not possible. The second experiment is intended to assess how important is to have accurate measures of the camera’s position and orientation with the road plane. In other words, how does the uncertainty of the camera pose estimation reflect on the final accuracy of the projection. For this purpose, errors in the yaw and pitch angles of the camera were introduced, and the IPM accuracy was calculated. Fig. 15 shows the resulting IPM of mappings with some errors (a), (b), (d) and (e) and the resulting image with no errors (c). Fig. 16 shows the decrease in IPM accuracy with the increase of error in yaw and pitch. The pitch angle is the most relevant for the projection accuracy, since a half degree error changes the accuracy from 0.85 to 0.30. Variations in yaw also drop the accuracy value to 0.30, but only after a 3.5 degree deviation. This is consistent with the concerns of several researchers worldwide that mention on-board camera’s pitch estimation to be a cumbersome problem. In [30], for example, it is stated that “a small error in the vehicle’s roll/pitch estimation leads to a massive terrain classification error forcing the vehicle off the road. Such situations occur even for roll/pitch errors below 0.5 degrees”. 7.4 IPM accuracy using LRF In order to test the usage of the LRF on the IPM projection the ATLASMV robot was used. An obstacle with 0.2m height (green box in Fig. 10(c)) was placed over the calibration grid in front of the robot at several distances and in several positions (to the left, right or in front of the robot). For each obstacle position the accuracy was computed. Fig. 17 shows the η IPM results both for the classic and the proposed IPM approach. The laser polygon introduced in Section 6.4 is able to classify pixels that view the obstacle as unmappable. Because of this, the proposed IPM approach (Fig. 17, dashed lines) consistently gets better accuracy results than the classic IPM (Fig. 17, dotted lines). When the obstacle is very close (0.3m, Fig. 18 (a)), using a classic IPM operation would be catastrophic (0.33 accuracy ratio) but the proposed approach remains accurate enough (0.75). In theory, the performance of IPM should increase when the obstacle is moved to a higher distance. This is not always observable in Fig. 17: for example the curve Obstacle Front, Proposed IPM shows a decrease from 0.3 to 0.5m. We believe that these variations in the performance measurement methodology might possibly be caused by noise in the input images. In fact, some other experiences also show a decrease in performance when the distance increases. For example, Obstacle Left, Proposed IPM, also decreases performance from 1m to 1.25m. Nonetheless, the main point is that, in the cases where an obstacle is present in the image, despite these variations in performance measurements, it is evident that the proposed IPM always shows a better performance when compared to the classic IPM. Fig. 18 shows the IPM resultant images for some of the tested scenarios. 7.5 Tests in real environments For the final validation of the proposed approach, several tests in real road scenarios with a full scale vehicle were done. The test platform used was the ATLASCAR (Fig. 12). The platform is equipped with three cameras (each with a different focal distance lens) and several lasers. Several hours of data from urban and highway roads were used for validating the algorithm. The proposed approach is less time consuming, is able to deal with pitch/roll variations due to brake/turning maneuvers, and using the LRF copes with obstacles present in the projection area. Fig. 19 provides a qualitative comparison of the classical IPM with the proposed multimodal IPM. It is possible to observe that in the presence of other vehicles or obstacles, the classical IPM produces several artifacts on the resultant image. On the contrary, the multimodal approach to IPM is able to cope with obstacles and removes them from the resultant image. Even in free road scenarios, as is the case of Fig. 19 (fourth line), the artifacts produced by the parked cars could reduce the effectiveness of a road detection approach. The flexibility of the proposed approach can also handle the usage of several input cameras. In Fig. 20 , the three cameras on-board the ATLASCAR (Fig. 12), each with different focal distance lenses, are used to obtain a more detailed mapping of the road. Fig. 20(a) shows images from the three cameras. The IPM is mapped to the road plane and the distribution of pixels supplied by each camera is shown in Fig. 20(b). Using a single camera to map the road (the green camera), shows the classical problems of lack of accuracy at long distances (the yellow traffic pattern in Fig. 20(c)). However, if multiple cameras are employed, the tele-camera (blue camera) can provide a high resolution view at long distances, which leads to a high resolution view of the yellow pattern of the road (Fig. 20(d)). Several video sequences showing results from classical and proposed IPM can be found at 8 Conclusions The current paper presents a flexible mathematical model for performing IPM. The methodology is to fuse laser data with vision data in order to improve the accuracy of the IPM projection. The fusion mechanism is based on the intersection of polygons defined according to several criteria: the algorithm computes the polygons generated from the image boundaries, the laser obstacles and the desired area of perception; then, the combination of these polygons (projection polygon) is projected back to the image plane, resulting in the image projection polygon. The image projection polygon is defined in the image coordinate system and can therefore be directly used to as a criterion to indicate which pixels are to be mapped through IPM, and which should not be mapped. Different test platforms, from a small scale robot to a full scale autonomous vehicle, were used to obtain both quantitative and qualitative results. Results show that the proposed approach is computed in less time than the classic IPM, and that the IPM image produced by the proposed approach has higher accuracy when obstacles are present in the road. A study of the influence of errors in the camera’s pose estimation to the IPM projection accuracy which corroborates previous findings is also presented. Finally, several hours of data both from urban roads as well as highways were qualitatively analyzed to evaluate the robustness and efficiency of the proposed approach. In sum, this paper proposes a novel algorithm that solves a common problem of the classical IPM: the disrupting of the IPM image when obstacles are present in the road. The proposed solution is to fuse the information from the images with the data from a LRF in order to specify in the image which pixels are viewing the ground plane and should therefore be mapped using IPM. Results show that the proposed method is more efficient than classical IPM. Acknowledgements Part of this work was developed under the grant SFRH/BD/43203/2008, from the Portuguese National Science Foundation – FCT, and supported by the Spanish Research Project reference TIN2011-25606. References [1] Federal Highway Administration, Data Fusion for Delivering Advanced Traveler Information Services, United States Department of Transportation Intelligent Transportation Systems Joint Program Office, 2003, pp. 247–254. [2] N.-E. El Faouzi H. Leung A. Kurian Data fusion in intelligent transportation systems: progress and challenges – a survey Inform. Fusion 12 1 2011 4 10 [3] C. Stiller F.P. León M. Kruse Information fusion for automotive applications – an overview Inform. Fusion 12 4 2011 244 252 [4] A. Sathyanarayana P. Boyraz J.H. Hansen Information fusion for robust ‘context and driver aware’ active vehicle safety systems Inform. Fusion 12 4 2011 293 303 [5] V. Kastrinaki M. Zervakis K. Kalaitzakis A survey of video processing techniques for traffic applications Image Vision Comput. 21 4 2003 359 381 [6] N. Simond M. Parent Obstacle detection from ipm and super-homography IROS 2007 IEEE 4283 4288 [7] P. Cerri, P. Grisleri, Free space detection on highways using time correlation between stabilized sub-pixel precision IPM images, in: Proceedings of the IEEE International Conference on Robotics and Automation, Barcelona, Spain, 2005, pp. 2223–2228. [8] M. Guanglin, P. Su-Birm, S. Miiller-Schneiders, A. Ioffe, A. Kummert, Vision-based pedestrian detection-reliable pedestrian candidate detection by combining IPM and a 1D profile, in: Proceedings of the IEEE Intelligent Transportation Systems Conference, Seattle, Washington, USA, 2007, pp. 137–142. [9] C. Lundquist T.B. Schön Joint ego-motion and road geometry estimation Inform. Fusion 12 4 2011 253 263 [10] M. Aly, Real time detection of lane markers in urban streets, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Eindhoven, Netherlands, 2008, pp. 7–12. [11] J. McCall, O. Achler, M. Trivedi, Design of an instrumented vehicle test bed for developing a human centered driver support system, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Parma, Italy, 2004, pp. 483–488. [12] A. Muad, A. Hussain, S. Samad, M. Mustaffa, B. Majlis, Implementation of inverse perspective mapping algorithm for the development of an automatic lane tracking system, in: Proceedings of the IEEE International Conference TENCON, vol. A, Chiang Mai, Tailandia, 2004, pp. 207–210. [13] H. Mallot H. Bülthoff J. Little S. Bohrer Inverse perspective mapping simplifies optical flow computation and obstacle detection Biol. Cybernet. 64 3 1991 177 185 [14] D. Pomerleau, RALPH: rapidly adapting lateral position handler, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Detroit, USA, 1995, pp. 506–511. [15] T. Ehlgen T. Pajdla D. Ammon Eliminating blind spots for assisted driving IEEE Trans. Intell. Transp. Syst. 9 4 2008 657 665 [16] H. Fang M. Yang R. Yang C. Wang Ground-texture-based localization for intelligent vehicles IEEE Trans. Intell. Transp. Syst. 10 3 2009 463 468 [17] S. Li Y. hai Easy calibration of a blind-spot-free fisheye camera system using a scene of a parking space IEEE Trans. Intell. Transp. Syst. 12 1 2011 232 242 [18] J. McCall, M. Trivedi, Performance evaluation of a vision based lane tracker designed for driver assistance systems, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Las Vegas, NV, USA, 2005, pp. 153–158. [19] F. Dornaika J. Álvarez A. Sappa A. López A new framework for stereo sensor pose through road segmentation and registration IEEE Trans. Intell. Transp. Syst. 12 4 2011 954 966 [20] C. Guo, S. Mita, D. McAllester, Stereovision-based road boundary detection for intelligent vehicles in challenging scenarios, in: Proceedings of the IEEE International Conference on Intelligent Robots and Systems, St. Louis, MO, USA, 2009, pp. 1723–1728. [21] M. Bertozzi, A. Broggi, A. Fascioli, Real-time obstacle detection on a massively parallel linear architecture, in: 3rd International Conference on Algorithms and Architectures for Parallel Processing, Melbourne, Australia, 1997, pp. 535–542. [22] M. Bertozzi A. Broggi GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection IEEE Trans. Image Proc. 7 1 1998 62 81 [23] S. Tuohy, D. O’Cualain, E. Jones, M. Glavin, Distance determination for an automobile environment using inverse perspective mapping in OpenCV, in: Proceedings of the IET Irish Signals and Systems Conference, Cork, Ireland, 2010, pp. 100–105. [24] A. Broggi, P. Cerri, S. Ghidoni, P. Grisleri, H.G. Jung, Localization and analysis of critical areas in urban scenarios, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Eindhoven, Netherlands, 2008, pp. 1074–1079. [25] S. Tan J. Dale A. Anderson A. Johnston Inverse perspective mapping and optic flow: a calibration method and a quantitative analysis Image Vision Comput. 24 2 2006 153 165 [26] L. Luo, I. Koh, S. Park, R. Ahn, J. Chong, A software-hardware cooperative implementation of bird’s-eye view system for camera-on-vehicle, in: Proceedings of the IEEE International Conference on Network Infrastructure and Digital Content, Beijing, China, 2009, pp. 963–967. [27] P. Coulombeau, C. Laurgeau, Vehicle yaw, pitch, roll and 3D lane shape recovery by vision, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Versailles, France, 2002, pp. 619–625. [28] R. Labayrade, D. Aubert, A single framework for vehicle roll, pitch, yaw estimation and obstacles detection by stereovision, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Columbus, OH, USA, 2003, pp. 31–36. [29] M. Nieto, L. Salgado, F. Jaureguizar, J. Cabrera, Stabilization of inverse perspective mapping images based on robust vanishing point estimation, in: Proceedings of the IEEE Intelligent Vehicles Symposium, Istanbul, Turkey, 2007, pp. 315–320. [30] S. Thrun M. Montemerlo A. Aron Probabilistic terrain analysis for high-speed desert driving Proceedings of the Robotics Science and Systems Conference, University of Pennsylvania 2006 University of Pennsylvania Philadelphia, PA 21 28 [31] A. Broggi, S. Cattani, P.P. Porta, P. Zani, A laserscanner-vision fusion system implemented on the terramax autonomous vehicle, in: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Beijing, China, 2006, pp. 111–116. [32] A.F. Massimo Bertozzi, Alberto Broggi, An extension to the inverse perspective mapping to handle non-flat roads, in: IEEE International Conference on Intelligent Vehicles. Proceedings of the 1998 IEEE International Conference on Intelligent Vehicles, 1998, pp. 305–310. [33] A. Sappa F. Dornaika D. Ponsa D. Gerónimo A. López An efficient approach to on-board stereo vision system pose estimation IEEE Trans. Intell. Transp. Syst. 9 3 2008 476 490 [34] V. Santos, J. Almeida, E. Avila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, Atlascar – technologies for a computer assisted driving system on board a common automobile, in: Proceedings of the IEEE International Conference on Intelligent Transportation Systems, Madeira Island, Portugal, 2010, pp. 1421–1427. [35] O.D. Evans Y. Kim Efficient implementation of image warping on a multimedia processor Real-Time Imaging 4 6 1998 417 428 "
    },
    {
        "doc_title": "Hierarchical nearest neighbor graphs for building perceptual hierarchies",
        "doc_scopus_id": "84951750360",
        "doc_doi": "10.1007/978-3-319-26535-3_74",
        "doc_eid": "2-s2.0-84951750360",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Agglomerative clustering",
            "Graphs",
            "Hierarchical",
            "Incremental learning",
            "Nearest",
            "Neighbour",
            "Open-ended environments"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Humans tend to organize their knowledge into hierarchies, because searches are efficient when proceeding downward in the tree-like structures. Similarly, many autonomous robots also contain some form of hierarchical knowledge. They may learn knowledge from their experiences through interaction with human users. However, it is difficult to find a common ground between robots and humans in a low level experience. Thus, their interaction must take place at the semantic level rather than at the perceptual level, and robots need to organize perceptual experiences into hierarchies for themselves. This paper presents an unsupervised method to build view-based perceptual hierarchies using hierarchical Nearest Neighbor Graphs (hNNGs), which combine most of the interesting features of both Nearest Neighbor Graphs (NNGs) and self-balancing trees. An incremental construction algorithm is developed to build and maintain the perceptual hierarchies. The paper describes the details of the data representations and the algorithms of hNNGs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The RACE Project: Robustness by Autonomous Competence Enhancement",
        "doc_scopus_id": "84933054481",
        "doc_doi": "10.1007/s13218-014-0327-y",
        "doc_eid": "2-s2.0-84933054481",
        "doc_date": "2014-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Control architecture",
            "Early integration",
            "European program",
            "General systems",
            "Learn+",
            "Object categories",
            "Perceptual memory",
            "Performance based",
            "Plan execution",
            "Systems architecture"
        ],
        "doc_abstract": "© 2014, Springer-Verlag Berlin Heidelberg.This paper reports on the aims, the approach, and the results of the European project RACE. The project aim was to enhance the behavior of an autonomous robot by having the robot learn from conceptualized experiences of previous performance, based on initial models of the domain and its own actions in it. This paper introduces the general system architecture; it then sketches some results in detail regarding hybrid reasoning and planning used in RACE, and instances of learning from the experiences of real robot task execution. Enhancement of robot competence is operationalized in terms of performance quality and description length of the robot instructions, and such enhancement is shown to result from the RACE system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A perceptual memory system for grounding semantic representations in intelligent service robots",
        "doc_scopus_id": "84911489131",
        "doc_doi": "10.1109/IROS.2014.6942861",
        "doc_eid": "2-s2.0-84911489131",
        "doc_date": "2014-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational issues",
            "High-level reasoning",
            "Intelligent Service robots",
            "Low level control",
            "Memory requirements",
            "Multiplexing schemes",
            "Perceptual memory",
            "Semantic representation"
        ],
        "doc_abstract": "© 2014 IEEE.This paper addresses the problem of grounding semantic representations in intelligent service robots. In particular, this work contributes to addressing two important aspects, namely the anchoring of object symbols into the perception of the objects and the grounding of object category symbols into the perception of known instances of the categories. The paper discusses memory requirements for storing both semantic and perceptual data and, based on the analysis of these requirements, proposes an approach based on two memory components, namely a semantic memory and a perceptual memory. The perception, memory, learning and interaction capabilities, and the perceptual memory, are the main focus of the paper. Three main design options address the key computational issues involved in processing and storing perception data: a lightweight, NoSQL database, is used to implement the perceptual memory; a thread-based approach with zero copy transport of messages is used in implementing the modules; and a multiplexing scheme, for the processing of the different objects in the scene, enables parallelization. The system is designed to acquire new object categories in an incremental and open-ended way based on user-mediated experiences. The system is fully integrated in a broader robot system comprising low-level control and reactivity to high-level reasoning and learning.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive teaching and experience extraction for learning about objects and robot activities",
        "doc_scopus_id": "84937567763",
        "doc_doi": "10.1109/ROMAN.2014.6926246",
        "doc_eid": "2-s2.0-84937567763",
        "doc_date": "2014-10-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Continuous interactions",
            "Human users",
            "Intelligent Service robots",
            "Interactive learning",
            "Learning methods",
            "Perceptual domain",
            "Robot architecture",
            "Teaching activities"
        ],
        "doc_abstract": "© 2014 IEEE.Intelligent service robots should be able to improve their knowledge from accumulated experiences through continuous interaction with the environment, and in particular with humans. A human user may guide the process of experience acquisition, teaching new concepts, or correcting insufficient or erroneous concepts through interaction. This paper reports on work towards interactive learning of objects and robot activities in an incremental and open-ended way. In particular, this paper addresses human-robot interaction and experience gathering. The robot's ontology is extended with concepts for representing human-robot interactions as well as the experiences of the robot. The human-robot interaction ontology includes not only instructor teaching activities but also robot activities to support appropriate feedback from the robot. Two simplified interfaces are implemented for the different types of instructions including the teach instruction, which triggers the robot to extract experiences. These experiences, both in the robot activity domain and in the perceptual domain, are extracted and stored in memory, and they are used as input for learning methods. The functionalities described above are completely integrated in a robot architecture, and are demonstrated in a PR2 robot.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic information and safety systems for driving assistance",
        "doc_scopus_id": "84907250800",
        "doc_doi": "10.5565/rev/elcvia.629",
        "doc_eid": "2-s2.0-84907250800",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The main object of this thesis is the study of algorithms for automatic information processing and representation, in particular information provided by onboard sensors (2D and 3D), to be used in the context of driving assistance. The work focuses on some of the problems facing todays Autonomous Driving (AD) systems and Advanced Drivers Assistance Systems (ADAS). The document is composed of two parts. The first part describes the design, construction and development of three robotic prototypes, including remarks about onboard sensors, algorithms and software architectures. These robots were used as test beds for testing and validating the developed techniques; additionally, they have participated in several autonomous driving competitions with very good results. The second part presents several algorithms for generating intermediate representations of the raw sensor data. They can be used to enhance existing pattern recognition, detection or navigation techniques, and may thus benefit future AD or ADAS applications. Since vehicles often contain a large amount of sensors of different natures, intermediate representations are particularly advantageous; they can be used for tackling problems related with the diverse nature of the data (2D, 3D, photometric, etc.), with the asynchrony of the data (multiple sensors streaming data at different frequencies), or with the alignment of the data (calibration issues, different sensors providing different measurements of the same object). Within this scope, novel techniques are proposed for computing a multi-camera multi-modal inverse perspective mapping representation, executing color correction between images for obtaining quality mosaics, or to produce a scene representation based on polygonal primitives that can cope with very large amounts of 3D and 2D data, including the ability of refining the representation as new information is continuously received.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An interactive open-ended learning approach for 3D object recognition",
        "doc_scopus_id": "84905034271",
        "doc_doi": "10.1109/ICARSC.2014.6849761",
        "doc_eid": "2-s2.0-84905034271",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Descriptors",
            "High level knowledge",
            "Leave-one-out cross validations",
            "Nearest-neighbor classifications",
            "open-ended learning",
            "Precision and recall",
            "Three-dimensional object"
        ],
        "doc_abstract": "Three-dimensional object detection and recognition is increasingly in manipulation and navigation applications in autonomous service robots. It involves clustering points of the point cloud from an unstructured scene into objects candidates and estimating features to recognize the objects under different circumstances such as occlusions and clutter. This paper presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In this paper, 'open-ended' implies that the set of object categories to be learned is not known in advance. The training instances are extracted from actual experiences of a robot, and thus become gradually available, rather than being available at the beginning of the learning process. This paper focuses on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D unstructured scenes in an open-ended manner? (2) How to acquire and utilize high-level knowledge obtained from the user (e.g. category label) to improve the system performance? This approach starts with a pre-processing phase to remove unnecessary information and prepare a suitable point cloud. Clustering is then applied to detect object candidates. Subsequently, all object candidates are described based on a 3D shape descriptor called spin-image. Finally, a nearest-neighbor classification rule is used to assign category labels to the detected objects. To examine the performance of the proposed approach, a leave-one-out cross validation algorithm is utilized to compute precision and recall. The experimental results show the fulfilling performance of this approach on different types of objects. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Grounding language in perception for scene conceptualization in autonomous robots",
        "doc_scopus_id": "84904861654",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84904861654",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cognitive architectures",
            "Grounding language",
            "Human supervision",
            "Learning frameworks",
            "Linguistic descriptions",
            "Natural languages",
            "Spatial structure",
            "Textual description"
        ],
        "doc_abstract": "In order to behave autonomously, it is desirable for robots to have the ability to use human supervision and learn from different input sources (perception, gestures, verbal and textual descriptions etc). In many machine learning tasks, the supervision is directed specifically towards machines and hence is straight forward clearly annotated examples. But this is not always very practical and recently it was found that the most preferred interface to robots is natural language. Also the supervision might only be available in a rather indirect form, which may be vague and incomplete. This is frequently the case when humans teach other humans since they may assume a particular context and existing world knowledge. We explore this idea here in the setting of conceptualizing objects and scene layouts. Initially the robot undergoes training from a human in recognizing some objects in the world and armed with this acquired knowledge it sets out in the world to explore and learn more higher level concepts like static scene layouts and environment activities. Here it has to exploit its learned knowledge and ground language into perception to use inputs from different sources that might have overlapping as well as novel information. When exploring, we assume that the robot is given visual input, without explicit type labels for objects, and also that it has access to more or less generic linguistic descriptions of scene layout. Thus our task here is to learn the spatial structure of a scene layout and simultaneously visual object models it was not trained on. In this paper, we present a cognitive architecture and learning framework for robot learning through natural human supervision and using multiple input sources by grounding language in perception. Copyright © 2014, Association for the Advancement of Artificial Intelligence. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Color correction for onboard multi-camera systems using 3D Gaussian mixture models",
        "doc_scopus_id": "84865045058",
        "doc_doi": "10.1109/IVS.2012.6232141",
        "doc_eid": "2-s2.0-84865045058",
        "doc_date": "2012-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Color correction",
            "Color space",
            "Corrected image",
            "Gaussian Mixture Model",
            "Image data",
            "Local color",
            "Multicamera systems",
            "Performance comparison",
            "Processing time",
            "Single-step"
        ],
        "doc_abstract": "The current paper proposes a novel color correction approach for onboard multi-camera systems. It works by segmenting the given images into several regions. A probabilistic segmentation framework, using 3D Gaussian Mixture Models, is proposed. Regions are used to compute local color correction functions, which are then combined to obtain the final corrected image. An image data set of road scenarios is used to establish a performance comparison of the proposed method with other seven well known color correction algorithms. Results show that the proposed approach is the highest scoring color correction method. Also, the proposed single step 3D color space probabilistic segmentation reduces processing time over similar approaches. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Color correction using 3D Gaussian mixture models",
        "doc_scopus_id": "84864127490",
        "doc_doi": "10.1007/978-3-642-31295-3_12",
        "doc_eid": "2-s2.0-84864127490",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color correction",
            "Color space",
            "Corrected image",
            "Gaussian Mixture Model",
            "Image mosaicing",
            "Large datasets",
            "Local color",
            "Processing time",
            "Single-step"
        ],
        "doc_abstract": "The current paper proposes a novel color correction approach based on a probabilistic segmentation framework by using 3D Gaussian Mixture Models. Regions are used to compute local color correction functions, which are then combined to obtain the final corrected image. The proposed approach is evaluated using both a recently published metric and two large data sets composed of seventy images. The evaluation is performed by comparing our algorithm with eight well known color correction algorithms. Results show that the proposed approach is the highest scoring color correction method. Also, the proposed single step 3D color space probabilistic segmentation reduces processing time over similar approaches. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D-2D laser range finder calibration using a conic based geometry shape",
        "doc_scopus_id": "84864114823",
        "doc_doi": "10.1007/978-3-642-31295-3_37",
        "doc_eid": "2-s2.0-84864114823",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "2D laser scanners",
            "2d lasers",
            "3D point cloud",
            "Calibration method",
            "Inertial sensor",
            "Laser calibration",
            "Laser range finders",
            "Object geometries",
            "Sensor data",
            "Sensor fusion",
            "Stereo cameras"
        ],
        "doc_abstract": "The AtlasCar is a prototype that is being developed at the University of Aveiro to research advanced driver assistance systems. The car is equipped with several sensors: 3D and 2D laser scanners, a stereo camera, inertial sensors and GPS. The combination of all these sensor data in useful representations is essential. Therefore, calibration is one of the first problems to tackle. This paper focuses on 3D/2D laser calibration. The proposed method uses a 3D Laser Range Finder (LRF) to produce a reference 3D point cloud containing a known calibration object. Manual input from the user and knowledge of the object geometry are used to register the 3D point cloud with the 2D Lasers. Experimental results with simulated and real data demonstrate the effectiveness of the proposed calibration method. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised local color correction for coarsely registered images",
        "doc_scopus_id": "80052901688",
        "doc_doi": "10.1109/CVPR.2011.5995658",
        "doc_eid": "2-s2.0-80052901688",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Color segmentation",
            "Local color",
            "Mean shift",
            "Recent performance",
            "Registered images"
        ],
        "doc_abstract": "The current paper proposes a new parametric local color correction technique. Initially, several color transfer functions are computed from the output of the mean shift color segmentation algorithm. Secondly, color influence maps are calculated. Finally, the contribution of every color transfer function is merged using the weights from the color influence maps. The proposed approach is compared with both global and local color correction approaches. Results show that our method outperforms the technique ranked first in a recent performance evaluation on this topic. Moreover, the proposed approach is computed in about one tenth of the time. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "ATLASCAR - Technologies for a computer assisted driving system on board a common automobile",
        "doc_scopus_id": "78650471844",
        "doc_doi": "10.1109/ITSC.2010.5625031",
        "doc_eid": "2-s2.0-78650471844",
        "doc_date": "2010-12-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Active safety",
            "Computer assisted",
            "Data gathering",
            "Driver assistance",
            "Driving systems",
            "Intelligent vehicles",
            "Vehicle autonomy"
        ],
        "doc_abstract": "The future of intelligent vehicles will rely on robust information to allow the proper feedback to the vehicle itself, to issue several kinds of active safety, but before all, to generate information for the driver by calling his or her attention to potential instantaneous or mid-term risks associated with the driving. Before true vehicle autonomy, safety and driver assistance are a priority. Sophisticated sensorial and perceptive mechanisms must be made available for, in a first instance, assisting the driver and, on a latter phase, participate in better autonomy. These mechanisms rely on sensors and algorithms that are mostly available nowadays, but many of them are still unsuited for critical situations. This paper presents a project where engineering and scientific solutions have been devised to settle a full featured real scale platform for the next generation of ITS vehicles that are concerned with the immediate issues of navigation and challenges on the road. The car is now ready and running, and the data gathering has just begun. ©2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Teaching computer science in higher education: Enabling learning roadmaps for post secondary courses",
        "doc_scopus_id": "78751558488",
        "doc_doi": "10.1109/FIE.2010.5673218",
        "doc_eid": "2-s2.0-78751558488",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "E-learning 2.0",
            "Learning roadmaps",
            "Paradigm",
            "Pedagogy",
            "Platform",
            "Strategies"
        ],
        "doc_abstract": "This paper tends to present how eLearning strategies and policies used, in a Portuguese district (Aveiro), are enhancing learning and knowledge for heterogeneous audiences with constraints that are beyond the usual. Besides time to learn and physical distance, they face new challenges like, the need to get specialized in areas where economy is growing; where companies need to adapt to new markets; where the technology revolution is a priority. Experience achieved tells the professionals observed have shown great interest in eLearning actions and for a hybrid methodology. By a simple empirical evaluation, professionals agree that information and communication technologies (ICT) are, in fact, very helpful for those who have several constraints to learn. The use of Learning Management Systems (LMS) tends to accomplish the learning assessments and skills, although there are a lot of improvements that can be done through ICT, especially to promote self-study. The paper presents experiences done with a new learning application that enables a more intelligent and friendly content organization and promotes actor's participation powering efficient learning. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning Roadmap Studio",
        "doc_scopus_id": "70449572518",
        "doc_doi": "10.1109/ICETC.2009.13",
        "doc_eid": "2-s2.0-70449572518",
        "doc_date": "2009-11-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            }
        ],
        "doc_keywords": [
            "ELearning 2.0",
            "Learning Roadmaps",
            "Paradigm",
            "Pedagogy",
            "Platform",
            "Strategies"
        ],
        "doc_abstract": "Technologies and learning platforms approaches are changing each day, one of the reasons for it is that organizations, enterprises and institutions are growing and producing more knowledge, whereas workers are becoming knowledge workers and need to be adapted for the fast change and share of information. From past experience, it has been denoted that strategies and pedagogical processes are tasks that can be created, enriched and boosted by actors who participate in learning and training processes: course managers, teachers and students. The challenge posed to the different actors involved also accelerates the changes that have been happening in education and training, empowering a society based on knowledge. Thus, it has been developed a new platform, Learning Roadmap Studio, which powers eLearning 2.0 concepts, that tends to promote more efficient learning and training. For teachers and course managers, it enables the creation, edition and deployment of learning roadmaps, that can be edit by students in order to communicate and share their knowledge among the learning community. For the students, the learning roadmap aims at promoting self-study and supervised study, endowing the pupil with the capabilities to find the relevant information and to capture the concepts in the study materials. The outcome will be a stimulating learning process together with an organized management of those materials. It is not intended to create new learning management systems. Instead, it is presented as an application that enables the edition and creation of learning processes and strategies, giving primary relevance to teachers, instead of focusing on tools, features and contents. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Aprend.E-Electronic integrated system for learning and training",
        "doc_scopus_id": "84869048035",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84869048035",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "High school",
            "Integrated systems",
            "Learning and training",
            "Long life",
            "Professional training"
        ],
        "doc_abstract": "This abstract presents Aprend.E-Electronic Integrated System for Learning and Training of High School Aveiro Norte, a new challenge of the University of Aveiro for professional training and long life training. It describes the motivation that induced its implementation, advantages for students, teachers and for all those whom the system serves.",
        "available": false,
        "clean_text": ""
    }
]