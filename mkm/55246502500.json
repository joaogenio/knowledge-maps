[
    {
        "doc_title": "Novel Modeling Method using Set Theory for Interference Study between Multiple FMCW Radars",
        "doc_scopus_id": "85128551636",
        "doc_doi": "10.1063/5.0081634",
        "doc_eid": "2-s2.0-85128551636",
        "doc_date": "2022-04-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Physics and Astronomy (all)",
                "area_abbreviation": "PHYS",
                "area_code": "3100"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 American Institute of Physics Inc.. All rights reserved.Recently, the number of systems based on FMCW radar technology has been increasing dramatically, due in part to the availability of low-cost development and evaluation kits that permit rapid prototyping and testing of radar-based applications. In a real scenario where it is possible to have multiple radars emitting simultaneously, interference between radars can arise due to the periodic nature of FMCW. Moreover, FMCW radars take advantage of the large bandwidth in order to achieve a high resolution in range. Therefore, the probability of two radars using the same spectrum range simultaneously is very high. In this work, the interference between FMCW radars systems is investigated using set theory. The FMCW signals are modeled as sets of time-frequency tuples and they are described as sets, reducing the analysis complexity of the interference between multiple radars. The proposed model was implemented in MATLAB and it was compared and validated in real scenarios with two radars operating and producing interference in the same indoor environment. It is shown that set theory considerable reduces the simulation time in comparison to traditional methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A general approach to hand-eye calibration through the optimization of atomic transformations",
        "doc_scopus_id": "85103787190",
        "doc_doi": "10.1109/TRO.2021.3062306",
        "doc_eid": "2-s2.0-85103787190",
        "doc_date": "2021-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Atomic transformation",
            "Calibration algorithm",
            "Calibration problems",
            "Calibration procedure",
            "Initial configuration",
            "Nonlinear least squares methods",
            "Optimization procedures",
            "Robot operating system"
        ],
        "doc_abstract": "© 2021 IEEE.This article proposes a general approach to solve the hand-eye calibration problem. The system is general since it is able to calibrate any number of cameras and, moreover, is able to simultaneously perform the calibration of several instances of the two common hand-eye calibration use cases: eye-on-hand and eye-to-base. The calibration is solved with a nonlinear least squares method, and the reprojection error is used as a metric to guide the optimization procedure. Our approach is seamlessly integrated with the robot operating system framework and allows for the interactive positioning of sensors and labeling of data, facilitating both the data acquisition and labeling and the calibration procedures. Results show that the proposed approach is able to handle any calibration use case with a minimal initial configuration. The approach is compared with several other state-of-the-art hand-eye calibration algorithms. Results show that the proposed approach produces very accurate calibrations when compared to the state of the art.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Camera to LiDAR calibration approach through the optimization of atomic transformations",
        "doc_scopus_id": "85103644407",
        "doc_doi": "10.1016/j.eswa.2021.114894",
        "doc_eid": "2-s2.0-85103644407",
        "doc_date": "2021-08-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Atomic transformation",
            "Detection sensors",
            "Geometric optimization",
            "Light detection and ranging",
            "Multiple cameras",
            "Multiple sensors",
            "Ranging sensors",
            "Sensor modality",
            "State of the art"
        ],
        "doc_abstract": "© 2021 Elsevier LtdThis paper proposes a camera-to-3D Light Detection And Ranging calibration framework through the optimization of atomic transformations. The system is able to simultaneously calibrate multiple cameras with Light Detection And Ranging sensors, solving the problem of Bundle. In comparison with the state-of-the-art, this work presents several novelties: the ability to simultaneously calibrate multiple cameras and LiDARs; the support for multiple sensor modalities; the calibration through the optimization of atomic transformations, without changing the topology of the input transformation tree; and the integration of the calibration framework within the Robot Operating System (ROS) framework. The software pipeline allows the user to interactively position the sensors for providing an initial estimate, to label and collect data, and visualize the calibration procedure. To test this framework, an agricultural robot with a stereo camera and a 3D Light Detection And Ranging sensor was used. Pairwise calibrations and a single calibration of the three sensors were tested and evaluated. Results show that the proposed approach produces accurate calibrations when compared to the state-of-the-art, and is robust to harsh conditions such as inaccurate initial guesses or small amount of data used in calibration. Experiments have shown that our optimization process can handle an angular error of approximately 20 degrees and a translation error of 0.5 meters, for each sensor. Moreover, the proposed approach is able to achieve state-of-the-art results even when calibrating the entire system simultaneously.",
        "available": true,
        "clean_text": "serial JL 271506 291210 291817 291820 291862 291866 291870 291883 31 Expert Systems with Applications EXPERTSYSTEMSAPPLICATIONS 2021-03-18 2021-03-18 2021-04-02 2021-04-02 2021-09-11T14:55:41 S0957-4174(21)00335-3 S0957417421003353 10.1016/j.eswa.2021.114894 S300 S300.1 FULL-TEXT 2021-09-11T14:12:26.883148Z 0 0 20210815 2021 2021-03-18T13:20:21.732409Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref 0957-4174 09574174 true 176 176 C Volume 176 47 114894 114894 114894 20210815 15 August 2021 2021-08-15 2021 article fla © 2021 Elsevier Ltd. All rights reserved. ACAMERALIDARCALIBRATIONAPPROACHTHROUGHOPTIMIZATIONATOMICTRANSFORMATIONS PINTODEAGUIAR A 1 Introduction 2 Proposed approach 2.1 Objective function 2.1.1 Camera modality sub-function 2.1.2 3D LiDAR sub-function 2.2 Normalization of multi-modal residuals 3 Calibration framework 3.1 Calibration configuration 3.2 Initial parameter estimation 3.3 Labeling data 3.4 Collecting data 3.5 Visualizing the optimization 4 Results 4.1 Methodology 4.2 Metrics 4.3 Evaluation 4.4 Impact of the number of collections used for training 4.5 Impact of the number of incomplete collections used for training 4.6 Impact of the accuracy of the initial estimate 5 Conclusions CRediT authorship contribution statement Acknowledgements References AGARWAL 2010 29 42 S COMPUTERVISIONECCV2010 BUNDLEADJUSTMENTINLARGE DEAGUIAR 2020 105535 A ALVAREZ 2014 1532 1542 S BADUE 2021 113816 C BESL 1992 239 256 P BRADSKI 2000 120 125 G DURRANTWHYTE 2006 99 110 H FABBRI 2020 1 R IEEETRANSACTIONSPATTERNANALYSISMACHINEINTELLIGENCE CAMERAPOSEESTIMATIONUSINGFIRSTORDERCURVEDIFFERENTIALGEOMETRY FISCHLER 1981 381 395 M FOOTE 2013 T 2013IEEECONFERENCETECHNOLOGIESFORPRACTICALROBOTAPPLICATIONSTEPRA TFTRANSFORMLIBRARY FREMONT 2008 V 2008IEEEINTERNATIONALCONFERENCEMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMS EXTRINSICCALIBRATIONBETWEENAMULTILAYERLIDARACAMERA FURGALE 2013 P 2013IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMS UNIFIEDTEMPORALSPATIALCALIBRATIONFORMULTISENSORSYSTEMS GAO 2003 930 943 X GARRIDOJURADO 2016 481 491 S GIRSHICK 2006 57 68 M SELECTEDPAPERSFREDERICKMOSTELLER UNBIASEDESTIMATESFORCERTAINBINOMIALSAMPLINGPROBLEMSAPPLICATIONS GUINDEL 2017 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS GUINDEL 2017 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS HORNEGGER 1999 J PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISION REPRESENTATIONISSUESINMLESTIMATIONCAMERAMOTION HU 2019 D 2019IEEECVFCONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR DEEPCHARUCODARKCHARUCOMARKERPOSEESTIMATION HUANG 2020 134101 134110 J HUANG 2009 L 2009IEEEINTELLIGENTVEHICLESSYMPOSIUM ANOVELMULTIPLANARLIDARCOMPUTERVISIONCALIBRATIONPROCEDUREUSING2DPATTERNSFORAUTOMATEDNAVIGATION SUKIM 2019 52 E LIAO 2017 Y 20172NDINTERNATIONALCONFERENCEADVANCEDROBOTICSMECHATRONICSICARM JOINTKINECTMULTIPLEEXTERNALCAMERASSIMULTANEOUSCALIBRATION MAJUMDER 2018 165 172 S MELENDEZPASTOR 2017 28 38 C MIRZAEI 2012 452 467 F OLIVEIRA 2020 103558 M PANDEY 2010 336 341 G DEPAULA 2014 1997 2007 M PENATESANCHEZ 2013 2387 2400 A PRADEEP 2014 211 225 V EXPERIMENTALROBOTICS CALIBRATINGAMULTIARMMULTISENSORROBOTABUNDLEADJUSTMENTAPPROACH REHDER 2016 383 398 J ROMERORAMIREZ 2018 38 47 F DOSSANTOS 2016 429 444 F SANTOS 2019 684 698 L SARIFF 2006 N 20064THSTUDENTCONFERENCERESEARCHDEVELOPMENT OVERVIEWAUTONOMOUSMOBILEROBOTPATHPLANNINGALGORITHMS STURM 2014 610 613 P COMPUTERVISIONAREFERENCEGUIDE PINHOLECAMERAMODEL VERMA 2019 3906 3912 S 2019IEEEINTELLIGENTTRANSPORTATIONSYSTEMSCONFERENCEITSC AUTOMATICEXTRINSICCALIBRATIONBETWEENACAMERAA3DLIDARUSING3DPOINTPLANECORRESPONDENCES WANG 2017 851 W ZHOU 2018 L 2018IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS AUTOMATICEXTRINSICCALIBRATIONACAMERAA3DLIDARUSINGLINEPLANECORRESPONDENCES ZUNIGANOEL 2019 2862 2869 D PINTODEAGUIARX2021X114894 PINTODEAGUIARX2021X114894XA 2023-04-02T00:00:00.000Z 2023-04-02T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 2021-06-03T23:22:39.891Z FCT-Foundation for Science and Technology DFA/BD/5318/2020 FST Foundation for Science and Technology André Silva Pinto de Aguiar thanks the FCT-Foundation for Science and Technology, Portugal for the Ph.D. Grant DFA/BD/5318/2020. 0 item S0957-4174(21)00335-3 S0957417421003353 10.1016/j.eswa.2021.114894 271506 2021-09-11T14:12:26.883148Z 2021-08-15 true 9751065 MAIN 16 57422 849 656 IMAGE-WEB-PDF 1 gr13 45835 216 756 gr11 28097 274 677 gr12 59533 303 538 gr8 27086 384 378 gr9 42489 272 378 gr10 58390 207 713 gr1 29720 318 557 gr2 55394 479 355 gr3 38297 253 622 gr4 37647 227 623 gr5 26654 299 579 gr6 40044 273 623 gr7 40265 371 623 gr13 5245 63 219 gr11 5067 89 219 gr12 23664 123 219 gr8 5563 164 161 gr9 31085 157 219 gr10 12344 64 219 gr1 7208 125 219 gr2 18248 163 121 gr3 8216 89 219 gr4 8040 80 219 gr5 6728 113 219 gr6 11125 96 219 gr7 8820 131 219 gr13 351003 956 3346 gr11 183510 1214 2997 gr12 236019 859 1526 gr8 85285 1089 1071 gr9 138857 770 1071 gr10 390325 917 3155 gr1 178096 1411 2468 gr2 186632 1360 1008 gr3 285181 1120 2756 gr4 283146 1007 2760 gr5 154239 1324 2563 gr6 333409 1212 2761 gr7 305954 1645 2760 si1 1410 si10 2986 si100 5085 si101 3098 si102 5085 si103 11404 si104 26868 si105 2558 si106 1532 si107 1801 si108 4612 si109 4612 si11 2695 si110 8384 si111 9023 si112 20021 si113 10650 si114 3851 si115 9840 si116 10690 si117 1605 si118 10873 si119 14075 si12 5109 si120 10023 si121 22946 si122 25518 si123 2953 si124 12549 si125 7059 si126 9217 si127 20950 si128 4004 si129 10219 si13 2691 si130 17821 si131 5303 si132 5414 si133 7411 si134 6972 si135 12531 si136 7930 si137 7827 si138 16004 si139 14295 si14 2550 si140 23792 si141 16827 si142 18205 si143 15700 si144 9279 si145 11992 si15 2986 si16 2695 si17 5109 si18 2691 si19 2691 si2 1547 si20 2986 si21 2987 si22 5110 si23 3561 si24 1577 si25 1577 si26 1577 si27 1577 si28 7752 si29 3636 si3 1604 si30 5795 si31 31502 si32 13339 si33 4040 si34 3746 si35 4040 si36 3238 si37 13902 si38 6183 si39 1577 si4 14074 si40 52652 si41 1986 si42 8772 si43 7253 si44 1342 si45 1532 si46 8310 si47 1801 si48 8877 si49 3673 si5 2691 si50 4440 si51 3673 si52 3673 si53 23103 si54 4440 si55 4557 si56 5742 si57 18062 si58 1657 si59 3051 si6 2550 si60 2420 si61 2953 si62 3474 si63 12159 si64 5856 si65 3474 si66 8631 si67 14386 si68 1797 si69 1532 si7 2986 si70 20483 si71 35017 si72 14473 si73 3043 si74 1801 si75 4612 si76 6993 si77 1532 si78 1801 si79 5742 si8 2695 si80 7284 si81 2953 si82 18315 si83 19115 si84 1412 si85 1412 si86 1549 si87 7284 si88 15206 si89 4609 si9 5109 si90 8698 si91 37769 si92 11842 si93 18597 si94 9944 si95 3272 si96 4819 si97 13888 si98 5861 si99 27245 am false 7565333 ESWA 114894 114894 S0957-4174(21)00335-3 10.1016/j.eswa.2021.114894 Elsevier Ltd Fig. 1 (a) AgRob V16 model and the respective referential frames represented as red-green–blue axes. (b): Transformation tree for AgRob V16 robotic platform. The majority of the frames not to be calibrated were omitted for simplicity. Each sensor has an associated atomic transformation, denoted by the solid edges. Dashed edges denote transformations that are not optimized (they can be static or dynamic). Each sensor has a corresponding link to which the data it collects is attached, denoted in the figure by solid thin ellipses. Very few approaches in the literature are capable of calibrating such a system while preserving the initial structure of the transformation graph. Fig. 2 Difference between the initial position of the pattern corners, and the final position of these same projected points, after the optimization has been completed. Squares denote the position of the detected pattern corners; crosses denote the initial position of each projected corner; points denote the current position of the projected points. Fig. 3 Two examples of the pattern boundary points extraction from LiDAR data. The colored points represent the clustered LiDAR scans considering the spherical component θ , and the crosses represent the boundary points extracted using the maximum and minimum values of the spherical component ϕ , for each cluster. Fig. 4 (a): calibration pattern and respective ground truth boundary points represented as blue lines; (b): misaligned boundary points observed by the 3D LiDAR with the ground-truth points at the start of the calibration procedure; (c): optimization result - ground-truth points and projected boundary points aligned. It is noteworthy that the orthogonal distance aligned the z coordinate of the ground-truth pattern and 3D LiDAR points. Fig. 5 Example of the developed interactive tool operation for AgRob V16 system. Here, the reference frames of the cameras that compose the stereo camera system and the 3D LiDAR sensor were moved. When the user positions the sensors in the desired pose, the initial estimate for the pose of each sensor is saved to use in calibration. Fig. 6 (a): labeling image data using a charuco pattern; (b): labeling 3D LiDAR data on pattern (solid blue points) using the semi-automatic proposed approach. Note that, our approach works with partial detections, i.e., collections of data where the pattern is not fully detected. This figure presents an example of an partial detection for each type of sensor. For the camera, only a portion of the corners of the pattern were detected. For the 3D LiDAR, the pattern is not fully observable due to the low vertical resolution. Even though, our approach is able to calibrate the system with these detections. Fig. 7 (a): ROS calibration configuration - simultaneous visualization of all the collections of data and respective alignment between ground-truth points and labeled points; (b): graphics representing the objective function minimization - individual residuals value, and the total error vs iterations. Fig. 8 Chain of transformations representing the generic configuration to extract an atomic transformation from a sensor to sensor calibration. Preserving the chain of transformation of the anchored sensor, and taking into account the sensor to sensor calibration T ̂ , we recover the atomic transformation child 1 T parent 1 . Fig. 9 Camera reprojection error from one camera image into the other. Squares represent the expected corner pixel coordinates, and crosses the projected result. Fig. 10 3D LiDAR to camera reprojection error metric calculation. (a) represents the annotation procedure, where four classes are annotated, each one representing a single side of the pattern; (b) red curves represent the approximation of each one of the classes by a polynomial function, in order to account for the distortion in the image; (c) reprojection error calculation - blue dots represent the 3D LiDAR pattern boundary reprojected points and yellow lines the difference of each one of the points with the labelled ground truth. Fig. 11 Reprojection error dispersion for the camera to camera calibration using ATOM full configuration. Each color represents the error associated with one individual collection. (a) is the representation of this error before calibrating (using the initial guess), and (b) after the calibration. Fig. 12 Point cloud projection into the left camera image using the 3D LiDAR to camera calibration. The color represents the points depth. It is worth noting that, this procedure was done using the ATOM full calibration result, with a collection from the test dataset, just like in all the evaluation pipeline. If the system is accuratly calibrated, changes in point’s color, which denotes a variation of the measured range, should coincide with transitions between far and near objects in the image. Fig. 13 Impact of the initial angle (a) and distance (b) estimation error on the optimization error and execution time. The considered optimization error is the root means squared (RMS) error. Table 1 Description of the datasets used for train and evaluation. Two datasets were used for training (calibration) and one for testing (evaluation). The datasets contain incomplete collections, i.e., collections were the pattern is not detected by at least one sensor, and partial collections, i.e., collections were the pattern is partially detected by at least one sensor. Dataset Nr. Collections Observations Total Incomplete cPartial train-1 42 5 c38 Dataset contains incomplete collections. train-2 56 0 c24 The dataset does not contain the point cloud generated by the ZED camera. test-3 15 0 c8 Dataset with low number of collections, only used for test. Table 2 Summary of the methods used and evaluated in the experiments. Method Calibration Properties OpenCV (Bradski, 2000) pairwise reprojection error, intrinsics calibration Stereo camera factory calibration pairwise reprojection error, intrinsics calibrations ICP average (Besl & McKay, 1992) pairwise reprojection error, average of result of all collections ICP best (Besl & McKay, 1992) pairwise reprojection error, best result of all collections ATOM pairwise [this paper] pairwise reprojection error, angle-axis, intrinsics calibration ATOM full [this paper] full calibration reprojection error, angle-axis, intrinsics calibration Table 3 Performance comparison of methods for camera to camera calibration. Method Train dataset e R (rad) e t (m) e x (px) e y (px) e rms (px) OpenCV (Bradski, 2000) train-1 Not able to calibrate due to partial pattern detections. sATOM pairwise 0.009 0.003 0.551 ± 0.800 0.780 ± 1.090 1.049 ATOM full 0.008 0.003 0.547 ± 0.759 0.638 ± 1.034 0.974 OpenCV (Bradski, 2000) train-2 0.006 0.003 0.582 ± 0.648 0.622 ± 0.966 0.863 ATOM pairwise 0.010 0.006 0.655 ± 1.055 0.677 ± 0.982 1.020 ATOM full 0.008 0.005 0.594 ± 0.912 0.696 ± 1.033 0.974 ZED’s factory calibration - 0.007 0.006 2.220 ± 0.765 0.486 ± 0.823 1.757 Table 4 Performance comparison of methods for camera to 3D LiDAR calibration. Method Type Train dataset e x (px) e y (px) e rms (px) ICP average (Besl & McKay, 1992) left camera - 3D LiDAR train-1 47.210 ± 31.374 19.058 ± 28.233 44.307 ICP best (Besl & McKay, 1992) left camera - 3D LiDAR 9.111 ± 11.950 2.625 ± 7.967 10.492 ATOM pairwise right camera - 3D LiDAR 3.054 ± 4.727 1.031 ± 2.689 3.869 ATOM pairwise left camera - 3D LiDAR 3.648 ± 4.846 1.260 ± 2.869 4.101 ATOM full right camera - 3D LiDAR 3.351 ± 4.874 0.950 ± 2.279 3.811 ATOM full left camera - 3D LiDAR 3.398 ± 4.923 1.100 ± 2.602 3.942 ICP average (Besl & McKay, 1992) left camera - 3D LiDAR train-2 - - - ICP best (Besl & McKay, 1992) left camera - 3D LiDAR - - - ATOM pairwise right camera - 3D LiDAR 7.574 ± 6.393 1.776 ± 3.181 6.715 ATOM pairwise left camera - 3D LiDAR 7.560 ± 5.535 1.619 ± 2.795 6.432 ATOM full right camera - 3D LiDAR 7.702 ± 5.441 1.648 ± 2.781 6.537 ATOM full left camera - 3D LiDAR 8.117 ± 5.692 1.687 ± 2.838 6.765 Table 5 Impact of the number of collections in ATOM’s full calibration performance. The dashed entries correspond to results not generated since, for the camera-to-LiDAR case, the mean rotation and translation errors are not available. Type Nr. Collections e R (rad) e t (rad) e x (px) e y (px) e rms (px) camera - camera 1 0.051 0.053 8.205 ± 7.201 16.089 ± 3.346 13.534 5 0.017 0.016 3.540 ± 4.575 1.108 ± 1.449 4.233 10 0.009 0.004 1.377 ± 1.318 0.932 ± 0.991 1.645 20 0.008 0.003 0.515 ± 0.719 0.658 ± 1.056 0.976 30 0.008 0.003 0.547 ± 0.759 0.638 ± 1.034 0.974 right camera - 3D LiDAR 1 - - 4.348 ± 6.314 1.936 ± 4.431 5.487 5 - - 4.202 ± 5.179 1.144 ± 2.342 4.466 10 - - 3.305 ± 4.742 0.984 ± 2.387 3.820 20 - - 3.355 ± 4.744 1.005 ± 2.412 3.774 30 - - 3.352 ± 4.874 0.950 ± 2.279 3.811 left camera - 3D LiDAR 1 - - 5.126 ± 6.686 1.527 ± 3.435 5.566 5 - - 3.381 ± 4.447 1.058 ± 2.503 3.730 10 - - 2.937 ± 4.430 1.115 ± 2.791 3.712 20 - - 3.411 ± 4.887 1.258 ± 2.959 4.046 30 - - 3.398 ± 4.924 1.100 ± 2.602 3.942 Table 6 Impact of the number of incomplete collections in ATOM full calibration performance. The dashed entries correspond to results not generated since, for the camera-to-LiDAR case, the mean rotation and translation errors are not available. Type Nr. Collections e R (rad) e R (rad) e x (px) e x (px) e rms (px) Complete Incomplete camera - camera 10 0 0.011 0.011 1.159 ± 1.479 0.848 ± 1.182 1.457 10 2 0.006 0.003 0.821 ± 0.711 0.606 ± 0.900 1.031 10 4 0.009 0.006 1.680 ± 2.001 0.784 ± 0.945 2.017 10 5 0.010 0.005 0.778 ± 0.990 0.641 ± 0.908 1.076 right camera - 3D LiDAR 10 0 - - 5.104 ± 6.517 1.370 ± 3.085 5.496 10 2 - - 3.272 ± 4.800 1.068 ± 2.634 3.920 10 4 - - 3.414 ± 4.934 1.040 ± 2.515 3.964 10 5 - - 3.734 ± 4.956 1.111 ± 2.540 4.069 left camera - 3D LiDAR 10 0 - - 5.113 ± 6.479 1.535 ± 3.430 5.576 10 2 - - 3.112 ± 4.665 1.192 ± 2.970 3.930 10 4 - - 2.995 ± 4.541 1.137 ± 2.886 3.849 10 5 - - 3.720 ± 5.012 1.274 ± 2.907 4.169 A Camera to LiDAR calibration approach through the optimization of atomic transformations André Silva Pinto de Aguiar Conceptualization Methodology Software Writing - original draft a c ⁎ Miguel Armando Riem de Oliveira Conceptualization Methodology Software Writing - review & editing b Eurico Farinha Pedrosa Conceptualization Methodology Software Writing - review & editing b Filipe Baptista Neves dos Santos Writing - review & editing Supervision a a INESC TEC – INESC Technology and Science; 4200-465 Porto, Portugal INESC TEC – INESC Technology and Science; 4200-465 Porto Portugal INESC TEC - INESC Technology and Science; 4200-465, Porto, Portugal b Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal c School of Science and Technology, University of Trás-os-Montes e Alto Douro; 5000-801 Vila Real, Portugal School of Science and Technology University of Trás-os-Montes e Alto Douro; 5000-801 Vila Real Portugal School of Science and Technology, University of Trás-os-Montes e Alto Douro; 5000-801 Vila Real, Portugal ⁎ Corresponding author. This paper proposes a camera-to-3D Light Detection And Ranging calibration framework through the optimization of atomic transformations. The system is able to simultaneously calibrate multiple cameras with Light Detection And Ranging sensors, solving the problem of Bundle. In comparison with the state-of-the-art, this work presents several novelties: the ability to simultaneously calibrate multiple cameras and LiDARs; the support for multiple sensor modalities; the calibration through the optimization of atomic transformations, without changing the topology of the input transformation tree; and the integration of the calibration framework within the Robot Operating System (ROS) framework. The software pipeline allows the user to interactively position the sensors for providing an initial estimate, to label and collect data, and visualize the calibration procedure. To test this framework, an agricultural robot with a stereo camera and a 3D Light Detection And Ranging sensor was used. Pairwise calibrations and a single calibration of the three sensors were tested and evaluated. Results show that the proposed approach produces accurate calibrations when compared to the state-of-the-art, and is robust to harsh conditions such as inaccurate initial guesses or small amount of data used in calibration. Experiments have shown that our optimization process can handle an angular error of approximately 20 degrees and a translation error of 0.5 meters, for each sensor. Moreover, the proposed approach is able to achieve state-of-the-art results even when calibrating the entire system simultaneously. Keyword Computer vision Geometric optimization Atomic transformations 1 Introduction Nowadays, autonomous robotic systems are endowed with high-quality onboard sensors of different modalities, i.e. sensors that output different types of data, such as cameras and Light Detection And Ranging (LiDAR) sensors. To move autonomously while safely avoiding any kind of obstacle, these vehicles need to perform complex tasks such as Simultaneous Localization and Mapping (Durrant-Whyte & Bailey, 2006) and Path Planning (Sariff & Buniyamin, 2006) which require calibrated sensor data. The quality of the onboard sensors data is also crucial, since the robot should have a clear perception of the environment. For example, in agriculture, the automation of tasks such as crop monitoring or harvesting is a complex challenge that requires data of high quality sensors (Santos et al., 2016), such as, for example long-range 3D LiDARs. To perform data fusion, and take advantage of all the sensors present in the robotic system, it is essential to know the spatial relationship between all sensors (Melendez-Pastor, Ruiz-Gonzalez, & Gomez-Gil, 2017; Majumder & Pratihar, 2018). To do so, the most common approach is to perform extrinsic calibration, i.e., to compute the transformation between the sensors’ reference frames. The standard approach to perform extrinsic calibration is to find associations between data incoming from each sensor to be calibrated. Thus, a cost function that minimizes the error between associations is used. Most of the calibration procedures use a pattern that can be detected independently of the sensor modality, so that data correspondences can be found. Using these concepts, camera to 3D LiDAR extrinsic calibration have been approached in several works. A minority of works perform calibration without using a pattern. In those, the characteristics of the environment are used as features to compute intrinsic and extrinsic calibration. In autonomous driving, for example, lane detection and vanishing point tracking are common approaches (Badue et al., 2021; de Paula, Jung, & da Silveira, 2014; Álvarez, Llorca, & Sotelo, 2014). The great majority of the works found in the literature follow a pairwise calibration between a monocular camera and a 3D LiDAR, a concept introduced by Huang and Barth (2009). In this work, the extrinsic coefficients are computed solving a closed-form equation, and refined with a maximum likelihood estimation. Similarly, Verma, Berrio, Worrall, and Nebot (2019) use a standard chessboard to calibrate a perspective/fisheye camera and a 3D LiDAR using a Genetic Algorithm. Wang, Sakurada, and Kawaguchi (2017) propose a work where the corners of the pattern are automatically detected for both a panoramic camera and a 3D LiDAR so that the calibration can be performed. For the LiDAR case, authors propose a detection based on the intensity of reflectance of the beams. Fremont and Bonnifait (2008) and Guindel, Beltran, Martin, and Garcia (2017) use circle-based patterns to perform the extrinsic calibration. Mirzaei, Kottas, and Roumeliotis (2012) propose the estimation of a 3D LiDAR intrinsic parameters, as well as the extrinsic calibration with a monocular camera, through the minimization of a non-linear least squares cost function. The calibration is used to build photorealistic 3D reconstruction of indoor and outdoor scenes. Pandey, McBride, Savarese, and Eustice (2010) calibrate a 3D LiDAR with an omnidirectional camera also using a standard planar pattern. To calibrate the system, the sensors should observe the pattern from at least three different points of view. With this input, the extrinsic coefficients are calculated with a non-linear optimization technique. With the same purpose, Huang and Grizzle (2020) use a pattern of known dimension and geometry and estimates the pattern to LiDAR pose automatically using a fitting algorithm. Although all these works perform successful extrinsic calibrations between 3D range sensors and monocular cameras, pairwise calibration is a major shortcoming since most robotic systems present more than two sensors to be calibrated. In a system with more than two sensors, one would have to combine multiple pairwise calibrations. The problem is that the number of the combinations of pairwise calibrations required grow quickly. For example, Zhou, Li, and Kaess (2018) presente a system with a 3D LiDAR and a stereo camera system. However, to calibrate the three sensors (LiDAR and two cameras), two calibrations have to be performed: LiDAR to left camera, and LiDAR to the right camera. In the same way, with the purpose of fusing point clouds of multiple stereo cameras, Dhall, Chelani, Radhakrishnan, and Krishna (2017) use a 3D LiDAR to perform pairwise calibration with all the cameras in the system. Only after obtaining the transformation between the range sensor and each camera of the stereo system, the transformation between the stereo cameras can be found. Then the point clouds can be fused. Similarly, Kim and Park (2019) perform six pairwise calibrations between a 3D LiDAR and six monocular cameras mounted in an hexagonal plate that constitute an omnidirectional camera. To overcome this limitation, few works exist that consider multi-sensor, multi-modal calibration in a non-pairwise fashion. For example, Zuniga-Noel, Ruiz-Sarmiento, Gomez-Ojeda, and Gonzalez-Jimenez (2019) propose a method to estimate the extrinsic calibration between multiple sensors such as LiDARs, depth cameras and RGB cameras. The calibration procedure is separated in two parts: a motion-based approach that estimates 2D extrinsic parameters and a method that uses the observation of the ground plane to estimate the remaining ones. It is worth noting that this framework requires the robotic platform to be moving. Liao, Li, Ju, Liu, and Jiang (2017) propose a joint objective function to simultaneously calibrate three RGB cameras with respect to an RGB-D camera. Rehder, Siegwart, and Furgale (2016), propose an approach for joint estimation of both temporal offsets and spatial transformations between sensors. This approach is one of few that is not designed for a particular set of sensors, since its methodology does not rely on unique properties of specific sensors. It is able to calibrate systems containing both cameras and LiDARs. Pradeep, Konolige, and Berger (2014), present a joint calibration of the joint offsets and the sensors locations for a PR2 robot. This method takes sensor uncertainty into account and is modelled in a similar way to the bundle adjustment problem. The two major shortcomings of the state-of-the art in extrinsic calibration are: most of the methods perform pairwise calibration, which can be exhaustive for a robotic system with many sensors to be calibrated; and the majority of the works are focused on specific sensor modalities, rather than working in a more general way. To overcome these issues, this work proposes an extrinsic calibration framework with the following contributions: • Support for calibration of multiple sensors (i.e. N ⩾ 2 ); • The ability to handle multiple sensor modalities; • Calibration without changing the topology of the input transformation tree; • Integration of the system within the Robot Operating System (ROS) framework, with interactive tools to collect data, set the initial estimates, and visualize the calibration. Our previous works have focused on the calibration of intelligent vehicles. These platforms are often characterized by the large amount of sensors of different modalities mounted onboard. As such, these previous works presented a methodology based on atomic transformations for multi-sensor, multi-modal robotic systems (Guindel, Beltran, Martin, & Garcia, 2017; Oliveira, Castro, Madeira, Dias, & Santos, 2019; Oliveira et al., 2020). In this work, we extend our framework - Atomic Transformation Optimization Method (ATOM) 1 1 (Oliveira et al., 2019) - to also consider the calibration of 3D LiDARs along with the other supported modalities. Atomic transformations are geometric transformations that are not aggregated, i.e., they are indivisible. As such, this article presents the methodologies implemented to simultaneously calibrate a 3D LiDAR sensor with multiple cameras using a Bundle Adjustment optimization scheme (Agarwal, Snavely, Seitz, & Szeliski, 2010). As our approach is not focused on a single robotic platform, we present the calibration of a different robotic platform in comparison with our previous works - the agricultural robot AgRob V16 (de Aguiar, dos Santos, dos Santos, de Jesus Filipe, & de Sousa, 2020; Santos et al., 2019). The remainder of this paper is organized as follows: Section 2 details the optimization procedure and how it is cast as a Bundle Adjustment problem; Section 3 describes the ROS (Quigley et al., 2009) calibration setup, i.e., the steps from the robotic platform configuration until the data collection; Section 4 details the experimental results; and finally, Section 5 provides conclusions and future work. 2 Proposed approach ATOM is a calibration framework that simultaneously calibrates sensors of different modalities though the optimization of atomic transformations. This concept is supported by a well-defined optimization pipeline, that defines a set of optimization parameters and minimizes a cost function that supports different input modalities. This function f, which depends on the optimization parameters Φ , is known as the objective function. Our approach minimizes f to calibrate all the sensors of generic multi-modal robotic platforms simultaneously. In this process, the definition of a tree graph which contains topological information about the relationship between reference frames is required. In this tree, nodes are reference frames and edges correspond to the transformation between nodes. This data structure allows for the definition of unique paths between the graph nodes, i.e., enables an efficient retrieval of the transformations between any two frames in the tree. Fig. 1 represents the robotic platform to be calibrated (AgRob V16) with its respective referential frames, and the transformation tree considered for the calibration. The design of the transformation tree leads to the definition of the optimization parameters to calibrate the system. An extrinsic calibration can be viewed as a pose estimation problem, where the pose of each sensor is estimated. Thus, the set of parameters to optimize Φ , must together define the pose of each sensor. To perform such a calibration, we propose to maintain the initial structure of the transformation tree, calibrating only one atomic transformation per sensor. Since the system contains camera sensors, it is also possible to introduce the intrinsic parameters of each camera in the set Φ . In this way, the set of parameters will be composed of different modalities and so, there is the need to design an objective function that is able to characterize sensors of in a multi-modal fashion. As previously discussed, pairwise approaches for projecting the objective function result in complex graphs with many combinations of relationship definitions. For every existing pair of sensors, these relationships must be established according to the combined modality of the pair of sensors, which leads to a problem of scalability for which there is no solution in the literature. To solve this issue, we formulate our solution in Bundle Adjustment problem, in that the structure of the objective function is designed in a sensor to calibration pattern paradigm. Also, for every collection of data, the transformation that takes the corners of the calibration pattern to the world is optimized. In other words, the poses of the pattern are jointly estimated with the poses of the sensors. To perform this iterative procedure, the set of optimization parameters Φ must be initialized. The first guess for the pattern pose is obtained w.r.t. one of the cameras to be calibrated, resulting in a transformation cam i T p , where p denotes pattern and a T b represents the transformation from frame b to a. As will be detailed later on, our calibration framework allows the definition of an initial guess for the pose of each sensor and, consequently, for the transformations to be calibrated. With this definition, it is possible to compute the pose of any particular sensor j as an aggregate homogeneous transformation w A s j , obtained from the chain of transformations for that particular sensor present in the topological transformation graph: (1) w A s j = ∏ i ∈ R i T i + 1 = ∏ i ∈ K i T i + 1 · child T ^ parent · ∏ i ∈ L i T i + 1 , where child T parent represents the transformation to be calibrated, i T i + 1 for i ∈ K represent the prior links to the frame parent, i T i + 1 for i ∈ L the later to the frame child, and R is the set that contains all the frames present in the chain of transformation of sensor j. So, to obtain the homogeneous transformation from the pattern to the world, the following calculation is applied: (2) w T p = w A cam m · cam m T p , where p refers to the pattern, w states for world, and cam m for the mth camera sensor. So, the set of parameters Φ to be optimized is composed of the transformation represented in (2) along with the poses of each sensor to be calibrated, and, in the case of cameras, their intrinsics and distortion parameters: (3) Φ = x m = 1 , r m = 1 , i m = 1 , d m = 1 , … , x m = M , r m = M , i m = M , d m = M ︷ cameras , x n = 1 , r n = 1 , … , x n = N , r n = N ︷ LiDARs , . . . ︷ Other modalities , x k = 1 , r k = 1 , … , x k = K , r k = K ︷ Calibration pattern , where m refers to the mth camera to be calibrated, n states for the nth LiDAR to be calibrated, k refers to the pattern detection of the kth collection of data, x is a translation vector [ t x , t y , t z ] , r is a rotation represented thought the angle-axis parameterization [ r 1 , r 2 , r 3 ] (where the vector r is used to represent the axis and its norm represents the angle), i is a vector with each camera intrinsic parameters [ c x , c y , f x , f y ] , and d is a vector of each camera distortion coefficients [ d 0 , d 1 , d 2 , d 3 , d 4 ] . The intrinsic and distortion parameters of each camera can be initialized using any camera calibration toolbox, or in some cases, these parameters are also provided by the manufacture. The angle-axis parameterization was chosen because it has three components and three degrees of freedom, which means that it does not introduce more sensitivity than the one inherent to the problem itself (Hornegger & Tomasi, 1999), unlike the rotation matrix which has nine degrees of freedom for the also three components, or the euler angles that loose a degree of freedom when two axis are aligned. Using angle-axis representation, we have six optimization parameters per sensor that represent the pose of each one, i.e., the geometric transformation that will be calibrated. The optimization procedure, as will be explained, consists of the minimization of an objective function by the definition of residuals that are calculated as an error (in pixels for RGB cameras and in meters for 3D LiDARs) between the re-projected position of the calibration pattern, and the position of the pattern detected by each sensor. 2.1 Objective function To be able to consider multiple modalities in the same optimization process, we propose to structure the objective function F ( Φ ) as a composition of as many sub-functions f i ( . ) as desired modalities. The objective function F ( Φ ) from (4) is minimized using a non-linear least squares approach. 2 2 In this work we used the least-squares solver provided by Least-squares finds a local minimum of a scalar cost function, with bounds on variables, by having an m-dimensional real residual function on n real variables. As such, we choose this minimization approach as its is the best fit for our problem. So, for each new modality added to the calibration, a sub-function associated with it is designed and incorporated in F ( Φ ) , which allows for the minimization of the error associated with the pose of sensor of that specific modality. This is one of the reasons why the proposed approach is scalable. Thus, the optimization procedure can be defined as: (4) arg min Φ F ( Φ ) = arg min Φ 1 2 ∑ i ‖ f i ( { Φ i } ) ‖ 2 , where f i ( . ) is the objective sub-function for the i-th sensor considering the set of k optimization parameters { Φ i } . Thus, the final cost to be minimized is computed by the sum of the squared sub-function values for each set of optimization parameters. The value for all these sub-functions is a vector with the residuals associated to whit re-projection of the points of the calibrated pattern. For our use-case, the goal is to calibrate a stereo camera system (two cameras) and a 3D LiDAR sensor. So, the objective function is composed of the vector values of three sub-functions, two for the cameras and one for the 3D LiDAR. Each sub-function is detailed in the next sub-sections. 2.1.1 Camera modality sub-function When the sensors to be calibrated are cameras, their calibration is performed as a bundle adjustment (Agarwal et al., 2010), as described in our previous work, Oliveira et al. (2020). Thus, the created sub-function is based on the average geometric error corresponding to the image distance (in pixels) between a projected point and a detected one. So, the goal of the cost sub-function for camera sensors is to adjust the initial estimate for the intrinsic and distortion parameters, and position of the pattern corners, in order to minimize the re-projection error f cam , given by: (5) f cam = ‖ x c = 1 - x ̂ c = 1 ‖ . . . , ‖ x c = C - x ̂ c = C ‖ , where ‖ . ‖ represents the Euclidean distance between two vectors, c is the index of the pattern corners, x c denotes the ground-truth pixel coordinates of the measured points given by the pattern detection, and x ̂ are the projected points, given by the relationship between a 3D point in the world and its projection onto the image. To perform such calibration, 3D pattern points have to be found and re-projected onto the image plane. For each collection of data, the camera(s) to be calibrated capture the pattern. By knowing the real size of the pattern, and the size of each square that composes it, the 3D coordinates of the corners can be found in the local pattern reference frame. Then, each corner is located in the plane z = 0 , since the corners are in the XoY plane of the local pattern reference frame. Thus, each corner in the local pattern referential frame p p is transformed to the camera referential frame as follows: (6) p cam = cam T w · w T p · p p . Note that, p cam and p p are homogeneous vectors of the 3D corner coordinates in each reference frame, so that (6) is valid. Finally, to re-project each 3D corner from camera’s reference frame p c = i cam to the image plane, taking into account each camera intrinsic and distortion parameters, the pinhole camera model (Sturm, 2014) is used: (7) x ̂ c = i = K · p c = i cam ″ , where K is the matrix that contains the intrinsic parameters i and, (8) p c = i cam ′ = p c = i cam · 1 z cam = x cam z cam , y cam z cam , 1 T , (9) p c = i cam ″ = x cam ′ · ( 1 + d 0 l 2 + d 1 l 2 + d 4 l 6 ) + 2 d 2 · x cam ′ y cam ′ + d 3 · ( l 2 + 2 x cam ′ 2 ) y cam ′ · ( 1 + d 0 l 2 + d 1 l 2 + d 4 l 6 ) + d 2 · ( l 2 + 2 y cam ′ 2 ) + 2 d 3 · x cam ′ y cam ′ , where l = ( x cam z cam ) 2 + ( y cam z cam ) 2 , and d j is the jth component of the distortion vector d . From (6)–(9), it is possible to conclude that all the desired parameters to optimize are being considered: the pattern to world transformation w T p present in (6) that can be computed as the inverse of (2); the world to camera transformation cam T w also present in (6); and finally, the intrinsic i and distortion parameters d considered in (7)–(9). The use of these parameters to project the 3D corners in the image plane, together with the minimization of the geometric re-projection error, lead to the parameter configuration that optimize the sub-function f cam . Thus, it is expected that the re-projected points become closer to the ground-truth corners during the optimization. Fig. 2 shows the difference between the initial position of the pattern corners, and the final position of these same projected points, after the optimization has been completed. It is possible to observe that the pixels corresponding to the projection of the final position of the points (dots in Fig. 2) almost perfectly match the ground-truth point (squares in Fig. 2). 2.1.2 3D LiDAR sub-function For the case of 3D LiDARs, the sub-function f lidar considers two types of residuals: orthogonal distance and limit points distance. To compute both, this approach also uses the calibration pattern and, in specific, its boundary points. As will be detailed later on, our calibration framework has a semi-automatic labelling procedure that allows to save, for each collection of data, the LiDAR 3D points that are on the pattern. As in case of the camera sensor, this approach formulates the cost sub-function by minimizing the residuals w.r.t. some ground-truth. Here, the ground-truth 3D points are, once again, generated in the pattern reference frame by knowing the three dimensional structure of the pattern, such as its height and width. Thus, by knowing the size of the pattern, the size of each pattern square, and the pattern origin (bottom left corner), the coordinates of the boundary points defined in the local pattern’s reference frame are computed. It is important to note that, the size of the board between the pattern grid and the end of the physical pattern had to be measured so that this step could be implemented. Also, as explained before, all the calculated pattern limit points have coordinate z = 0 , since the pattern’s reference frame is in the XoY plane. After calculating the ground-truth boundary points of the pattern, two things are required: the pattern boundary points observed by the 3D LiDAR sensor and the homogeneous transformation that converts 3D points from the LiDAR referential frame to the local pattern reference frame. Given a set of labelled 3D LiDAR cartesian points on the pattern p lidar = x c = i , y c = i , z c = i , the boundary points are calculated using a spherical parameterization for each 3D point. After computing the spherical coordinates of each 3D LiDAR point on pattern p s , lidar = r c = i , θ c = i , ϕ c = i , two limit points are calculated considering the set of 3D LiDAR points on pattern belonging to a given horizontal scan of the original point cloud. As the labelled set of 3D LiDAR points on pattern is an unordered point cloud, the horizontal scans are computed by clustering the points considering their θ value. So, points with the same θ value belong to the same horizontal scan. Finally, to extract the two limit points per horizontal scan, the ϕ component maximum and minimum values of each set are computed, resulting in the two most distant points, corresponding to points in the pattern boundaries. The result of this procedure is represented in Fig. 3 . The final step before computing the residuals that constitute the cost sub-function f lidar is to convert the set of labelled 3D LiDAR points on pattern, as well as the computed boundary points, to the patterns’ reference frame. This is done using the homogeneous transformations computed in (1) and (2): (10) p p = p T w · w A lidar · p lidar , where p T w is the transformation matrix from the world to the pattern reference frame, and w A lidar the transformation matrix from the world to the LiDAR sensor reference frame. Similarly to the cameras’ case, (10) shows that the optimization parameters include the sensor pose and the pattern pose. After this, the two residual types can be computed. The first, orthogonal distance, is the absolute z value of the coordinates of the projected 3D LiDAR points. As they are on patterns’ referential frame, it is intended that their z coordinate is zero. Therefore, any value different from zero means that the optimization parameters (sensor pose and pattern pose) are not yet correct. The second residual type is the Euclidean distance of x and y components between the ground-truth pattern boundary points, and the LiDAR 3D points on pattern boundary calculated as described before. For each LiDAR point on the pattern boundary, the residual is computed as the distance between x and y coordinates, in the calibration pattern frame, of the respective LiDAR boundary point and the closest point that belongs to the limit of the physical board that is being detected. This being said, the 3D LiDAR cost sub-function is as follows: (11) f lidar = z l = 1 lidar , p , ‖ p q = 1 , xy boardlimit - p q = 1 , xy p ‖ , . . . , z l = L lidar , p , ‖ p q = Q , xy boardlimit - p q = Q , xy p ‖ , where z l = i lidar , p is the z coordinate of the ith 3D LiDAR point projected into the patterns’ referential frame, p q = j , xy boardlimit are the x and y coordinates of the jth 3D LiDAR boundary point on the same referential, and p q = j , xy p are the x and y coordinates of the corresponding ground-truth boundary point. Fig. 4 shows the ground-truth pattern boundary points representation (blue lines on the left of Fig. 4), the calculated boundary points at the start of the calibration procedure (blue circles on the middle of Fig. 4), and the result of the optimization procedure with the ground-truth points and the boundary points observed by the LiDAR aligned (right representation on Fig. 4). 2.2 Normalization of multi-modal residuals We propose a full calibration method where sensors of different modalities contribute to a global vector residual of residuals. While the camera sub-function provides a set of residuals that are expressed in pixels, the LiDAR sub-function provides a set of residual expressed in meters. This mismatch in units of measurements may display highly disparities in error magnitudes, which could result in unwanted behaviours in the optimization processes due to differences in scale. For example, a residual of 1 pixel has higher influence (or weight) in the optimization path than a residual of 0.5 meters. Yet, our knowledge about the system tells us that the opposite should be considered. As result, the parameters that influence the residuals with higher scale will dominate the optimization, while the other parameters are perceived to already be close to their optimal state. To handle the different scales in multi-model residuals in Eq. 4, we employ a normalization factor to the optimization. Let C = { c } be the set of existing residual classes (e.g C = { pixels , meters } ) and c ( i ) ∈ C is the residual class for the ith sensor, then the optimization is defined as (12) arg min Φ F ( Φ ) = arg min Φ 1 2 ∑ i f i ( { Φ i } ) η c ( i ) 2 , where η c ( i ) is the normalization factor for residuals created by the sensor sub-function f i . Note that the normalization factor η c ( i ) is defined per residual class and not per sensor. The normalization values per class are given by the arithmetic mean of the same class residuals before the optimization. For example, the normalization for the class pixels, with η c ( i ) = η pixels , is given by (13) η pixels = 1 n ∑ j ‖ f j ( { Φ j } ) ‖ 1 : ∀ f j ∈ { pixels } , where n is the total number of residuals that are part of the considered class and ‖ . ‖ 1 is the L1 norm. Note that the normalization factors are constant values during optimization, calculated once with the residuals that result from the initial guess. 3 Calibration framework The ROS (Quigley et al., 2009) has become the standard framework for the development of robotic solutions. As referenced before, the proposed calibration procedure requires the creation of a transformation tree, from which atomic transformations are optimized. For this purpose, ROS provides a tree graph referred to as tf tree (Foote, 2013). With this tool, it is possible to define a data structure as the one present in Fig. 1. Also, the Robot Operating System Visualization (RVIZ) tool supports additional functionalities, such as robot visualization, collision detection, etc. In fact, this visualization procedure is interactive, in that if any transformation between two links changes, the robotic platforms and sensors affected by these links change its pose accordingly. This interactive procedure is possible since the optimizations’ cost function always recomputes the aggregate transformations. Therefore, a change in one atomic transformation in the chain affects the global sensor pose, and consequently, the error to minimize. So, if atomic transformations change due to the calibration procedure, the tf tree will automatically adjust the robots and sensors poses accordingly. It should be emphasized that, due to all these functionalities, the calibration procedure should not change the structure of the tf tree. Our approach preserves the predefined structure of the tf tree, since, during optimization, only the values of some atomic transformations contained in the chain are estimated, securing the topology of the tree. To the best of our knowledge, our approach is one of few which maintains the structure of the transformation graph before and after optimization. Given all of the above, we state an extensive integration with ROS as a key component of the proposed approach. The ROS calibration framework is segmented in five main components: configuration, initial estimate, data labelling, data collection, optimization procedure. Each will be described in detail in the following sections. 3.1 Calibration configuration The configuration defines the parameters which will be used throughout the calibration procedure, from the definition of the sensors to be calibrated to a description of the calibration pattern. The proposed approach, detailed in Section 2, is based on the optimization of atomic transformations. These were combined through the use of the topological information contained in a tree. The transformation tree is generated from a ROS Unified Robot Description Format (URDF). Additional information must be given to define which, out of the set of atomic transformations, will be optimized during the calibration procedure. Also, a description of the calibration pattern must be provided. All this information is defined in a calibration configuration file. 3.2 Initial parameter estimation Optimization procedures suffer from the known problem of local minima. This problem tends to occur when the initial solution is far from the optimal parameter configuration, and may lead to failure in finding adequate parameter values. To avoid this, the setup of the a plausible initial guess for the entire parametric optimization system is essential. Our approach supports different modalities of parameters, as stated in (3). Thus, each modality requires a specific type of initialization. For cameras intrinsic i and distortion d parameters, the initialization is performed using any state-of-the-art camera calibration toolbox, or using the calibration provided by the manufacture. To initialize the transformations of sensors in general, we developed an interactive tool which parses the configuration URDF file and creates a 3D visualization tool for ROS interactive marker associated with each sensor. Fig. 5 shows an example of the developed tool. Here, we can see the user changing each sensor reference frame, dragging the respective interactive markers. With this tool the user can move and rotate the markers relative to each sensor. This provides a simple, interactive method to easily generate plausible first guesses for the poses of the sensors. Immediate visual feedback is provided to the user by the observation of the 3D models of the several components of the robot model and how they are put together, e.g. where each camera or LiDAR is positioned w.r.t. the vehicle. Also, for multi-sensor systems, it is possible to observe how well the data from a pair of sensors overlap. An example of this procedure can be watched at Concerning the atomic transformations associated with the calibration pattern w T p present in (2), these are initialized by defining a new branch in the transformation tree which connects the pattern to the frame to which it is fixed. For example, for AgRob V16 case, w T p is estimated through (2) where cam m T p is estimated solving the Perspectiva-n-Point (PnP) for the detected pattern corners (Gao, Hou, Tang, & Cheng, 2003; Fabbri, Giblin, & Kimia, 2020; Penate-Sanchez, Andrade-Cetto, & Moreno-Noguer, 2013), and w A cam m by deriving its topology from the tf tree and using the initial values for each atomic transformation in the chain. 3.3 Labeling data The labeling of data refers to the annotation of the portions of data which captures the calibration pattern. A labeling procedure is executed for the data of each sensor, and can be automatic, semi-automatic or even manual in some cases. The information that is stored depends on the modality of the sensor, but for cameras it is always the pixel coordinates of the corners observed in the pattern. The standard calibration pattern that is used for camera calibration is a chessboard pattern. The images are labelled using one of the many available image-based chessboard detectors (Czyzewski, 2017). Our system is also compatible with charuco boards (Garrido-Jurado, Muñoz-Salinas, Madrid-Cuevas, & Medina-Carnicer, 2016). These have the advantage of being able to detect the pattern even when it is partially occluded. Also in this case we make use of off the shelf detectors, (e.g. Romero-Ramirez, Muñoz-Salinas, & Medina-Carnicer, 2018; Hu, DeTone, & Malisiewicz, 2019). For the calibration of AgRob V16, a labeling algorithm for 3D LiDARs was developed. This method is semi-automatic and is initialized by a setup of a seed point. To label the pattern points viewed by the 3D LiDAR, the user drags an interactive marker to a point located in the pattern - the seed point. This constitutes the non-automatic stage of the procedure. After that, the algorithm clusters a set of points (that are intended to belong to the pattern) using an Euclidean distance threshold computed using the a priori known dimensions of the pattern. Despite being simple and fast, this approach reveals lack of precision, since it includes many outliers in the labeling procedure. The pattern used is rectangular. Thus, the Euclidean distance threshold has to be higher than the smaller side of the rectangle. This means that, if the pattern is close to another object, points from this object will be labeled as pattern points. To overcome this issue, a Random Sample Consensus (RANSAC) (Fischler & Bolles, 1981) algorithm is executed to fit the set of labeled points in a plane (the pattern plane), eliminating the outliers. RANSAC is an iterative algorithm, and it is performed a maximum number of times M. To find points that belong to the plane, the point to plane distance is computed in each iteration i for each point j as (14) D ij = | a i x j + b i y j + c i z j + d i | a i 2 + b i 2 + c i 2 where p j = [ x j , y j , z j ] T is the jth point on the cluster. With this, a point is considered as inlier if its distance to the plane D ij is smaller than a given threshold D threshold . The final set of inliers corresponds to the one found in the iteration i that gives the higher number of points belonging to the plane. Fig. 6 shows an example of the labeling procedure for cameras and 3D LiDARs proposed in ATOM. It is worth noting that, our approach works with partial detections, as represented in the figure. This interactive data labeling procedure is showcased in 3.4 Collecting data In most robotic systems, the data coming from the sensors is streamed at different frequencies. However, to compute the associations between the data of multiple sensors, temporal synchronization of the sensor data is required. Of course, this only becomes an issue when calibrating multi-sensor robotic systems. For now, the synchronization problem is solved trivially by collecting data (and the corresponding labels) at user defined moments in which the scene has remained static for a certain period of time. In static scenes, the problem of data de-synchronization is not observable, which warrants the assumption that for each captured collection the sensor data is ‘adequately’ synchronized. This can be done using, for example, a tripod to held the pattern before collecting each snapshot of data (Rehder et al., 2016; Furgale, Rehder, & Siegwart, 2013). In this work, the problem is approached in a simpler way, where the pattern is hold by the user, as shown in Fig. 2, remaining static by sufficient amount of time to ensure the synchronization between all the sensors. To save the scene data captured by all the sensors in the calibration system, the user can do it with just two mouse clicks on the interactive ROS-based tool (on RVIZ) developed. We refer to these recordings of data as data collections. Each one of them contains the values of all atomic transformations that exist in the system at a given timestamp, a copy of the robot configuration file, sensor data and labels, and high level information about each sensor, such as the topological transformation chain, extracted from the transformation tree. This information is stored in a dataset file that will be read by the optimization procedure afterwards. Also, a video showing the procedure for collecting data is provided for AgRob V16 calibration here It should be pointed out that, the set of collections should contain as many different poses as possible. As such, collections should preferably have different distances and orientations w.r.t. the calibration pattern, so that the calibration returns more accurate results. This concern is common to the majority of calibration procedures. 3.5 Visualizing the optimization The immediate visualization of the calibration is essential for several reasons: it provides the user the necessary data so that he can detect failures on the calibration, such as outlier data collections; it gives feedback about the cost function residuals minimization, which can serve to detect possible local minima, and to make sure that the optimization procedure is converging. Fig. 7 shows the three main visualization features of ATOM. Our calibration framework provides a simultaneous visualization of all the data collections, as well as immediate feedback of the alignment between ground-truth points and labeled points for optimization, images with the reprojection, robot meshes, the position of the reference frames, etc. Also, optimization graphics are provided with residuals values and the total error for each iteration. This configuration is similar to the standard one which is used during the initial parameter estimation, the data labeling and collection, but contains a couple of key distinctions. As mentioned above, the calibration procedure uses a dataset file which contains information about each of the stored collections. These collections contain data gathered in a a set of sequential instants in time. The ROS calibration configuration publishes data from all collections simultaneously, as if those time instants were packed together and processed as if they had occurred all at the same time. Collisions in topic names and reference frames are avoided by adding a collection related prefix to each designation. Also, the original transformation tree is replicated for each collection. A video with an example of a calibration execution for AgRob V16 is provided 4 Results To test and validate the performance of the proposed approach, an extensive evaluation procedure was developed. Our calibration framework, ATOM, was used to calibrate three configurations of the AgRob V16 sensing system. Two of them were pairwise calibrations between two cameras of a stereo system, and a single camera and a 3D LiDAR, which we denote as ATOM pairwise. The third was a calibration between all three sensors (two cameras and 3D LiDAR), which we call ATOM full, where results for particular pairs of sensors are obtained using a full calibration. In this procedure, three datasets were used, as represented in Table 1 . Two of them (train-1, train-2) were used for training, i.e., to perform the calibrations, and the third one (test-3) was used to test the calibration with specific metrics that will be detailed later on. The datasets contain incomplete and partial collections, i.e., collections where the pattern is not detected for all the sensors, and collections where the pattern is only partially visible, respectively. This section is divided in two parts: the evaluation of ATOM’s performance against state-of-the-art approaches, such as OpenCV stereo calibration (Bradski, 2000), and ICP point cloud alignment (Besl & McKay, 1992); the characterization of ATOM w.r.t. several characteristics of the datasets, such as the number of incomplete/partial collections, and the accuracy of the initial guess. Table 2 makes a summary of the calibration experiments that were carried out. OpenCV and the stereo camera factory calibration were used to get the camera-to-camera extrinsic calibration, and ICP was explored to get the camera-to-LiDAR calibration. This last calibration was obtained by the alignment of the 3D LiDAR point cloud, and the 3D point cloud provided by the stereo camera software development kit. Two versions of ICP were used as camera-to-LiDAR extrinsic calibration: the one corresponding to the collection where the fitting of point clouds was more accurate, and the average of the transformations obtained in all collections. Finally, as referenced before, ATOM was calibrated both in pairwise and full modes, and both approaches are evaluated. 4.1 Methodology One of the key characteristics of our evaluation procedure is the use of separate datasets to perform the calibration and generate the results. As discussed in Section 3, ATOM provides a data collection procedure, where datasets are generated and visualized on RVIZ. Datasets are composed of several collections, each one containing data of all the sensors, initial atomic transformations, pattern labelled points, and other information. To evaluate our calibration framework, two datasets where initially collected - train-1 and train-2. Then, three calibrations were executed over each one of the datasets, two pairwise and one using all the sensors to be calibrated in AgRob V16 system. These calibrations generate a json file similar to the one generated at the end of the data collection procedure, but with the calibrated atomic transformations. After obtaining all these calibration configurations, a third dataset was recorded - test-3. It was used to evaluate the accuracy of the calibrations obtained. Using the metrics that will be described later on, the chain of transformations of each calibration was loaded and used to compute errors using the labelled data of the test dataset. In this way, possible influence of using the same data to calibrate and test is eliminated, and a more rigorous evaluation is achieved. Unlike ATOM, both OpenCV and ICP perform sensor-to-sensor calibration, instead of calibrating atomic transformations without changing the topology of the chain of transformations. Thus, to evaluate these methods in the same way ATOM is evaluated, the atomic transformations set to be calibrated had to be recovered from the sensor-to-sensor calibrations. This problem is formulated in Fig. 8 . Let link 2 T base be the entire chain of transformations from the base link to the data link of the anchored sensor, T ̂ be the sensor to sensor calibration obtained, parent 1 T base be the chain of transformations from the base link to the parent link of the atomic transformation to be calibrated child 1 T parent 1 , and link 1 T child 1 the chain of transformation from the atomic transformation child link, to the non-anchored sensor data link. The entire chain of transformations relationships can be formulated as follows: (15) T ̂ · link 2 T base = link 1 T child 1 · child 1 T parent 1 · parent 1 T base . So, from (15), we can extract the atomic transformation of the non-anchored sensor as follows: (16) child 1 T parent 1 = ( link 1 T child 1 ) - 1 · T ̂ · link 2 T base · ( parent 1 T base ) - 1 . The advantage of this approach is that it is generic. For example, from OpenCV, a camera to camera metric is obtained. Thus, the procedure consists in anchoring one of the cameras, and use the obtained transformation to recover the atomic transformation marked for calibration in the original ATOM configuration. In the same way, ICP provides a camera to LiDAR calibration. Once again, we anchor one of these sensors, and apply the exact same routine to extract the non-anchored sensor atomic transformation to be calibrated. In this way, we are able to obtain a calibrated system without changing the initial chain topology, which allows the direct comparison of these state-of-the-art approaches with ATOM, using exactly the same metrics. These metrics are described in the next section. 4.2 Metrics To evaluate the camera to camera calibration performance, the methodology used is based on three different metrics: the mean rotation error (rad), the mean translation error (m), and the reprojection error (px). To compute the reprojection error, the idea is to use the calibration result to project the detected pattern corners of one camera image into the image of the second calibrated camera image and compare the projected pixel coordinates with the ground truth pattern corners coordinates in the image of the anchored camera. To transform pixels from one camera into another, we start from projecting the 3D world coordinates of the pattern corners into the image of a camera, using (6)–(7). Since the 3D pattern corners are defined in the local pattern reference frame, they all lie in the plane z = 0 . Thus, (6)–(7) can be simplified to the following: (17) p cam = K · cam T p ′ · p p ′ , where cam T p ′ is a portion of the matrix cam T w · w T p , without the component z of the rotation, as follows: (18) cam T p ′ = r 11 r 12 t x r 21 r 22 t y r 31 r 32 t z , and p p ′ is the pattern corner, represented as a vector in its homogeneous form, without the z component, i.e., p p ′ = x y 1 T . Using the fact that the 3D coordinates of the pattern’s corners are the same for both cameras, (17) can be applied to the two of them, so that we can find a relation between both expressions. This resulted in the following formulation: (19) p cam 2 = K cam 2 · cam 2 T p ′ · cam 1 T p ′ - 1 · K cam 1 - 1 · p cam 1 , where cam 1 and cam 2 refer to the cameras that were calibrated. This formulation provides the relationship between pixel coordinates of the pattern corners in both camera images. However, (19) requires the camera to pattern transformation matrix for both cameras. This can be a problem since, some approaches, unlike ATOM, do not estimate the camera to pattern transformation while performing the camera to camera calibration. In addition to this, ATOM estimates these transformations for a training dataset. So, the usage of a test dataset to evaluate all the frameworks, denies the use of the estimated pattern pose from ATOM. To overcome this, the pattern pose w.r.t. one of the cameras cam 1 T p is computed using the PnP algorithm. Then, using the output json file from each calibration, we recover the camera to camera transformation cam 1 T cam 2 , through the chain of transformations. In this manner, it is possible to determine the transformation of the other camera to the pattern, as follows: (20) cam 2 T p = cam 1 T cam 2 - 1 · cam 1 T p . From this expression, we can derive cam 2 T p ′ and cam 1 T p ′ , and successfully project pixels from one image into the other. With this information, the reprojection error is computed as follows: (21) e xy = p projected - p expected . From (21), the error can be decomposed in its x and y components. Also, considering the reprojection error for all the N collections, the root mean square error is calculted as follows: (22) e rms = 1 N ∑ e xy 2 . Fig. 9 illustrates a resulting corner reprojection from one camera into the other using the ATOM full calibration. For the calculation of the mean rotation and translation errors to evaluate the camera to camera calibration, we consider the following observation: the chain of transformations from the base link to the pattern reference frame that passes from each one of the calibrated cameras should be equal. This happens since the calibration pattern pose in reference with the base link is fixed, and any chain of transformations that link these two referentials should represent the same spatial relationship. Thus, the difference in rotation and translation can be quantified, assessing the inequality between the two chains of transformation. Once again, this formulation requires the pattern pose w.r.t. each one of the cameras, that is again extracted solving the PnP problem. With this information, we can state that (23) base R cam 1 base t cam 1 0 1 cam 1 R p cam 1 t p 0 1 = base R cam 2 base t cam 2 0 1 cam 2 R p cam 2 t p 0 1 . Now, we can define the rotation and translation difference as (24) Δ R = base R cam 1 · cam 1 R p - 1 · base R cam 2 · cam 2 R p (25) Δ t = base R cam 1 · cam 1 t p + base t cam 1 - base R cam 2 · cam 2 t p - base t cam 2 Finally, we can define the mean rotation error as (26) e R = 1 N ∑ i | | angle ( Δ R i ) | | , where angle is the angle-axis representation of the rotation, and the mean translation error as (27) e t = 1 N ∑ i | | Δ t i | | . To evaluate the 3D LiDAR to camera calibration, we used the reprojection error (px) and its corresponding root mean square error (px) considering all the test collections N. In this case, the mean rotation and translation errors were not used due to the difficulty of estimating the pattern pose w.r.t. the LiDAR sensor with precision. The process of calculating the reprojection error to evaluate the camera to LiDAR calibration consists in three main steps: 1. Label the pixels that belong to the boundaries of the pattern in the image. 2. Reproject the pattern boundary points in the LiDAR’s referential frame (detailed in Section 2.1) p boardlimit into the image. 3. Compute root mean square between labeled and projected points. Fig. 10 shows these steps. An annotation tool was developed to perform the labelling. This tool allows the user to manually annotate individual points corresponding to four classes, each one representing one side of the pattern in each image. Then, in order to account for the image distortion, we approximate each one of the pattern sides by a polynomial, fitting the labelled points. In this step, a simple linear regression would not suffice because images have distortion which transforms straight ines into curves. So, a polynomial is more suitable for modeling this phenomenon. Figs. 10a and 10b show these two steps. After having the annotations for all the images of the camera to be calibrated in the test dataset (test-3), the reprojection error is calculated. To do so, the 3D LiDAR labelled points that belong to the pattern boundaries are reprojected into the image using (6)–(9). So, for each collection, the error between each projected point and the closest ground truth point belonging to one of the polynomial curves is calculated as in (21). Fig. 10c shows an example of the reprojection result and the corresponding error for each point. Considering the reprojection errors calculated for each one of the N collections, the root mean square error is also calculated using (22). It should be emphasised that, all of these metrics are publicly available, and are integrated in the ATOM software framework. A specific package called ATOM evaluation was created and can be easily used by the user to evaluate the calibrations performed. 4.3 Evaluation The evaluation procedure applies the previously described metrics to compare ATOM with state-of-the-art calibration methods. Note that these state-of-art methods are pairwise and as such not able to calibrate the entire system simultaneously. The comparison with ATOM, which is a general, full calibration approach, against specialized pairwise methods is not entirely fair. However, since the nature of all the metrics used in the evaluation is also pairwise, it is ATOM that is at a disadvantage in comparison with the other methods. For the camera-to-camera calibration scenario, two versions of ATOM were calibrated in two diffeernt training datasets, and evaluated in the same test dataset. The first is a pairwise calibration between both cameras, and the second a full calibration of the entire AgRob V16 system, with the same two cameras and a 3D LiDAR. It is worth noting that, the calibrations performed consider each camera intrinsic parameters, as well as the pairwise extrinsic calibration between them. To compare ATOM with the state of the art, the OpenCV stereo calibration toolbox (Bradski, 2000) was used to calibrate exactly the same configuration. Additionally, the factory’s intrinsic and extrinsic calibrations were evaluated. Table 3 summarizes all these experiments. Starting by the analysis of ATOM pairwise and ATOM full, we can see that both versions present similar performances, with marginal differences with respect to all the metrics calculated. For example, for the train-1 dataset, we can see a root mean square error difference of 0.075 px and for train-2 0.046 px. Thus, we can conclude that, ATOM allows to optimize an entire robotic system without a significant loss of performance, when comparing with a specific pairwise calibration between the two sensors of interest. In what concerns OpenCV, Table 3 shows that, for the train-1 dataset, it is not able to calibrate. This happens since this framework requires collections where all the pattern corners are detected, i.e., non partial detections. So, since the majority of the collections present in this dataset are partial, OpenCV is not able to calibrate. This is a limitation since, to accomplish a dataset without partial collections, its variety can be limited due to the impossibility of collecting, e.g., collections with the pattern far away from the cameras. On the other hand, for the train-2 dataset, OpenCV achieves the smaller reprojection root mean square error. In this, the number of partial collections is low, and OpenCV, a specialized pairwise calibrator for cameras, performs an accurate intrinsic and extrinsic calibration. Even so, ATOM full shows errors only slightly higher than OpenCV for this dataset, showing that it is capable of achieving a state-of-the-art performance, even considering a non pairwise approach. Concerning the camera factory calibration, it is clear that it presents the less accurate calibration. This can be explained since the calibration the same for all the equipments. Finally, to analyse the impact of the ATOM’s calibration, Fig. 11 shows the dispersion of the reprejection error per collection, before and after calibrating with ATOM full. As expected, the dispersion of the error before calibrating is higher in almost all collections. On the contrary, after calibrating the cameras with ATOM full, the dispersion drastically reduces, with the exception of one collection (represented in brown). This collection can represent a degenerate of data collection, due to, for example, de-synchronization of the data from from the sensors. In order to evaluate the camera to LiDAR calibration, ATOM was used to calibrate both modalities in three different manners: two ATOM pairwise versions, one between the LiDAR and each camera, and the ATOM full version that comprises all three sensors. To compare our approach with the state-of-the-art, ICP (Besl & McKay, 1992) was used to calibrate the left camera and the LiDAR in two different ways: one considering the average of the calibration obtained in all the collections, and other considering only the collection that presents the best alignment between the two point clouds. Note that, ICP only calibrates the left camera w.r.t. the LiDAR since the stereo camera point cloud extracted directly using the manufacture’s API is defined in this camera referential. Table 4 summarizes all this information. Similarly to the camera-to-camera case, here we can verify that ATOM pairwise and ATOM full result in a similar calibration performance, with marginal reprojection error differences. So, once again, this leads to the conclusion that ATOM full can be used to calibrate all the robotic system without any significant loss of performance while evaluating calibrations between pairs of sensors. In this set of tests, a consistent decrease of performance of all the calibration configurations from train-1 to train-2 dataset is observed. This consistency can be caused by synchronization errors between sensors while collecting the calibration data. Looking for the ICP performance on train-1 dataset, firstly, we can conclude that the ICP average is highly affected by outliers, i.e., collections where the calibration fails. This can be inferred by the high standard deviations present in the x and y reprojection error compoenents. ICP best, despite being significantly less accurate than ATOM, presents a better performance than ICP average. The overall bad performance of ICP can be explained by the difficulty of aligning a dense point cloud (provided by the stereo camera), and a sparse one (provided by the laser). It is worth noting that, the train-2 dataset does not contain the stereo camera point cloud, so here ICP can not be used for calibration. To have a visual perception of the ATOM full performance on the calibration of the left camera and the LiDAR, Fig. 12 shows the reprojection of the LiDAR 3D points in the left camera image. In this Fig., color represents the points depth. Here, the transitions of the objects can be sharply observed, which is a good indicator for the calibration performance. 4.4 Impact of the number of collections used for training One of the major questions in general for calibration procedures is the minimum amount of data required to calibrate sensors with precision. In this section we propose an evaluation of ATOM full calibration using different numbers of training collections. The calibration of the three combinations of sensors is evaluated for five different levels of collections used. Table 5 presents the results obtained for each configuration. Starting by the analysis of the camera-to-camera calibration, here we can see that the increase of the number of collections leads to a increase in performance. Using a single collection, as expected, results in higher reprojection, rotation and translation errors. While increasing the number of training collections, the performance increases, with the best performance being observed with the maximum number of collections. Looking at the performance of the calibration of the LiDAR with both cameras, we can see that, as expected using a single collection also results in a higher reprojection error. However, in this case, this difference is not significant, and while increasing the number of collections, the performance saturates. Thus, we can conclude that, the increase of the number of collections has a positive impact in the final performance of all the calibration configurations. However, the camera-to-camera calibration is more sensible to the lower number of collections than the camera-to-LiDAR calibration. 4.5 Impact of the number of incomplete collections used for training The proposed calibration framework, ATOM, supports collections where the pattern is not detected by all the sensors in the calibration system. For example, suppose that the pattern is, for a specific collection, is viewed by the right camera and the LiDAR but not by the left camera. The current section intends to evaluate the impact of this type of collections, and conclude if the presence of incomplete collections has, or not, correlation with changes on ATOM’s performance. Table 6 summarizes the results obtained for three sensor configurations with four different values of incomplete collections, maintaining the same number of training collections. ATOM can deal with incomplete collections, as long as the pattern is detected by at least one sensor. If not, the calibration system can not compute any residual, and that collection must be discarded. On the other hand, if, a single sensor does not detect the pattern for a specific collection, this leads to a reduction of the number of residuals used on the optimization procedure. This being said, results do not show any correlation between the increase of the number of incomplete collections and the performance of ATOM. So, this leads to the conclusion that ATOM can deal with the decrease of the number of residuals (as long as sufficient number of collections are provided). This conclusion consistent with the one taken from Table 5, where it was shown that, ATOM can calibrate accurately for a reasonable low number of collections. 4.6 Impact of the accuracy of the initial estimate The initial estimate (or initial guess) of the calibration parameters has an impact in the outcome of the optimization. A good initial estimate provides a sufficient approximation that allows the optimization process to find the optimal solution that best represents the real calibration of the system. In turn, a inaccurate estimate may lead the optimization process to an unrecoverable state where it is not possible to achieve the optimal solution. This is known as the problem of local minima. In this section, we are interested in assessing the robustness of ATOM to the accuracy of the initial estimate. More specifically, the angle and distance error to the optimal state that our method can handle and the additional execution times that result from said errors. To characterize the robustness to the angle error, we start with the optimal angle and then add the angle error to the Euler components of the rotation. The sign of the error is provided by a fair binomial sampling (Girshick, Mosteller, & Savage, 2006). The tolerance to the distance error is found by sampling an uniform offset from the optimal positions that sits on a sphere with a radius equal to the distance error. Because random sampling is used, we run the experiment for each error 10 times and the reported values are the mean of the runs. By increasing the error in several steps, we can pinpoint the error at which our optimization process will fail. Note that the error is applied to all pose parameters that are being estimated. The results are presented in Fig. 13 . The obtained results show that our optimization process can handle an angle error of approximate 20 degrees and a distance error of 0.5 meters, for each sensor. In our opinion, these errors provide a sufficient margin of tolerance for a practical usage of our manual procedure for initial parameter estimation, described in Section 3.2. The execution times are strictly related to the convergence of the optimization. Inside the margin of tolerance, the execution times are mostly constant (with some fluctuations). This means the optimization process adequately handles the imposed error with a proper convergence. 5 Conclusions This paper solves the problem of camera-to-LiDAR calibration using the optimization of atomic transformations. To do so, this work formulates the calibration as a Bundle Adjustment problem, minimizing the reprojection error of sensors that can have different modalities. Our approach, ATOM, provides several advantages when compared with the current state-of-the-art: (i) it offers a framework to simultaneously calibrate any number of sensors; (ii) it improves the optimization of different sensor modalities by introducing the multi-modal normalization. (iii) it maintains the topology of the input transformation tree; (iv) it supports incomplete and partial collections of data, which makes the detection procedure more flexible and robust; (v) it uses a common calibration pattern, which generalizes the approach; (vi) it has seamless integration with ROS, setting a complete framework for camera to LIDAR calibration. Results show that the proposed approach presents similar performance in comparison with the state-of-the-art, even calibrating the entire robotic system simultaneously. These results demonstrate that ATOM can achieve the same performance of specialized methods in pairwise calibration between specific sensors, while running a complete calibration with multiple sensors of different modalities. Furthermore, our framework proved to be robust to inaccurate initial guesses and small number of collections. Finally, the use of a generic calibration pattern constitutes a major advance since, in the current state-of-the-art, many approaches use built in–house patterns. Future work aims to test ATOM in more advanced robotic systems, with multiple 3D LiDARs. Additionally, the problem of data synchronization while collecting data for calibration will be addressed thought the use of simple concepts such as data interpolation, and more advanced ones, such as generative adversarial networks to generate synchronized sensor data. CRediT authorship contribution statement André Silva Pinto de Aguiar: Conceptualization, Methodology, Software, Writing - original draft. Miguel Armando Riem de Oliveira: Conceptualization, Methodology, Software, Writing - review & editing. Eurico Farinha Pedrosa: Conceptualization, Methodology, Software, Writing - review & editing. Filipe Baptista Neves dos Santos: Writing - review & editing, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements André Silva Pinto de Aguiar thanks the FCT-Foundation for Science and Technology, Portugal for the Ph.D. Grant DFA/BD/5318/2020. References Agarwal et al., 2010 S. Agarwal N. Snavely S.M. Seitz R. Szeliski Bundle adjustment in the large K. Daniilidis P. Maragos N. Paragios Computer Vision – ECCV 2010 2010 Springer, Berlin Heidelberg Berlin, Heidelberg 29 42 Agarwal, S., Snavely, N., Seitz, S.M., & Szeliski, R. (2010). Bundle adjustment in the large. In K. Daniilidis, P. Maragos, & N. Paragios (Eds.), Computer Vision – ECCV 2010 (pp. 29–42). Berlin, Heidelberg: Springer Berlin Heidelberg. de Aguiar et al., 2020 A.S.P. de Aguiar F.B.N. dos Santos L.C.F. dos Santos V.M. de Jesus Filipe A.J.M. de Sousa Vineyard trunk detection using deep learning – An experimental device benchmark Computers and Electronics in Agriculture 175 2020 105535 de Aguiar, A.S.P., dos Santos, F.B.N., dos Santos, L.C.F., de Jesus Filipe, V.M., & de Sousa, A.J.M. (2020). Vineyard trunk detection using deep learning – an experimental device benchmark. Computers and Electronics in Agriculture, 175, 105535. Álvarez et al., 2014 S. Álvarez D. Llorca M. Sotelo Hierarchical camera auto-calibration for traffic surveillance systems Expert Systems with Applications 41 2014 1532 1542 Álvarez, S., Llorca, D., & Sotelo, M. (2014). Hierarchical camera auto-calibration for traffic surveillance systems. Expert Systems with Applications, 41, 1532–1542. Badue et al., 2021 C. Badue R. Guidolini R.V. Carneiro P. Azevedo V.B. Cardoso A. Forechi L. Jesus R. Berriel T.M. Paixão F. Mutz L. de Paula Veronese T. Oliveira-Santos A.F. De Souza Self-driving cars: A survey Expert Systems with Applications 165 2021 113816 Badue, C., Guidolini, R., Carneiro, R.V., Azevedo, P., Cardoso, V.B., Forechi, A., Jesus, L., Berriel, R., Paixão, T.M., Mutz, F., de Paula Veronese, L., Oliveira-Santos, T., & De Souza, A.F. (2021). Self-driving cars: A survey. Expert Systems with Applications, 165, 113816. Besl and McKay, 1992 P. Besl N.D. McKay A method for registration of 3-d shapes IEEE Transactions on Pattern Analysis and Machine Intelligence 14 1992 239 256 Besl, P., & McKay, N.D. (1992). A method for registration of 3-d shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14, 239–256. Bradski, 2000 G. Bradski The opencv library Dr Dobb’s J. Software Tools 25 2000 120 125 Bradski, G. (2000). The opencv library. Dr Dobb’s J. Software Tools, 25, 120–125. Czyzewski, 2017 Czyzewski, M. A. (2017). An extremely efficient chess-board detection for non-trivial photos. ArXiv, abs/1708.03898. Czyzewski, M.A. (2017). An extremely efficient chess-board detection for non-trivial photos. ArXiv, abs/1708.03898. Dhall et al., 2017 Dhall, A., Chelani, K., Radhakrishnan, V., & Krishna, K.M. (2017). Lidar-camera calibration using 3d–3d point correspondences. Dhall, A., Chelani, K., Radhakrishnan, V., & Krishna, K.M. (2017). Lidar-camera calibration using 3d-3d point correspondences. Durrant-Whyte and Bailey, 2006 H. Durrant-Whyte T. Bailey Simultaneous localization and mapping: part i IEEE Robotics & Automation Magazine 13 2006 99 110 Durrant-Whyte, H., & Bailey, T. (2006). Simultaneous localization and mapping: part i. IEEE Robotics & Automation Magazine, 13, 99–110. Fabbri et al., 2020 R. Fabbri P. Giblin B. Kimia Camera pose estimation using first-order curve differential geometry IEEE Transactions on Pattern Analysis and Machine Intelligence 2020 1 Fabbri, R., Giblin, P., & Kimia, B. (2020). Camera pose estimation using first-order curve differential geometry. IEEE Transactions on Pattern Analysis and Machine Intelligence, (pp. 1–1). Fischler and Bolles, 1981 M.A. Fischler R.C. Bolles Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography Communications of the ACM 24 1981 381 395 Fischler, M.A., & Bolles, R.C. (1981). Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24, 381–395. Foote, 2013 T. Foote tf: The transform library 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA) 2013 IEEE Foote, T. (2013). tf: The transform library. In 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA). IEEE. Fremont and Bonnifait, 2008 V. Fremont P. Bonnifait Extrinsic calibration between a multi-layer lidar and a camera 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 IEEE Fremont, V., & Bonnifait, P. (2008). Extrinsic calibration between a multi-layer lidar and a camera. In 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE. Furgale et al., 2013 P. Furgale J. Rehder R. Siegwart Unified temporal and spatial calibration for multi-sensor systems 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems 2013 IEEE Furgale, P., Rehder, J., & Siegwart, R. (2013). Unified temporal and spatial calibration for multi-sensor systems. In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. Gao et al., 2003 X.-S. Gao X.-R. Hou J. Tang H.-F. Cheng Complete solution classification for the perspective-three-point problem IEEE Transactions on Pattern Analysis and Machine Intelligence 25 2003 930 943 Gao, X.-S., Hou, X.-R., Tang, J., & Cheng, H.-F. (2003). Complete solution classification for the perspective-three-point problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25, 930–943. Garrido-Jurado et al., 2016 S. Garrido-Jurado R. Muñoz-Salinas F. Madrid-Cuevas R. Medina-Carnicer Generation of fiducial marker dictionaries using mixed integer linear programming Pattern Recognition 51 2016 481 491 Garrido-Jurado, S., Muñoz-Salinas, R., Madrid-Cuevas, F., & Medina-Carnicer, R. (2016). Generation of fiducial marker dictionaries using mixed integer linear programming. Pattern Recognition, 51, 481–491. Girshick et al., 2006 M.A. Girshick F. Mosteller L.J. Savage Unbiased estimates for certain binomial sampling problems with applications S.E. Fienberg D.C. Hoaglin Selected Papers of Frederick Mosteller 2006 Springer, New York New York, NY 57 68 Girshick, M.A., Mosteller, F., & Savage, L.J. (2006). Unbiased estimates for certain binomial sampling problems with applications. In S.E. Fienberg, & D.C. Hoaglin (Eds.), Selected Papers of Frederick Mosteller (pp. 57–68). New York, NY: Springer New York. Guindel et al., 2017 C. Guindel J. Beltran D. Martin F. Garcia Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 IEEE Guindel, C., Beltran, J., Martin, D., & Garcia, F. (2017a). Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE. Guindel et al., 2017 C. Guindel J. Beltran D. Martin F. Garcia Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 IEEE Guindel, C., Beltran, J., Martin, D., & Garcia, F. (2017b). Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE. Hornegger and Tomasi, 1999 J. Hornegger C. Tomasi Representation issues in the ML estimation of camera motion Proceedings of the Seventh IEEE International Conference on Computer Vision 1999 IEEE Hornegger, J., & Tomasi, C. (1999). Representation issues in the ML estimation of camera motion. In Proceedings of the Seventh IEEE International Conference on Computer Vision. IEEE. Hu et al., 2019 D. Hu D. DeTone T. Malisiewicz Deep ChArUco: Dark ChArUco marker pose estimation 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019 IEEE Hu, D., DeTone, D., & Malisiewicz, T. (2019). Deep ChArUco: Dark ChArUco marker pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. Huang and Grizzle, 2020 J.-K. Huang J.W. Grizzle Improvements to target-based 3d LiDAR to camera calibration IEEE Access 8 2020 134101 134110 Huang, J.-K., & Grizzle, J.W. (2020). Improvements to target-based 3d LiDAR to camera calibration. IEEE Access, 8, 134101–134110. Huang and Barth, 2009 L. Huang M. Barth A novel multi-planar LIDAR and computer vision calibration procedure using 2d patterns for automated navigation 2009 IEEE Intelligent Vehicles Symposium 2009 IEEE Huang, L., & Barth, M. (2009). A novel multi-planar LIDAR and computer vision calibration procedure using 2d patterns for automated navigation. In 2009 IEEE Intelligent Vehicles Symposium. IEEE. Kim and Park, 2019 E. su Kim E. S.-Y. Extrinsic calibration between camera and LiDAR sensors by matching multiple 3d planes Sensors 20 2019 52 su Kim, E., & Park, S.-Y. (2019). Extrinsic calibration between camera and LiDAR sensors by matching multiple 3d planes. Sensors, 20, 52. Liao et al., 2017 Y. Liao G. Li Z. Ju H. Liu D. Jiang Joint kinect and multiple external cameras simultaneous calibration 2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM) 2017 IEEE Liao, Y., Li, G., Ju, Z., Liu, H., & Jiang, D. (2017). Joint kinect and multiple external cameras simultaneous calibration. In 2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM). IEEE. Majumder and Pratihar, 2018 S. Majumder D.K. Pratihar Multi-sensors data fusion through fuzzy clustering and predictive tools Expert Systems with Applications 107 2018 165 172 Majumder, S., & Pratihar, D.K. (2018). Multi-sensors data fusion through fuzzy clustering and predictive tools. Expert Systems with Applications, 107, 165 – 172. Melendez-Pastor et al., 2017 C. Melendez-Pastor R. Ruiz-Gonzalez J. Gomez-Gil A data fusion system of gnss data and on-vehicle sensors data for improving car positioning precision in urban environments Expert Systems with Applications 80 2017 28 38 Melendez-Pastor, C., Ruiz-Gonzalez, R., & Gomez-Gil, J. (2017). A data fusion system of gnss data and on-vehicle sensors data for improving car positioning precision in urban environments. Expert Systems with Applications, 80, 28 – 38. Mirzaei et al., 2012 F.M. Mirzaei D.G. Kottas S.I. Roumeliotis 3d LIDAR–camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization The International Journal of Robotics Research 31 2012 452 467 Mirzaei, F.M., Kottas, D.G., & Roumeliotis, S.I. (2012). 3d LIDAR–camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization. The International Journal of Robotics Research, 31, 452–467. Oliveira et al., 2019 Oliveira, M., Castro, A., Madeira, T., Dias, P., & Santos, V. (2020). A general approach to the extrinsic calibration of intelligent vehicles using ros. In M.F. Silva, J. Luís Lima, L.P. Reis, A. Sanfeliu, & D. Tardioli (Eds.), Robot 2019: Fourth Iberian Robotics Conference (pp. 203–215). Cham: Springer International Publishing. Oliveira, M., Castro, A., Madeira, T., Dias, P., & Santos, V. (2020a). A general approach to the extrinsic calibration of intelligent vehicles using ros. In M.F. Silva, J. Luís Lima, L.P. Reis, A. Sanfeliu, & D. Tardioli (Eds.), Robot 2019: Fourth Iberian Robotics Conference (pp. 203–215). Cham: Springer International Publishing. Oliveira et al., 2020 M. Oliveira A. Castro T. Madeira E. Pedrosa P. Dias V. Santos A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Robotics and Autonomous Systems 131 2020 103558 Oliveira, M., Castro, A., Madeira, T., Pedrosa, E., Dias, P., & Santos, V. (2020b). A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach. Robotics and Autonomous Systems, 131, 103558. Pandey et al., 2010 G. Pandey J. McBride S. Savarese R. Eustice Extrinsic calibration of a 3d laser scanner and an omnidirectional camera IFAC Proceedings Volumes 43 2010 336 341 Pandey, G., McBride, J., Savarese, S., & Eustice, R. (2010). Extrinsic calibration of a 3d laser scanner and an omnidirectional camera. IFAC Proceedings Volumes, 43, 336–341. de Paula et al., 2014 M. de Paula C. Jung L. da Silveira Automatic on-the-fly extrinsic camera calibration of onboard vehicular cameras Expert Systems with Applications 41 2014 1997 2007 de Paula, M., Jung, C., & da Silveira, L. (2014). Automatic on-the-fly extrinsic camera calibration of onboard vehicular cameras. Expert Systems with Applications, 41, 1997–2007. Penate-Sanchez et al., 2013 A. Penate-Sanchez J. Andrade-Cetto F. Moreno-Noguer Exhaustive linearization for robust camera pose and focal length estimation IEEE Transactions on Pattern Analysis and Machine Intelligence 35 2013 2387 2400 Penate-Sanchez, A., Andrade-Cetto, J., & Moreno-Noguer, F. (2013). Exhaustive linearization for robust camera pose and focal length estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 2387–2400. Pradeep et al., 2014 V. Pradeep K. Konolige E. Berger Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach Experimental Robotics 2014 Springer Berlin Heidelberg 211 225 Pradeep, V., Konolige, K., & Berger, E. (2014). Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach. In Experimental Robotics (pp. 211–225). Springer Berlin Heidelberg. Quigley et al., 2009 Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., & Ng, A. Y. (2009). Ros: an open-source robot operating system. In ICRA workshop on open source software (p. 5). Kobe, Japan volume 3. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., & Ng, A.Y. (2009). Ros: an open-source robot operating system. In ICRA workshop on open source software (p. 5). Kobe, Japan volume 3. Rehder et al., 2016 J. Rehder R. Siegwart P. Furgale A general approach to spatiotemporal calibration in multisensor systems IEEE Transactions on Robotics 32 2016 383 398 Rehder, J., Siegwart, R., & Furgale, P. (2016). A general approach to spatiotemporal calibration in multisensor systems. IEEE Transactions on Robotics, 32, 383–398. Romero-Ramirez et al., 2018 F.J. Romero-Ramirez R. Muñoz-Salinas R. Medina-Carnicer Speeded up detection of squared fiducial markers Image and Vision Computing 76 2018 38 47 Romero-Ramirez, F.J., Muñoz-Salinas, R., & Medina-Carnicer, R. (2018). Speeded up detection of squared fiducial markers. Image and Vision Computing, 76, 38–47. Santos et al., 2016 F.N. dos Santos H. Sobreira D. Campos R. Morais A.P. Moreira O. Contente Towards a reliable robot for steep slope vineyards monitoring Journal of Intelligent & Robotic Systems 83 2016 429 444 dos Santos, F.N., Sobreira, H., Campos, D., Morais, R., Moreira, A.P., & Contente, O. (2016). Towards a reliable robot for steep slope vineyards monitoring. Journal of Intelligent & Robotic Systems, 83, 429–444. Santos et al., 2019 L. Santos F. Santos J. Mendes P. Costa J. Lima R. Reis P. Shinde Path planning aware of robot’s center of mass for steep slope vineyards Robotica 38 2019 684 698 Santos, L., Santos, F., Mendes, J., Costa, P., Lima, J., Reis, R., & Shinde, P. (2019). Path planning aware of robot’s center of mass for steep slope vineyards. Robotica, 38, 684–698. Sariff and Buniyamin, 2006 N. Sariff N. Buniyamin An overview of autonomous mobile robot path planning algorithms 2006 4th Student Conference on Research and Development 2006 IEEE Sariff, N., & Buniyamin, N. (2006). An overview of autonomous mobile robot path planning algorithms. In 2006 4th Student Conference on Research and Development. IEEE. Sturm, 2014 P. Sturm Pinhole camera model K. Ikeuchi Computer Vision: A Reference Guide 2014 Springer, US Boston, MA 610 613 Sturm, P. (2014). Pinhole camera model. In K. Ikeuchi (Ed.), Computer Vision: A Reference Guide (pp. 610–613). Boston, MA: Springer US. Verma et al., 2019 S. Verma J.S. Berrio S. Worrall E. Nebot Automatic extrinsic calibration between a camera and a 3D lidar using 3D point and plane correspondences 2019 IEEE Intelligent Transportation Systems Conference (ITSC) 2019 IEEE Auckland, New Zealand 3906 3912 Verma, S., Berrio, J.S., Worrall, S., & Nebot, E. (2019). Automatic extrinsic calibration between a camera and a 3D lidar using 3D point and plane correspondences. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 3906–3912). Auckland, New Zealand: IEEE. Wang et al., 2017 W. Wang K. Sakurada N. Kawaguchi Reflectance intensity assisted automatic and accurate extrinsic calibration of 3d LiDAR and panoramic camera using a printed chessboard Remote Sensing 9 2017 851 Wang, W., Sakurada, K., & Kawaguchi, N. (2017). Reflectance intensity assisted automatic and accurate extrinsic calibration of 3d LiDAR and panoramic camera using a printed chessboard. Remote Sensing, 9, 851. Zhou et al., 2018 L. Zhou Z. Li M. Kaess Automatic extrinsic calibration of a camera and a 3d LiDAR using line and plane correspondences 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2018 IEEE Zhou, L., Li, Z., & Kaess, M. (2018). Automatic extrinsic calibration of a camera and a 3d LiDAR using line and plane correspondences. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. Zuniga-Noel et al., 2019 D. Zuniga-Noel J.-R. Ruiz-Sarmiento R. Gomez-Ojeda J. Gonzalez-Jimenez Automatic multi-sensor extrinsic calibration for mobile robots IEEE Robotics and Automation Letters 4 2019 2862 2869 Zuniga-Noel, D., Ruiz-Sarmiento, J.-R., Gomez-Ojeda, R., & Gonzalez-Jimenez, J. (2019). Automatic multi-sensor extrinsic calibration for mobile robots. IEEE Robotics and Automation Letters, 4, 2862–2869. "
    },
    {
        "doc_title": "Segmentation and Manipulation of Cork Strips in Bulk",
        "doc_scopus_id": "85107151302",
        "doc_doi": "10.1109/ICARSC52212.2021.9429769",
        "doc_eid": "2-s2.0-85107151302",
        "doc_date": "2021-04-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Computer vision techniques",
            "Conveyor belts",
            "Cork stoppers",
            "Industry sectors",
            "Motion planners",
            "Natural cork",
            "Punching machine",
            "Rgb-d cameras"
        ],
        "doc_abstract": "© 2021 IEEE.The production of cork stoppers is the largest application of natural cork, which is an ever-growing industry sector. Many attempts have been made to increase the automation of this process, such as the use of automated cork punching machines, but not all steps of this process are fully efficient such as the manipulation of cork strips prior to perforation, which is still a hand labor. This paper presents a system based on an RGBD camera and a 6 DoF robotic arm that manipulates cork strips which are disposed in bulk, either in a container or in a conveyor belt. It uses computer vision techniques to segment a single cork strip from the bunch and motion planners to control the robotic arm in order to grab the selected cork strip. On the experiments made, the system was able to correctly grab a cork strip with 92% success rate and with a frequency of 6 strips per minute.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Magnetic Mapping for Robot Navigation in Indoor Environments",
        "doc_scopus_id": "85124795947",
        "doc_doi": "10.1109/IPIN51156.2021.9662528",
        "doc_eid": "2-s2.0-85124795947",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Earth magnetic fields",
            "Indoor environment",
            "Localisation",
            "Localization in indoor environment",
            "Magnetic anomalies",
            "Magnetic disturbance",
            "Magnetic mapping",
            "Metallic objects",
            "Particle filter",
            "Robot navigation"
        ],
        "doc_abstract": "© 2021 IEEE.Magnetic disturbances of the Earth's magnetic field (magnetic anomalies) occur normally in indoor environments due to the presence of ferromagnetic artefacts, such as reinforced concrete or steel infrastructures and other local metallic objects. In conventional robot navigation, which uses the direction of the Earth's magnetic field to determine orientation, these anomalies are seen as undesirable. However, if the environment is rich in anomalies with sufficient local variability, these can be mapped and used as features for localization purposes. The work presented here aims at exploiting these magnetic features optimally for navigation of mobile robots, by employing advanced interpolation techniques that permit precise mapping of magnetic fields in indoor environments that cannot be surveyed according to a regular grid during map acquisition and may suffer from large gaps in the magnetic coverage. First, the issues addressed to create a magnetic map are depicted, namely data acquisition, methods of interpolation employed, and map validation processes. Subsequently, the results of multiple localization experiments are presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Neural Network Classifier and Robotic Manipulation for an Autonomous Industrial Cork Feeder",
        "doc_scopus_id": "85115444254",
        "doc_doi": "10.1007/978-3-030-86230-5_34",
        "doc_eid": "2-s2.0-85115444254",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Convolutional neural network",
            "Cork",
            "Deep learning",
            "High quality",
            "Image processing technique",
            "Network robotics",
            "Neural networks classifiers",
            "Robotic manipulation",
            "Specific orientation",
            "Universal robot"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.This paper presents a solution for an autonomous cork puncher feeder with a robotic arm using image processing techniques and a convolutional neural network. Due to the need for cork strips to be inserted into the puncher with a specific orientation, to produce high quality cork stoppers, the identification of the orientation of each cork strip on the conveyor belt is a necessity. In response to this problem a convolutional neural network is used to analyse images processed with subtracted background, to create a robust solution for cork strips classification. In the tests carried out, a classification accuracy of 100% was obtained in a test data set with 12 different cork strips.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach",
        "doc_scopus_id": "85085374137",
        "doc_doi": "10.1016/j.robot.2020.103558",
        "doc_eid": "2-s2.0-85085374137",
        "doc_date": "2020-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Extrinsic calibration",
            "Multi-modal approach",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization",
            "State-of-the-art approach"
        ],
        "doc_abstract": "© 2020 Elsevier B.V.This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2020-05-19 2020-05-19 2020-05-28 2020-05-28 2020-08-03T02:56:00 S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 S300 S300.1 FULL-TEXT 2020-08-03T02:04:37.606251Z 0 0 20200901 20200930 2020 2020-05-19T15:45:05.788726Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref vitae 0921-8890 09218890 true 131 131 C Volume 131 7 103558 103558 103558 202009 September 2020 2020-09-01 2020-09-30 2020 article fla © 2020 Elsevier B.V. All rights reserved. AROSFRAMEWORKFOREXTRINSICCALIBRATIONINTELLIGENTVEHICLESAMULTISENSORMULTIMODALAPPROACH OLIVEIRA M 1 Introduction 2 ROS based calibration setup 2.1 Configuration of the calibration procedure 2.2 Interactive positioning of sensors 2.3 Interactive data labelling 2.4 Collecting data 2.5 Sensor poses from partial transformations 3 Calibration procedure 3.1 Optimization parameters 3.2 Objective function 3.2.1 Camera sub-function 3.2.2 Laser sub-function 3.3 Sensors pose calibration: Optimization 4 Results 4.1 Camera to camera 4.2 Complete system calibration 5 Conclusions and future work Acknowledgements References MUELLER 2017 1 6 G 2017IEEE20THINTCONFINTELLIGENTTRANSPORTATIONSYSTEMSITSC CONTINUOUSSTEREOCAMERACALIBRATIONINURBANSCENARIOS WU 2015 2638 2642 L 2015IEEEINTCONFMECHATRONICSAUTOMATIONICMA BINOCULARSTEREOVISIONCAMERACALIBRATION ROUSU 2016 896 900 L 2016IEEEADVANCEDINFORMATIONMANAGEMENTCOMMUNICATESELECTRONICAUTOMATIONCONTROLCONFIMCEC AUTOMATICCALIBRATIONSYSTEMFORBINOCULARSTEREOIMAGING LING 2016 1771 1778 Y 2016IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROS HIGHPRECISIONONLINEMARKERLESSSTEREOEXTRINSICCALIBRATION DINH 2019 815 826 V VASCONCELOS 2012 2097 2107 F PEREIRA 2016 326 337 M ALMEIDA 2012 312 319 M IMAGEANALYSISRECOGNITION 3D2DLASERRANGEFINDERCALIBRATIONUSINGACONICBASEDGEOMETRYSHAPE GUINDEL 2017 1 6 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS KWON 2018 1451 1454 Y 201818THINTCONFCONTROLAUTOMATIONSYSTEMSICCAS AUTOMATICSPHEREDETECTIONFOREXTRINSICCALIBRATIONMULTIPLERGBDCAMERAS KHAN 2016 1960 1965 A 2016IEEEINTCONFROBOTICSBIOMIMETICSROBIO CALIBRATIONACTIVEBINOCULARRGBDVISIONSYSTEMSFORDUALARMROBOTS BASSO 2018 1315 1332 F QIAO 2013 253 256 Y 2013INTCONFCOMPUTATIONALPROBLEMSOLVINGICCP ANEWAPPROACHSELFCALIBRATIONHANDEYEVISIONSYSTEMS ZHANG 2011 1 6 C 2011IEEEINTCONFMULTIMEDIAEXPO CALIBRATIONBETWEENDEPTHCOLORSENSORSFORCOMMODITYDEPTHCAMERAS CHEN 2019 2685 2694 G QILONGZHANG 2004 2301 2306 G 2004IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROSIEEECATNO04CH37566VOL3 EXTRINSICCALIBRATIONACAMERALASERRANGEFINDERIMPROVESCAMERACALIBRATION HASELICH 2012 25 28 M 2012IEEEINTCONFEMERGINGSIGNALPROCESSINGAPPLICATIONS CALIBRATIONMULTIPLECAMERASA3DLASERRANGEFINDER CHEN 2016 448 453 Z 20169THINTCONGRESSIMAGESIGNALPROCESSINGBIOMEDICALENGINEERINGINFORMATICSCISPBMEI EXTRINSICCALIBRATIONALASERRANGEFINDERACAMERABASEDAUTOMATICDETECTIONLINEFEATURE VELAS 2014 M CALIBRATIONRGBCAMERAVELODYNELIDAR LEE 2017 64 69 G 2017IEEEINTCONFMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI CALIBRATIONVLP16LIDARMULTIVIEWCAMERASUSINGABALLFOR360DEGREE3DCOLORMAPACQUISITION LEVINSON 2013 J ROBOTICSSCIENCESYSTEMS AUTOMATICONLINECALIBRATIONCAMERASLASERS DEZHIGAO 2010 6211 6215 J 20108THWORLDCONGRESSINTELLIGENTCONTROLAUTOMATION AMETHODSPATIALCALIBRATIONFORCAMERARADAR SANTOS 2010 1421 1427 V 13THINTIEEECONFINTELLIGENTTRANSPORTATIONSYSTEMS ATLASCARTECHNOLOGIESFORACOMPUTERASSISTEDDRIVINGSYSTEMBOARDACOMMONAUTOMOBILE LIAO 2017 305 310 Y 20172NDINTCONFADVANCEDROBOTICSMECHATRONICSICARM JOINTKINECTMULTIPLEEXTERNALCAMERASSIMULTANEOUSCALIBRATION REHDER 2016 383 398 J PRADEEP 2014 211 225 V EXPERIMENTALROBOTICS12THINTSYMPOSIUMEXPERIMENTALROBOTICS CALIBRATINGAMULTIARMMULTISENSORROBOTABUNDLEADJUSTMENTAPPROACH OLIVEIRA 2020 203 215 M ROBOT2019FOURTHIBERIANROBOTICSCONFERENCE AGENERALAPPROACHEXTRINSICCALIBRATIONINTELLIGENTVEHICLESUSINGROS BRADSKI 2000 G QUIGLEY 2009 M ICRAWORKSHOPOPENSOURCESOFTWARE ROSOPENSOURCEROBOTOPERATINGSYSTEM FOOTE 2013 1 6 T 2013IEEECONFERENCETECHNOLOGIESFORPRACTICALROBOTAPPLICATIONSTEPRA TFTRANSFORMLIBRARY HORNEGGER 1999 640 647 J PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISIONVOL1 REPRESENTATIONISSUESINMLESTIMATIONCAMERAMOTION AGARWAL 2010 29 42 S BUNDLEADJUSTMENTINLARGE OLIVEIRAX2020X103558 OLIVEIRAX2020X103558XM 2022-05-28T00:00:00.000Z 2022-05-28T00:00:00.000Z © 2020 Elsevier B.V. All rights reserved. 2020-05-03T22:04:14.426Z FCT Fundação para a Ciência e a Tecnologia CYTED CYTED Ciencia y Tecnología para el Desarrollo item S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 271599 2020-08-03T02:04:37.606251Z 2020-09-01 2020-09-30 true 2168899 MAIN 11 62586 849 656 IMAGE-WEB-PDF 1 gr3 35928 345 552 gr9 19670 275 339 pic2 6890 131 112 gr1 33542 289 490 gr12 23770 202 495 gr11 36248 266 489 gr7 37441 309 371 fx1002 8338 79 360 gr4 28117 314 378 gr6 18379 143 385 fx1001 8242 73 367 gr5 45234 242 376 gr2 42926 236 553 gr8 52449 600 489 gr10 49550 264 489 fx1003 6813 77 283 gr3 8344 137 219 gr9 6226 164 202 pic2 19499 164 140 gr1 5132 129 219 gr12 5757 89 219 gr11 7781 119 219 gr7 20274 163 196 fx1002 2821 48 219 gr4 6100 163 197 gr6 10154 81 219 fx1001 2829 43 219 gr5 25359 141 219 gr2 10115 94 219 gr8 4814 163 133 gr10 11178 118 219 fx1003 3134 60 219 gr3 277021 1527 2446 gr9 136675 1219 1500 pic2 61746 583 499 gr1 243828 1282 2171 gr12 231425 895 2191 gr11 268102 1178 2168 gr7 362638 1370 1643 fx1002 30951 210 958 gr4 207176 1390 1675 gr6 147160 632 1706 fx1001 30843 193 975 gr5 416481 1072 1667 gr2 344316 1047 2451 gr8 390098 2658 2167 gr10 424428 1169 2168 fx1003 24651 206 753 si115 6585 si97 32421 si36 1420 si95 6165 si111 9455 si44 6967 si7 7552 si94 1102 si42 7085 si121 31424 si5 7072 si74 5953 si89 2139 si119 9249 si71 1806 si10 1146 si30 8639 si12 235 si35 7139 si8 12045 si37 8297 si54 6126 si16 25662 si41 1679 si117 27686 si80 14084 si88 1722 si51 1990 si91 10025 si85 7503 si6 5529 si110 14513 si21 22352 si26 1589 si32 5724 si45 21948 si47 2672 si24 1842 si27 1665 si40 5395 si112 8789 si118 8959 si18 5806 si55 7274 si59 21073 si1 1359 si87 1814 si70 2168 si38 3203 si52 22498 si83 5598 si53 5544 si77 23977 si4 916 si49 2727 si17 4071 si90 10327 si3 3897 si50 3168 si29 1813 si39 21584 si22 37437 si48 1268 si114 16710 si34 1274 si23 2199 si28 1675 si92 3705 si25 1781 si75 36143 si31 1407 am false 3012923 ROBOT 103558 103558 S0921-8890(20)30398-5 10.1016/j.robot.2020.103558 Elsevier B.V. Fig. 1 Two methodologies for solving the calibration of complex systems using pair-wise approaches: (a) sequential pairwise; (b) one level pyramid using a reference sensor. The estimated transformations use the arrangements shown in solid colour arrows. Other possible arrangements are presented in dashed grey lines. Note that in both cases only a subset of the available transformations is used. Fig. 2 The proposed calibration procedure: (a) initialization from xacro files and interactive first guess; (b) data labelling and collecting. Fig. 3 Interactive labelling of 2D LiDAR data: (a) creation of interactive marker on the sensor body, (b) dragging and dropping the marker on top of the data cluster containing the chessboard plane, (c) and (d) subsequent automatic tracking of the chessboard plane. Fig. 4 Conceptual transformation graph for a complex robotic system. Each sensor has a respective calibration partial transformation, denoted by the solid edges. Dashed edges contain transformations which are not optimized (they may be static or dynamic). Each sensor has a corresponding link to which the data it collects is attached, denoted in the figure by the solid thin ellipses. Very few approaches in the literature are capable of calibrating such a system while preserving the initial structure of the graph of transformations. Fig. 5 Example of reprojection of chessboard corners during the optimization procedure: squares denote the position of the detected chessboard corners (ground truth points); crosses denote the initial position of each projected corner; points denote the current position of the projected corners. Fig. 6 Chessboard: graphics visualization of the created grid and boundary correspondent to the real chessboard (a); image of the real chessboard (b). Fig. 7 ATLASCAR2: Autonomous vehicle from the Department of Mechanical Engineering of the University of Aveiro; the sensors are indicated by the Ellipses. Fig. 8 Pixel coordinates errors between projected (expected) chessboard corners and the ground truth indexes, from top left camera to top right camera, for each collection, before the optimization procedure (a) and after the optimization procedure (b). Fig. 9 Flowchart representing the results comparison structure. Each ellipse represents a JSON file and each rectangle identifies a programmed application. Fig. 10 Pixel coordinate errors between projected and expected chessboard corners. The kalibr results are not visible because of the selection of the axes range. Fig. 11 Average error per sensor during a full system calibration procedure. Errors for cameras in pixels, for LiDARs in metres. Fig. 12 Left laser (dots surrounded by red circles) and right laser (dots surrounded by green circles) data overlaid onto a representation of the chessboard, taking in consideration the pose of the chessboard and the LiDARs as estimated by the calibration for one particular collection: (a) and (b) the start of the optimization (initial guess); (c) and (d) the end of the optimization (calibration results). Table 1 Average errors and standard deviations along both directions, before and after the optimization. Values in pixels. Values Average error Standard deviation Initial Final Initial Final x error 2.25 1.64 2.68 1.69 y error 17.09 0.53 3.32 0.62 Both 17.34 1.83 8.71 1.51 Table 2 Average errors and standard deviations, in pixels, for the distances in x axis and y axis, for the proposed approach, the OpenCV stereo calibration and the kalibr calibration method. For kalibr, two datasets were used for training: the train dataset, which was also used to train all other approaches, and the test dataset, which was used to evaluate all approaches. Values in pixels. Calibration method Average error Standard deviation x y x y Proposed approach (left) 2.218 1.633 1.223 0.584 Proposed approach (right) 2.080 1.797 1.253 0.608 OpenCV stereo calibrate 1.251 0.903 1.509 0.767 Kalibr (train) [26] 67.383 8.887 0.832 1.722 Kalibr (test) [26] 1.187 17.999 1.369 2.225 A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Miguel Oliveira a b Afonso Castro b ⁎ Tiago Madeira a Eurico Pedrosa a Paulo Dias a c Vítor Santos a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal, Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro b Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal Department of Mechanical Engineering, University of Aveiro c Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro ⁎ Corresponding author. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups. Keywords Extrinsic calibration ROS Optimization Bundle adjustment Intelligent vehicles OpenCV 1 Introduction Intelligent vehicles require a considerable amount of on-board sensors, often of multiple modalities (e.g. camera, Light Detection And Ranging (LiDAR), etc.) in order to operate consistently. The combination of the data collected by these sensors requires a transformation or projection of data from one sensor coordinate frame to another. The process of estimating these transformations between sensor coordinate systems is called extrinsic calibration. An extrinsic calibration between two sensors requires an association of data from one sensor to the data of another. By knowing these data associations, an optimization procedure can be formulated to estimate the parameters of the transformation between those sensors that minimizes the distance between associations. Most calibration approaches make use of calibration patterns, i.e., objects that are robustly and accurately detected by distinct sensor modalities. Although there have been many solutions available in the literature, on the topic of calibration, there is no straightforward solution for the calibration of multiple sensors in intelligent vehicles, or robots in general. There are multiple factors that contribute to this lack of solutions. The majority of works on calibration focus on sensor to sensor pairwise calibrations: between only cameras [1–5] or between cameras and LiDARs [6–10]. When considering pairwise combinations of sensors, there are several possibilities, according to the modality of each of the sensors in the pair. Most of them have been addressed in the literature: RGB to RGB camera calibration [1–5,7]; RGB to depth camera (RGB-D cameras) calibration [11–16]; camera to 2D LiDAR [6,7,10,15,17–20]; 2D LiDAR to 3D LiDAR [8]; camera to 3D LiDAR [10,21,22]; and camera to radar [23]. Nonetheless, all these approaches have the obvious shortcoming of operating only with a single pair of sensors, which is not directly applicable to the case of intelligent vehicles, or more complex robotic systems in general. To be applicable in those cases, pairwise approaches must be arranged in a graph-like sequential procedure, in which one sensor calibrates with another, that then relates to a third sensor, and so forth. Another option is to establish one sensor as the reference sensor and link all other sensors to it. In this case, the graph of transformations between sensors results in a one level pyramid, which contains the reference sensor on top and all other sensors at the base. One example is [7], in which a methodology for calibrating the ATLASCAR2 autonomous vehicle [24] is proposed, wherein all sensors are paired with a reference sensor. Sequential pairwise approaches have three major shortcomings: (i) transformations are estimated using only data provided from the selected sensor tandem, despite the fact that data from additional sensors could be available and prove relevant to the overall accuracy of the calibration procedure; (ii) sensitivity to cumulative errors, since the transformations are computed in a sequence (in fact, this does not occur in [7], since the pose of a specific sensor only depends on the reference sensor pose); (iii) structure of transformation graph is enforced by the nature of the calibration procedure, rather than being defined by the preference of the programmer, which could compromise some robot functionalities. Fig. 1 shows a conceptual example in which these problems are visible. There are a few works which address the problem of calibration from a multi-sensor, simultaneous optimization, perspective. In [25], a joint objective function is proposed to simultaneously calibrate three RGB cameras with respect to an RGB-D camera. Authors report a significant improvement in the accuracy of the calibration. In [26], an approach for joint estimation of both temporal offsets and spatial transformations between sensors is presented. This approach is one of few that is not designed for a particular set of sensors, since its methodology does not rely on unique properties of specific sensors. It is able to calibrate systems containing both cameras and LiDARs. Moreover, the approach does not require the usage of calibration patterns for the LiDARs, using the planes present in the scene for that purpose. In [27], a joint calibration of the joint offsets and the sensors locations for a PR2 robot is proposed. This method takes sensor uncertainty into account and is modelled in a similar way to the bundle adjustment problem. Our approach is similar to [27], in the sense that we also employ a bundle adjustment-like optimization procedure. However, our approach is not focused on a single robotic platform, rather it is a general approach that is applicable to any robotic system, which also relates it with [26]. This paper is an extension of [28], where the general approach was originally proposed. This extension focuses on the comparison of this work against state of the art calibration approaches, i.e. methodologies provided by Open Source Computer Vision Library (OpenCV) [29] as well as the calibration method from [26]. Robot Operating System ROS [30] based architectures are the standard when developing robots. There are several ROS based calibration packages available. 1 1 2 2 3 3 In addition, some approaches are well integrated with ROS since the input data for the calibration is provided as a rosbag file. Despite this, no approach provides a complete solution for the calibration of intelligent vehicles. Thus, the seamless integration with ROS became a core component of the proposed approach. To that end, the proposed calibration procedure is self-configured using the standard ROS robot description files, the Unified Robot Description Format (URDF), and provide several tools for sensor positioning and data labelling based on RVIZ interactive markers. The remainder of this paper is organized as follows: Section 2 describes the methodologies used to set up an optimization procedure which calibrates the system. In this section, several auxiliary tools for labelling data and positioning sensors are described; Section 3 details the optimization procedure and how it is cast as a bundle adjustment problem; Section 4 provides comparisons with established OpenCV calibration methodologies; finally, Section 5 provides conclusions and future work. 2 ROS based calibration setup A schematic of the proposed calibration procedure is displayed in Fig. 2. It consists of five components: configuration; interactive positioning of sensors; interactive labelling of data; collection of data; and finally, the optimization procedure. Each component will be described in detail in the following subsections. 2.1 Configuration of the calibration procedure Robotic platforms are described in ROS using a xml file called URDF. We propose to extend the URDF description files of a robot in order to provide information necessary for configuring how the calibration should be carried out. A new URDF element, named calibration, is introduced specifically for the purpose of calibrating. Each calibration element describes a sensor to be calibrated. The element contains information about the calibration parent and child links, which define the partial transformation that is optimized. 2.2 Interactive positioning of sensors Optimization procedures suffer from the known problem of local minima. This problem tends to occur when the initial solution is far from the optimal parameter configuration. Thus, it is expected that, by ensuring an accurate first guess for the sensor poses, there is less likelihood of falling into local minima. We propose to solve this problem in an interactive fashion: the system parses the URDF robot description and creates an rviz interactive marker associated with each sensor. It is then possible to move and rotate the interactive markers. This provides a simple, interactive method to manually calibrate the system or, alternatively, to easily generate plausible first guesses for the poses of the sensors. Real time visual feedback is provided by the observation of the bodies of the robot model (e.g. where a LiDAR is placed w.r.t. the vehicle), and also by the data measured by the sensors (e.g. how well the measurements from two LiDARs match). An example of this procedure can be watched at 2.3 Interactive data labelling Since the goal is to propose a calibration procedure that operates on multi-modal data, a calibration pattern adequate to all available sensor modalities must be selected. A chessboard pattern is a common calibration pattern, in particular for RGB and RGB-D cameras. To label image data, one of the many available image-based chessboards detectors is used (Find Chessboard Corners OpenCV function 4 4 ). In the case of 2D LiDAR data, it is not possible to robustly detect the chessboard, since there are often multiple planes in the scene derived from other structures, such as walls and doors. To solve this, we propose an interactive approach which requires minimal user intervention: rviz interactive markers are positioned along the LiDAR measurement planes and the user drags the marker to indicate where in the data the chessboard is observed. This is done by clustering the LiDAR data, and selecting the cluster which is closer to the marker. This interactive procedure is done only once, since it is then possible to track the chessboard robustly. Fig. 3 shows an example of the labelling of 2D LiDAR data. This interactive data labelling procedure is showcased in 2.4 Collecting data Usually, different sensors stream data at different frequencies. However, to compute the associations between the data of multiple sensors, temporal synchronization is required. While some approaches require hardware synchronization to operate [26], in the current method this is solved trivially by collecting data (and the corresponding labels) at user defined moments in which the scene has remained static for a certain period of time. In static scenes, the problem of data desynchronization is not observable, which warrants the assumption that for each captured collection the sensor data is synchronized. We refer to these snapshot recordings of multi-sensor data as data collections. This information is stored in a JSON file that will be read by the optimization procedure afterwards. The JSON file contains abstract information about the sensors, such as the sensor transformation chain, among others, and specific information about each collection, i.e., sensor data, partial transformations, and data labels. It is important to note that the set of collections should contain as many different poses as possible. As such, collections should preferably have different distances and orientations w.r.t. the chessboard so that the calibration becomes more reliable. This concern is common to the majority of calibration procedures. 2.5 Sensor poses from partial transformations The representation of a complex, multi-sensor system requires the creation of a transformation graph. For this purpose, ROS uses a graph tree referred to as tf tree [31]. One critical factor for any calibration procedure is that it should not change the structure of that existing tf tree. The reason for this is that the tf tree, derived from the URDF files by the robot state publisher, 5 5 also supports additional functionalities, such as robot visualization or collision detection. If the tf tree changes due to the calibration, those functionalities may be compromised or require some redesigning. To accomplish this, we propose to compute the pose of any particular sensor (i.e., the transformation from the Reference Link, also known as World, to that Sensor) as an aggregate transformation A , obtained after the chain of transformations for that particular sensor, extracted from the topology of the tf tree: (1) where i T i + 1 represents the partial transformation from the ith to the i + 1 link, and p a r e n t and c h i l d are the indexes of the calibration parent and calibration child links in the sensor chain, respectively. Our approach preserves the predefined structure of the tf tree, since, during optimization, only one partial transformation contained in the chain is altered (the one in blue in Eq. (1)). This computation is performed within the optimization’s cost function. Therefore, a change in one partial transformation affects the global sensor pose, and consequently, the error to minimize. The optimization may target multiple links of each chain, and is agnostic to whether the remaining links are static or dynamic, since all existing partial transformations are stored for each data collection. To the best of our knowledge, our approach is one of few which maintains the structure of the transformation graph before and after optimization. This is a feature that is often overlooked, yet it is of critical practical importance for the selection of a calibration framework. Taking the example of Fig. 4, consider that Sensor 1 is mounted on top of a pan and tilt unit, where LinkA T Link C corresponds to the pan movement, and LinkC T Sensor 1 represents the tilt motion. For this particular case, Eq. (1) becomes: (2) where I is the identity matrix (since there are no prior links), and the pan and tilt motions are coloured in red to denote that these transformations are dynamic and, as a consequence, may also change from collection to collection. Another example is the one of Sensor 2: it contains an aggregate transformation that also includes the partial transformation optimized w.r.t. Sensor 1, resulting in the following aggregate transformation: (3) which is also directly derived from Eq. (1). Such complex arrangements are seldom tackled by a single calibration approach, even less in a transparent way by the same general formalism. The proposed optimization of partial transformations achieves this goal. We consider the ability to preserve the structure of the tf tree as a key feature of the proposed framework: from a practical standpoint, since it facilitates the integration into ROS, both before and after the optimization; and, moreover, from a conceptual perspective, since this formulation is general and adequate to handle most calibration scenarios. 3 Calibration procedure The goal of general optimization procedures is to find the parameter configuration that results in the smallest function value. This function, which depends on the optimization parameters Φ is known as the objective function. For the purpose of calibrating the multi-modal sensors of a robotic platform, like the ATLASCAR2 intelligent vehicle, the objective of this optimization is to estimate the pose of each sensor relatively to a reference link (base link for ATLASCAR2 case). 3.1 Optimization parameters An extrinsic calibration translates into a pose estimation. Thus, the set of parameters to optimize, defined as Φ , must contain parameters that together define the pose of each sensor. As discussed in the beginning of Section 2, we propose to maintain the initial structure of the transformation graph, and thus only optimize one partial transformation per sensor. In the example of Fig. 4, these partial transformations are denoted by solid arrows. Since the usage of camera sensors is considered, it is also possible to introduce the intrinsic parameters of each camera in the set Φ . Our goal is to define an objective function that is able to characterize sensors of different modalities. Pairwise methodology for devising the cost function results in complex graphs of exhaustive definition of relationships. For every existing pair of sensors, these relationships must be established according to the modality of each of the sensors, and, although most cases have been addressed in literature, as discussed in Section 1, a problem of scalability remains inherent to such a solution. To address this issue, we propose to structure the cost function in a sensor to calibration pattern paradigm, similar to what is done in bundle adjustment. That is, the positions of 3D points in the scene are jointly refined with the poses of the sensors. These 3D points correspond to the corners of the calibration chessboard. What is optimized is actually the transformation that takes these corners from the frame of reference of the chessboard to the world, for every collection. All variables must have some initial value, so that the optimizer may compute the first error, and start to refine the values in order to obtain the minimum of the cost function. The first guess for each chessboard is obtained by computing the pose of a chessboard detection in one of the cameras available. The output is a transformation from the chessboard reference frame to the camera’s reference frame. Since we already have the first guess for the poses of each sensor, calculated as an aggregate transformation A (see Eq. (1)), to obtain the transformation from the chessboard reference frame to the world (an external and absolute frame), the following calculation is applied: (4) chess T world = camera A world ︷ Eq. (1) ⋅ chess T camera ︷ chess detection , where chess and camera refer to chessboard and camera coordinate frames, respectively. Thus, the set of parameters to be optimized Φ , contains the transformation represented in Eq. (4), for each collection, along with the poses of each sensor: (5) Φ = [ x m = 1 , r m = 1 , i m = 1 , d m = 1 , … , x m = M , r m = M , i m = M , d m = M , ︷ Cameras x n = 1 , r n = 1 , … , x n = N , r n = N , ︷ LiDARs … , ︷ Other modalities x k = 1 , r k = 1 , … , x k = K , r k = K ︷ Calibration object ] where m refers to the mth camera, of the set of M cameras, n refers to the nth LiDAR, of the set of N LiDARs, k refers to the chessboard detection of the kth collection, contained in the set of K collections, x is a translation vector [ t x , t y , t z ] , r is a rotation represented through the axis/angle parameterization [ r 1 , r 2 , r 3 ] (where the vector [ r 1 , r 2 , r 3 ] is used to represent the axis and its norm the angle), i is a vector of a camera’s intrinsic parameters [ f x , f y , c x , c y ], and d is a vector of camera’s distortion coefficients [ d 0 , d 1 , d 2 , d 3 , d 4 ]. The initial estimate for the intrinsic parameters is obtained using any intrinsic camera calibration tool. The axis/angle parameterization was chosen because it has 3 components and 3 degrees of freedom, making it a fair parameterization, since it does not introduce more numerical sensitivity than the one inherent to the problem itself [32]. At this point, there are six parameters per sensor, related to the pose of each one, to be enhanced. These values compose the geometric transformation that will be calibrated. The cost function will compute the residuals based on an error (in pixels for RGB cameras and in millimetres for LiDARs) between the re-projected position of the chessboard, estimated by all transformations, and the position of the calibration pattern detected by each sensor. 3.2 Objective function The cost function for this optimization, F ( Φ ) , can be thought of as the sum of several sub-functions that compose a vector function, where, for every modality of sensor added to the calibration, a new sub-function is defined accordingly, which allows for the minimization of the error associated with the pose of sensors of that modality. Thus, the optimization procedure can be defined as: (6) arg min Φ F ( Φ ) = 1 2 ∑ i f i ( Φ i 1 , … , Φ i k ) 2 where f i ( ⋅ ) is the objective sub-function for the i th sensor with the respective parameters block { Φ i 1 , … , Φ i k } , being k the parameters number of each objective sub-function. In other words, the scalar cost function of this optimization is the sum of the squares of the returned values from a vector function, divided by two. Each different sensor has an inherent sub-function, that depends on the sensor modality. The value of all these sub-functions is a vector with the errors (residuals) associated to the re-projection of the calibration pattern points. Since for the ATLASCAR2 intelligent vehicle we are considering four sensors (two cameras and two 2D LiDARs), the objective function is composed by the vector values of four sub-functions, two of each type. Each different sub-function is detailed in the next sub-sections. 3.2.1 Camera sub-function When the sensors are cameras, their calibration is performed as a bundle adjustment [33], and as such, the sub-function created is based on the average geometric error corresponding to the image distance between a projected point and a detected one. The 3D points corresponding to the corners of the calibration chessboard are captured by one or more cameras in each collection. Each camera is defined by its pose relative to a reference link and intrinsic parameters. After the desired acquisitions are completed, the 3D points are projected from the world into the images and the 2D coordinates are compared to the ones obtained by detection of the calibration pattern in the corresponding images. The positions of the 3D points in the world are obtained by applying the transformation described in Eq. (4) to the chessboard corner points defined in the chessboard detection’s reference frame. The goal of this cost sub-function is to adjust the initial estimation of the camera parameters and the position of the points, in order to minimize the average reprojection error f camera , given by: (7) f camera = ℓ 2 ( x c = 1 , x ˆ c = 1 ) ℓ 2 ( x c = 2 , x ˆ c = 2 ) ⋯ ℓ 2 ( x c = C , x ˆ c = C ) ⊺ where ℓ 2 is the Euclidean distance between two vectors, c denotes the index of the chessboard corners, x c denotes the pixels coordinates of the measured points (given by chessboard detection), and x ˆ c are the projected points, given by the relationship between a 3D point in the world and its projection on an image plane. By knowing the real size of the chessboard squares, the 3D coordinates of all corners relatively to the chess frame can be inferred. Note that the z value will be, for every point, zero, since the chessboard is in the XoY plane. After obtaining the 3D coordinates of all corners in reference to the chessboard frame, the objective function computes the coordinates of the points relatively to the camera link through multiplying by the geometric transformation between the base link (reference frame for the ATLASCAR2 example) and the calibration pattern frame and by the transform between the camera link and the base link: (8) p c a m e r a = camera T world ⋅ world T chess ⋅ p c h e s s where p c h e s s refers to the x , y , z coordinates of a chessboard corner, defined in the local chessboard coordinate frame, and p c a m e r a refers to the x , y , z coordinates of the same chessboard corner, defined in the camera link. In fact, both p c h e s s and p c a m e r a are the homogenized matrices of the coordinates so that Eq. (8) is mathematically correct. Note that the parameters to be optimized define the chessboard to world transformation, and that the world to camera transformation is computed from an aggregate of several partial transformations, one of which is defined by other parameters being optimized; furthermore, the intrinsic matrix is dependent on parameters which are accounted for in the optimization. As is expected, the re-projected points become closer to the ground truth corners during the optimization procedure. Fig. 5 shows the difference between the initial position of the chessboard corners, projected from the 3D world to the camera image, and the final position of these same projected points, after the optimization has been completed. It is possible to observe that the pixels corresponding to the projection of the final position of the points (dots in Fig. 5) almost perfectly match the ground truth points (squares in Fig. 5). 3.2.2 Laser sub-function Finally, for the case of 2D LiDARs, the sub-function only considers the two border points, among all the measurements that are related to the chessboard plane, to compute the error associated to the pose of the LiDAR and the chessboard. In order to calculate the residuals that this cost sub-function should return, the detected points’ 3D coordinates from the chessboard frame are required. During the calibration setup stage, when the information of a time stamp is saved, the ranges of all measurements that the LiDAR is detecting are stored, as well as the information about this same LiDAR and the indexes of the ranges that correspond to the plane where the chessboard is. With the optimization parameters of the chessboard pose and the LiDAR pose (computed accordingly to Eq. (1)), both relative to the base link, the 3D coordinates of each labelled measurement of the point cloud in the chessboard frame are known: (9) x y z 1 chess = chess T world ⋅ world T lidar ⋅ x y z 1 lidar . Finally, with the coordinates from the chessboard frame, of both the first and the last points of the cluster extracted in the labelling stage, it is possible to compute the error evaluated by this cost sub-function. The error is based on the distance between each one of the limit points (the first and the last index) of the selected ranges and the chessboard surface boundaries. There are two computed distances for each point: orthogonal and longitudinal. The orthogonal distance is the z absolute value of the coordinates, in the calibration pattern frame, of the LiDAR data measurement. In an ideal setting, the z value should be zero, since the chessboard plane is on the XoY plane. This is why any value different from zero means that the optimization parameters (sensor pose and chess pose) are not yet correct. The longitudinal distance is the Euclidean distance between the x and y coordinates, in the calibration pattern frame, of the LiDAR data measurement and the x and y coordinates of the closest point that belong to the limit of the physical board that is being detected. In order to compute this distance, it is essential to create a group of points that represent the boundaries of the chessboard. By knowing the size of the board, the size of each chess square, and that the chess frame origin matches with the first (top left) chess corner, the coordinates were calculated and the points of the board boundaries were manually defined. The size of the border between the chess corner grid and the end of the physical chessboard had to be measured so that this step could be implemented. In Fig. 6, we can see the grid of the chess corners and a line around it: that line marks the limit of the board. This solid line has some points within it, which are going to be compared to the LiDAR data measured ones. Again, the optimizer will search for the closest limit point to each one of the studied LiDAR data measurement coordinates and then compute the longitudinal distance. Thus, the LiDAR sub-function f lidar is defined as: (10) f lidar = | z 1 chess | ℓ 2 ( p 1 b o a r d l i m i t , p 1 c h e s s ) | z 2 chess | ℓ 2 ( p 2 b o a r d l i m i t , p 2 c h e s s ) ⊺ where (11) p b o a r d l i m i t = x y boardlimit , (12) p c h e s s = x y chess , and z chess is the third coordinate value of the range measurement points transformed to the chessboard’s coordinate frame. 3.3 Sensors pose calibration: Optimization The cost function F ( Φ ) from Eq. (6) is minimized using a least-squares approach. 6 6 In this work we used the least-squares solver provided by SciPy: Least-squares finds a local minimum of a scalar cost function, with bounds on the variables, by having an m-dimensional real residual function of n real variables. As such, we choose this minimization approach as it is the best fit for our problem. 4 Results To assess the performance of the proposed calibration approach, we used an intelligent vehicle as test bed. The ATLASCAR2 [24] is an electric vehicle (Mitsubishi i-MiEV) with several sensors onboard. In this work four sensors were considered: two 2D LiDARs and two RGB cameras. Thus, two different modalities of sensors are used. The sensors are designated as follows: left laser, right laser, top left camera and top right camera. Fig. 7 shows the ATLASCAR2 vehicle. The proposed approach is used to calibrate the four selected sensors simultaneously. Nonetheless, as discussed above, there are no approaches which provide an off-the-shelf multi-sensor multi-modal calibration. As such, in order to evaluate this approach, we provide comparisons against other pairwise methodologies, which are abundant in the field, as was mentioned in Section 1. Note that, in the following comparisons, the results given by the proposed approach for a particular pair of sensors are obtained using a complete system calibration. On the other hand, the alternative methodologies calibrate a single pair of sensors. In this sense, the comparison methodology is not favourable to the proposed approach, since the other approaches are specialized in the case being evaluated. In the following lines, two tests are detailed: the first is a camera-to-camera evaluation which compares several calibration methods in a pairwise fashion, while the second characterizes the proposed joint optimization over time providing global metrics. 4.1 Camera to camera The methodology used to compute the error of the calibrated poses of the top right camera and the top left camera is based on the distance between pixel coordinates. These coordinates are, on the one hand, the detected chessboard corners (ground truth) of the top right camera and, on other hand, the coordinates of the projections of those corners, to the top left camera, using the transformation between the cameras which is the output of the calibration. To transform pixels from one camera to the other, we start from the projection of the 3D world coordinates to the image of a camera: (13) p = K ⋅ R t ⋅ P where P refers to the 3D homogeneous coordinates of the corners as viewed in the chessboard frame; p is a vector composed by the u , v and w values, in which: x pixel = u ∕ w and y pixel = v ∕ w , allowing for the direct extraction of image coordinates from this vector; R t is the non-homogeneous geometric transformation matrix from the camera frame to the chessboard frame, K represents the camera’s intrinsic matrix. Eq. (13) can be applied to each camera separately. Since the 3D chessboard corner coordinates are defined in the chessboard frame, the value of Z will be 0 for all corners, because they all lie on the XoY plane: Z chess = 0 . As a result, Eq. (13) may be simplified as follows: (14) u v w = f x 0 c x 0 f y c y 0 0 1 ⋅ r 11 r 12 t x r 21 r 22 t y r 31 r 32 t z ⋅ X Y 1 chess corners , which is equivalent to: (15) p camera = K ⋅ camera T chess ′ ⋅ P chess , where the geometric transformation matrix camera T chess ′ is a portion of the camera T chess matrix, as detailed in Eq. (15). We use (15) for both cameras, and relate both expressions by the 3D coordinates of the chessboard corners (which are the same for both cameras), resulting in: (16) p cam2 = K cam2 ⋅ cam2 T chess ′ ⋅ cam1 T chess ′ -1 ⋅ K cam1 -1 ⋅ p cam1 where cam1 and cam2 refer to the top left and top right cameras, respectively. This formulation provides the relation between image coordinates of the chessboard corners for both camera images of each collection. Notice, however, that calibration methods output the transformation between sensors, in this case between cameras, while Eq. (16) requires transformations from the cameras to the chessboard. Some approaches, as for example the proposed approach, also estimate the pose of the chessboards (see parameters of the calibration objects in Eq. (5)). Thus, at first glance, one could think of using these transformations directly in Eq. (16). However, these chessboard poses are estimated for a given training dataset, and cannot be accurately used for other datasets. Moreover, as said before, not all calibration approaches output the pose of the chessboards (e.g. OpenCV stereo calibrate). Instead, calibration approaches provide the transformation between cameras. By arbitrarily selecting one camera from which the chessboard pose is determined through the solvePNP function (we have used cam1, but tests have shown that the alternative provided similar results) and using the transformation cam1 T cam2 estimated by the calibration approaches, it is possible to determine the transformation of the other camera to the chess, as follows: (17) cam2 T chess = cam1 T cam2 -1 ︷ calibration ⋅ cam1 T chess ︷ s o l v e P n P . From this expression the partial matrices cam1 T chess ′ and cam2 T chess ′ are derived. Then, we apply Eq. (16) to compute the corner coordinates on the top right camera image, as projected from the detection of the top left camera image. The error is computed by measuring the difference between expected and projected corner coordinates on the top right camera image: (18) x error y error top right camera = x y projected − x y e x p e c t e d Fig. 8 shows the errors related to the projection of the chessboard corners from the top left camera to the top right camera before and after the optimization of the position and orientation parameters of the cameras. These results can be better evaluated through the calculated mean error and standard deviation values, as shown in Table 1: Next, the proposed approach was compared with other calibration methodologies: stereo calibrate function 7 7 provided by OpenCV and the kalibr calibration method [26]. The kalibr method requires hardware synchronization and receives a bag file as input, unlike the other approaches, which make use of the datasets collected as described in Section 2. Because of this, two different calibrations are provided for kalibr: the first in which the training dataset is used, and a second which uses the test dataset, i.e. the dataset which is used to evaluate all approaches. The results for the proposed approach are presented for two different scenarios, taking into account the camera (top left or top right) which was used for creating the initial values of the chessboard poses (see Eq. (4)). These two variants are used to assess the impact of the selection of the camera for providing initial estimates on the final calibration estimates. In this experiment, calibration of a pair of sensors composed by the top left camera (cam1) and the top right camera (cam2) is evaluated. The dataset used for running the calibration procedures, i.e. the training dataset, is composed of 27 collections (27 images per camera). The test dataset to be used to evaluate the estimated sensor-to-sensor transformations has 15 collections. Images from the train and test datasets are similar. In order to make this comparison fair, the three distinct calibration procedures are given the exact same information. Moreover, the procedures were implemented in such a way that the returned estimated parameters, and remaining data, are organized similarly to the proposed approach. This means that each distinct approach will output a final JSON file with the estimated position and orientation of the sensors. Taking all this into account, a specific tool was created for visualizing the results of the different calibration procedures named Results Visualization, which imports the JSON files outputted by each one of the several approaches. Fig. 9 shows a flowchart of this framework, built specifically to compare the proposed methodology with standard pairwise approaches. Fig. 10 shows the pixel errors of the three distinct calibration approaches. Note that the methodology described above is computed separately for each collection. The performance of the kalibr method is clearly below the other two. We suspect there are several factors contributing to this. The first is that this method requires hardware synchronization, not ensured in the used datasets. Another is that the kalibr method reads data from a bag file and thus we have no control over the images which are selected to run the calibration. In an attempt to address the problem, we ran a kalibr calibration using the test dataset as input (kalibr test in Table 2). It may be that the selection of images is not working well, which in turn causes a poor calibration performance. Also, due to the limited duration of the bag file of the experiment, only around 20 to 30 images have been selected, a total similar to the datasets we have used. It could be that kalibr requires a larger number of images. In any case, we believe these results are not representative of kalibr. The proposed approach and the stereo calibration display similar errors, which means that the proposed approach is on par with a state of the art calibration approach. Moreover, the largest error of each of the compared methodologies occurs for the same collection (in this case, for collection 4, the dark green). This also shows a high degree of consistency between the proposed approach and the stereo calibration. Table 2 shows the average error and the standard deviation of all tested calibration approaches. These results exhibit reprojection errors in the order of some pixels, which is the normal range of values for these methods and experimental setups. Moreover, the obtained values are very similar between the proposed approach and the stereo calibrate. As such, results show that the proposed approach is able to calibrate all sensors on-board the ATLASCAR2 using a single optimization procedure. Furthermore, the accuracy of this joint calibration framework we propose is the same as when using state of the art pairwise calibration methods. 4.2 Complete system calibration This section will provide results concerning a full system calibration. Note that, in Section 4.1, the results focus only on the evaluation of the camera sensors, despite the fact that the complete system was also calibrated. In this section, the goal is to characterize all the sensors and not just the cameras. Because of this, it is not possible to compare the full system calibration (taking into account all the sensors) with other approaches since, as described in Section 1, there is no calibration framework available, in particular a multi-sensor and multi-modal one. Fig. 11 shows the average error per sensor over the cost function evaluations, for a full system calibration test. The average error per sensor is estimated after the several error measurements computed for each particular sensor. For example, a camera cost sub-function returns as many residuals as chessboard corners (see Eq. (7)), while the LiDAR sub sub-function returns four measurements (see Eq. (10)). The average error for camera sensors is provided in pixels, while for LiDAR sensors the error is in metres. The first takeaway is that the optimization is working as intended, since the minimization of the errors of all sensors can be observed. This shows that the multi-sensor, multi-modal optimization (the joint minimization of all the sensor’s parameters) is in fact possible. Furthermore, the final errors values (after the optimization is finished) are around a few pixels for camera sensors (2.8 and 3.3 pixels for the top left camera and the top right camera, respectively), and around a few centimetres for the LiDARs (0.017 and 0.033 metres for the left laser and right laser, respectively). These values are on par with the state of the art, even when considering calibration results for pairwise approaches. Another important insight is the reason why the top left camera residual starts with a low error: Section 3.1, in particular Eq. (4), described how the initial poses of the chessboards were estimated using one camera sensor, which is arbitrarily selected. In this test, the top left camera was selected to produce the initial chessboard pose estimates. Thus, since the corner detection in the top left camera images are used to compute the initial chessboard poses, the reverse procedure of projecting the chessboard corners back to the image results in corner coordinates that are naturally very close to the detections at the beginning of the optimization. Fig. 12 shows the data from the LiDARs along with a representation of the chessboard. For a better visualization, a single collection is displayed. The four images correspond to different stages of the optimization process. It is possible to see an improvement during the calibration (i.e. from Fig. 12(a) and (b), the beginning of the optimization, to (c) and (d), the end of the optimization, since the data from both LiDARs is much closer to the chessboard plane (c) and (d) when compared to (a) and (b). This shows that the proposed approach is also capable of calibrating LiDARs within a joint optimization framework. 5 Conclusions and future work This paper proposes an extrinsic calibration methodology that is general, in the sense that the number of sensors and their modalities are not restricted. The approach is compliant with the ROS framework, having also the advantage of not altering the tf tree. To accomplish this, the problem is formalized as an optimization procedure of a set of partial transformations, which accounts for specific links in the transformation chains of the sensors. Additionally, the work contributes with a set of interactive tools for the positioning of the sensors and labelling of data, which facilitate the creation of a first guess and significantly ease the calibration procedure. Results show that the proposed approach is able to achieve similar accuracy when compared to state of the art methodologies, implemented in OpenCV. Moreover, these results are obtained by performing a complete calibration of the system, rather than one of a single pair of sensors. In other words, the proposed approach calibrates all sensors at once, with similar performance as the pairwise approaches. This confirms that the proposed approach is adequate for the calibration of complex robotic systems, as are most intelligent vehicles. Future work will focus on the extension to additional sensor modalities, e.g., 3D LiDARs, RGB-D cameras, Radio Detection And Ranging (RaDAR), etc. Given the scalability of the proposed framework, it is expected that this should be more or less straightforward. Finally, the ultimate goal is to produce a multi-sensor, multi-modal calibration package that may be released to the community. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This Research was funded by National Funds through the FCT — Foundation for Science and Technology, in the context of the project UIDB/00127/2020, as well as CYTED/TICs4CI — Aplicaciones TICS para Ciudades Inteligentes . References [1] Mueller G.R. Wuensche H. Continuous stereo camera calibration in urban scenarios 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC) 2017 1 6 10.1109/ITSC.2017.8317675 G. R. Mueller, H. Wuensche, Continuous stereo camera calibration in urban scenarios, in: 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC), 2017, 1–6. DOI: 10.1109/ITSC.2017.8317675. [2] Wu L. Zhu B. Binocular stereovision camera calibration 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA) 2015 2638 2642 10.1109/ICMA.2015.7237903 L. Wu, B. Zhu, Binocular stereovision camera calibration, in: 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA), 2015, 2638–2642. DOI: 10.1109/ICMA.2015.7237903. [3] Rou Su L. JingLiang Zhong B. QiaoLiang Li SuWen Qi HuiSheng Zhang TianFu Wang An automatic calibration system for binocular stereo imaging 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC) 2016 896 900 10.1109/IMCEC.2016.7867340 Rou Su, JingLiang Zhong, QiaoLiang Li, SuWen Qi, HuiSheng Zhang, TianFu Wang, An automatic calibration system for binocular stereo imaging, in: 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC), 2016, 896–900. DOI: 10.1109/IMCEC.2016.7867340. [4] Ling Y. Shen S. High-precision online markerless stereo extrinsic calibration 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) 2016 1771 1778 10.1109/IROS.2016.7759283 Y. Ling, S. Shen, High-precision online markerless stereo extrinsic calibration, in: 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2016, 1771–1778. DOI: 10.1109/IROS.2016.7759283. [5] Dinh V.Q. Nguyen T.P. Jeon J.W. Rectification using different types of cameras attached to a vehicle IEEE Trans. Image Process. 28 2 2019 815 826 10.1109/TIP.2018.2870930 V. Q. Dinh, T. P. Nguyen, J. W. Jeon, Rectification using different types of cameras attached to a vehicle, IEEE Trans. on Image Processing 28 (2) (2019) 815–826. DOI: 10.1109/TIP.2018.2870930. [6] Vasconcelos F. Barreto J.P. Nunes U. A minimal solution for the extrinsic Calibration of a Camera and a laser-rangefinder IEEE Trans. Pattern Anal. Mach. Intell. 34 11 2012 2097 2107 F. Vasconcelos, J. P. Barreto, U. Nunes, A minimal solution for the extrinsic calibration of a camera and a laser-rangefinder, IEEE Trans. on Pattern Analysis and Machine Intelligence 34 (11) (2012) 2097–2107. [7] Pereira M. Silva D. Santos V. Dias P. Self calibration of multiple lidars and cameras on autonomous vehicles Robot. Auton. Syst. 83 2016 326 337 M. Pereira, D. Silva, V. Santos, P. Dias, Self calibration of multiple lidars and cameras on autonomous vehicles, Robotics and Autonomous Systems 83 (2016) 326–337. [8] Almeida M. Dias P. Oliveira M. Santos V. 3d-2d laser range finder calibration using a conic based geometry shape Image Analysis and Recognition 2012 312 319 M. Almeida, P. Dias, M. Oliveira, V. Santos, 3d-2d laser range finder calibration using a conic based geometry shape, in: Image Analysis and Recognition, 2012, 312–319. [9] A. Geiger, F. Moosmann, O. Car, B. Schuster, Automatic camera and range sensor calibration using a single shot, in: Proc. IEEE Int. Conf. Robot. Autom., 2012, pp. 3936–3943. [10] Guindel C. Beltrán J. Martín D. García F. Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 1 6 C. Guindel, J. Beltrán, D. Martín, F. García, Automatic extrinsic calibration for lidar-stereo vehicle sensor setups, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) (2017) 1–6. [11] Kwon Y.C. Jang J.W. Choi O. Automatic sphere detection for extrinsic calibration of multiple rgbd cameras 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS) 2018 1451 1454 Y. C. Kwon, J. W. Jang, O. Choi, Automatic sphere detection for extrinsic calibration of multiple rgbd cameras, in: 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS), 2018, 1451–1454. [12] Khan A. Aragon-Camarasa G. Sun L. Siebert J.P. On the calibration of active binocular and rgbd vision systems for dual-arm robots 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO) 2016 1960 1965 10.1109/ROBIO.2016.7866616 A. Khan, G. Aragon-Camarasa, L. Sun, J. P. Siebert, On the calibration of active binocular and rgbd vision systems for dual-arm robots, in: 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), 2016, 1960–1965. DOI: 10.1109/ROBIO.2016.7866616. [13] Basso F. Menegatti E. Pretto A. Robust intrinsic and extrinsic calibration of rgb-d cameras IEEE Trans. Robot. 34 5 2018 1315 1332 10.1109/TRO.2018.2853742 F. Basso, E. Menegatti, A. Pretto, Robust intrinsic and extrinsic calibration of rgb-d cameras, IEEE Trans. on Robotics 34 (5) (2018) 1315–1332. DOI: 10.1109/TRO.2018.2853742. [14] Qiao Y. Tang B. Wang Y. Peng L. A new approach to self-calibration of hand-eye vision systems 2013 Int. Conf. on Computational Problem-Solving (ICCP) 2013 253 256 10.1109/ICCPS.2013.6893596 Y. Qiao, B. Tang, Y. Wang, L. Peng, A new approach to self-calibration of hand-eye vision systems, in: 2013 Int. Conf. on Computational Problem-Solving (ICCP), 2013, 253–256. DOI: 10.1109/ICCPS.2013.6893596. [15] Zhang C. Zhang Z. Calibration between depth and color sensors for commodity depth cameras 2011 IEEE Int. Conf. on Multimedia and Expo 2011 1 6 10.1109/ICME.2011.6012191 C. Zhang, Z. Zhang, Calibration between depth and color sensors for commodity depth cameras, in: 2011 IEEE Int. Conf. on Multimedia and Expo, 2011, 1–6. DOI: 10.1109/ICME.2011.6012191. [16] Chen G. Cui G. Jin Z. Wu F. Chen X. Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction IEEE Sens. J. 19 7 2019 2685 2694 10.1109/JSEN.2018.2889805 G. Chen, G. Cui, Z. Jin, F. Wu, X. Chen, Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction, IEEE Sensors Journal 19 (7) (2019) 2685–2694. DOI: 10.1109/JSEN.2018.2889805. [17] Qilong Zhang G. Pless R. Extrinsic calibration of a camera and laser range finder (improves camera calibration) 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), Vol. 3 2004 2301 2306 10.1109/IROS.2004.1389752 Qilong Zhang, R. Pless, Extrinsic calibration of a camera and laser range finder (improves camera calibration), in: 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), 3, 2004, 2301–2306 3. DOI: 10.1109/IROS.2004.1389752. [18] Häselich M. Bing R. Paulus D. Calibration of multiple cameras to a 3d laser range finder 2012 IEEE Int. Conf. on Emerging Signal Processing Applications 2012 25 28 M. Häselich, R. Bing, D. Paulus, Calibration of multiple cameras to a 3d laser range finder, in: 2012 IEEE Int. Conf. on Emerging Signal Processing Applications, 2012, 25–28. [19] Chen Z. Yang X. Zhang C. Jiang S. Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) 2016 448 453 Z. Chen, X. Yang, C. Zhang, S. Jiang, Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature, in: 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2016, 448–453. [20] Velas M. Spanel M. Materna Z. Herout A. Calibration of rgb camera with velodyne lidar 2014 M. Velas, M. Spanel, Z. Materna, A. Herout, Calibration of rgb camera with velodyne lidar, 2014. [21] Lee G. Lee J. Park S. Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 64 69 10.1109/MFI.2017.8170408 G. Lee, J. Lee, S. Park, Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition, in: 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI), 2017, 64–69. DOI: 10.1109/MFI.2017.8170408. [22] Levinson J. Thrun S. Automatic online calibration of cameras and lasers Robotics: Science and Systems 2013 J. Levinson, S. Thrun, Automatic online calibration of cameras and lasers, in: Robotics: Science and Systems, 2013. [23] Dezhi Gao J. Duan J. Xining Yang S. Zheng B. A method of spatial calibration for camera and radar 2010 8th World Congress on Intelligent Control and Automation 2010 6211 6215 10.1109/WCICA.2010.5554411 Dezhi Gao, J. Duan, Xining Yang, B. Zheng, A method of spatial calibration for camera and radar, in: 2010 8th World Congress on Intelligent Control and Automation, 2010, 6211–6215. DOI: 10.1109/WCICA.2010.5554411. [24] Santos V. Almeida J. Ávila E. Gameiro D. Oliveira M. Pascoal R. Sabino R. Stein P. Atlascar - technologies for a computer assisted driving system, on board a common automobile 13th Int. IEEE Conf. on Intelligent Transpor Tation Systems 2010 1421 1427 10.1109/ITSC.2010.5625031 V. Santos, J. Almeida, E. vila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, Atlascar - technologies for a computer assisted driving system, on board a common automobile, in: 13th Int. IEEE Conf. on Intelligent Transpor tation Systems, 2010, 1421–1427. DOI: 10.1109/ITSC.2010.5625031. [25] Liao Y. Li G. Ju Z. Liu H. Jiang D. Joint kinect and multiple external cameras simultaneous calibration 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM) 2017 305 310 10.1109/ICARM.2017.8273179 Y. Liao, G. Li, Z. Ju, H. Liu, D. Jiang, Joint kinect and multiple external cameras simultaneous calibration, in: 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM), 2017, 305–310. DOI: 10.1109/ICARM.2017.8273179. [26] Rehder J. Siegwart R. Furgale P. A general approach to spatiotemporal calibration in multisensor systems IEEE Trans. Robot. 32 2 2016 383 398 10.1109/TRO.2016.2529645 J. Rehder, R. Siegwart, P. Furgale, A general approach to spatiotemporal calibration in multisensor systems, IEEE Trans. on Robotics 32 (2) (2016) 383–398. DOI: 10.1109/TRO.2016.2529645. [27] Pradeep V. Konolige K. Berger E. Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach Experimental Robotics: The 12th Int. Symposium on Experimental Robotics 2014 Springer Berlin Heidelberg Berlin, Heidelberg 211 225 10.1007/978-3-642-28572-1˙15 V. Pradeep, K. Konolige, E. Berger, Calibrating a Multi-arm Multi-sensor Robot: A Bundle Adjustment Approach, Springer Berlin Heidelberg, Berlin, Heidelberg, 2014, 211–225. DOI: 10.1007/978-3-642-28572-1˙15. [28] Oliveira M. Castro A. Madeira T. Dias P. Santos V. A general approach to the extrinsic calibration of intelligent vehicles using ros Robot 2019: Fourth Iberian Robotics Conference 2020 Springer International Publishing Cham 203 215 M. Oliveira, A. Castro, T. Madeira, P. Dias, V. Santos, A general approach to the extrinsic calibration of intelligent vehicles using ros, in: Robot 2019: Fourth Iberian Robotics Conference, Springer International Publishing, Cham, 2020, 203–215. [29] Bradski G. The OpenCV Library, Dr. Dobb’s J. Softw. Tools 2000 G. Bradski, The OpenCV Library, Dr. Dobb’s Journal of Software Tools. [30] Quigley M. Conley K. Gerkey B.P. Faust J. Foote T. Leibs J. Wheeler R. Ng A.Y. Ros: an open-source robot operating system ICRA Workshop on Open Source Software 2009 M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, A. Y. Ng, Ros: an open-source robot operating system, in: ICRA Workshop on Open Source Software, 2009. [31] Foote T. Tf: The transform library 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA) 2013 1 6 10.1109/TePRA.2013.6556373 T. Foote, tf: The transform library, in: 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA), 2013, 1–6. DOI: 10.1109/TePRA.2013.6556373. [32] Hornegger J. Tomasi C. Representation issues in the ML estimation of camera motion Proceedings of the Seventh IEEE International Conference on Computer Vision, Vol. 1 1999 640 647 10.1109/ICCV.1999.791285 J. Hornegger, C. Tomasi, Representation issues in the ml estimation of camera motion, 1, 1999, 640–647 1. DOI: 10.1109/ICCV.1999.791285. [33] Agarwal S. Snavely N. M. Seitz S. Szeliski R. Bundle Adjustment in the Large 2010 29 42 10.1007/978-3-642-15552-9˙3 S. Agarwal, N. Snavely, S. M. Seitz, R. Szeliski, Bundle adjustment in the large, 2010, 29–42. DOI: 10.1007/978-3-642-15552-9˙3. Afonso Castro is a junior web developer. He has an M.Sc. Degree in Mechanical Engineering from the Department of Mechanical Engineering of the University of Aveiro (2019). His master specialization was in robotics and, more precisely, sensor calibration. During his M.Sc. Dissertation development, Afonso Castro has published a research article entitled “A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS” for ROBOT2019: Fourth Iberian Robotics Conference, where he has participated and presented the mentioned work. "
    },
    {
        "doc_title": "Fast Grid SLAM based on particle filter with scan matching and multithreading",
        "doc_scopus_id": "85085940206",
        "doc_doi": "10.1109/ICARSC49921.2020.9096191",
        "doc_eid": "2-s2.0-85085940206",
        "doc_date": "2020-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Fast scan",
            "Multi-threading",
            "Particle filter",
            "Pose refinement",
            "Rao-blackwellized particle filter",
            "Real-time operation",
            "Scan matching",
            "Space management"
        ],
        "doc_abstract": "© 2020 IEEE.This paper presents a SLAM solution based on Rao-Blackwellized particle filters supported by a fast scan matching algorithm for pose refinement and a flexible space management data structure. By taking advantage of independence between particles its computational efficiency is further improved through multithreading. We have evaluated the efficiency of the solution by using several publicly available datasets and compared the results with the popular solution GMapping. The obtained results show the proposed approach provides a fast and accurate particle filter SLAM suitable for real-time operations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TIMAIRIS: Autonomous Blank Feeding for Packaging Machines",
        "doc_scopus_id": "85087543982",
        "doc_doi": "10.1007/978-3-030-34507-5_7",
        "doc_eid": "2-s2.0-85087543982",
        "doc_date": "2020-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Alternative solutions",
            "Computer vision system",
            "Current packaging",
            "Different shapes",
            "Industrial environments",
            "Mobile manipulator",
            "Modes of operation",
            "Multi-Modal Interactions"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Current packaging machine vendors do not provide any automated mechanism for blank feeding and the state of the art is to have a human operator dedicated to feed the blank piles to the packaging machine. This is a tedious, repetitive and tiring task. This also results in problems with unintentional errors, such as using the wrong pile of blanks. An alternative solution is the use of a fixed robotic arm surrounded by a protective cage. However, this solution is restricted to a single packaging machine, a unique type of blank shapes and does not cooperate with humans. TIMAIRIS is a joint effort between IMA S.p.A., Italy, (IMA) and the Universidade de Aveiro, Portugal, (UAVR), promoted by the European Robotics Challenges (EuRoC) project. Together, we propose a system based on a mobile manipulator for flexible, autonomous and collaborative blank feeding of packaging machines on industrial shop floor. The system provides a software architecture that allows a mobile robot to take high level decisions on how the task should be executed, which can depend on variables such as the number of packaging machines to feed and the rate of blank consumption at each one. Through a computer vision system, blanks of different shapes and sizes are correctly identified for adequate manipulation. The manipulation of the piles of blanks is performed using a single arm using compliant modes of operation to increase manipulation safety and robustness. Additionally, it has a safe navigation system that allows the robot to be integrated in an industrial environment where humans are present. Finally, it provides an enhanced multimodal interaction between human and robot that can be adapted to the environment and operator characteristics to make communication intuitive, redundant and safe.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Asynchronous mmWave Radar Interference for Indoor Intrusion Detection",
        "doc_scopus_id": "85079098351",
        "doc_doi": "10.1007/978-3-030-36150-1_30",
        "doc_eid": "2-s2.0-85079098351",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Common environment",
            "Energy variations",
            "FMCW radar",
            "Frequency spectra",
            "Indoor intrusion",
            "Interference patterns",
            "Millimetre-wave radar",
            "mm-Wave"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.This paper describes the development of a system for indoor intrusion detection that takes advantage of interference between asynchronous millimetre-wave radars. The approach exploits the information embedded in the interference pattern observed in the Doppler domain when two or more radars operate in a common environment and share the same frequency spectrum. By continuously monitoring the interference, it is possible to detect the corresponding energy variations. A sharp decrease in the interference energy is thus interpreted as an intrusion of an object or a person. Within this approach the source of the interference can be identified taking advantage of beam-forming of MIMO radars. Compared with the standard configuration, which exploits the reflection of radar signals, the proposed setup has the advantage of maximizing the energy available for intrusion detection and an increased capacity of obstacles and walls penetration. When combined with the capacity of mobile robots to dynamically position the radars, this scheme permits the implementation of highly versatile intrusion detection solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Machine Learning Methods for Radar-Based People Detection and Tracking by Mobile Robots",
        "doc_scopus_id": "85079096158",
        "doc_doi": "10.1007/978-3-030-36150-1_31",
        "doc_eid": "2-s2.0-85079096158",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Indoor environment",
            "Learning models",
            "Machine learning approaches",
            "Machine learning methods",
            "Mobile robotic",
            "Moving objects",
            "Non-persons",
            "People detection"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.This paper reports a machine learning approach for people detection and tracking in indoor environments using a compact radar system deployed by a mobile robot. The set-up described in the paper includes a series of experiments carried out in an indoor scenario involving walking people and dummies representative of other moving objects. In these experiments, distinct learning models (a neural network and a random forest) were explored with different combinations of radar features to achieve person versus non-person classification.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs",
        "doc_scopus_id": "85070213536",
        "doc_doi": "10.1016/j.robot.2019.06.006",
        "doc_eid": "2-s2.0-85070213536",
        "doc_date": "2019-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "European robotics challenges (EuRoC)",
            "Manufacturing industries",
            "Mobile manipulation",
            "Realistic environments",
            "Robotics technology",
            "Scientific competition",
            "Skill-based"
        ],
        "doc_abstract": "© 2019As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2019-08-02 2019-08-02 2019-08-08 2019-08-08 2019-09-13T06:57:28 S0921-8890(17)30794-7 S0921889017307947 10.1016/j.robot.2019.06.006 S300 S300.1 FULL-TEXT 2020-01-13T11:13:04.440713Z 0 0 20191001 20191031 2019 2019-08-02T15:14:58.064687Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 120 120 C Volume 120 9 103227 103227 103227 201910 October 2019 2019-10-01 2019-10-31 2019 article fla © 2019 Published by Elsevier B.V. SKILLBASEDANYTIMEAGENTARCHITECTUREFOREUROPEANROBOTICSCHALLENGESINREALISTICENVIRONMENTSEUROCCHALLENGE2STAGEIIREALISTICLABS LIM G 1 Introduction 1.1 Motivation 1.2 Related work 2 European robotics challenges (EuRoC) 2.1 Challenge 2: shop floor logistics and manipulation 2.1.1 Benchmarking 2.1.2 Showcase: autonomous packaging 2.2 EuRoC platform 2.3 TIMAIRIS software architecture 3 Agent architecture 3.1 Skill-based anytime agent architecture 3.2 Resource management scheme 4 Solving benchmarking tasks 4.1 Task 1: production logistics 4.1.1 SLC detection 4.1.2 Manipulation and planning strategy 4.2 Task 2: product assembly 4.2.1 Fixture detection 4.2.2 Bolt, nut and washer detection 4.3 Manipulation and planning strategy 5 Solving showcase tasks 5.1 Manipulation of stacked non rigid objects 5.2 Manipulator for packaging 5.3 Showcase perception 5.4 Motion planing 5.5 Task planning 5.6 Safe human–robot collaboration 6 Challenge evaluation 6.1 Benchmark evaluation 6.2 Showcase evaluation 7 Conclusion Acknowledgments References PRATT 2013 10 12 G KITANO 1997 340 347 H PROCEEDINGSFIRSTINTERNATIONALCONFERENCEAUTONOMOUSAGENTS ROBOCUPROBOTWORLDCUPINITIATIVE SICILIANO 2014 1 7 B ISRROBOTIK201441STINTERNATIONALSYMPOSIUMROBOTICSPROCEEDINGS EUROCTHECHALLENGEINITIATIVEFOREUROPEANROBOTICS BISCHOFF 2010 15 16 R CULLY 2015 503 A HILDEBRANDT 2016 21 27 A AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2016INTERNATIONALCONFERENCE AFLEXIBLEROBOTICFRAMEWORKFORAUTONOMOUSMANUFACTURINGPROCESSESREPORTEUROPEANROBOTICSCHALLENGESTAGE1 ZADEH 2017 81 103 S BAKLOUTI 2017 9 14 E NILSSON 1999 205 226 K CHATZILYGEROUDIS 2018 236 250 K INSAURRALDE 2015 87 104 C DEAN 1988 49 54 T AAAIVOL88 ANALYSISTIMEDEPENDENTPLANNING GREFENSTETTE 1992 189 195 J MACHINELEARNINGPROCEEDINGS1992 APPROACHANYTIMELEARNING PEDROSA 2015 457 468 E PROGRESSINARTIFICIALINTELLIGENCE17THPORTUGUESECONFERENCEARTIFICIALINTELLIGENCEEPIA2015 ASKILLBASEDARCHITECTUREFORPICKPLACEMANIPULATIONTASKS AMARAL 2017 198 203 F AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2017IEEEINTERNATIONALCONFERENCE SKILLBASEDANYTIMEAGENTARCHITECTUREFORLOGISTICSMANIPULATIONTASKSEUROCCHALLENGE2STAGEIIREALISTICLABSBENCHMARKING LIM 2017 159 164 G AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2017IEEEINTERNATIONALCONFERENCE RICHROBUSTHUMANROBOTINTERACTIONGESTURERECOGNITIONFORASSEMBLYTASKS LIM 2017 15 27 G IBERIANROBOTICSCONFERENCE HUMANROBOTCOLLABORATIONSAFETYMANAGEMENTFORLOGISTICSMANIPULATIONTASKS MOKHTARI 2016 993 1005 V INTELLIGENTAUTONOMOUSSYSTEMS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES LIM 2019 G BALOGH 2005 17 R THRUN 2006 661 692 S BUEHLER 2009 M DARPAURBANCHALLENGEAUTONOMOUSVEHICLESINCITYTRAFFICVOL56 LIM 2017 336 341 G 2017IEEEINTERNATIONALCONFERENCEMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI NEURALREGULARIZATIONJOINTLYINVOLVINGNEURONSCONNECTIONSFORROBUSTIMAGECLASSIFICATION RUSSELL 2010 S ARTIFICIALINTELLIGENCEAMODERNAPPROACH LIM 2018 231 236 G 2018IEEEINTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC MOBILEMANIPULATIONFORAUTONOMOUSPACKAGINGINREALISTICENVIRONMENTSEUROCCHALLENGE2STAGEIISHOWCASE LIM 2019 1 11 G LIM 2013 387 395 G INTELLIGENTAUTONOMOUSSYSTEMSVOL12 ONTOLOGYREPRESENTATIONINSTANTIATIONFORSEMANTICMAPBUILDINGBYAMOBILEROBOT LIM 2011 492 509 G RUSU 2008 927 941 R RUSU 2009 R SEMANTIC3DOBJECTMAPSFOREVERYDAYMANIPULATIONINHUMANLIVINGENVIRONMENTS OLIVEIRA 2016 614 626 M PEDROSA 2016 35 40 E 2016INTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC ASCANMATCHINGAPPROACHSLAMADYNAMICLIKELIHOODFIELD COHENOR 1995 453 461 D RUSU 2011 1 4 R ROBOTICSAUTOMATIONICRA2011IEEEINTERNATIONALCONFERENCE 3DPOINTCLOUDLIBRARYPCL LOWE 1999 1150 1157 D COMPUTERVISION1999PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCEVOL2 OBJECTRECOGNITIONLOCALSCALEINVARIANTFEATURES TUDICO 2017 498 509 A PORTUGUESECONFERENCEARTIFICIALINTELLIGENCE IMPROVINGBENCHMARKINGMOTIONPLANNINGFORAMOBILEMANIPULATOROPERATINGINUNSTRUCTUREDENVIRONMENTS LIMX2019X103227 LIMX2019X103227XG 2021-08-08T00:00:00.000Z 2021-08-08T00:00:00.000Z © 2019 Published by Elsevier B.V. 2019-08-10T21:40:24.934Z S0921889017307947 National Funds China National Funds for Distinguished Young Scientists FCT - Foundation for Science and Technology UID/CEC/00127/2013 This work was supported by the EuRoC Project under Grant no. 608849 and by National Funds through the FCT - Foundation for Science and Technology , in the context of the project UID/CEC/00127/2013 . Dr. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D. degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Marie Curie individual fellow in the School of Computer Science at University of Manchester, UK. His research interests lie in the area of artificial intelligence and machine learning for autonomous robots, including perception, semantics, cognition and spatiotemporal representations on neuromorphic architectures. Eurico Pedrosa is a Post-Doc Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his Informatics Engineering degree from University of Aveiro in 2010 and a Computer Science Ph.D. degree from Aveiro University in 2018. His research interest are focused on intelligent robotics, robotic navigation including localization and mapping (SLAM), space representation using volumetric grids and most recently the application of radar sensors in indoor robotics. Filipe Amaral is a Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his MSc degree in Computer and Telematics Engineering from University of Aveiro in 2014. His current research interests are in the area of autonomous mobile robotics. Prof. Dr. Artur Pereira was born in Vila Nova de Famalicão, Portugal, in April 1960. He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 2003. He is currently an Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Instituto de Engenharia Electrónica e Informática de Aveiro. The main focus of his research is robotics at the architectural and software levels, with emphasis on simulation, navigation, localization, mapping, and machine learning. Nuno Lau is Assistant Professor at Aveiro University, Portugal and Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), where he leads the Intelligent Robotics and Systems group (IRIS). He got is Electrical Engineering Degree from Oporto University in 1993, a DEA degree in Biomedical Engineering from Claude Bernard University, France, in 1994 and the Ph.D. from Aveiro University in 2003. His research interests are focused on Intelligent Robotics, Artificial Intelligence, Multi-Agent Systems and Simulation. Nuno Lau participated in more than 15 international and national research projects, having the tasks of general or local coordinator in about half of them. Nuno Lau won more than 50 scientific awards in robotic competitions, conferences (best papers) and education. He has lectured courses at Phd and MSc levels on Intelligent Robotics, Distributed Artificial Intelligence, Computer Architecture, Programming, etc. Nuno Lau is the author of more than 150 publications in international conferences and journals. He was President of the Portuguese Robotics Society from 2015 to 2017, and is currently the Vice-President of this Society. Prof. Dr. José Luís Azevedo is currently Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Institute of Electronics and Informatics Engineering of Aveiro (IEETA). He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1998. His current research interests are in the area of cooperative autonomous mobile robotics. Prof. Dr. Bernardo Cunha was born in 1959 in Porto, Portugal. He earned his doctoral degree in electrical engineering at the University of Aveiro, Portugal, in 1999. He is a full time teacher at Universidade de Aveiro in the computer architecture area and an investigator at the Instituto de Engenharia Electrónica e Informática de Aveiro. Current research interests are centered in the area of cooperative autonomous mobile robotics. Simone Badini is a Mechanical Designer in the Research and Development department of IMA Spa since 2013. IMA Spa is a world leader company in the design and manufacture of automatic machines for the processing and packaging of pharmaceuticals, cosmetics, food, tea and coffee and tobacco. He got is M.Sc. degree in Mechanical Engineering from University of Bologna, Italy in 2012. He is currently project manager for the integration of cobot and autonomous mobile robot in the production lines for the IMA group. item S0921-8890(17)30794-7 S0921889017307947 10.1016/j.robot.2019.06.006 271599 2020-01-13T11:13:04.440713Z 2019-10-01 2019-10-31 true 3280747 MAIN 14 53587 849 656 IMAGE-WEB-PDF 1 gr11 88037 82 219 gr6 76125 134 219 gr10 99763 151 219 gr1 84425 55 219 gr12 78236 62 219 pic3 95379 163 140 gr13 84239 60 219 gr17 92453 164 193 gr2 91612 129 219 gr19 18466 78 219 fx1002 5080 40 219 gr9 80285 162 219 pic5 90620 164 140 pic8 92385 164 140 gr4 86357 129 219 gr7 83216 163 155 gr8 78279 144 219 pic4 91410 163 140 gr5 84112 164 138 gr3 101514 163 219 gr18 77492 164 204 pic1 90433 163 140 gr16 82949 47 219 fx1001 5682 68 219 gr14 84362 115 219 gr20 34694 98 219 pic2 92082 164 140 pic6 88104 163 140 gr21 22647 162 219 gr15 93327 108 219 pic7 96532 163 140 gr11 115147 142 378 gr6 102249 210 342 gr10 127663 261 378 gr1 119793 128 506 gr12 100014 106 376 pic3 101319 132 113 gr13 105403 103 376 gr17 149053 321 378 gr2 129409 223 378 gr19 42698 110 309 fx1002 27829 150 816 gr9 109290 277 375 pic5 98434 132 113 pic8 101766 132 113 gr4 113348 190 323 gr7 120919 318 302 gr8 117887 327 496 pic4 97712 132 113 gr5 117350 225 189 gr3 125523 242 325 gr18 101664 243 302 pic1 99455 132 113 gr16 120354 112 525 fx1001 36645 252 816 gr14 114768 194 370 gr20 52488 146 325 pic2 99552 131 112 pic6 98208 132 113 gr21 59417 279 378 gr15 119748 187 378 pic7 107374 132 113 gr11 364806 629 1674 gr6 175979 929 1514 gr10 495261 1154 1674 gr1 351578 568 2243 gr12 185445 470 1668 pic3 169551 583 500 gr13 214287 455 1666 gr17 622945 1422 1674 gr2 342433 988 1675 gr19 90504 487 1369 fx1002 103050 398 2169 gr9 245828 1230 1663 pic5 158735 584 500 pic8 168594 584 500 gr4 258891 842 1433 gr7 296978 1410 1339 gr8 281811 1449 2197 pic4 140145 583 500 gr5 232097 999 840 gr3 443350 1074 1440 gr18 160189 1077 1340 pic1 155377 583 500 gr16 409379 497 2326 fx1001 144071 670 2169 gr14 245424 859 1639 gr20 178264 647 1440 pic2 165958 583 499 pic6 138608 583 500 gr21 195782 1237 1675 gr15 306022 828 1676 pic7 174577 583 500 si27 26913 si33 1898 si34 1618 si31 7355 si3 2005 si14 1133 si6 2114 si38 3009 si21 1832 si37 4191 si18 3593 si36 1780 si16 8122 si2 1418 si8 1661 si1 1607 si9 4591 si26 2321 si19 1613 si32 6363 am 19107071 ROBOT 3227 103227 S0921-8890(17)30794-7 10.1016/j.robot.2019.06.006 Fig. 1 Human operators in a packaging industry. Fig. 2 KUKA KMR mobile manipulator. Fig. 3 Initial setup of the table for task 2. Fig. 4 Showcase environment in the gazebo simulator. Fig. 5 EuRoC software framework for C2 [23]. Fig. 6 Use case diagram of skill-based anytime agent architecture (SAAA). Fig. 7 A higher level overview of SAAA. Fig. 8 Sequence diagram of SAAA. Fig. 9 A skill-based architecture for safe human–robot collaboration. Fig. 10 Example of detection of two SLCs on a shelf. The top image is what is perceived by our detection system (plus depth). The bottom image contains a superimposed 3D model of the SLC for each detection, including the center of the SLC (red dot) and a possible pick point (green dot) . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Example of nut and bolt detection. The top image contains the detection of a nut and the bottom image contains the detection of a bolt. The same detection algorithm is used for nuts and bolts. Fig. 12 Washer detection in the fixture. The top image is what is acquired by the camera. The bottom image is the result of the segmentation by intensity. Fig. 13 Snapshots of bent blank piles. Fig. 14 Gripper and blank magazine in simulator. Fig. 15 Grpeer and blank magazine in the realistic environment. Fig. 16 Two pallets and blank piles and their detection results. The red lines in center and right images indicate the pose of pallets, while the green circles indicate the positions of blank piles. Fig. 17 A sequential result of the pose estimation of a blank pile. Fig. 18 Drawing pin filter to detect outlines. Fig. 19 A task plan for 9 blank piles on two pallets. Fig. 20 Human tracking and HRI during the EuRoC evaluation. Fig. 21 Interaction tree for the showcase task. Table 1 Benckmark task 1. Team Metric 1 Metric 2 Bonus Time AutoMAP 5 5 6.22 4:56 MTC-LU-UoB-Airbus 5 4 0 18:13 RSAII 5 5 3.32 9:14 NimbRo Logistics 5 5 3.45 8:54 TIMAIRIS 5 5 10 3:04 Table 2 Benckmark task 2. Team Metric 1 Metric 2 Bonus Time AutoMAP 5 5 4.16 15.01 MTC-LU-UoB-Airbus 0 1 0 8:54 RSAII 5 5 3.78 16:31 NimbRo Logistics 4 4 0 41:27 TIMAIRIS 5 5 10 6:15 Table 3 Quantifiable evaluation. Objectives Metrics Targets Events Percent (%) O1 O1M1 6+5+3 6+5+3 100 O1 O1M2 6 6 100 O1 O1M3 5 5 100 O1 O1M4 2 2 100 O2 O2M1 6 6 100 O2 O2M2 6 6 100 O2 O2M3 6 6 100 O2 O2M4 2 2 100 O3 O3M1 3 3 100 O3 O3M2 12 12 100 O4 O4M1 4 4 100 O4 O4M2 2 2 100 Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs Gi Hyun Lim a 1 Eurico Pedrosa a 1 Filipe Amaral a Artur Pereira a ⁎ Nuno Lau a ⁎ José Luís Azevedo a Bernardo Cunha a Simone Badini b a IEETA, Universidade de Aveiro, Aveiro, Portugal IEETA, Universidade de Aveiro Aveiro Portugal IEETA,Universidade de Aveiro, Aveiro, Portugal b IMA, Via Emilia 428-442, 40064 Ozzano dell’Emilia, Italy IMA Via Emilia 428-442 Ozzano dell’Emilia 40064 Italy IMA,Via Emilia 428-442, 40064 Ozzano dellEmilia, Italy ⁎ Corresponding authors. 1 These authors contributed equally to this work. As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging. Keywords Skill-based Anytime agent architecture Mobile manipulation Autonomous packaging European robotics challenges (EuRoC) 1 Introduction 1.1 Motivation Several robotics challenges and competitions have been launched for the exchange of research results and for comparative evaluations including manipulation, such as the DARPA Robotics Challenge [1], the RoboCup leagues [2] and the European Robotics challenges (EuRoC) project [3], since neither resources such as robots’ platforms, source codes and datasets are available to the public nor are simulators sufficiently mature to present real environments. To obtain a good result in a robotics challenge or competition, challengers need to develop a robotic system that completes given challenge tasks within a limited time. EuRoC is a research project where a robotic competition is conducted with the aim to develop and present solutions to the European manufacturing industry [3]. The EuRoC consortium has launched three industry-relevant challenges: C1 - Reconfigurable Interactive Manufacturing Cell, C2 - Shop Floor Logistics and Manipulation, and C3 - Plant Servicing and Inspection. In particular, the Challenge 2 (C2) addresses the SRA2009 [4] scenarios: Logistics and Robotic Co-Workers. Mobile manipulators are provided by the EuRoC host as a suggested solution to utilize as logistic carriers and dexterous manipulators. Each challenge team should consist of both a research group and an industry partner to show the use case in a realistic environment. Our work is being developed within the Challenge 2 context. The Challenge 2 host especially set the working time regulations to share the resources among all challenger teams and to make sure that they have the same amount of effective lab time in the realistic environment where the challengers access robotics platforms and benchmark infrastructures. As the use case scenario of TIMAIRIS, which is a collaborative challenge team between the Intelligent Robotics and Intelligent Systems (IRIS) research group from the University of Aveiro (UAVR) in Portugal and IMA S.p.A industry from Italy, an autonomous blank feeding task is investigated because it is not easily solved by a robot. Proof of such assumption is that the industrial state-of-the-art solution for this problem is to use human operators to perform it, as shown in Fig. 1. This is a tedious, repetitive and tiring task and human operators may occasionally refrain from collecting blank piles from more distance pallets. This paper presents a skill-based anytime agent architecture (SAAA) and the result of TIMAIRIS team in the second stage of C2. TIMAIRIS is one of 6 challenger teams advancing to the final stage out of 103 teams in the simulation contest, The second stage consists of three phases to be performed in a real environment: Benchmarking, Freestyle and Showcase. In the realistic labs of the EuRoC challenge tasks, the main constraint is time, not only the running time (online efficiency) in an evaluation matrix but also development time (offline efficiency) to complete the tasks in a limited time. Each challenger team is allowed to take a limited fixed time to access an experimental environment in which EuRoC hosts offer support and the mandatory robot platform is available. Especially, for the three phases of the stage II, each team has evenly-distributed eleven weeks (54 working days) to exclusively access the environment. Challenger teams are required to find efficient and robust skills and complete evaluations in the given time. SAAA provides opportunities to increase efficiency not only in autonomous execution for shop floor logistics and manipulation but also in its use of development during the challenge competition. 1.2 Related work To meet the requirements and constraints on a robotic challenge, challenger teams need an adaptive, flexible, robust and efficient robotics architecture [5,6]. Several reactive architectures are proposed to find a solution that makes a robot complete tasks on-time. By managing mission time, Zadeh et al. [7] developed an autonomous reactive architecture of unmanned vehicles in realistic ocean environment. Synchronization between high level mission and low level path planning is configured to control mission time and to guarantee termination of the mission with the best sequence of tasks fitted to available time. Baklouti et al. [8] proposed a reactive control architecture for wheelchair robot navigation without human intervention and prior knowledge of the world. Nilsson and Johansson [9] introduced a layered Open Robot Control (ORC) architecture to meet industrial demands such as computing efficiency and simple factory-floor operation with integration of online and offline robot programming. Offline programming typically done by robot programmers requires abstract modeling, whereas online programming typically done by production engineers or robot operators utilizes physical robots in real environments. Recently several trial and error methods are proposed to adapt for use by damaged robots [5,10]. Those focus on the online time challenge by reducing the number of trials to recover from damage to complete tasks. Cully et al. [5] introduced an intelligent trial and error algorithm which enables a robot to discover a compensatory behavior from damage without requiring pre-programmed contingency plans. The algorithm conducts experiments based on high-performing policies for the intact robot work on the damaged robot. While traditional reinforcement learning (RC) methods for robots need to reset their learning environments and robots to an initial state, Chatzilygeroudis et al. [10] proposed a reset-free trial-and-error learning for robot damage recovery. Insaurralde and Petillot [11] proposed a capability-oriented robot architecture that enables multiple unmanned vehicles to collaborate to autonomously carry out underwater intervention missions in a fault-tolerant manner. On the other hand, anytime approaches use iterative improvement techniques in problem solving, planning, scheduling [12] and learning [13]. Those address the online time challenge by returning results at any time. To adapt to realistic industrial environments, this paper proposes a skill-based anytime agent architecture (SAAA) by integrating previous work [14–17]. The architecture consists of a solver, modular skills and task representations. The separation of task s that organize orders of objects to assemble into graphs from an agent processing algorithm [18,19] allows a robot to start at any task state. It is not necessary to reset the robot and its environment to run from an initial state. During development, a robotics challenger team needs to repeat a subtask or to execute a skill from a specific state to refine the subtask or the skill until the time of evaluation. On the other hand a human co-worker will continue or restart a task after a task failure or a task completion at the runtime. To the best of our knowledge, robotic system architectures and anytime algorithms consider only online efficiency while the proposed architecture also takes offline efficiency into account. 2 European robotics challenges (EuRoC) Many robotic challenges have been so far launched to find competitive solutions for their applications [20]. The DARPA Grand Challenge [21] and its successive events, DARPA Urban Challenge [22] and DARPA Robotics Challenge [1], have drawn worldwide attention as robotic competitions for autonomous ground vehicles on an off-road course, for autonomous operation in a urban environment and for autonomous emergency maintenance robots, respectively. The main objective of the DARPA challenges is bridging the gap between fundamental discoveries of academia and military use. Since 1997, RoboCup [2] has been held annually to foster AI and intelligent robotics research. Initially the target of competition was a world cup with real robots, nowadays there are also other leagues such as RoboCup Industrial, RoboCup Rescue and RoboCup@Home. Especially, the RoboCup Industrial league defines two tasks: logistics and manipulation; but robots in this league have size constraints. To match the Strategic Research Agenda (SRA) for robotics [4], the EuRoC project was launched in 2014. It aims at exploiting synergies among all the actors of robotics and manufacturing to accelerate the transference of state-of-the-art technologies from academia to industry. After qualification, the members of each team are required to organize from both communities. The main motivation of the European Robotics Challenges (EuRoC) is to make use of robotics products and services by strengthening collaboration between the industrial and the research community [3]. The EuRoC is a research project based on robotics competitions that drives innovation in robotics and manufacturing through a number of application experiments. The Challenge Advisory Board of EuRoC is in charge of the evaluation of the competition project empowered on robotic platforms and benchmark infrastructures to get rid of contestants’ burden on platform-related low-level problems and maintenance. As a realization in this context, the EuRoC initiative has launched and run three challenges in parallel with different motivations and objectives. The Shop Floor Logistics and Manipulation or Challenge 2 (C2) covers application scenarios about robotic co-workers and logistics robots in industrial, professional service and domestic service sectors, which is matched to the Strategic Research Agenda (SRA) for robotics in Europe [4]. In the co-worker scenario, a robot becomes an assistant or collaborator that works literally hand-in-hand with its human counterparts in unstructured environments. Here, the robot needs to communicate with a human to take an order, to ask confirmation, and to reply to a question. Each EuRoC challenge is structured in three successive stages, over the period of 4 years: Stage I QUALIFYING: Simulation Contest, Stage II REALISTIC LABS: Benchmarking, Freestyle and Showcase and Stage III FIELD TESTS: Pilot Experiments. 2.1 Challenge 2: shop floor logistics and manipulation The Shop Floor Logistics and Manipulation challenge or Challenge 2 of EuRoC project consists of a assembly of tasks designed to be solved by a mobile robotic platform operating in industrial environment. This challenge is divided in three main stages where three different scenarios were designed and considered. Stages I and II were already completed and the Stage III will be done at the end of the EuRoC project. Briefly, the first stage was based on a stationary platform and the achieved solutions were integrated in a simulation environment. The Stage II consists in a scenario where a real Light-Weight-Robot (LBR iiwa) equipped with a two-finger jaw-gripper mounted on a mobile base (KUKA omniRob) was controlled through an intranet-based framework (similar to the one that was used in Stage I), as shown in Fig. 2. The final stage, Stage III, will be an application of all the achieved solutions in a real scenario of industry with real work pieces provided by an end-user, showing the real capabilities of this innovative solution and the contribution to the manufacturing dynamics improvement. 2.1.1 Benchmarking For the Stage II in the Challenge 2, the EuRoC host has provided a benchmark environment at DLR in a realistic factory set-up with elements from real end users. It consists of two tasks in simplified manufacturing environment: logistics and assembly. The first task of the Benchmarking phase addresses production logistics. In this task, the mobile manipulator has to bring 5 Small Load Carriers (SLCs) which are distributed in the room to a fixed target area on a table. One SLC is placed on the goal table out of the target area, two are placed on another table and two are located in a shelf. The room has enough space for the robot to move between the tables and the shelf. The start position is in the middle of the room. The setup of the room will be static over the development and evaluation. The SLCs are filled with a various number of nuts and washers from the Benchmarking task 2. For each correctly delivered SLC (its footprint has to be inside the target area) one point is awarded (max 5 points). For each SLC, if all parts in it remain within it during the transport to the goal area, another point is awarded (max 5 points). If the task is completed successfully extra points could be awarded for the execution time. The amount of points is inversely related to the best teams time (max 10 points). The second task of Benchmarking is about mobile manipulation for basic pre-assembly of products. In this task, the robot has to assemble five bolts. The layout of the room and the start position of the robot are the same as in the previous benchmark task. The assembly table is identified as “Pickup table”. On that table, there are 5 nuts, 5 bolts and 5 washers within a fixed area. Next to the parts are two fixtures attached to the table. One of the fixtures holds the washers and the other has a gap with the bolt’s head shape to allow the nut to be screwed. The parts are arranged to minimize occlusions (see Fig. 3). The bolts are upright, the nuts are flat and the washers are placed in the fixture to give a good orientation for grasping. The translation between the different parts may vary. The nuts and bolts are touching a virtual line parallel to the fixtures 10 cm and 20 cm behind, respectively. For each nut correctly screwed, one point is awarded (max 5 points). Applying a washer onto the bolt before screwing a nut gives another point (max 5 points). As in the previous task, if the task is completed successfully extra points are awarded for the execution time. Again, the amount of points is inversely related to the best teams time (max 10 points). 2.1.2 Showcase: autonomous packaging As the third and final challenge of the Stage II, the showcase runs in an simplified environment that, although not located in an industrial plant, includes the most important elements of the real environment where the task will be executed as a final product, as shown in Fig. 4. It uses real pallets with real piles of blanks and a prototype of the blank feeding mechanism of a packaging machine that includes all relevant features for the task to be performed. In this task the following issues are going to be covered: the platform will be able to recognize empty pallets, plan and replan its actions; an initial version of the multimodal interface will be used, integrating the possibility of gesture commands in the interaction with the platform (gestures will be used mostly for safe navigation); the robot will be able to navigate in an environment that includes a few humans and interact with them; the manipulation of the selected type of blanks will be demonstrated. To solve the blank feeding task using a mobile robot, several technical issues are identified. • Manipulation of stacked objects: The blank pile is not a unique rigid solid object. Not only it can bend under its own weight, but blanks can also easily break free from the pile due to the fact that a considerable amount of low friction blanks are stacked on top of each other. • Single arm manipulation: The manipulation of a blank pile is performed using a single arm with a gripper. In order not to miss any blanks out of a stack on a pallet, picking up the blank pile requires several manipulation steps and a specialized gripper. • Smooth placing of the blank pile: Blank piles have to be placed in the feeding mechanism avoiding hard collisions with the blanks that are already there and with the four blank guiding rods that prevent blanks from slipping out of the inclined magazine surface. • Shared workspace with humans: The robot has to be able to navigate in an environment that is shared with humans. This introduces safety concerns but, at the same time, it also rises the opportunity to take advantage of the human–robot proximity to explore human–robot interactions (HRI) to control the robot [16]. • Robust detection of blanks: Blanks are provided in untied piles, piled up on pallets close to each other, which can cause erroneous detection. • Handling global localization errors: The feeding of the blank magazine is an example of a manipulation action that requires a high level of precision for a proper feeding while preventing collisions between the blank magazine and the manipulator. 2.2 EuRoC platform As an objective of the EuRoC, robotics platforms and benchmark infrastructures have been developed to make challengers focus on the challenging research without efforts on platform-related low-level problems and maintenance. Especially for C2, two mobile manipulators are suggested to address logistics and robotic co-workers scenarios, as shown in Fig. 2. EuRoC hosts and robot manufacturers also provide programming and simulation frameworks, and open interfaces on lowest levels. Available EuRoC SW tools 2 2 are also provided for use in the EuRoC. Fig. 5 shows the EuRoC software framework for C2. Challengers are required to develop high-level software components with the similar functionalities but different scenarios to test and validate in meaningful contexts typically for shop floor logistics and manipulation in C2. 2.3 TIMAIRIS software architecture TIMAIRIS software is composed of three main components: a skill-based agent architecture (SAAA) [15], a human tracker and safety manager subsystem [17] and a realistic simulator [14]. TIMAIRIS software uses a skill-based anytime agent architecture [15] that has been evolving from the one used for Stage I simulation tasks [14]. The separation of task-dependent representations and a generic agent processing algorithm allow the robot to start at any task state. Human collaborators or robot operators need to repeat a task after recovering from failure or to test the skill from a given starting point. Having the same architecture performing successfully for such a different set of tasks demonstrates that this team solution is remarkably flexible and well adapted for EuRoC C2 platform and its capabilities. Ensuring safety, industrial robots need to share an environment with humans and to work hand in hand. To realize safe human–robot collaboration, a human tracker and safety manager subsystem has been integrated into the SAAA [17]. The human tracker keeps on tracking humans in a workspace. The safety manager infers whether it is in a safe state or not based on system states and human tracking information. Then, human–robot interaction module takes an order from the operator to resume or stop the paused task using gestures [16,24]. A realistic simulator has been developed for the Challenge 2 (see Fig. 4), using, as starting point, the Stage II simulator provided by the EuRoC host. The simulator allows the execution of the complete Challenge 2 tasks and was an essential tool in the development of TIMAIRIS’ Challenge 2 software. 3 Agent architecture 3.1 Skill-based anytime agent architecture To address the requirements and achieve the objectives of C2, a skill-based anytime agent architecture (SAAA) has been developed to solve the logistics and manipulation tasks [15]. This paper extends the previous architecture in the way that a plan manages more than one object and a robot explores workspace that cannot be covered by its cameras without moving the platform. In particular, the new architecture has been used to solve the Production Logistics and Product Assembly tasks previously described. These tasks have four objectives: perception, manipulation, planning and human robot interaction. Fig. 6 shows the use case diagram of SAAA at development time and/or runtime for human–robot collaboration. A robot and two types of humans are involved in the use case: a developer and a co-worker in a scenario for autonomous packaging. A robot developer sets a robot state to refine a skill which is executed at the state. It is possible that the developer repeat to execute the skill to improve performance and reduce runtime without reseting the robot and its environment. For example, a placing skill will be executed after picking and transport skills in pick-and-place tasks. To refine the placing skill, it is not efficient to run whole pick-and-place task by reseting to an initial state. A human co-worker may provide a command to start and stop a task or to change a sequence of objects to be delivered. A mobile manipulator (see Fig. 2) sequentially executes skills to complete the Production Logistics and Product Assembly task. In a higher level view of the skill-based framework, as shown in Fig. 7, the functional components are represented by boxes. A Perception module collects sensory data and processes them to extract information used by high-level modules. A Skill is the capacity of doing a particular task, such as, picking and placing an object or moving the end-effector of the manipulator to a desired pose. Perception modules and Skills collect and transmit sensory-motor data via Sensor interface and Effector interface, respectively. The Action Planner provides a plan for the target object. A plan contains a sequence of skills and their corresponding objects. The Solver is responsible for taking decisions on how to solve the current task based on the current sensory data and available Skills. To solve these tasks, an agent is required to perceive the environment through sensors and act upon that environment using actuators [25]. A skill-based agent architecture is implemented to develop a generic solution capable of handling shop floor logistics and assembly tasks by analyzing the properties of the environment. Henceforth, the proposed method has been utilized for several packing scenarios by increasing the realization during the EuRoC challenges [16,17,24,26,27]. Fig. 8 shows the sequence diagram of SAAA for autonomous packaging. To complete a task, a robot developer can set the state of a robot to start a new task from the initial state or to repeat the previous skill to continue the previously uncompleted task. When the robot start a task, the Solver [15] reads the Order graph [14] of the task and also tries to get a command from Interaction which is a module that tracks humans [17] and gets a command from a co-worker via gesture recognition [16]. The Solver requests a plan from Planner with the states of the robot and its environment, and executes a sequence of skills by following the plan. It continues until completing the task or terminated by the robot developer. when the robot stops, the developer repeats to set the state and to start the robot. The proposed algorithm is summarized in 1. In each task, the set O of objects to be manipulated, including their properties and place zones, is known in advance. The algorithm starts by building an Order Graph which represents the order of objects. The order is restricted by a direct acyclic graph (DAG), which represents a dependency graph between objects in terms of order of manipulation [14]. Leafs represent objects that need to be handled first. The dummy object λ is added to represent the graph root. To ensure that all object are eventually detected [28], a set of search poses S is estimated by the procedure buildSearchSpace with the insurances that all combined poses cover the whole working space. The set S is encoded as a circular list so that the search for poses never ends. Additionally, to improve the search [29], the vision system on the pan tilt unit is used to detect objects in the environment and the obtained poses are put in the head of S in the order defined by G . It may not detect any object, but, if it does, the system can gain in execution times due to good initial search poses. The algorithm then executes nested loops that, making the robot move around the search poses, finishing when only the root node ( λ ) remains in the Order Graph. Each search pose is explored to see if a leaf object is there. A plan is a tuple [19] consisting of a sequence of skills and their corresponding objects that allows to properly pick objects, move the end-effector of the manipulator and place the objects in the target position [30]. Those skills depend on a priori calculation of the pick and place pose. The specific plan depends on the task being solved. The Action Planner can estimate the state of task based on the input object. The plan could be related with several objects. For instance, in the assembly process several parts are added in sequence until the final assembly is produced. If the execution of the skill succeeds, the leaf corresponding to the processed object is removed from the graph. Because Order Graph and plan are separated from the agent processing algorithm, only three procedures are task dependent, buildOrderGraph, buildSearchSpace and makePlan. This formulation has two significant advantages. First, the agent can start execution at any task state, even if the agent meets a failure while execution, as it can continue the task after recovering from the failure. Second, to solve a task the developer only has to focus on the creation of a plan supported by a set of available skills. 3.2 Resource management scheme Fig. 9 shows an extension of the skill-based architecture for safe human–robot collaboration. The Human tracker keeps on tracking humans in the workspace by using laser scanners. The Human–Robot Interaction (HRI) module communicates with a human by recognizing gestures [24] and by providing information via multi-modal interfaces [16]. The Safety manager infers whether it is in a safe state or not based on system states and human tracking information. When the Safety manager decides to pause an executing task, it requests Solver to gaze at the nearest human operator, and to trigger HRI to interact with him. Then, HRI takes an order from the operator to resume or stop the paused task using gestures. To continuously monitor humans, the Human tracker and Safety manager need to be continuously active, while all other modules including skills and perceptions modules just run on request. That should cause conflicts over resources. For example, when the robot wants to recognize the pose of a blank magazine to feed a blank pile, a perception module tries to rotate the pan–tilt camera system to the blank magazine on the table. If, at the moment, a human operator approaches, the Safety manager also tries to rotate the same camera system to gaze the operator. Based on the skill-based agent architecture [14,15], a resource management scheme is added to the system architecture, as shown in Algorithm 2. When a robot starts, each module which needs to run continuously is launched and becomes a daemon. The Solver builds a resource map which lists all necessary resources for each daemon module. At every spin, which means a wake-up for all subscriptions, services, timers and so on in ROS (Robot Operating System), 3 3 the daemon checks the availability of its resources. If available, it runs normal procedures and release all resources at the end of the procedures. 4 Solving benchmarking tasks The objective of our team is to solve the tasks proposed for the EuRoC Benchmarking phase with high precision, accuracy and robustness, as fast as possible. Time is a key factor in solving the tasks at hand: not only it is an evaluation metric, but, for industrial applications, it a matter of productivity by reducing the time for development. In this section, we present our approach to solve two tasks of the Benchmarking phase -Production Logistics and Product Assembly- using the presented architecture. 4.1 Task 1: production logistics The goal of this task is to find all SLCs present in the room such as tables, shelves and workbenchs and place them in the designated target area in a workbench. Their approximate locations are known, on top of two tables and in a shelf, but their exact positions on those locations are unknown. To solve this task we start by designating several search poses that guarantees that eventually all SLCs are detected with the stereo camera mounted in the pan and tilt unit. Once an SLC is in sight, its pose should be detected, in order to pick it up. Then the SLC is placed somewhere else, in an intermediate or target area. This last step has several variations that affect the overall execution time because of the locations of SLCs. 4.1.1 SLC detection The detection of SLCs has two stages: first, color segmentation is used to define candidate regions in the image, that may contain at least an SLC; second, a 3D point cloud for each region is generated and matched against 3D template of an SLC to find its position and orientation, i.e. its pose. Color segmentation, in the HSV color space, is initially used to detect an SLC due to the SLC’s color homogeneity and good contrast with the environment. The output of the color segmentation is then used to generates blobs that represent SLC candidates. The resulting blobs must have a minimum size to be valid. The overlapping of SLCs in the image may generate a single blob for multiple SLCs, but that is not a concern as the disambiguation is deferred to the next stage. Once color segmentation is completed, the resulting blobs are used as masks to generate a point cloud for each candidate. While 3D point cloud generation and matching processes are heavy, the color segmentation is not. Thus, the SLC detection is time-optimized by reducing many candidates from the color segmentation. Let P be a point cloud and { p i } the set of points of P . The calculation of SLC’s pose relies on the processing of the point cloud P . To reduce the computational complexity of processing a point cloud with a high number of points, P is decimated by applying a voxelization filter. Then, to remove possible outliers in the point cloud a statistical outlier removal [31] is used. To address the possibility of SLCs overlap, the point cloud is divided into clusters using a euclidean cluster extractor [32], the cluster with the highest number of points being assigned to P and the remaining points being discarded – thanks to our agent architecture, discarded SLCs will be detected in the next cycle. The position of the SLC is approximately calculated from the centroid c of P , given by c = 1 n ∑ i = 1 n p i , while its orientation is initially provided by the Principal Component Analysis (PCA) of the projection of P in the X O Y plane [33]. However, this approach is not enough, as the view of the SLC may provide a partial point cloud that skews the centroid from the real center, and the orientation of the PCA has an ambiguity of π radians. The final pose of the SLC is calculated by matching P against a 3D template of the SLC that has its centroid in the origin and its bearing defined by the X axis. Before matching the point cloud, P is transformed to its origin, i.e. the inverse pose of P is applied to itself, then, the transformation that results from the matching between the template and P is the correction of the initial pose calculation. Note that because the orientation given by PCA is ambiguous, we do the matching with the initial orientation and with another rotated by π radians. To correct the pose, we use the matching transformation that provided the best matching. The algorithm used for matching is the adaptation of the scan matching algorithm proposed by Pedrosa et al. [34] to three dimensions. An example of SLC detection is shown in Fig. 10. 4.1.2 Manipulation and planning strategy To solve this task three manipulation skills were used. The pick_object and place_object are based on the skills trained in a simulation environment [14] with some minors adaptations. The pick_object_shelf skill is also derived from the pick_object skill but taking in account the space between the shelves of the shelf. So in this skill instead of approaching the object using a vertical movement it is approached at an angle of 45 degrees. This allows the arm to reach the objects on the shelf without hitting the upper level of the self. Our initial approach was to pick an SLC, hold it and deliver it to the goal area. With this approach the robot has to move across the room 4 times, 2 for the SLCs on the pickup table and 2 for the SLCs in the shelf. It takes around 15 min to complete the job, being most of this time spent on navigation between the goal table and the pickup table and shelf. In order to optimize this process a second approach was developed. Because a considerable amount of time was being spent on navigation, we manage to transport the SLCs on top of the robot platform. This way, navigation was reduced to one trip to each side: first, the two SLCs in the table are picked up and placed in the top of the robot; then, the robot moves to the shelf, pick up another SLC and also put it in its top surface; finally, the fourth SLC is picked up and held in the gripper, while the robot navigates to the target table. On the target table, the four SLCs are put in the target area, after which the fifth SLC, the one in the target table, is detected, picked and placed. With this improvement the time dropped to around 10 min. After this pick and place strategy was implemented, some improvements were accomplished, by parallelizing some steps. For example, after picking an SLC, while placing it on the top of the robot, movement to the next observation and picking position can be performed. Task time was now reduced to around 8 min. A significant amount of time was still being spent in picking the SLCs from the top of the robot and placing them on the goal area (3 SLCs from the top of the robot plus the fourth that was transported on the gripper). So our final solution was to rearrange the SLCs on top of the robot so that the manipulator can pick two at the same time. This way one of the picking from the robot’s body was eliminated. Also, by placing the fourth SLC on top of the robot aligned with the third and by picking both at once, the number of places in the target area was reduced to 3 – 2 places holding two SLCs and one holding only one SLC. The 4th SLC is placed on top of the robot while it is navigating to the target table. Also, during navigation the arm could pick the 3rd and 4th SLCs so that when the robot reaches the target table the SLCs are already grasped. These improvements dropped the time to 5 min. Furthermore, speed increasing was previously prepared through parameters in a configuration file. During the evaluation, 6 out of 7 attempts were successful and with decreasing times, which lead to the final task time of 3 min and 4 s just by increasing the speed of the movements. 4 4 4.2 Task 2: product assembly The goal of this task is to pre-assemble a set of bolts, nuts and washers using a single manipulator. Since the single arm manipulation is not able to assemble two parts, two fixtures are provided to help in the assembly: one contains the washers in an approximate upright position to facilitate picking; the other has a well with the shape of the bolt head shallow enough to hold the bolt in place when a torque is applied, i.e. it secures the bolt when screwing the nut. The specifications of the task includes the approximate locations of the assembly pieces and fixtures, but not their exact positions. To solve the task we start by defining an observation point for the camera that is mounted in the arm, that provides a complete view of all necessary elements for the task and allows the manipulation of all parts without changing the position of the platform. This is important as, if the platform does not move, the relative positions of all objects are maintained with high precision after the first detection (see Fig. 3). The robot has to detect the bolt, pick it and place it in the bolt fixture, then it has to detect the washer, pick it and place it in the bolt, and finally it has to detect a nut, pick it and screw it in the bolt. Positions are calculated using the pinhole model instead of the point cloud provided by the stereo rig. this strategy can be pursued because the dimensions of all elements in the task and a precise distance from the camera to the working table, that is inferred from the pose of the arm are known in advance. This is done to speed up detection, because the generation of depth information is computationally expensive. 4.2.1 Fixture detection The detection of the fixtures, although executed only once, is an important step. The robot does not start from the working area but has to navigate there. Thus, location errors are inevitable. The assembly area is well defined, therefore, the fixtures are used as reference points to the rest of the elements. The detection of the fixtures is done using HSV color segmentation. We start by detecting both fixtures from the observation point. Once the resulting blobs are obtained, for each blob the rotated rectangle that best encloses it is calculated. The detection from the observation point is not very precise, therefore using the information from the rotated rectangle, the camera is approximated to each fixture and the detection is repeated individually. The information about the fixture that contains the washers is used in the detection of the washers, as their relative position to the fixture is known and so the search space in the image can be reduced. The center of the rotated rectangle that derives from the bolt’s fixture coincides with screwing place. 4.2.2 Bolt, nut and washer detection The detection algorithm for the bolts and nuts is the same. We explore their shape similarity when viewed from the top, i.e. when the object is located at the image center. The detection of a bolt/nut starts by performing color segmentation, then blobs are created from the resulting segmentation. Only the blob closer to the image center and in the vicinity of the bolts virtual line is considered, the rest being discarded. We perform a shape analysis to find the vertices of its convex shape and to find afterwards two consecutive edges that resemble the bolt/nut hexagonal top in length and angle (Fig. 11). Those edges are then used to calculate the center of the bolt/nut, and its orientation. The orientation is required for the nut so that it can be picked up by its edges. The described approach assumes that the object to be detected is as much as possible in the center of the image. This is achieved by generating a hint list for bolts and nuts that gives a rough approximation of their positions. The list is populated by running the color segmentation once for the bolts and nuts from the observation position. The resulting blobs are then used as hints. When it is time to detect a bolt/nut the hint is used to approximate the camera to the object in focus. The detection of the washers uses a different strategy. Instead of color segmentation, we do an intensity filtering that keeps the pixels with higher intensity (Fig. 12). Then, from the resulting segmentation we extract several blobs validated by their size. The centers of these blobs are then used to calculate the positions of the washers. 4.3 Manipulation and planning strategy To place the bolt in the screwing fixture, the place_bolt skill takes advantage of the compliant movement of the arm. To insert the bolt head into the fixture, after aligning the bolt with the center of the fixture, the bolt is pushed against the fixture while rotating it. As soon as a drop in the bolt’s height is detected, the action finishes, as it means the bolt’s head entered the fixture. For the place_nut skill the strategy is similar. While using compliant movement of the arm, the robot starts rotating the nut while at the same time pushes it against the bolt tip. Based on how much the height of the nut dropped during the first rotation, the number of rotations is adjusted in order to be fully screwed on the bolt. In the pick_washer skill, since they are in an upright position slightly tilted, the gripper approaches the washers just like in a normal pick but with the fingers adjusted for the washer diameter. Then the gripper moves down slowly in order to push the adjacent washer and create space for the picking. Because they are tilted the robot pushes them in the opposite direction of the tilt in order to became upright and aligned with the fingers of the gripper. To place the washers on the bolt, the place_washer skill aligns the inner bottom edge of the washer with the top of the bolt, just touching it on the side. Then the washer is rotated by 60 degrees while pushing it against the bolt. This tilts the bolt a little bit making it push the washer to the correct position when it is released. The planning strategy for this task is straightforward. Not considering the perception parts, it corresponds to the following sequence of actions: first, a bolt is picked up and placed in the fixture; then, a washer is picked up and placed in the bolt; next, a nut is picked up and screwed in the bolt; finally, the assembly is picked up and placed in a target region. The same procedure applies to the other bolts, washers and nuts. The speed of the movements and the movements connecting the different skills were parameterized and optimized during evaluation, starting with safer speeds and proceeding to faster ones after the task had been completed with success. In 7 attempts to solve the task, only one of them failed to complete all the objectives. With this strategy the best achieved task time was 6 min and 15 s. 5 5 5 Solving showcase tasks 5.1 Manipulation of stacked non rigid objects Solving the showcase tasks introduced a number of technological issues. The blank pile is not a unique rigid solid object, as shown in Fig. 13. It is a stack of cardboards, that can bend under its own weight and can also be disrupted, since some blanks can break free from the pile, due to low friction between them. To prevent the breakdown of a pile, any manipulation procedure has to be aware of this fact. Blanks are provided in untied piles, piled up on pallets close to each other, which can cause erroneous detections. Blank piles have to be placed in a feeding mechanism avoiding hard collisions with blanks that can already be there and with the four blank guiding rods that prevent blanks from slipping out of the inclined magazine surface. This feeding task requires a high level of precision for a proper feeding, while preventing collisions between the blank magazine and the manipulator. The manipulation has to be performed using a single arm. Since the gripper provided by the host is not appropriate, a solution has to be revised, taking into account the aforementioned issues. Aside from the gripper design, a proper sequence of actions has to be planned to accomplish the showcase tasks. 5.2 Manipulator for packaging Regarding the gripper design, the TIMAIRIS team decided to used the gripper provided with the robotic arm and adapt it to the required manipulations. Two fingers were designed, able to grasp a pile of blanks with a maximum height of 80 mm. The closing force of 80N (or at least 50N) should be enough at least for manipulating the smaller piles. In order to grasp a pile, it should be partially dragged out of the table, as shown in Fig. 13. To do so, two thin shafts were attached to the fingers, as shown in Fig. 14(a) and Fig. 15(a). These thin shafts can slide along their own axes thanks to the presence of a spring, ensuring the existence of contact between the shafts and the interlayer and thus preventing the loss of any blank during the dragging of the pile. Warehouse or blank magazine is composed of a plate inclined by 55 ∘ and four aluminum legs that sustain the plate. The plate is ad hoc built for the industrial partner’s blanks or cardboards. On the plate, four thin rods are mounted that guide the blanks once the robot delivers the whole stack, as shown in Fig. 14(b) and Fig. 15(b). Moreover, a proximity sensor is added and connected to a led yellow light. The proximity sensor checks if the number of cardboards is below a given threshold, in which case the light is turned on, as shown in Fig. 15(b). 5.3 Showcase perception To accomplish the showcase tasks a number of perception activities must be implemented. The piles of blanks are put over two pallets, thus these pallets must be detected and located relative to the robot. The poses of the piles themselves must be precisely estimated, so the picking up could be well performed. Finally, the blank magazine must be detected and located in order to perform the feeding procedure. The detection of the pallets is done by processing the stereo images captured using the pan–tilt camera. It unfolds into three steps: first, voxel grid filtering [35] based on the height of the pallets is used to remove irrelevant points from the point cloud [36]; second, the filtered points are projected on a horizontal plane and clustering is applied to define candidate regions; finally, these regions are matched against a 3D template to find the poses of the pallets. The same approach is applied for a first rough estimation of the poses of the piles in the pallets, by using the average height of the piles and an appropriate template. Fig. 16 illustrates the detection of both pallets and piles. However, to reliably pick a blank pile up from a pallet, a robust pose estimation of piles is necessary. On the one hand, in real industry scenarios, the piles on a pallet are tightly-aligned and put close to each other. On the other hand, the patterns printed on the blanks, even for the same blank shape, are variable and can change with end users’ demands. Therefore, detection and pose estimation of blanks cannot rely on conventional approaches such as color segmentation or local feature matching such as SIFT [37]. The approximate piles’ poses estimated so far can be used to position the TCP camera, in order to obtain more reliable ones. The camera is positioned over a pile and an image is captured, as shown in Fig. 17(a). Then, a Canny edge detector is applied to that image, as shown in Fig. 17(b). The detected edges come from the blanks’ outline but also from the printed patterns on the top blank. A drawing pin filter which classifies each point with various number of neighbors is applied to detect the outline of a blank pile [26]. The kernel is defined as K D P = [ x i ] = p + h n , if x i is in the center h n , otherwise p > h × | [ x ] | , n = ∑ x i , where p and h are the values of pin and head, respectively. The p is larger than the sum of heads and n is the normalization factor of the kernel. In this work, a drawing pin kernel of size 5 × 5 with p = 50 and h = 1 is used, as shown in Fig. 18. The basic assumption is that the edges of outlines are straight lines and their points have few neighbors except those in the line, while the points in printed patterns have many neighbors. In the filtering algorithm, points with more than the kernel size are discarded, as shown in Fig. 17(c). Finally, to find the precise pose of the pile, it is matched against a template, starting from the roughly estimated pose in a spiral direction. Fig. 17(d) shows the result of the match. 5.4 Motion planing After grasping the blank pile, the arm is put in a transport pose. This pose is defined considering two requirements. In one side, the arm should be completely inside the robot footprint. This way, navigation to the blank magazine table will be much safer. In the other side, the arm pose should minimize the necessary motions to place the pile in the magazine. All of these manipulations have to be done while maintaining the blank pile in a pose that prevents it from breaking apart. In this challenge, motions have been generated in the simulator in advance and then the results are applied to the real robot. In order to achieve the best results specific filters had to be used to enhance and complete the trajectories that were derived from existing motion planners. Several different motion planners have also been tested and benchmarked for tasks of different complexities [38]. 5.5 Task planning The manipulation of the blank pile, aside from the gripper, also raises planning issues. The first relates to the order by which blank piles must be manipulated. Due to their dispositions in the pallet, certain blank piles are blocked by others and cannot be dragged before these are removed. Therefore, the order by which blank piles are manipulated has to be planned. The plan considers the blank piles available in all the pallets and gives preference to the pallet with the lowest number of blank piles, since an empty pallet can be replaced with a fully packed one. Fig. 19 shows a task plan with the sequence and dragging directions to pick up all blank piles on two pallets. The dragging itself is also a challenge. The gripper and the arm have to maintain pressure against the pallet while dragging the blank pile. Too much pressure and the pallet could move, not enough pressure and the pile can be disrupted. Therefore, the robotic arm has to be used in compliant mode and the pressure should be properly calculated. 5.6 Safe human–robot collaboration For small batch production, changing or adding new features, such as continuously changing printed patterns of blanks, can be a burden on both human operators and autonomous robots. Since industrial environments are noisy, where machines produce a continuous whirring sound, verbal communication is difficult for humans and impractical for robots. Thus, gestures have been considered as a practical alternative way of communication. Until now, gesture recognition systems in HRI have focused on small number of implicit interactions such as pointing and handing over. With respect to pointing, the target object must be in the field of view of both human and robot. Fig. 20 shows an example of human tracking and human–robot interaction during the EuRoC evaluation. When the robot detects an operator entering a region for safety handling, it stops the executing task and points the pan–tilt to the operator. To ask to fetch objects in a cluttered and unstructured environment, HRI systems need to have a rich vocabulary to designate an object [16]. Fig. 21 shows an interaction tree for the showcase task. That represents commands by sequentially encoding 8 gestures, which consist of 6 numbers 0 to 5 and thumb up/down. Gesture recognition module recognizes a one-hand gesture provided by the operator. Then HRI module tries to build a command with a sequence of gestures. Following the command, the robot continues or stops the paused task. 6 Challenge evaluation 6.1 Benchmark evaluation During the benchmark phase of the Stages II, all the Challenge 2 participants have been evaluated by developing same two tasks under the same conditions: the robot platform, EuRoC software architecture and working time. As the benchmark task 1, the robot has to pick up and place 5 SLCs into the target area to show the logistics capabilities with different payload from different location to a fixed place. The task has two metrics: a number of delivered SLCs regardless of whether parts are lost (metric 1) and a number of delivered SLCs without losing parts (metric 2). Table 1 shows the results, which are their best results out of three trials. The bonus is calculated as inverse proportion to the time difference from the best on that between the best and worst. As the benchmark task 2, the robot is required to screw nuts on 5 bolts placed in a fixed area on a table using a fixture. The task also has two metrics: a number of washers successfully put onto bolts (metric 1) and a number of bolts with a successfully screwed on nut (metric 2) Table 2 shows the results of the benchmark task 2. The bonus is calculated as same as that of the task 1 among the records of teams, who finish the assembly task. TIMAIRIS has completed the both tasks without failures in the shortest times. 6.2 Showcase evaluation TIMARIS’ showcase addresses all of these issues (and others) in the form of three challenges and two extra demos, with quantifiable metrics. The objectives and metrics are organized so that the highlights described above can be evaluated in a comprehensive and robust way. The first objective (O1) is focused on perception and includes 4 metrics: detecting the pose of blank piles, pallets and blank magazine (O1M1); recognizing the need for blank feeding (O1M2); identifying the number of piles in each pallet (O1M3); identifying the presence of humans in the vicinity of the robot (O1M4). The second objective is focused on manipulation and navigation and includes 4 metrics: correctly picking a blank pile (O2M1); transporting the piles to the blank magazine (O2M2); placing the blank piles in the blank magazine (O2M3); stopping manipulation and navigation if a human is close to the robot (O2M4). The third objective evaluates the planning capabilities and includes 2 metrics: providing a plan to pick the blank piles (O3M1); adapting navigation paths (O3M2). The last objective evaluates human robot interaction and includes 2 metrics: recognizing gesture commands provided by the human (O4M1); tracking a human for interaction (O4M2). The achieved metrics are presented in Table 3. TIMAIRIS completed the showcase with every objective and metric being accomplished as can be seen from Table 3 where targets is the number of trials in predefined task scenarios and events is the number of successes during the showcase evaluation. The metrics of O1M1 are composed of three metrics for detecting the pose of blank piles, pallets and blank magazine. In what regards Perception, Manipulation/Navigation and Planning, i.e. Objectives 1, 2 and 3 previously specified, all metrics have been achieved during the execution of the first part of the showcase evaluation that consisted of three challenges. Tracking humans with the pan–tilt camera, Metric 2 of Objective 4, was also achieved during this first part of the showcase evaluation. The gesture recognition was demonstrated during the first part of the showcase evaluation, where gesture commands have been used to start challenge execution. Safety has been demonstrated in manipulation and navigation several times. During the showcase, the gesture recognition, which includes automatic correction methods, has been presented independently and the results showed that it is very robust and adequate for this kind of interaction (see the linked video 6 6 ). The results obtained in the showcase phase provide an excellent base for the development of the pilot experiment. The pilot experiment environment is a real industrial setting but, as already referred, all the developments of showcase are directly applicable in this final environment. Some new features will have to be addressed that also depend on the speedup of the final prototype, such as considering several packaging machines and different types/shapes of blanks, enhanced safety and more challenging navigation issues. Still, the showcase results are a complete and very solid basis for the work that needs to be done in the pilot experiments. 7 Conclusion To enable the paradigm shift in the packaging industry, many developed technologies must be considered as the integration cannot be performed and only the complete set allows the execution of the task. The developed system has gradually evolved from the qualification stage in a limited time. During the Stage 2 of the EuRoC project, the proposed architecture has demonstrated the feasibility of practical use of for Shop Floor Logistics and Manipulation. It is believed that SAAA is efficient not only in autonomous execution for autonomous packaging tasks but also in its use of development during the competitive challenges and that the simple architecture and distributed skills make the system efficient. The results of benchmarking tasks in which TIMAIRIS took first place among 5 Stage 2 challenger teams was possible that our team just focused on improving individual skills to complete all tasks and to reduce running time. One of important development during the showcase is showing that autonomous blank feeding is possible in a realistic industrial environment. That is a pending task of the industry partner IMA S.p.A which is a world leading company in the design and manufacture of automatic machines for the processing and packaging. As the result of the Stage II evaluations, TIMAIRIS has been selected to advance to the final stage of the EuRoC project. Some new features will have to be addressed that also depend on the speedup of the final prototype to complete the pilot experiments. Still, the results of the Stage II are a complete and very solid basis for the work that needs to be done in the final stage of the EuRoC project. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was supported by the EuRoC Project under Grant no. 608849 and by National Funds through the FCT - Foundation for Science and Technology , in the context of the project UID/CEC/00127/2013. References [1] Pratt G. Manzo J. The DARPA robotics challenge [competitions] IEEE Robot. Autom. Mag. 20 2 2013 10 12 G. Pratt, J. Manzo, The DARPA robotics challenge [competitions], IEEE Robotics & Automation Magazine 20 (2) (2013) 10–12 [2] Kitano H. Asada M. Kuniyoshi Y. Noda I. Osawa E. Robocup: the robot world cup initiative Proceedings of the First International Conference on Autonomous Agents 1997 ACM 340 347 H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, E. Osawa, Robocup: The robot world cup initiative, in: Proceedings of the first international conference on Autonomous agents, ACM, 1997, pp. 340–347 [3] Siciliano B. Caccavale F. Zwicker E. Achtelik M. Mansard N. Borst C. Achtelik M. Jepsen N.O. Awad R. Bischoff R. EuRoC-The challenge initiative for european robotics ISR/Robotik 2014; 41st International Symposium on Robotics; Proceedings of 2014 VDE 1 7 B. Siciliano, F. Caccavale, E. Zwicker, M. Achtelik, N. Mansard, C. Borst, M. Achtelik, N. O. Jepsen, R. Awad, R. Bischoff, EuRoC-the challenge initiative for european robotics, in: ISR/Robotik 2014 41st International Symposium on Robotics; Proceedings of, VDE, 2014, pp. 1–7 [4] Bischoff R. Guhl T. The strategic research agenda for robotics in europe [industrial activities] IEEE Robot. Autom. Mag. 1 17 2010 15 16 R. Bischoff, T. Guhl, The strategic research agenda for robotics in europe [industrial activities], IEEE Robotics & Automation Magazine 1 (17) (2010) 15–16 [5] Cully A. Clune J. Tarapore D. Mouret J.-B. Robots that can adapt like animals Nature 521 7553 2015 503 A. Cully, J. Clune, D. Tarapore, J.-B. Mouret, Robots that can adapt like animals, Nature 521 (7553) (2015) 503 [6] Hildebrandt A.-C. Schuetz C. Wahrmann D. Wittmann R. Rixen D. A flexible robotic framework for autonomous manufacturing processes: report from the european robotics challenge stage 1 Autonomous Robot Systems and Competitions (ICARSC), 2016 International Conference on 2016 IEEE 21 27 A.-C. Hildebrandt, C. Schuetz, D. Wahrmann, R. Wittmann, D. Rixen, A flexible robotic framework for autonomous manufacturing processes: Report from the European Robotics Challenge Stage 1, in: Autonomous Robot Systems and Competitions (ICARSC), 2016 International Conference on, IEEE, 2016, pp. 21–27 [7] Zadeh S.M. Powers D.M. Sammut K. An autonomous reactive architecture for efficient AUV mission time management in realistic dynamic ocean environment Robot. Auton. Syst. 87 2017 81 103 S. M. Zadeh, D. M. Powers, K. Sammut, An autonomous reactive architecture for efficient auv mission time management in realistic dynamic ocean environment, Robotics and Autonomous Systems 87 (2017) 81–103 [8] Baklouti E. Amor N.B. Jallouli M. Reactive control architecture for mobile robot autonomous navigation Robot. Auton. Syst. 89 2017 9 14 E. Baklouti, N. B. Amor, M. Jallouli, Reactive control architecture for mobile robot autonomous navigation, Robotics and Autonomous Systems 89 (2017) 9–14 [9] Nilsson K. Johansson R. Integrated architecture for industrial robot programming and control Robot. Auton. Syst. 29 4 1999 205 226 K. Nilsson, R. Johansson, Integrated architecture for industrial robot programming and control, Robotics and Autonomous Systems 29 (4) (1999) 205–226 [10] Chatzilygeroudis K. Vassiliades V. Mouret J.-B. Reset-free trial-and-error learning for robot damage recovery Robot. Auton. Syst. 100 2018 236 250 K. Chatzilygeroudis, V. Vassiliades, J.-B. Mouret, Reset-free trial-and-error learning for robot damage recovery, Robotics and Autonomous Systems 100 (2018) 236–250 [11] Insaurralde C.C. Petillot Y.R. Capability-oriented robot architecture for maritime autonomy Robot. Auton. Syst. 67 2015 87 104 C. C. Insaurralde, Y. R. Petillot, Capability-oriented robot architecture for maritime autonomy, Robotics and Autonomous Systems 67 (2015) 87–104 [12] Dean T.L. Boddy M.S. An analysis of time-dependent planning. AAAI, vol. 88 1988 49 54 T. L. Dean, M. S. Boddy, An analysis of time-dependent planning., in: AAAI, Vol. 88, 1988, pp. 49–54 [13] Grefenstette J.J. Ramsey C.L. An approach to anytime learning Machine Learning Proceedings 1992 1992 Elsevier 189 195 J. J. Grefenstette, C. L. Ramsey, An approach to anytime learning, in: Machine Learning Proceedings 1992, Elsevier, 1992, pp. 189–195 [14] Pedrosa E. Lau N. Pereira A. Cunha B. A skill-based architecture for pick and place manipulation tasks Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015 2015 Springer 457 468 E. Pedrosa, N. Lau, A. Pereira, B. Cunha, A skill-based architecture for pick and place manipulation tasks, in: Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015, Springer, 2015, pp. 457–468 [15] Amaral F. Pedrosa E. Lim G.H. Shafii N. Pereira A. Azevedo J.L. Cunha B. Reis L.P. Badini S. Lau N. Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC challenge 2, stage II-realistic labs: benchmarking Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on 2017 IEEE 198 203 F. Amaral, E. Pedrosa, G. H. Lim, N. Shafii, A. Pereira, J. L. Azevedo, B. Cunha, L. P. Reis, S. Badini, N. Lau, Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC Challenge 2, Stage II-Realistic Labs: Benchmarking, in: Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on, IEEE, 2017, pp. 198–203 [16] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Dias P. Azevedo J.L. Cunha B. Reis L.P. Rich and robust human-robot interaction on gesture recognition for assembly tasks Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on 2017 IEEE 159 164 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, P. Dias, J. L. Azevedo, B. Cunha, L. P. Reis, Rich and robust human-robot interaction on gesture recognition for assembly tasks, in: Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on, IEEE, 2017, pp. 159–164 [17] Lim G.H. Pedrosa E. Amaral F. Dias R. Pereira A. Lau N. Azevedo J.L. Cunha B. Reis L.P. Human-robot collaboration and safety management for logistics and manipulation tasks Iberian Robotics Conference 2017 Springer 15 27 G. H. Lim, E. Pedrosa, F. Amaral, R. Dias, A. Pereira, N. Lau, J. L. Azevedo, B. Cunha, L. P. Reis, Human-robot collaboration and safety management for logistics and manipulation tasks, in: Iberian Robotics conference, Springer, 2017, pp. 15–27 [18] Mokhtari V. Lim G.H. Lopes L.S. Pinho A.J. Gathering and conceptualizing plan-based robot activity experiences Intelligent Autonomous Systems 13 2016 Springer 993 1005 V. Mokhtari, G. H. Lim, L. S. Lopes, A. J. Pinho, Gathering and conceptualizing plan-based robot activity experiences, in: Intelligent Autonomous Systems 13, Springer, 2016, pp. 993–1005 [19] Lim G.H. Shared representations of actions for alternative suggestion with incomplete information Robot. Auton. Syst. 2019 G. H. Lim, Shared representations of actions for alternative suggestion with incomplete information, Robotics and Autonomous Systems. [20] Balogh R. I am a robot–competitor: a survey of robotic competitions Int. J. Adv. Robot. Syst. 2 2005 17 R. Balogh, I am a robot–competitor: A survey of robotic competitions, International Journal of Advanced Robotic Systems 2 (2005) 17 [21] Thrun S. Montemerlo M. Dahlkamp H. Stavens D. Aron A. Diebel J. Fong P. Gale J. Halpenny M. Hoffmann G. Stanley: the robot that won the DARPA grand challenge J. Field Robot. 23 9 2006 661 692 S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann, et al., Stanley: The robot that won the darpa grand challenge, Journal of field Robotics 23 (9) (2006) 661–692 [22] Buehler M. Iagnemma K. Singh S. The DARPA Urban Challenge: Autonomous Vehicles in City Traffic, vol. 56 2009 Springer M. Buehler, K. Iagnemma, S. Singh, The DARPA urban challenge: autonomous vehicles in city traffic, Vol. 56, springer, 2009 [23] A. Dömel, S. Kriegel, M. Brucker, M. Suppa, Autonomous pick and place operations in industrial production, in: Ubiquitous Robots and Ambient Intelligence (URAI), 2015 12th International Conference on, 2015, pp. 356–356, [24] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Azevedo J.L. Cunha B. Neural regularization jointly involving neurons and connections for robust image classification 2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 IEEE 336 341 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, J. L. Azevedo, B. Cunha, Neural regularization jointly involving neurons and connections for robust image classification, in: 2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), IEEE, 2017, pp. 336–341 [25] Russell S. Norvig P. Artificial Intelligence: A Modern Approach third ed. 2010 Prentice Hall S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, 3rd Edition, Prentice Hall, 2010 [26] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Azevedo J.L. Cunha B. Badini S. Mobile manipulation for autonomous packaging in realistic environments: euroc challenge 2, stage ii, showcase 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC) 2018 IEEE 231 236 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, J. L. Azevedo, B. Cunha, S. Badini, Mobile manipulation for autonomous packaging in realistic environments: Euroc challenge 2, stage ii, showcase, in: 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), IEEE, 2018, pp. 231–236 [27] Lim G.H. Lau N. Pedrosa E. Amaral F. Pereira A. Luís Azevedo J. Cunha B. Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges Adv. Robot. 2019 1 11 G. H. Lim, N. Lau, E. Pedrosa, F. Amaral, A. Pereira, J. Luís Azevedo, B. Cunha, Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges, Advanced Robotics (2019) 1–11 [28] Lim G.H. Yi C. Suh I.H. Ko D.W. Hong S.W. Ontology representation and instantiation for semantic map building by a mobile robot Intelligent Autonomous Systems, vol. 12 2013 Springer 387 395 G. H. Lim, C. Yi, I. H. Suh, D. W. Ko, S. W. Hong, Ontology representation and instantiation for semantic map building by a mobile robot, in: Intelligent Autonomous Systems 12, Springer, 2013, pp. 387–395 [29] G.H. Lim, M. Oliveira, S.H. Kasaei, L.S. Lopes, Hierarchical nearest neighbor graphs for building perceptual hierarchies, in: 22nd International Conference on Neural Information Processing, ICONIP2015, 2015. [30] Lim G.H. Suh I.H. Suh H. Ontology-based unified robot knowledge for service robots in indoor environments IEEE Trans. Syst., Man, Cybern. A 41 3 2011 492 509 10.1109/TSMCA.2010.2076404 G. H. Lim, I. H. Suh, H. Suh, Ontology-based unified robot knowledge for service robots in indoor environments, Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on 41 (3) (2011) 492 –509 doi:101109/TSMCA.20102076404 [31] Rusu R. Marton Z. Blodow N. Dolha M. Towards 3D point cloud based object maps for household environments Robot. Auton. Syst. 56 11 2008 927 941 R. B. Rusu, Z. C. Marton, N. Blodow, M. Dolha, Towards 3D point cloud based object maps for household environments, Robotics and Autonomous Systems 56 (11) (2008) 927–941 [32] Rusu R.B. Semantic 3D object maps for everyday manipulation in human living environments (Ph.D. thesis) 2009 Computer Science department, Technische Universitaet Muenchen, Germany R. B. Rusu, Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments, Ph.D. thesis, Computer Science department, Technische Universitaet Muenchen, Germany (October 2009) [33] Oliveira M. Lopes L.S. Lim G.H. Kasaei S.H. Tomé A.M. Chauhan A. 3D object perception and perceptual learning in the RACE project Robot. Auton. Syst. 75 2016 614 626 M. Oliveira, L. S. Lopes, G. H. Lim, S. H. Kasaei, A. M. Tomé, A. Chauhan, 3d object perception and perceptual learning in the race project, Robotics and Autonomous Systems 75 (2016) 614–626 [34] Pedrosa E. Pereira A. Lau N. A scan matching approach to slam with a dynamic likelihood field 2016 International Conference on Autonomous Robot Systems and Competitions, ICARSC 2016 IEEE Portugal, Bragança 35 40 10.1109/ICARSC.2016.23 E. Pedrosa, A. Pereira, N. Lau, A Scan Matching Approach to SLAM with a Dynamic Likelihood Field, in: 2016 International Conference on Autonomous Robot Systems and Competitions (ICARSC), IEEE, Portugal, Bragança, 2016, pp. 35–40 doi:101109/ICARSC.201623 [35] Cohen-Or D. Kaufman A. Fundamentals of surface voxelization Graph. Models Image Process. 57 6 1995 453 461 D. Cohen-Or, A. Kaufman, Fundamentals of surface voxelization, Graphical models and image processing 57 (6) (1995) 453–461 [36] Rusu R.B. Cousins S. 3D is here: point cloud library (PCL) Robotics and Automation (ICRA), 2011 IEEE International Conference on 2011 IEEE 1 4 R. B. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: Robotics and Automation (ICRA), 2011 IEEE International Conference on, IEEE, 2011, pp. 1–4 [37] Lowe D.G. Object recognition from local scale-invariant features Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, vol. 2 1999 Ieee 1150 1157 D. G. Lowe, Object recognition from local scale-invariant features, in: Computer vision, 1999 The proceedings of the seventh IEEE international conference on, Vol. 2, Ieee, 1999, pp. 1150–1157 [38] Tudico A. Lau N. Pedrosa E. Amaral F. Mazzotti C. Carricato M. Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments Portuguese Conference on Artificial Intelligence 2017 Springer 498 509 A. Tudico, N. Lau, E. Pedrosa, F. Amaral, C. Mazzotti, M. Carricato, Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments, in: Portuguese Conference on Artificial Intelligence, Springer, 2017, pp. 498–509 Dr. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D. degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Marie Curie individual fellow in the School of Computer Science at University of Manchester, UK. His research interests lie in the area of artificial intelligence and machine learning for autonomous robots, including perception, semantics, cognition and spatiotemporal representations on neuromorphic architectures. Eurico Pedrosa is a Post-Doc Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his Informatics Engineering degree from University of Aveiro in 2010 and a Computer Science Ph.D. degree from Aveiro University in 2018. His research interest are focused on intelligent robotics, robotic navigation including localization and mapping (SLAM), space representation using volumetric grids and most recently the application of radar sensors in indoor robotics. Filipe Amaral is a Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his MSc degree in Computer and Telematics Engineering from University of Aveiro in 2014. His current research interests are in the area of autonomous mobile robotics. Prof. Dr. Artur Pereira was born in Vila Nova de Famalicão, Portugal, in April 1960. He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 2003. He is currently an Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Instituto de Engenharia Electrónica e Informática de Aveiro. The main focus of his research is robotics at the architectural and software levels, with emphasis on simulation, navigation, localization, mapping, and machine learning. Nuno Lau is Assistant Professor at Aveiro University, Portugal and Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), where he leads the Intelligent Robotics and Systems group (IRIS). He got is Electrical Engineering Degree from Oporto University in 1993, a DEA degree in Biomedical Engineering from Claude Bernard University, France, in 1994 and the Ph.D. from Aveiro University in 2003. His research interests are focused on Intelligent Robotics, Artificial Intelligence, Multi-Agent Systems and Simulation. Nuno Lau participated in more than 15 international and national research projects, having the tasks of general or local coordinator in about half of them. Nuno Lau won more than 50 scientific awards in robotic competitions, conferences (best papers) and education. He has lectured courses at Phd and MSc levels on Intelligent Robotics, Distributed Artificial Intelligence, Computer Architecture, Programming, etc. Nuno Lau is the author of more than 150 publications in international conferences and journals. He was President of the Portuguese Robotics Society from 2015 to 2017, and is currently the Vice-President of this Society. Prof. Dr. José Luís Azevedo is currently Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Institute of Electronics and Informatics Engineering of Aveiro (IEETA). He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1998. His current research interests are in the area of cooperative autonomous mobile robotics. Prof. Dr. Bernardo Cunha was born in 1959 in Porto, Portugal. He earned his doctoral degree in electrical engineering at the University of Aveiro, Portugal, in 1999. He is a full time teacher at Universidade de Aveiro in the computer architecture area and an investigator at the Instituto de Engenharia Electrónica e Informática de Aveiro. Current research interests are centered in the area of cooperative autonomous mobile robotics. Simone Badini is a Mechanical Designer in the Research and Development department of IMA Spa since 2013. IMA Spa is a world leader company in the design and manufacture of automatic machines for the processing and packaging of pharmaceuticals, cosmetics, food, tea and coffee and tobacco. He got is M.Sc. degree in Mechanical Engineering from University of Bologna, Italy in 2012. He is currently project manager for the integration of cobot and autonomous mobile robot in the production lines for the IMA group. "
    },
    {
        "doc_title": "Robot self position based on asynchronous millimetre wave radar interference",
        "doc_scopus_id": "85076670191",
        "doc_doi": "10.1109/IPIN.2019.8911809",
        "doc_eid": "2-s2.0-85076670191",
        "doc_date": "2019-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            }
        ],
        "doc_keywords": [
            "Experimental validations",
            "FMCW radar",
            "Low computational complexity",
            "Millimeter waves (mmwave)",
            "Millimetre-wave radar",
            "mm-Wave",
            "Robot positions",
            "Simple expression"
        ],
        "doc_abstract": "© 2019 IEEE.This paper proposes a method that can be used in robots equipped with millimeter wave (mmWave) radars to estimate its position. The method takes advantage of the interference produced by other radars located in the same environment with well known position. Additionally, to compute the robot position it is used only the Angle-of-Arrival (AoA) of each radar interference. The robot position is estimated by the minimization of a cost function presenting a simple expression with low computational complexity. The performance of the proposed method is simulated for different scenarios and an experimental validation is also presented. The experimental results show that is possible to estimate the robot position with an error bellow 20 cm using only three radars with well known position.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges",
        "doc_scopus_id": "85066896804",
        "doc_doi": "10.1080/01691864.2019.1617780",
        "doc_eid": "2-s2.0-85066896804",
        "doc_date": "2019-07-03",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Industrial environments",
            "Industrial robotics",
            "Mobile manipulation",
            "Mobile manipulator",
            "Object manipulation",
            "Perception systems",
            "Pose estimation",
            "stacked objects"
        ],
        "doc_abstract": "© 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.Object manipulation tasks such as picking up, carrying and placing should be executed based on the information of objects which are provided by the perception system. A precise and efficient pose estimation system has been developed to address the requirements and to achieve the objectives for autonomous packaging, specifically picking up of stacked non-rigid objects. For fine pose estimation, a drawing pin shaped kernel and pinhole filtering methods are used on the roughly estimated pose of objects. The system has been applied in a realistic industrial environment as a challenging scenario for the Challenge 2–Shop Floor Logistics and Manipulation on a mobile manipulator in the context of the European Robotics Challenges (EuRoC) project.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Non-Linear Least Squares Approach to SLAM using a Dynamic Likelihood Field",
        "doc_scopus_id": "85039043621",
        "doc_doi": "10.1007/s10846-017-0763-7",
        "doc_eid": "2-s2.0-85039043621",
        "doc_date": "2019-03-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Least-squares optimization",
            "Levenberg- Marquardt methods",
            "Likelihood field",
            "Low computational complexity",
            "Non-linear least squares",
            "Nonlinear least squares problems",
            "Scan matching",
            "SLAM"
        ],
        "doc_abstract": "© 2017, Springer Science+Business Media B.V., part of Springer Nature.This paper presents a fast scan matching approach to online SLAM supported by a dynamic likelihood field. The dynamic likelihood field plays a central role in the approach: it avoids the necessity to establish direct correspondences; it is the connection link between scan matching and the online SLAM; and it has a low computational complexity. Scan matching is formulated as a non-linear least squares problem that allows us to solve it using Gauss-Newton or Levenberg-Marquardt methods. Furthermore, to reduce the influence of outliers during optimization, a loss function is introduced. The proposed solution was evaluated using an objective benchmark designed to compare different SLAM solutions. Additionally, the execution times of our proposal were also analyzed. The obtained results show that the proposed approach provides a fast and accurate online SLAM, suitable for real-time operation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mobile manipulation for autonomous packaging in realistic environments: EuRoC challenge 2, stage II, showcase",
        "doc_scopus_id": "85048892736",
        "doc_doi": "10.1109/ICARSC.2018.8374188",
        "doc_eid": "2-s2.0-85048892736",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.European Robotics Challenges (EuRoC) project has been launched to find competitive solutions by exploiting synergies across research institutes to industrial end-users. This paper reports the research conducted by the TIMAIRIS team to fulfill EuRoC C2 Stage II tasks. TIMAIRIS is one of the 6 EuRoC finalists (Stage III) from an initial group of 102 teams. The packaging industry is very interested in recent advances in robotics but is still quite conservative in the way it uses automation, since shapes and printed patterns of blanks vary a lot to comply with end users' demands. The use of programmable logic controllers (PLCs) is widely common but the use of more sophisticated decision mechanisms is not so common. A shop floor logistics and manipulation system has been developed and demonstrated in a realistic environment for autonomous packaging.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A sparse-dense approach for efficient grid mapping",
        "doc_scopus_id": "85048861103",
        "doc_doi": "10.1109/ICARSC.2018.8374173",
        "doc_eid": "2-s2.0-85048861103",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.The regular volumetric grid is a popular method used in mapping to represent the environment, however for large three-dimensional environments it requires a large amount of memory that may not even be available. In this paper we present a sparse-dense data structure to manage the space of a volumetric grid that provides improvements over the octree data structure, a data structure popularized by the OctoMap mapping framework. Furthermore, we propose an online data compression scheme supported by a cache mechanism that further improves the space efficiency of our approach without compromising time efficiency. The approach is evaluated using public available datasets that show an increase in space and memory efficiency over OctoMap without compromising accuracy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human-Robot Collaboration and Safety Management for Logistics and Manipulation Tasks",
        "doc_scopus_id": "85042220407",
        "doc_doi": "10.1007/978-3-319-70836-2_2",
        "doc_eid": "2-s2.0-85042220407",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Human-robot collaboration",
            "Industrial context",
            "Manipulation task",
            "Mobile manipulator",
            "Reasoning methods",
            "Region-based filtering",
            "Safety concerns",
            "Safety management"
        ],
        "doc_abstract": "© Springer International Publishing AG 2018.To realize human-robot collaboration in manufacturing, industrial robots need to share an environment with humans and to work hand in hand. This introduces safety concerns but also provides the opportunity to take advantage of human-robot interactions to control the robot. The main objective of this work is to provide HRI without compromising safety issues in a realistic industrial context. In the paper, a region-based filtering and reasoning method for safety has been developed and integrated into a human-robot collaboration system. The proposed method has been successfully demonstrated keeping safety during the showcase evaluation of the European robotics challenges with a real mobile manipulator.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Neural regularization jointly involving neurons and connections for robust image classification",
        "doc_scopus_id": "85042368378",
        "doc_doi": "10.1109/MFI.2017.8170451",
        "doc_eid": "2-s2.0-85042368378",
        "doc_date": "2017-12-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Classification performance",
            "Classification results",
            "Experimental analysis",
            "Fully connected neural network",
            "Fully-connected layers",
            "Prediction performance",
            "Regularization methods",
            "Regularization technique"
        ],
        "doc_abstract": "© 2017 IEEE.This paper presents an integrated neural regularization method in fully-connected neural networks that jointly combines the cutting edge of regularization techniques; Dropout [1] and DropConnect [2]. With a small number of data set, trained feed-forward networks tend to show poor prediction performance on test data which has never been introduced while training. In order to reduce the overfitting, regularization methods commonly use only a sparse subset of their inputs. While a fully-connected layer with Dropout takes account of a randomly selected subset of hidden neurons with some probability, a layer with DropConnect only keeps a randomly selected subset of connections between neurons. It has been reported that their performances are dependent on domains. Image classification results show that the integrated method provides more degrees of freedom to achieve robust image recognition in the test phase. The experimental analyses on CIFAR-10 and one-hand gesture dataset show that the method provides the opportunity to improve classification performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC Challenge 2, Stage II - Realistic Labs: Benchmarking",
        "doc_scopus_id": "85026869156",
        "doc_doi": "10.1109/ICARSC.2017.7964075",
        "doc_eid": "2-s2.0-85026869156",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "Continuous development",
            "Effective solution",
            "Manipulation task",
            "Planning strategies",
            "Production quality",
            "Robotic technologies",
            "Scientific competition"
        ],
        "doc_abstract": "© 2017 IEEE.Nowadays, the increase of robotic technology application to industry scenarios is notorious. Proposals for new effective solutions are in continuous development once industry needs a constantly improvement in time as well as in production quality and efficiency. The EuRoC research project proposes a scientific competition in which research and industry manufacturers joint teams are encouraged to develop and test solutions that can solve several issues as well as be useful in manufacturing improvement. This paper presents the TIMAIRIS architecture and approach used in the Challenge 2 - Stage II - Benchmarking phase, namely regarding the perception, manipulation and planning strategy that was applied to achieve the tasks objectives. The used approach proved to be quite robust and efficient, which allowed us to rank first in the Benchmarking phase.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Efficient localization based on scan matching with a continuous likelihood field",
        "doc_scopus_id": "85026864903",
        "doc_doi": "10.1109/ICARSC.2017.7964053",
        "doc_eid": "2-s2.0-85026864903",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Gauss Newton",
            "Levenberg-Marquardt method",
            "Localization algorithm",
            "Loss functions",
            "Mobile robot localization",
            "Nonlinear least squares problems",
            "Scan matching",
            "State of the art"
        ],
        "doc_abstract": "© 2017 IEEE.This paper presents a fast scan matching approach to mobile robot localization supported by a continuous likelihood field. The likelihood field plays a central role in the approach, as it avoids the necessity to establish direct correspondences; it is the connection link between scan matching and robotic localization, and it provides a reduced computational complexity. Scan matching is formulated as a non-linear least squares problem and solved by the Gauss-Newton and Levenberg-Marquardt methods. Furthermore, to reduce the influences of outliers during optimization, a loss function is introduced. The proposed solution was evaluated using a publicly available dataset and compared with AMCL, a state-of-the-art localization algorithm. Our proposal shows to be a fast and accurate localization algorithm suitable for any type of operation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rich and robust human-robot interaction on gesture recognition for assembly tasks",
        "doc_scopus_id": "85026864168",
        "doc_doi": "10.1109/ICARSC.2017.7964069",
        "doc_eid": "2-s2.0-85026864168",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Human robot Interaction (HRI)",
            "Manufacturing enterprise",
            "Mobile manipulator",
            "Multiple feature fusion",
            "Puzzle games",
            "Robotics technology",
            "Small and medium sized enterprise"
        ],
        "doc_abstract": "© 2017 IEEE.The adoption of robotics technology has the potential to advance quality, efficiency and safety for manufacturing enterprises, in particular small and medium-sized enterprises. This paper presents a human-robot interaction (HRI) system that enables a robot to receive commands, provide information to a human teammate and ask them a favor. In order to build a robust HRI system based on gesture recognition, three key issues are addressed: richness, multiple feature fusion and failure verification. The developed system has been tested and validated in a realistic lab with a real mobile manipulator and a human teammate to solve a puzzle game.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments",
        "doc_scopus_id": "85028989811",
        "doc_doi": "10.1007/978-3-319-65340-2_41",
        "doc_eid": "2-s2.0-85028989811",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Complex task",
            "Mobile manipulator",
            "Motion planners",
            "Motion planning problems",
            "Robotic agents",
            "Simulation environment",
            "Unstructured environments"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.This paper presents the use, adaptation and benchmarking of motion planning tools that will be integrated with the KUKA KMR iiwa mobile robot. The motion planning tools are integrated in the robotic agent presented in [1]. The adaptation consists on algorithms developed to increase the robustness and the efficiency to solve the motion planning problems. These algorithms combine existing motion planners with a trajectory filter developed in this work. Finally, the benchmarking of different motion planners is presented. Three motion planning tasks with a growing level of complexity are taken in consideration for the tests in a simulation environment. The motion planners that provided the best results were RRTConnect for the two less complex tasks and PRM* for the most difficult task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Scan Matching Approach to SLAM with a Dynamic Likelihood Field",
        "doc_scopus_id": "85010460599",
        "doc_doi": "10.1109/ICARSC.2016.23",
        "doc_eid": "2-s2.0-85010460599",
        "doc_date": "2016-12-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Gauss-Newton methods",
            "Loss functions",
            "Low computational complexity",
            "Nonlinear least squares problems",
            "Real-time operation",
            "Scan matching",
            "SLAM",
            "SLAM approach"
        ],
        "doc_abstract": "© 2016 IEEE.This paper presents a fast scan matching approach to online SLAM supported by a dynamic likelihood field. The dynamic likelihood field plays a central role in the approach, as it avoids the necessity to establish direct correspondences, it is the connection link between scan matching and the online SLAM and it has a low computational complexity. Scan matching is formulated as a non-linear least squares problem and solved by the Gauss-Newton method. Furthermore, to reduce the influences of outliers during optimization, a loss function is introduced. The proposed solution was evaluated using an objective benchmark designed to compare SLAM solutions and its execution times were also analyzed. It shows to be a fast and accurate online SLAM approach, suitable for real-time operation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A skill-based architecture for pick and place manipulation tasks",
        "doc_scopus_id": "84945908825",
        "doc_doi": "10.1007/978-3-319-23485-4_45",
        "doc_eid": "2-s2.0-84945908825",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Manipulation task",
            "Pick and place",
            "Product customization",
            "Specific tasks",
            "Unstructured environments"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Robots can play a significant role in product customization but they should leave a repetitive, low intelligence paradigm and be able to operate in unstructured environments and take decisions during the execution of the task. The EuRoC research project addresses this issue by posing as a competition to motivate researchers to present their solution to the problem. The first stage is a simulation competition where Pick & Place type of tasks are the goal and planning, perception and manipulation are the problems. This paper presents a skill-based architecture that enables a simulated moving manipulator to solve these tasks. The heuristics that were used to solve specific tasks are also presented. Using computer vision methods and the definition of a set of manipulation skills, an intelligent agent is able to solve them autonomously. The work developed in this project was used in the simulation competition of EuRoC project by team IRIS and enabled them to reach the 5th rank.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Online SLAM based on a fast scan-matching algorithm",
        "doc_scopus_id": "84884726130",
        "doc_doi": "10.1007/978-3-642-40669-0_26",
        "doc_eid": "2-s2.0-84884726130",
        "doc_date": "2013-10-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "localization",
            "real-time",
            "Real-time operation",
            "Scan-matching",
            "Simultaneous localization and mapping",
            "SLAM approach"
        ],
        "doc_abstract": "This paper presents a scan-matching approach for online simultaneous localization and mapping. This approach combines a fast and efficient scan-matching algorithm for localization with dynamic and approximate likelihood fields to incrementally build a map. The achievable results of the approach are evaluated using an objective benchmark designed to compare SLAM solutions that use different methods. The result is a fast online SLAM approach suitable for real-time operations. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From an autonomous soccer robot to a robotic platform for elderly care",
        "doc_scopus_id": "84861984252",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861984252",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Developed countries",
            "Elderly care",
            "Hardware and software",
            "Home care",
            "Independent living",
            "Innovative solutions",
            "Number of peoples",
            "Nursing homes",
            "Robotic platforms",
            "Robotic soccer",
            "Soccer robot"
        ],
        "doc_abstract": "Current societies in developed countries face a serious problem of aged population. The growing number of people with reduced health and capabilities, allied with the fact that elders are reluctant to leave their own homes to move to nursing homes, requires innovative solutions since continuous home care can be very expensive and dedicated 24/7 care can only be accomplished by more than one care-giver. This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Spoken communication with CAMBADA@Home service robot",
        "doc_scopus_id": "84861961543",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861961543",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Automatic speech recognition",
            "Hands-free",
            "Home service robot",
            "In-line",
            "Mobile service robots",
            "NAtural language processing",
            "Natural response",
            "Robotic platforms",
            "Spoken languages",
            "Text to speech",
            "Two-component"
        ],
        "doc_abstract": "Spoken language is a natural way to control the human-robot interaction, especially for mobile service robots. It has some important advantages over other communication approaches: eyes and hands free, communication from a distance, even without being in line of sight and no need for additional learning for humans. In this paper, we present the spoken dialog framework integrated in our mobile service robot CAMBADA@Home, a robotic platform aimed at move into a living space and interact with users of that space. The proposed framework comprises three major spoken and natural language processing components: an Automatic Speech Recognition component to process the human requests, a Text-to-Speech component to generate more natural responses from the robot side, and a dialog manager to control how these two components work together.",
        "available": false,
        "clean_text": ""
    }
]